[Music] the struct and function of complex networks that receive the most citation in any mathematics paper between 2003 and 2011 I just take today 13,500 citations so I just want to say personally I'm so inspired by your work again very good thank you very much so I'm going to talk about network today I'm going to tell you about in doing in my group me and a large co-conspirators so I'm not a biologist mainly what I do is developing methods try and understand kind of techniques that we develop have been widely applied in your interest when I talk about network I just mean a bunch of dots going together by line in the jargon nodes and edges we're interested in network because they can be used as representations of structure of many different systems of this picture here for instance is a picture of a protein interaction network nodes in this network are protein the edges represent physical interacting with one form complexes with the other one so of course there's a vast amount of literature on the study of the individual node and of the individual edges in this network ie complexes with proteins interact with other protein but what we're interested in is the bigger picture what can you learn by looking at the network on a larger scale not just the local fails let's look at this one node of this one connected just to give you a little just so story of why this the worthwhile thing to do the world wide web network network weather knows our webpages and the edges are the hyperlink between you click on this web page it takes you to that other web page those of you who are old paiement like me will remember when the web first came out if you can remember back to 20 years or so ago if you tried doing a web search back then we had search engines like like remember any of these they were terrible they really did not work well right they you type something and if you're lucky you might get a page that was a baby related to what you were looking for it was a real art to find anything you didn't need and then in 1996 Google came along and it was a revelation suddenly it worked really well why did it work well well the trick that Google found was that you should be looking at the connection you should be looking at the structure of the network not just at the individual web pages earlier web engine and looked at what's on the pages look at the text oh you're looking for a web page about network theory okay well look for pages where the words somewhere contain the words network theory on the page right okay that's a good start what Google realize is that there's a useful information in the link between the web pages well very roughly speaking what Google does is it looks for web pages that lots of people link to lots of people link to this web page oh they must think this is an important web page or bump that one up the ranking and this turns out to be a very effective means of identifying which of the important web pages so they realized that there was information not just in the nodes of the network but in the links linking together in the structure of the network and if you consider money to be a good indication of success in this business then they've done very well after that become billionaires by exploiting the structure of that here's some other examples of network this one is an epidemiological network a network involved in the spread of disease so disease is spread from person to person the people are the nodes in this network and the edges connecting together in this case who's been an enclosed pleaded with meaning you know close enough to sneeze on plum wanna float enough to cough on someone that's how did it easy to get transmitted and you give someone the flu you see them so the blue travels over a network like this people and if you want to know how diseases spread predict how they're going to spread or find some weight with it writing then you need to know networks appear not just in biological field elsewhere as well is a technological example in the picture of the internet so the Internet is a network where the nodes are computed and the edges are data connections and you can certainly well imagine that the structure of the internet destructive this network effects our data that a well-designed internet would get data around efficiently and between computers and all the design one would not and you could ask questions like how could we improve the structure this are there any bottlenecks I mean weak points so you can certainly imagine there could be a connection between the structure and the function of this system the internet here's one more example from my own work this is a social network at the network of collaborations between scientists the nodes in this network are scientists and the edges are commit in the paper so this is sort of somehow communicating the structure on science who's working with whom and the groups that people work together in and so again you externally you could certainly believe that structure of this network is connected with different schools of thought you could find a group of people working on one thing over here and we believe that the structure of the network is correlated with the properties now these examples that I just showed you are all actually kind of unusual in one respect which is that I can make nice pictures of all of them they're all rather small rather sparse networks where I can draw a picture on a piece of paper on the screen here and you can roughly speaking what see what's going on this is actually rather rare most network that we look at are quite dense and they're quite a lot of nodes and you kind of make a picture of them you get something looks like a bit just some giant hairball very difficult to make out what's going on in the data if your data look like this mark the doll at dana-farber likes to call pictures like this ridiculous which I think it as a good a name as any apparently the official definition of a ridiculous it should be visually stunning scientifically worthless and published in science or nature I did not did it here's some other examples this is a picture that one incidentally was a protein interaction Network again this is a blue item cell and the reactions that turned one into another this is a portion of the world wide web I think you'll agree that's pretty impossible to tell anything about the echo this is another protein interactive network I find it hard to believe that anybody even bothered to promise that you know what are you supposed to know by looking at a picture like this so this is part of the problem that we're dealing with we have these very large data sets they're very dense you can't really look at that you can't see so you know eyeball analysis so one of the things we want to do is understand what the structure of these networks is even when you can't make a picture of it and that means we need to have some more rigorous and more formal way of analyzing and extracting such a problem by doing something complication so so one good question here is when I ask what do I when I ask I want to understand the structure of a network what I really mean by that what is structure in the network in the various ways you could define structure maybe structure is the thing that makes a network not random its lack of random or maybe structure is some property of the network that's correlated with the particular thing that you yourself are interested in but but I think probably the best definition of structure as I'll be talking about it in network is that it's something which allows me to give you a simple description of what's going on I in general if you haven't give me a big complicated network like this one I want to throw the whole thing then you know KP millions of nodes and millions of edges and I have to tell you where every idea is I have to give you a lot of information describe destructive network but it's also possible that I could give you just a very simple small piece of information and it would still be very useful feel like I could tell you that this network is divided into two big clumps of nodes one here and one here right and that could tell you something very useful about Stuxnet well yet it's just a single pin so structure is for me some simple thing that I can extract about this network something I can tell you about the network that doesn't require me to give you millions of pieces of information but the human beings frankly are not useful that's not useful to human being they're not good at understanding millions of pieces I give you a terabyte of data it may accurately describe the structure of this network but you as a human being can't reinvest it you need to somehow boil it down into something simple that actually descriptive and useful to us as human beings and back to what we call so let me give you a simple sort of illustrative example of that this is a fake example it's like an idealized example it's not real but it shows you the kind of things I'm thinking of I'm going to make a fake Network and it's going to have this particular kind of structure in it called community structure which is where the network divides up into a bunch of different parts this is only one kind of structure I'll go for many different times today but this is one of the ones I've looked at there are there are many other things that we will consider today such as core periphery structure dense core surrounded by three ranking structure where the nodes of the network are aligned in some hierarchy latent space structure where the nodes are arrayed in some space which is usually unseen it exists but we don't conserve it and we need to infer it by looking at the data higher a hierarchical structure where net one breaks up into Park and the pot break up there are many different kinds of structure these Network can have and I'll talk about several of these today but for this first example let me talk about community structure which is breaking up and networking FIFA so this is something that might very well be interesting in social network the clump of people here who will interact and another clump of people here who are correct okay that's interesting what is it that defines the two clumps why are they not interacting with each other it might also be interesting in metabolic network oh there's a clump of metabolites over here that all interact or it might be interesting on the worldwide web oh there's a clump of web pages here they're all linked right so people are interested just in being able to pull out this kind of stuff from networks and it's actually a big industry that evolve of people coming up with smart way but that's not what I'm going to do here I'm just going to give you a very simple example where I'm gonna make a fake network that has this kind of structure the way I'm going to do is among you something called of stochastic block model with a simple mathematical model of a network it works by taking some number of nodes and dividing them into groups they divided them into three groups somehow anyhow I like and then I'm going to throw down edges at random between those nodes with probabilities that I chew they're going to be two different processes one that I'll call P in is the probability that there's an edge between two nodes that are in the same blue all the way that I know somebody who's in my own circle of bread and P out is the probability that I know somebody in one of the other third probably an edge in Q notes that are in differences just to probably and if I choose PN to be large and P out to be small then I have traditional community structure I have dense connections within the groups and only a few connects between the groups like the picture that that's what we call community structure is comp of nodes over here and clump of nodes over here but not many so suppose that with my structure now I'm going to try and detect that structure in the network so this is a trick that we quite often do in this area we make up some fake network when we know what the structure is and then we ask can we actually detect at another good way of checking whether your methods are working and so here's one way of doing this I'm going to take this network I'm going to represent it as a matrix so I can if I put node in my network I can draw an n-by-n matrix and I put ones in it who represent where the edges are give the node numbers from 1 to n if there's an edge between node 13 and node 20 and I put a 1 in the 13 comma 20 L so just an N by n matrix with 1 written on the edges and zeros everywhere else the Scorpion JSONP matrix so now I'm going to calculate the spectrum the eigenvalues of that of that matrix they look like this so this is an actual example I said the adjacency matrix into standard QR algorithm it gives me the spectrum of that matrix and you see an interesting pattern you have almost all the eigenvalues in a sort of band here on the left and then you have a small number of outlying eigenvalues that separated from all the others over there on the way that's what you've seen in this numerical calculation this was an actual network that I did but we can also prove using that as the random matrix theory that this is exactly what whole calculation because the model is that so here's the thing it turns out that this gives you a very nice decomposition of the structure the network there are n minus 3 eigen values over here in the band it turns out that they are random isotopic vectors those eigenvectors that point randomly in any direct in other words there is absolutely nothing there on the other hand over here these outliers are definitely not random there's a lot of interesting stuff going on there that's where all structure is so this model network that I made up has structure in it namely that the nodes are divided into these three groups and there's more connections within groups and how the Queen would that's the structure but there's also random which is just that the edges themselves I just threw down at random like so the both structure and randomness in the network and this spectrum neatly divides the two thing this stuff on the left is purely isotropic random there is nothing there just random vectors this stuff on the right is where all the structure lies first of all you can show that the number of outlying eigenvalues there is equal to the number of communities in the network just look at this picture and I can immediately say 1 2 3 oh I know I have 3 groups in this network read it straight off by looking at it but more than that if I take so ignore the leading eigenvector I take the other outliers not the leading one of these 2 in this particular case but two eigenvectors there there are all n element eigenvectors in M elements and n node network so got two n element vectors and put them together to make a 2 by n matrix a very tall thin matrix and then regard the n rows of that matrix as coordinates so each row has two elements I'm going to record regard those as coordinates in a two dimensional space and plots that end dots that they represent if I do that I get this picture here so this is again the exact same network it's a wheel network I actually did the calculation and what you see is three very nice clump oh stop and if you work out which known each of those dots or on who you find that the three clumps are precisely the three so in other words those leading eigenvalues and eigenvectors tell you how many communities you haven't tell you exactly using everything else here on the left so the tectum very nicely separate the structure in the network from all the random stuff so this is the kind of thing we'd like to do effectively when i say i want to make a simple statement about the network view these three eigenvectors over here are my single thing they contain all the information about the structure and everything else being sort of filtered out that's the kind of thing that I want fail to do so we've spent quite a lot of time working with the spectral method here are just a few examples figures things we work on this one over here is a social and animal social network at the network of friendships between dolphin Oh this one here is a host-parasite interaction a bunch of these different things these methods are useful for extracting this kind of structure and network however they're not the site is they're not what I'm mainly going to talk about today I'm actually mainly going to talk about inference method today and the reason is that these spectral methods although they're sort of elegant in some ways they're really only good for this trick of community detection there are many other kinds of structure that we'd like to be able to act and they're not flexible in the way they can be adapted to many other kinds of stuff as well so I'm going to actually be talking about a more general class class with influenced methods based on ideas of inference I realize that there are people but I don't want to assume that everybody here so so before I get into talking about the actual calculations we've done let I'm gonna do a little sort of primer here on what I mean by statistical inference I'm going to do it not using Network ideas and just ordinary regular data measure in the lab whatever so here's my example of statistical inference just to sort of set the stage suppose i measure some bunch of number i just measure some numbers in the lab or something like that there's a n number I'll call them X sub i where i run on 1 through N and suppose of those numbers are represented by this little green line here and suppose that I know these numbers to be drawn I don't know which normal definition is in all this we have two parameters the mean standard deviation that assume that I don't know that all I know some normal distribution question is given the data that I observe and I work out what the mean and standard deviation of the normal distribution from which they would walk ah the answer is of course I can otherwise I wouldn't last how can I do it I can do it by conventional maximum likelihood method so the way this works is I write down first of all I write down the normal distribution there it is the top line there is just the regular normal distribution or mean new thing out then even - where exactly so that's it that's the probability of making a measurement x given the mean mu and now I ask well what's the probability that I made all of the measurements the end measurement that I made in my experiment while assuming they're independent that's just the product of the probabilities for all of them so there's the probability of mentioned X sub I a product I went and just take this expression from here back to put it in there and I get this here for the total progress which can be simplified so that's what we call the likelihood of the data it's the probability that I measured particular set of dick I did at this point I still don't know them mean mu and the standard deviation Sigma they're just represented an arbitrary symbol I don't know what the values of this however this probability of measuring the data certainly depends on the values of those symbol for instance if I mentioned measured a bunch of measurements here and my Gaussian is over here then those measurements look very unlikely they're very unlikely to have been generated from a Gaussian over here if I mention all over that write much more likely my Gaussian was over here where my data are so not all bad things are equally likely given the data that I have good so this expression here measures the probability of the data that I observed what I'm going to do is maximize what which values of MU and Sigma are most likely to look and maximize this likelihood here actually typically one maximizes the log likelihood it's just the log of that quantity logarithms so I'm going to take the logarithm of this thing here and it gives you that expression and then maximize this with respect and you Sigma so here it here's that expression again and maximize this with respect to MU for instance new only appears over here differentiate that with respect to MU that there's all people to zero I get that expression there and rearrange that from you and I get the MU is equal to the sum of my measurement divided by the number of methods and I mean that they teach you but when they teach you this in middle school they don't but you don't have to take it on face right there is actually a derivation for that expression and here it is this is whether you can do the same thing so standard deviation differentiate set it equal to zero and you get this expression which is also the one that's work so what you're doing is you're taking a model in this case the model is a very simply it's just a gal Keenan and you're fitting it through a set of data by maksim like theater in other words you're writing down the probability that you've got the data you observe if that model is correct and then you're maximizing that policy with respect to the parameters of the model so now we're going to do the exact same thing but we're going to do it with networks so it's gonna be a bit more complicated the model is now going to be a network model in other words a model that generates Network and the observed data are going to be a network I'm going to calculate the probability you deserve data IVs in network we're generated by that model and then maximize that positively bit it's exactly the same procedure just with more complicated data let's see where that gets it so the first example I'm going to give is the one we already looked at detecting communities in network so I had some network and I believe that there's some pump in this network and the model that I'm going to use to do the detecting model I'm going to fit is one that I've already described it's this stochastic block model where I throw down some number of nodes and they throw edge down edges between them I'm going to generalize it a little bit instead of just having two probabilities unity out I'm now going to have probabilities for every pair of mutti so there's been a probability second if you only got one one here or the next as we know it in Group one another node in Group one and only a one three is for connection between a node in Group 1 and no 2 group 3 suppose there's gonna be a whole matrix of probabilities those are parameters of my model and the other parameters in my model are still belongs to which group I also have to say which node is in which room in order to fully define a particular special case of this model now I'm actually not really interested in these Omega parameters the probabilities who belong for which group is the thing I'm really interested in because that's going to tell me how the networks break down I used oh me go here instead of P because I'm using a slightly different model it's a model but it's basically the same thing I can write down an expression which gives the probability of the observed graph or network be given the parameters that the Omegas energies and it's giving back here and then what I want to do is just maximize that with respect to this there's two sets of parameters there's the problem he's Omega and there's the who belongs to which group so jisub I here is which blue node I belongs to so the three group then T sub I could be one two or three so G is a discrete variable and maximizing things over discrete variable T R Omega is a continuous variable it's a probability and maximizing things other pieces easy so let's do the easy bit so we maximize this our only go because by differentiating and we get an expression like that looks like this so after max my T with respect to Omega my log likelihood at the following form it's a sum over R and s are groups of their three groups and are those one two three in sum over all pairs a group M sub RS is the number of edges that go between blue power and group s and n sub R and n sub s or the number of known so the way you need to think about this is you give me a candidate division of the network into group then that defines how big the groups are that then sub R and it defines how many edges run between every pair of them and within group so I can calculate all of the numbers here and plug them into this expression I'll get expression for the log length now I want to further maximize that with respect to whom belongs the which group so I can calculate the value of this quantity for any candidate division of the network that you tell me about now I want to calculate it or all candidate divisions and find the one that gets the higher and that will be the maximum likelihood fit of this model for this network the characters I said is that the squeak optimization is difficult and optimizing a between feeding now but something we have to do that America Lee I can't do that big equation so the final stage of this process is London picking miracle optimization but in the end it worked quite well so here's just one example well I'll give you a couple of examples so this example is this is a famous example from the social network this is a small social network of students at their students in a club a University study done in the 1970s so why do we care about this well it's a famous study because it just happened while this guy was studying this particular group of friends at university a dispute arose in the club they had a big argument about something rather and it's split into two and the claim is that if you look at the structure of the network of huge friends with whom that's what we're looking at here you can work out how the split was gonna happen so this network was measured by that he no he just thought you would easily happened to come in a year before this it happened and mention destructor the network we know what it looked like before now the claim is that you can work out how we're gonna submit by looking at something you take this network and feed it in the algorithm - like you and it splits it according to the colors that you see there the red and the blue and it turns out that that is almost exactly along the lines of what did happen in real life even though this network was measured a year before the stick so if you had known what you were looking at you could have predicted a year beforehand how this community here's another funny example this is a network that I got from my Michigan poly glider Anna m'q she did a study of political blocks so these are in websites which people expressed their opinion at politics and so the nodes are website and the edges of the network websites are which note which website link which other ones on the web some link on this website and click on it so she did a big study of this and she gave us the data and we fed that into our algorithm and it splits learn it worked up into two clear community with packing label blue and braid and and it turns out that those two clear communities correspond very closely through the political alignment of what these are all blogs about politics some liberal some of the conservative and turns out that even if you didn't know about liberal and conservative politics you could tell which logs were which just by looking into very communities the liberal ones site of a liberal ones and the deserved and once they know the only ones that there are very pigmented free and this algorithm that exactly wants so that is a good question in this particular case yes we are more generally you don't always have to do that but it's a difficult job actually working at how many so often what we do is we just try the analysis with a lot of different kinds of drum and then look at what we've got and you know if we learn something from one of those analyses and that's great we learnt that but but you're not necessarily saying this is the best and as I say that turned out to be it's not entirely a thought problem the model selection problem okay so so that gives you an idea of the kind of thing we're doing what I want to do in the remainder of the time is give you a series of examples of other applications of these kinds of ideas the very network I'm gonna start off with some examples that they're really just very slight variance on what we saw already here but then as we get towards the end of the hour also further afield so so here's one example which is really just a slight variant among the poor but an important one I think almost everything you'll see in the literature on network makes the assumption that the data we have are correct but of course that experimental data in very few experimental data are completely less so in other words usually you they claim that there's an edge there you just assume that that's not really right people say there's an edge there what they mean is well we're kind of 90% sure there's enough air and maybe kenderson so what we really have are probabilities we don't just have yes or no for each edge in the network we have some probability that exists so let's suppose that instead of just an adjacency matrix that's completely composed of zeros and ones we have an idea native of the probability that say probability that we believe and it is then based on the experiment that we've done so I'll call that matrix Q sub IJ that's the probability node I and J are connected by it so in the conventional interpretation these problems would all be 0 1 0 meaning the definition edge there I mean it definitely is an edge there but in reality of course that's not all though these are often sort of in the gray area in between so one way to handle data like this and the most common thing that people have done up now is they just place a threshold on it like they'll say anything about 50% I'll call that an edge anything below critical manner however this is obviously not a very good thing to do there's a huge amount of difference between edge which is there with 1% probability in an advocate there is 49% probability but those are both less than 50% right so they book big into the non edit right if you tell me I have a 1% chance of winning the lottery and what ok doesn't sound great didn't tell me I have a 49% John right so you're throwing away a lot of information if you just sort of hold it so it'd be much better if we actually make use of everything that's contained in these folders so we did some work on this is work I did with my so what we do is we say what we'd really like to do is write down a likelihood function for the probability of the data we have that's this matrix Q given the parameters of the model that we belong to what group in the same thing is before and we break that down as the probability of the actual kumoi the true network really which proteins interactively thorough or whatever it is which we don't know so that's the probability piece of the graph given the parameters of the model we don't know this it's not made in the experiment but we know the functional form because that's precisely the thing that comes out multiplied by the probability of the data given the unobserved larkey and then summed over over this part here on the Zipp ugundi we don't know but magically it turns out we don't need to know it because it said that actually we only need to know that we can still do the whole maximum likelihood thing and when we do that we can make sort of optimal influences Levy structure in networks where we only haven't sort of fuzzy data we're not really sure the answers we have error abound so just as an example here's one thing that we did so this is some tests of the kind that I'm talking about for where where we make up some data some fake data where we know what the structure is in the network and we see if people treat that so we compared what happens if you use our algorithm which is that didn't do line there so this vertical axis here is measuring how successful we were recovering the structure it's the measure of what fraction of the nodes we placed in it but basically this fire is better but that blue line is how well I'm ever did this red line is how well you do if you just threshold the data like I said before and then just apply conventional thing there's of course various different choices for where you could place the threshold and you do better or worse depending on where that is that's what the curve is showing but never does it come anywhere close how well you do if you actually do this properly okay so that's really just a slight variation on something what is it so here's something a little bit further afield we're still looking for groups in networks now but now we're going to acknowledge the fact that those groups might be a bit fuzzy so previously we said well let's look for ways in which this network breaks up but everybody has to belong to one group it's like hard division into the guys in this group and the guys in that group and that's often not what real groups if you think about a social network for instance you belong to a bunch of different groups you belong to your family and you belong to the people who you work with it your job so the groups are overlapping and like you're the person in the overlap you're belonging to several different groups so this is a common thing that happens in lots lots of network that a node can play a role in this group and it can play a role in that group as well we like to allow for that to happen so the way we do that is something similar to what we do before but I'm going to introduce an extra set of parameters here I'll call Peter sub IR that Peter sub IR is the fraction of the edges connected no tie that connected people power so I'm in a particular group and some of my edges connect me to this other this group and some of my just connect me to this other group so this is sort of measuring how strongly I'm connected to vary so this is sort of a generalization of the G parameters we had the port for everybody in one group or another group now I can so I can write down a log likelihood for this model well there it is right at the top and in principle at this point I noticed maximise it the way I've done with everything else zero you gonna do something slight different something a little more flexible so I'm gonna say suppose I knew what the parameters feed it and then we go if I did then I can calculate this quantity over here Q sub IJ is a probability it's the probability that nodes I and J belong to groups R and s that's what that thing is joint probability and it has a very nice import expression in terms of the parameters I know that well if I know this quantity here using that I can write down expressions for the parameters there are also quite simple they're given by these expressions here so if I know Q is a function the parameters and I know the parameters are function of Q then I can just solve forever what I do basically is I guess values of the parameters feed them into this equation here get Q take you feed it into these equations up at the top and get the parameters and I just keep on going back this is called an expectation maximization Alvin the technique in 80 an inference and it has a number of advantages but one of those is Marchese is that ah it's really just these three equations five line programming just go around it smooth over and over again and that that allows this method to get up to very large networks one about gold here somewhat like so one could in principle just directly maximize here's an example application to a social network so what you're seeing here is the colors represent who got cooking which several group in this network what you see is that most nodes get put in one group but there are some nodes that got lit up between several different little pie chart there and actually an interesting thing happened if you look at the nodes they get split between several groups you'll notice that often they're the node with a large number of connections there are some those that have got a lot of connection and those no typically are the ones that just don't comfortably in any group you know if you're connected there lots and lots of people all over the place then you don't really belong to this one you belong to many different groups so it's natural to fit those up between several different groups these nodes that are very highly connected give you problem in cases where you're trying to split the network up and be completely non overlapping group allowing the groups to overlap solve that problem the ones in the nodes in the overlap are and the ones and by putting them in the overlap you can allow them to get rid of it so this is really only a slight variation what we saw before almost all nodes are still in only one group but the problem though you can now deal with them back ok a little bit further afield still so far I've just been thinking about dividing networks up into groups you know such that a lot of connections within group and a few connections between group but there are much more general ways you could divide up or classify them it doesn't just have to be I'm in a group and I have lots of connections in my own group there could be many other things you could do for instance the definition of being in this group here could be that a person in this group here has lots of connections for this other group over here and a few connections for this group down here and not very many connected to this group and some connections or whatever you know I can think of some complicated thing that this pattern of connection is what defines being true right the definition of being in the group doesn't just have to be I have the connection there are many other ways people should be defined so I can talk about more general classifications kind of just classify the nodes of the network but what I want to do is I want to do that without saying what pattern I'm looking for I don't want to say I'm looking for traditional community structure where everybody connected in their own group I want to say just find me any pattern that gives a good description of this network so we can do that as well we can write down a model where you have some general probabilities for a vertex in a group are to be connected to other vertices anywhere else networks you can write down the likelihood that you can take its logarithm get a log likelihoods you can write down an e ml Gotham again fairly simple it's just three equation which you can iterate around this whole loop in your computer program pretty quickly and what it allows you to do is take Network and divide them up into group but in very general ways where you don't have to specify how you want to do the division beforehand so it could be traditional community structure where people connect their own loop but it could be other things as well and whatever it is the program will find that was it out first so here's an example this is an it is a lexical network Bacchae a language network in this network there are no represent word they're actually the 100 most common words that appear in the book david copperfield which I chose and the edges in the network represent which word here next to which other words adjacent these are nouns and adjectives in this particular example I got a little conjunctions in adverb and in the English language usually what you get is an adjective next to a noun and I think it's some or no like the leg but sometimes you get an adjective or another idea like the big red one sometimes you get a noun before another round like a big school but that can happen but most often it's adjectives next to net so that means that most of the connections in this network should be between an adjective and a noun in other words between nodes of unlike time not between not within group but between the nexus there are only rather few with English so take this network and you feed it into the algorithm they just described and low in the whole it split it up and if you group the nodes and they correspond very closely to the noun on the left and the adjective on the right to the nouns are in the black dot here and pretty clearly found the noun from the abacus in this network it doesn't know anything about language but it worked out what the noun but moreover the structure its found is not conventional to use destructor where most of the connections within the glue they're not so the connections are not between noun and now you can see that in the thicket most of the edges run between the two group so I didn't have to tell it this it just found the particular kind of structure that makes sense here's a more extremely example this is just an artificial example made up the test method so this is a network in which there are four groups of node who asked these eight keystone node in the middle that was sort of motivated by ideas that come from ecology in food when there are eight keystone nodes in the middle and we threw down a bunch of agency almost all the a didn't network are completely random so they mean nothing at all they're just there to try and throw the algorithm box game but what defines the structure is that every node is connected to pull of these teeth no node and which ones it's connected to define which group is it right so there's lots and lots of connections there which mean nothing at all but if you look carefully each node has two connections which tell me which group it did but we didn't tell the album this we didn't tell them that it's Keystone toads or that this is the definition we just feed this in through the algorithm and magically it comes up with the right answer rearrange the nodes into the four groups the colors here represent the four groups it is supposed to find and you know hasn't got it perfectly right pretty good and and it also separately picked out the eight nodes in the middle so in theory quite complicated form of the classification instruction network could be picked out using this method and moreover you don't need to know in advance what you you're looking for you just say finally something that describes okay in the last few minutes let me give you few other examples I mention core periphery structure this is a different kind of structure where there's sort of a dense core in the network and it's possible around the outside you can do that too using a stochastic block model again it's just a block model in which you have two groups the core and the periphery and there's lots of connection in in the core group and there's very few connection in the periphery group and they're sort of middle in them but there's still a stochastic model and the same kind of inference for what well here's just an application that we've used to pick out the core node in the hierarchy I mentioned it's sort of a generalization of community structure where you split the network up into groups and then you split those nets into smaller groups we can do that as well the way we do that is we represent the hierarchy by a tree like structure that represents how the node splits up and how those split up and how those pick up all the way down until you split the feeds with a single then this tree structure is your model and that's what you're thinking so you can use this you could fit it to a network and you can derive a tree structure I'm actually looking at the fish book just be more output often it's better to just look at the network itself and see how it breaks up into groups like this one up here at left and then how that group breaks up and so we're dividing and subdividing what this particular example here is time natan space vodka laden space structure is suppose I had some network that somehow embedded in a state we know then the network this somewhere in the face but I don't know where I just know that embedded in space somehow and that the positions of the node affect the probability they'll be connected so an example would be a road network so this is a picture of the interstate network of the United States the nodes are intersections between interstate edges so if two nodes are closer together in this map they're more likely to be connected by road they're very powerful so now suppose you're given only network itself but you're not given where the positions are question is can I infer the position just by looking at the structure so the positions of the nodes are now playing the role that was previously played by who belongs to which community it's the unobserved defining variable that's really telling you what the structure the network is so yes the answer is absolutely you can you can write down again a model for this and you can like an unlikely he put and then maximize it here's an example of that this is the neural network CL exam same we derived by white it out in the nineteen eighty and each of the nodes in this network is somewhere in the body and what we did is we took the neural network and we fed it through the tails of them then what are the positions of the nose what's your best estimate of the spatial position of those and that's what the colors represent here what you can see is that the color sort of nicely go through the spectrum from red to blue degree the yellow at that end it's not perfectly in it some big note here that the youth of the Swedish one here but basically it's got them in the right order it's got it worked out that these notes here or at one end and then sort of going down so we were only given the network we could estimate roughly how the neural network is laid out physically one and sort of nice special example of it is ranking or status in network here's some results for my project that I did with my former student Brian ball here we were looking at social network these are social networks that come from school so a large study not they we did so you come by some other people of friendship networks a month in u.s. one of the interesting things about the data that come from the study is that there's a directionality when you go into schools and you want to work out who's friends with whom you ask people you circulate questionnaires or you do interviews and say who are you friends with and what you find often is the person aces their friends with person B the person B doesn't say now historically the so she always said okay that's weird there's obviously something wrong with our data but perhaps there's not anything wrong with the data perhaps actually from person 8 is definitely person B and person who doesn't say that back maybe there's something going on so that was the hypothesis the hypothesis was that there is a ranking a latent base structure that we're not observing that the kids in the school are somehow arrayed along as I mentioned some sort of higher up the hierarchy and I'm a low down aha e and when you say your friend with someone you tend to say your friend in the beginning color than you are I mean I know this and the kids are the cooler than you I do not say that so that's the hypothesis right so we can check that again what we do is we come up with a model where there's a latent space variable that says where you are in the pecking order in the school and there's a probability that one person says they're friends with another person based on your relative position in the pecking order okay so you write down the model I'm going through in detail so I show you some of the result so what these two pots show are the probability to the person will say they're friends with someone else based on for the difference in length they're different than where they are because and there's an interesting thing that happens the left hand thought is four pairs of kids where they both said they were friends with the other kids in that case there's a huge spike at zero which says that almost all pairs of friends who stood in both bands they're almost at exactly the same height in the hierarchy apparently the kids in the school have a very clear idea of exactly where they come in the pecking order and they're friends with other people who are at the exactly the same level in almost all cases two kids say they're friends with each other then they're there at exactly the same level in high high or low but exactly the same level so the Delta but if you look at though one way one where one kids is different and the other one doesn't now it's just lopsided distribution there's still a big spike at zero but there's also this sort of long tail that goes off to the left there of people saying that they're friends with into a pool in there but also notice the way it sort of drops are basically it kids saying that they're friends with people who are just a little bit cooler than them right they don't say their friends would like the coolest kid in the class because nobody would believe them but they said that you know they they see their friends with this person a little bit cooler now so for those of the hypothesis we tested it by looking to see whether these inferred latest base positions be the ranking of the kids in the schools are correlated with other things that are widely accepted to be noted so for instance this one over here showed in degree in degrees just how many people said they were friends with you in other words the total overall popularity which is you need to be a measure of status and you find that there's a strong positive correlation there of the ranks that we infer with over another one is age it's it's why they accepted my sociologists that the older kids in school have higher faces on young kids indeed you find that the average ranking of it increases if you go through the great we did this kind of now axis 484 different school that to Jada set that we got many people of Duke University of networks in 84 different US schools and it's all extremely well all of the we look at very I see that I'm out of time here so I have to stop talking before I do I say thank you to my birthday collaborated on this bunch of students who worked on this with me Travis the only one who still here you see over there and things one postdoc Iago and two faculty collaborators right around here in School of Engineering thanks for them thanks [Applause] so the networks are the day you mean there's more than one different model that good yeah okay so saying like if I have one particular model what model that more than one maximum likelihood right well so yes it so that's absolutely true that there are so so this is this is an excellent question and there's several different aspects so one is there are some network where there are several different maxima and they look nothing like each other there's several different way of breaking on the network there are a few good examples that people come up with where there are competing ways of breaking up the network and they're both meaningful it several different meaningful ones they just are breaking up a long different line something like yeah I've you know I've got a social network and it's breaking up a long line of age but it's also breaking up again it could be more than one meaningful division but in most cases when there's the divisions are really completely different if what it's usually telling you is just that the signal in the network is not very strong but this isn't really a very strong division if there's a really strong division there then pretty much you sort of gravitate the world it that our algorithms not going to have any difficulty finding it so you can get roughly the same so if you're getting totally different results every time it probably means there isn't very far up to there and then you're not really interested you also get stuff like what you're talking about where I get slightly different vision and I think what that's telling you is just there's some fun and you can get at that by well that's running the album time but a better way of doing it is these EMF of them detail but they give you a full posterior distribution over all possible time of no hidden very link rather than just a single maximum likelihood assignment so that then you can actually by the width of the likelihood P you can work out that error on your assignment you can do complete statistic on the assignment so those kind of thing for certain throw up all and yeah if your peak was very good did you comment about using multivariate time series to elucidate mechanism networks okay so next question so most of these networks vary over time and relatively little has we in my group have actually just started thinking about that one my other student I'm sharing it back there my dear to do with this so we were hoping to find something exactly along the lines of what you're suggesting which is that I can by combining data on how these things vary over time I can get a better handle on the structural variable to describe the structures and networks and I can by looking at them in a sense the data when you're looking at network our network a network is a data point it's not a data set when we're fixing when we're when we're fitting data to this model what we really have is one measurement like we have one internet right we don't have lots of instance the model generates an ensemble of possible different internet but what we measured it and we want so in a sense it's a difficult inference problem if you really only have one measurement but if you have many over time evolving in some way and in principle you should be able to do better than that the simplest the purpose of it have been difficult in that if you have several measurements in the network they're independent but almost certainly they're not both the one we're looking at very highly correlated and that basically the feature that we're trying to incorporate but they say it's early days yet thanks for the documents really cool my question is water I guess what are advantages of this marathon in terms of you know accuracy and computational complexity over for example Markov processes or some modern algorithms like T distributed stochastic they were embedding that allow you you know take those big large data sets and embed them into smaller sub spaces and do this kind of clustering separation between clusters and meaningful lives realizations as well a lot of the kind of compilations I've been talking about here could be done by montano method we don't actually do them that way because we think that we basically does the same thing but in principle one could absolutely use other models like various variable model you can do that but see what they're doing is they're assuming a particular structure the model they're assuming for instance say that nodes that are closer together in the latent space are more likely to be connected we don't make that assumption they may be more likely to be connected but we're not assuming that oh we're all we're doing is we're asking the album to come up with some description come up with some assignment of nodes in this latest space plus something else that allows you to give a compact description of network that's something else might be the nodes closer together they're more likely connected but it might be that those over here are likely be connected with nodes over here and these nodes they're unlikely or anything it can be any sort of combination of probabilities depending on latest based there so so if you knew in advance what for the patent you were looking for in your data those kind of method would work and indeed might be faster but if you don't know what you're looking for and these methods are very flexible you just feed your network in and you say give me a good description of these data and it's that one robot structure like software or things modules within some kind of standard framework we'll folks that are new to this good plug and play and start to try out some of this stuff there are so there's a large amount of pop we're now available one of the nice things so this field that you know being going in earnest for 20 years or so and in the early days you had to write your own program but in ten years especially in the last five years some people who are a real expert you know my software that I use and is is you know me hacking up something not elegantly but there are people who know what they're doing on the computer science side very nice stuff if you're interested in this stuff two packages that I use a lot that I really recommend one is called get ete pho which is uh you know pull down menus type thing where it's very nice visual interface and you don't need the program at all you just has a lot of $10 another one it's something called Network X it's a Python library so it's the people who are programmers you write your own Python program but then the network algorithm is just called a library 17 for you and although I do like my own software to do some calculations these days and I use those packages a lot thank you thank you mark this is great you have an open mind which is a big question about the network especially the term structure by yourself the Silicon Valley which is become the epicenter ocean was long believed is Network but after there people don't believe that it is a class were of creativity that gentleman mention about classroom what is the classroom for me is next the real world is met so the struct they set up this able network is the way open the door the see Matt so my question actually ask you that is holistically do you have some kind of algorithm or idea to see how we can link Sun ideal structure this and this for example the relating space in hierarchies in three star but something in between is actually we can discover something rather than set of program and equations so the cluster there's the analysis what I think more important than the network any comments thank point I can look at a network so so absolutely you're right in a sense this is all just the first stage of the analysis that you were actually you have a data set then you actually want to understand what going on these kinds of methods would what they would give you is a simple description of your data it breaks up in this way or embedded nicely in this space or it has this kind of hierarchical function to it once you discover that that's really only the first step now you have to ask what do I understand in biological terms or in physical terms in social term that this is telling me so I'm going to have to do some additional analysis where I say go look at the groups that I found in the network and say what is it a character racism why is it that this bunch of people are all bored with each other and this month we're all going to be together but they're not talking to each other right that's not something that you're going to get out of the network itself that's something that you using some scientific insight and some understanding of this have to work out so this note these methods will couldn't tell you where you should be looking but they'll tell you want you so absolutely there's this is the first step and then there's many more thank you [Applause]