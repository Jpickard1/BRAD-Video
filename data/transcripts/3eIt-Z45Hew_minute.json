[
    {
        "start": 3.65,
        "text": "you could have been right well good afternoon it's my pleasure to introduce the speaker of our seminar today like the Michael shoulding who's an assistant professor in the internal medicine department I think I give you a more sort of you know the way I know Michael some kind of presentation you hear that these major problems in healthcare are expected to be solved by people who speak two languages they understand medicine and at the same time they're very good at engineering and computational aspect of the project and you're gonna hear from someone who has "
    },
    {
        "start": 64.979,
        "text": "this you know bilingual ability when you see his presentation you see a lot of math in there you realize that we're not talking with a regular clinician here or a regular researcher in health the good thing about Mike is when we started working together you can send him any mathematical paper any kind of statistical mathematical paper I mean like you know a few hours he's asking question how to improve that not only understands that the Foundation's it talks about improvement on the quality of his work I can give you two things first of all he's K our work was you know when they scored by NIAC was 11 the score and it's not really something that you always hear you know in in our world secondly I can tell you personally when we wrote an NSF grant Bailey a very competitive NSF grant workers are together which we spent a few days on it not months just a few days and Mike was "
    },
    {
        "start": 126.479,
        "text": "writing most of that I can tell you that my expectation was that just like any NSF grant that you know I've been working with NSF for like a couple of decades it will come it will get rejected so I told him would be rejected but when we hear that it might be funded my expectation was that like a hundred different questions will come and say address these addresses but Mike wrote the grant so well that the first time in history that I worked with with NSF at least no comment caper came back just do it as you suggested and it's perfect so this is the quality of a clinician who understands the computational aspect of that really really well and goes deep into like sophisticated mathematical aspect to find a solution so what I'm gonna do I'm gonna basically give the floor to Mike and I want you to join me thank you "
    },
    {
        "start": 188.4,
        "text": "Kevin that's very very kind so the title of my talk this afternoon is improving acute respiratory distress syndrome diagnosis using novel machine learning approaches I don't have any conflicts of interest related to this presentation so I want to start this afternoon by telling you guys a story about a patient who has recently hospitalized in the University of Michigan and I hope in telling you this story I'm gonna give you some framework and a sense of urgency about the problem that we're gonna talk about today so this is a 65 year old female and she undergoes spinal surgery for back problems and after the operation she's doing okay but then all of a sudden she developed really severe breathing problems and has to be put on mechanical ventilation support and so the on the left side here is the chest x-ray she gets right after her procedure and you may or may not have looked at chest x-rays before but this is fairly normal there's a heart in the middle there's lung fields the lung fields are "
    },
    {
        "start": 249.87,
        "text": "mostly black which is normal there may be a little scattered abnormalities lower but pretty much normal but 12 to 24 hours when she's in really severe distress she gets this chest x-ray on the right and it's markedly abnormal the lungs are full of this full of his white stuff what's this whites it could be white blood cells and flammer carry cells it could be fluid it's just clearly abnormal and since this was unexpected the pulmonary doctors were called to see this patient and you know the pulmonary doctors look at this patient carefully they review her history or they review her chest x-rays and they suspect that this is due to fluid in her lungs from heart failure and they recommend treating with diuretics which they think these water pills remove the fluid and then she'll improve quickly and so that's the recommendation so that's what's done the Bayesian has given treatment for heart failure for five days but five days later she's not improving at all and another chessick "
    },
    {
        "start": 311.37,
        "text": "sure is performed and if anything these abnormalities are worse and at that point her diagnosis revised and she was given the diagnosis of acute respiratory distress it so this is one of many examples of patients who are actually getting cared for in this healthcare system and they're actually getting misdiagnosed or they're diagnosed late or they're not diagnosed at all and you know there's one thing that's particularly problematic about this case for me so as it turns out the pulmonologist involved in this patients care a few weeks ago was myself so I am potentially you know an expert in a diagnosis of acute respiratory distress syndrome and yet even myself I got confused with this case and potentially initially misdiagnosed the patient so the problem that we're really talking about today is medical diagnosis what's medical diagnosis medical diagnosis is the process of uncovering the condition "
    },
    {
        "start": 373.56,
        "text": "or disease which is causing a patient signs or symptoms and it's really the fundamental or the critical first step to ensuring a patient gets the right treatment and it's also an underappreciated source of quality of care a quality of medical care and diagnostic error is actually the most common cause of paid malpractice claims in the United States and we're starting to recognize that medical diagnoses interactor it diagnosis is really actually a problem that we should seriously and in fact the Institute of Medicine converged a panel to think about this problem and think about ways of moving forward about the research that needs to be done to address the diagnosis in medicine in 2015 so today I'm going to tell you about the consequences of inadequate diagnosis of arid yes and then I'm going to tell you about some of our work to develop models to identify patients with arid yes with the hope that these models that could could be implemented at the bedside and help clinicians better identify patients "
    },
    {
        "start": 433.959,
        "text": "with their deaths and treat them better and so in talking about these models I'm going to tell you about some two concepts and machine learning learning with label uncertainty and learning with privileged information and then if we have time I'm going to discuss more briefly some of the other machine learning projects that were involved in in a or D s so what's a or D s sort of for the non clinicians in the room which I suspect are most of the people here today so acute respiratory distress syndrome is this severe inflammatory injury in the lungs basically where the lungs get filled with white blood cells and fluid and as a consequence of this patients get very low oxygen levels they need to be placed on mechanical ventilation or other invasive respiratory support they have a higher mortality because of this it Arius develops it's sort of a downstream consequence of a severe insult to the "
    },
    {
        "start": 494.499,
        "text": "body like sepsis pneumonia trauma and it's important that we recognize patients with air D s when they develop this because we have like very specific intentional treatments that we can provide to these patients that can help to improve outcomes the problem is that in clinical practice we actually know that we do a very poor job of identifying cases of era TS so this is a major study in JAMA from 2016 and it basically followed patients with air D s in 50 countries 500 ICUs and sort of the bottom lines from the study where that on the day that patients develop their T they were only recognized as such 34% of the time and 60% of patients were ultimately recognized at any point in their hospital stay so this is a problem and what's even more of a problem is when you look at whether or not these patients are getting treated appropriately so this is a study that's looking at a particular treatment for "
    },
    {
        "start": 556.889,
        "text": "areas called low tidal and ventilation and only 19 percent of patients in this study with their IDs were really adequately getting treated and this is all because these clinicians were really failing to recognize and diagnose these patients appropriately so what's going on here just kind of give you a better sense of why clinicians have so much trouble diagnosing Aird yes so this is sort of what happens when a patient shows up to the hospital so they show up they have this really severe illness like sepsis or trauma or pneumonia we gather a lot of data about this patient initially when they come in we get vitals we get laboratory values we might have information from a waveform like an EKG or a pulse oximeter and we have all this information we take this information we sort of come up with a working diagnosis or primary diagnosis in which we treat the patient for and some patients do "
    },
    {
        "start": 616.97,
        "text": "better but some patients do worse and patients who end up developing a RDS often don't develop this for say 12 to 24 to 48 hours after they present so they so they sort of evolve over time and hopefully at the right point a stupe clinician says gosh I need to get a chest x-ray and then hopefully interprets the chest secure correctly and identifies these abnormalities and then puts it all together and makes a diagnosis of era TS and because of the sort of the temporal nature of this data and the challenge is sort of putting it all together particularly given that you know clinicians working in the hospital particularly in the intensive care unit have lots of other stuff going on it's easy to miss another sort of issue with a RTS as a disease is that it's based on clinical criteria and the clinical criteria themselves have imperfect reliability so what does that mean so if "
    },
    {
        "start": 678.29,
        "text": "you ask two experts to review a case for era - yes they kind of agree that the case is erred yes with the kaepa score of about 0.5 so what does that mean just just as a review so capita is a measure of innovative reliability it's a score from zero to one one sort of being perfect agreement zero being any agreement is no is what would be expected from chance alone so a couple point five it's moderate that's not that good why is that a problem so when we can't identify the diagnosis of arid yes really well it really hurts our ability to study this disease so this is some simulation worker that I've done in the past sort of looking at whether when we when we diagnose this condition areas with low reliability can we actually study it well can we actually detect true Association so this is a simulation "
    },
    {
        "start": 738.56,
        "text": "study looking at a biomarker association with the development of era TS if if we diagnose erred yes really well with perfect reliability we can detect that biomarker quite well but as the reliability the diagnosis versus we just fail to actually study this well and detect these true Association and this problem is not just for epidemiologic or biomarker study this problem is also a problem for clinical trials if we want to study new therapies we can't identify these cases really reliably we really are limited in their ability to study this disease so the implications of low reliability diagnosis are it can potentially is contributing to our inability to recognize these patients in practice and treat them it's potentially an underappreciated source of bias and research and it weakens our ability to really understand this disease better and learn new treatments for this disease so what's the solution then well "
    },
    {
        "start": 801.26,
        "text": "our solution then is to develop data driven approaches to a RDS identification our goal is to develop novel machine learning approaches to assist clinicians in identifying patients with RDS as well as identify patients at high risk for developing disease in the future so I'm going to talk now and tell you more about some of the models that we've been developing for areas so so for these models our goal is to identify patients when they develop area so we're interested in using sort of all the data that's being collected on these patients from the time they come into the room to room and tell the time where they developed this condition so one of the immediate questions that we ask ourselves is what should we do about the chest x-ray so the chest area is a critical piece of information it's sort of critical to the overall diagnosis severity s so should we have a model "
    },
    {
        "start": 861.68,
        "text": "that includes information on the chest x-ray well and when we thought about this we said no and the reason why is because if our model requires information from a chest x-ray if the chest x-ray is not obtained in a live environment the model may not run very well so we don't want a model to be dependent on the chest x-ray information to make a good prediction or identification of era TS and so our our ideal model might actually be able to use just routinely available data to actually identify patients that are at high risk for developing areas so that that algorithm potentially can tell a clinician hey this patient is high-risk maybe you should get a chest x-ray and and confirm that diagnosis so the basic problem formulation here is you know given a set of training examples where we have input features X and output "
    },
    {
        "start": 922.49,
        "text": "labels Y where Y is one if arias is present and Y is negative one if arias is absent we want to find the function that map's X to Y and so for this work we use a data set of 401 patients patients were admitted to the University of Michigan in January in 2016 with moderately ohok low oxygen levels there was also patients additional patients admitted with respiratory failure and this is sort of the data set which we want to know whether or not the patient has erred yes and for this work we only polled about 24 elect help electronic health record variables including some vital signs laboratory values and respiratory support settings we did this a priority sort of deciding that we wanted to pull variables from the electronic health record which we had at least some suspicion might be associated with the development of error guess one sort of "
    },
    {
        "start": 984.029,
        "text": "key question how do you do this type of work well we have to know which paces in our cohort actually develop there yes and this is actually not a trivial problem because there's no icd-9 code to say oh the patient develop their 80s it may not be well documented in a chart so we actually have to have clinical experts review all the charts and look for themselves and sort of decide whether or not aired yes developed in this patient and what's more because I showed you Jade data earlier that even even among clinical experts the reliability isn't only moderate we actually need to have multiple people review the charts and then the label is the patient gets a label of air yes if the majority of experts feel that the diagnosis is consistent with their yeah so there's a lot of data that goes into labeling a case this is just sort of a screenshot of what you have to look at "
    },
    {
        "start": 1044.209,
        "text": "to sort of make the decision you have to look at the electronic health record you have to look at all the chest x-rays that were performed on the patient and then we created this survey tool which a expert clinician could use to review the chart and decide whether or not a or D s was present and we basically asked them based on your clinical judgment did the patient develop erred yes during the first six days of the hospitalization yes or no but then we also asked them can you please provide the time in which you felt that air has developed because sort of a key question these algorithms is we want to know not only did the patient develop parity yes but also when they developed areas so we had to build in the survey this question about what time over the course of the mission did they develop area and so the data looks something like this so we have patients in the dataset who "
    },
    {
        "start": 1105.5,
        "text": "are observed multiple times over the course of their admission there might be a patient who initially shows up does not have aired yes and then develops arias later a patient who shows up in his hazardous the whole time and then a patient who maybe has observed multiple times over the course of her admission and never develops and then this is our approach this is basically taking the data from the electronic health record pre-processing seeing it by normal normalizing it and then splitting the data at the patient level into two thirds for training and 1/3 for testing by patient we use the training set to do cross validation then we train the whole model on the training set and then we apply the model to the test set and we report results from the test set and I just want to stop for a second and just mention that most of the results I'm about to show you we're actually performed by Nick a PhD student in capons lab "
    },
    {
        "start": 1167.72,
        "text": "so yeah well done so here's the initial results so we looked at a couple of classifiers logistic regression random forests and support vector machine you know and the results aren't great the area under super operating curve the sort of the best one is support vector machine it's 0.71 it's not great and so I'm gonna tell you sort of been invested spend the rest of the time showing you how by extending these models we can get up get a lot better so because a lot of the talk today is about a extension to the support vector machine I just want to make sure that everyone in the room is sort of on the same page about the general idea of a support vector machine what's happening here so this is a depiction of the support vector machine so basically if you plot examples on sort of two of their features the goal "
    },
    {
        "start": 1229.639,
        "text": "of the support vector machine is to find a hyperplane or a decision boundary that separates positive and negative examples and not only that but it actually tries to find a decision boundary where that maximizes the distance between the decision boundary and the closest cases this is a sort so called maximum marginal classifier but in reality this isn't usually possible it's very difficult it's it's unlikely that you're going to have a problem where the data is linearly separable so you either choose to use a kernel or you choose to use or the combination of a kernel and it's so-called soft margin SVM so here our goal remains to find the best decision boundary that classify or that separates the positive and negative examples but for those cases in which we miss classify we add this variable sy this is a slack variable and we try to "
    },
    {
        "start": 1290.629,
        "text": "minimize the distance between those those examples that are misclassified from the margin and so that's what sort of depicted here in this optimization algorithm where we're trying to minimize W which maximizes the margin of the decision boundary while also minimizing the slacks for those examples that we miss classify and then the C parameter is the tuning parameter in which we optimize during cross-validation um one immediate problem with support vector machines is that it can get a bit problematic when your data is super imbalanced and our data is super imbalanced so we have 13,000 negative examples and 700 positive examples so that can sort of lead to a problematic decision boundary and the way that you can handle this is by performing something called class support vector "
    },
    {
        "start": 1350.69,
        "text": "machines with a class weighted cost function so the idea here is that you know if you have a bunch of negative examples of not very many positive example you might the the decision boundary might be overly biased or overly influenced by all those additional negative examples and it might lead to a boundary that doesn't perform as well so then what we can do is we can weight the cost function so such that we add a higher penalty to the miss classification of examples from the minority class so that's what's happening here so we change the C parameter such that the minority class is weighted differently the majority class and sometimes this can lead to better results and so we did that and we found that a support vector machine with the class weighted cost function performs slightly better in terms of accuracy but not a lot better and from the receiver "
    },
    {
        "start": 1411.2,
        "text": "right area under the receiver operating characteristic curve but then there's a more sort of insidious and fundamental problem with our problem or our data structure so you know we have observations on patient over time and we're using all these observations to train our algorithm the problem is that these observations for a given patient aren't necessarily independent and identically distributed and you know iid is one of the assumptions of machine learning and having ie iid data ensures that we're going to be able to find the best classifier and you know in a lot of situations this iid assumption is kind of ignored but we sort of suspected for this problem this is going to be a problem that if we are training a model with highly correlated data you could sort of learn a model that over fits the training examples and not perform as "
    },
    {
        "start": 1473.3,
        "text": "well in testing and so you can actually measure correlation between data points using this formula it's basically it measures the correlation between two vectors and when we did this for the data we found this interesting result so what I'm showing you here is that is the average correlation within a patient's data over time so for patients that are four examples that are really close together you can see the correlation is quite high but then over time as it over time as you look at examples or observations within a patient's that are further away the correlation sort of decreases and so at some point it seems like two two observations on the same patient farther and farther away become close to iid and so sorry with this intuition in this insight we devised a "
    },
    {
        "start": 1536.99,
        "text": "method or a slanting strategy to make the data look more IID so the approach here is we basically start by sampling a data point on a patient for right at the beginning and then we calculate the the correlation between that data point all the rest of the data on the patient and at some threshold etta we choose we we say okay after that threshold the data is sort of not correlated much anymore so let's sample that data point the first data point under that threshold that's a and then we basically start again we start with that new data point calculated correlation distance and then sample again under that crush whole data how do you choose Etta this sampling threshold well this is actually something that you could potentially cross validate and that's what we actually did and so this is what the results look like so on the left "
    },
    {
        "start": 1599.18,
        "text": "side here is the difference in the accuracy between training and testing and then the right side is the validation area under the receiver operating characteristic curve and so this is what happened so when you set the it's a very high what that means is your sampling really highly correlated data but then as the Etta is lowered the data becomes less correlated and so as the data is as the data is less correlated the difference between training and testing accuracy decreases suggesting that we're not overfitting the data as well but then at some point as Etta becomes too low this distance increases and then our overall performance decreases - so what's probably happening there is that we're under sampling the data and there's actually information in the data that we're losing by under sampling it and so we're not actually getting as good at performance and so we can actually find sort of the best sampling threshold Etta "
    },
    {
        "start": 1660.86,
        "text": "in which we can use in this problem and it's right around a correlation of 0.7 and so then when we do this we actually have like quite a bit better performance across all the classification algorithms compared to not sampling at all so this isn't necessarily a fair comparison either because as it turns out this correlations sampling strategy actually substantially improves the imbalance in the data set so now we're actually only using 1,100 negative examples and 700 positive examples where with Noah sampling we had substantially imbalanced data maybe a more fair comparison is actually doing a random sample such that the data remains somewhat more balanced where we were randomly sampling so that there's twice as many negative examples as positive examples and you can see here that the correlation based approach "
    },
    {
        "start": 1723.01,
        "text": "still works better so that's all well and good but mmm no thinking about air - yes as a clinical problem and thinking about these clinical experts potentially like myself who are reviewing charts and sort of deciding did a patient have a radius or maybe they didn't have arid yes you know there really is a spectrum there are some cases where we're just so confident yes this is clearly an arid yes case we know this the data looks like it their imaging looks like it and then there's cases where yeah it's not areas we're really confidence not areas but then there's some she's new there's some cases we just don't know very well that may be because the data is noise the the image quality isn't very good or we just don't know and so maybe we can use this insight from the clinical expert on the competence of a training label to better inform a model so how "
    },
    {
        "start": 1784.76,
        "text": "did we do this basically after we ask the expert based on your clinical judgment did the patient develop err yes we immediately followed that by the question how confident are you in your answer to your previous question with the following choices equivocal slightly confident moderately confident or highly confident and so now when we're training with some information about the label uncertainty we have training examples with features X and output class Y and a level of confidence on that training label so how do we incorporate that into a support vector machine framework it's actually fairly straightforward now we're adding a weighting Z to the slack variables which is additional pen that's specific to each individual case which is basically just taking the confidence on the label and transforming "
    },
    {
        "start": 1845.12,
        "text": "it such that the weighting becomes a number on a scale of 0 to 1 and so now in our in our support vector machine optimization problem where we're waiting cases with lower certainty in higher certainty differently so on the example here this case which we misclassified maybe had lower lower certainties in this case which had higher certainty and so we place more emphasis on the cases with higher certainty we try to get the right so when we do this we actually do quite a bit better so when we account for the label uncertainty in the diagnosis we actually do quite a bit better using both either this sort of random sampling approach or the correlation based sampling approach and now we're getting area under the operating curves "
    },
    {
        "start": 1907.37,
        "text": "of 0.85 5 which is actually pretty good so now I want to think a little bit more about how we could potentially use these chest x-rays the trainer model even though we don't want the model to be dependent on the chest x-ray when it comes time to implement the model the idea here is that you know we have all this we have all this training data and we have all these chest x-ray results and they must tell us something about the characteristic of the patient so can we use that information to learn a better model but then not have the model dependent on it information when it comes time for testing and so that thinking about chest x-rays in that way is thinking about chest x-rays as privileged information so what's this idea about privileged information so in machine learning in general argh "
    },
    {
        "start": 1968.17,
        "text": "is given a set of training examples we want to find the function among the collection of function that best approximates approximates a decision rule but in human learning along with examples we might have a teacher who sort of gives us an explanation about the examples some contexts and comparisons and we're able to learn with less examples quite effectively this is particularly the case when it comes to training and clinical medicine so you know to become a doctor um you may only see a few cases of a certain disease over the course of your training yet you could still become fairly expert at managing these cases and that's because while you're taking care of these patients or in training you have supervising physicians who are sort of guiding you and giving you insight into how to manage this case you have like "
    },
    {
        "start": 2028.35,
        "text": "you have access to additional information as you're sort of learning about this case and so in human learning we don't need that many examples to do very well and so that's the idea of privileged information in machine learning so this is additional information that's available about the example at the time of training but not at the time of testing and this it's actually the case that by using privileged information you can reduce the number of samples you need to find the Bayes optimal solution which basically means the best function from among those functions that we potentially could choose from and this is critical for scenarios where annotation of data is expensive and that's clearly the case for error - yes and for lots of other problems that asking all these clinical experts to annotate all this data takes time it's expensive so we'd rather not do that so "
    },
    {
        "start": 2088.619,
        "text": "for me in particular if this works well for err yes I like that because I don't have to spend so much time telling clinical experts please just go review these charts and tell me whether err yes is present um but I also would say that this type of privileged information it's available I think in a lot of other situations in health care this is just one example that I came up with off the top of my head say you wanted to train a classification algorithm to detect cells that were malignant or not in pathology slides and so you may have the label provided by the pathologist is it a malignancy or not but you might also have sort of text information this sort of describes the slide that the pathologist is looking at and maybe that text information might also give you insight onto why a certain group of "
    },
    {
        "start": 2150.13,
        "text": "cells as a malignancy or not so now in learning with privileged information our formulation looks something like this so we have features X still but then we also have these privileged features called X star and these features are available at training only and then we have this output class Y still that's negative 1 or Y so how do we handle privileged information in a support vector machine framework so now what we're doing is we're replacing the slack variable sy with a function of the privileged information and the idea here is that this privilege information again provides us additional insight as to whether or not it's okay or is it more or less okay that one of our examples we misclassified and so in solving this "
    },
    {
        "start": 2210.97,
        "text": "optimization problem however we have to learn the feature weights and an additional feature weights for the for the privileged information on this function in addition to all these other values and so when we did this oh I'm going to tell you one other thing before I tell you that so um in the optimization problem of support vector machine for privileged information the decision rule for new values of X including the dual form of the decision rule still doesn't require X star so this is a reminder again so when you do a support vector machine optimization problem you end up with a function and when you put a new a new data ax - that into that function the classification rule is positive or negative based on the sign of that function and so the rule formulation of "
    },
    {
        "start": 2273.28,
        "text": "this problem where you basically you take this new information in it's a linear combination of the support vectors of the decision function you can see still that you don't need X star after you learn the best optimization function you learn the right support vectors you still don't need this information that's privileged information to make a decision or a prediction about a new case so how does this perform well you know again it performs pretty good at least compared to the SVM without privileged information so this is prima memory work that we did as part of the NSF application where we can again see that incorporating this additional information of the problem that we have available at training time only allows us to do a lot better in terms of the performance of the algorithm so I'd like "
    },
    {
        "start": 2336.43,
        "text": "to just sort of leave you with this idea that learning with uncertainty and with privileged information I think is relevant to a lot of different medical problems because there's actually a lot of medical conditions that have some degree of diagnostic uncertainty I think people outside of medicine may not appreciate that you know there's there's some uncertainty in diagnosis for lots of conditions and and I would also say that there's a lot of privileged information available in electronic health record data that might be leveraged to improve the training of a model so just as example sepsis is an important clinical problem in medicine right now right it's a severe infection it causes a high mortality lots of bad outcomes and uh sepsis has pretty pretty imperfect precision in the diagnosis yet there's still a need to identify patients with "
    },
    {
        "start": 2399.579,
        "text": "sepsis in real-time and then at the same time there's data that's important to making the diagnosis of sepsis that may not be available so when a patient comes in with sepsis we often draw blood cultures and those cultural results aren't going to be available maybe for 24 to 48 hours so we can't rely on the cultural result to make a decision about whether or not a patient has sepsis but at the same time in retrospective data that we use to train an algorithm we actually have those cultural results right you will actually know which patients had cultures that were positive or negative and potentially that's privileged information that we could again use to improve the performance of our algorithm so now that I've talked about that in more detail I just want to mention some of the other projects that were currently working on the machine learning and image processing in the air yes so in addition to identifying "
    },
    {
        "start": 2460.609,
        "text": "patients at the time of arias onset we're also working on identifying patients who are at high risk for developing areas and using the data base pretty much when they show up to the hospital to predict whether they're going to develop RDS in the future and this is an important clinical problem because we're actually quite interested in determining whether or not among these high-risk patients we can do things to prevent Aird yes from developing and this work is supported by a Midas challenge grant so the data that we're using here is patients January through March 2016 and we're pulling out for this project sixty variables from the chronic health record labs vitals medications and in this case we're able to generate almost nine hundred features from these variables and we're using a l2 logistic regression model to make predictions about whether or not a patient's going to develop erred yes in the future so these are our preliminary "
    },
    {
        "start": 2521.809,
        "text": "results for this work as you can see our baseline electronic health record model performed pretty well and it performs a lot better than lips which is this lung injury prevention score which is sort of like the state of the art or for the prediction of whether a patient's going to develop air - yes and the other things is exciting about this model is all this data is pulled automatically from the electronic health record so we can actually make multiple predictions over time again to make predictions about whether or not a patient's going to develop arias in the future because we may not have all the information immediately available when they come in these patients may be progress and then they become high-risk at some future time point and we're excited about this work and our plans are to think about some different formulations thinking about incorporating time varying parameters with long short-term memory network framework and then thinking about using unsupervised learning to learn "
    },
    {
        "start": 2583.67,
        "text": "better future representations that the data before making predictions the other thing that we're working on right now again something that Nick is leading is image processing to detect chessick rays that are consistent with air - yes and I didn't tell you much detail about this before but when we looked at why clinicians disagree about the diagnosis of air yes it's mainly driven by a disagreement in chest X and so the question is can we develop image processing machine learning algorithms to quantify lung injury and help identify chessick tree that might be consistent with their ideas so this is kind of a cool problem and there's really two main steps in this problem the first step is segmenting a chest x-ray to identify lung windows or the lung regions as the regions of interest and if you have a chest x-ray that's "
    },
    {
        "start": 2643.7,
        "text": "performed on an outpatient setting a tester tree like this one that's actually not that hard to do this to identify the lung regions and this this this work was or this identification was using the random walker algorithm with one of many algorithms at which you can use but the problem is in the ICU the chest x-rays are a lot harder because they often have lots of other stuff they might have EKG lead they might have other wires and some of these simpler algorithms don't necessarily work as well for identifying lung regions so the approach that we're using here is using an algorithm first it's called um total variations of noising what's happening there so the idea is that when you have a lot of detail in an image like very very high detail some of that detail may it might actually be noise and it might not be unnecessary and so by using the total variation de noising algorithm "
    },
    {
        "start": 2705.29,
        "text": "what we can do is potentially remove some of these like high detailed structures that potential are are just getting in the way but preserves from the main old underlying detail of the image and then do that first and then use one of these algorithms to detect lung borders in addition to segmentation we have to do some work in feature extraction where we're actually extracting features from the lung that identify these cloudy patterns that might be consistent with lung injury and so here this is it's called a Hessian based blur detection algorithm is something that harm dirkson collaborated and labs were developing and in using this algorithm some other algorithms we can identify sort of areas of these images that are abnormal and use these as features in a classification algorithm to a detect "
    },
    {
        "start": 2765.609,
        "text": "images consistent with their IDs so this is so preliminary work we hope in the future to be able to share exciting results on the output of this work so I'm gonna leave you at this point with just a few conclusions and then I think there'll be time for questions so I hope I showed you today that machine learning approaches can offer pretty exciting solutions the problem of a RDS diagnosis and then I hope I also sort of shared with you how uncertainty in the labels and potentially privileged information is common in healthcare settings and we it might be important to consider that when we're building machine learning applications in healthcare and then finally I just want to let you guys know that there's really an urgent need to improve accuracy of diagnosis in healthcare and this is a real problem at the beginning to become more and more appreciated and I really think that sort "
    },
    {
        "start": 2827.479,
        "text": "of broadly data science can really offer exciting solutions to this problem of improving diagnosis so lots to people to thank that made this work all possible of course kayvon is an outstanding mentor and collaborator and the people in Kay bonds lab including Nick and then Oscar who recently joined up and super helpful I have great mentorship and I work with cool people including Jenna Wiens and romaji na na ma through and some undergraduate students in electrical engineering and with that I'm happy to take any questions yes they are the ask so if you wrongly diagnosed them with that like what happens with the patient is it like because depending on that if it's not a big problem to misdiagnosed "
    },
    {
        "start": 2887.84,
        "text": "them you could adjust your slack function to wait more happily on correctly getting so get more like a little bit positive yeah so it depends on what we decide to do next there are certain interventions that we might give to patients with their - yes that's sort of low-risk we could probably get away with giving them to everyone in fact often we do simply because we're not going to diagnose in there yes and there's other interventions that are much higher risk like potentially literally paralyzing the patients that they cannot move so we completely control their breathing function or turning them over or doing some of these other sort of presumably at this point only experimental therapies that are higher risk so it's likely that we might have to wanted an algorithm to adjust the cost function depending on that situation because certainly there's there's lower cost to getting false "
    },
    {
        "start": 2949.19,
        "text": "positives in certain situation but there's much higher cost in other yes have you considered have you tried Ensemble Ania notice you use a lot of single models yes we have not tried ensemble in that seems like a reasonable idea because we were sort of wanting to develop these new new methodological approaches we're sort of focusing on a single model I think ultimately the idea is that we will try to incorporate privileged learning a living with uncertainty and all these classification algorithms and then potentially using as an ensemble so the the 24 features that you use for prediction yeah where did the clinicians see those features when they scored the cases it's hard to know probably not so those 24 features are "
    },
    {
        "start": 3012.52,
        "text": "like labs and vital signs I mean they could have gone to the chart and saw all those features but it's unlikely it probably just reviewed some of the notes they looked at the images and sort of got an overall the skulls the stock dish fish salt of the case yeah yeah yeah salt and then so so I'm not sure but I doubt they were influenced very heavily by them and then they didn't have perfect agreement with each other I didn't so whatever that CAPA is should be a ceiling on the so I don't actually know exactly how asked at a Kappa and an area under the curve over an ROC should equate but there should be some ceiling I mean your your your your machine learning one would think wouldn't be able to do better than any individual scorer well I mean that's something that we're interested in so so when we use three clinicians so individual clinicians Capitol point five you can "
    },
    {
        "start": 3073.42,
        "text": "actually determine what cap it is for group of clinicians and for our work it's about I think point eight point eight five point ish so right by combining a group of clinicians we have a high reliability but your point is well-taken so there's other other ways that we could sort of evaluate we could see if our model is equivalent to any individual clinicians among the group or our model is better than any individual clinician this issue is coming up now because you know we're using groups of imperfect clinicians to build a gold standard and building for building algorithms so there are ways of looking at this and you're right you know but the hope is that our model can sort of replicate the group understanding of multiple clinicians and then perhaps be better than any individual yes hi thanks for coming ok you know I I want "
    },
    {
        "start": 3135.66,
        "text": "to follow on with this a little bit my question was really related I think and it has to do with you know various communities and how they trust this and whether you know you see are we gonna have to have national standards I mean you know talk about the FDA and these would be advices but you know it's like what would a clinician you know we have some examples you know they're not really all that good I mean you know from the point of view of you know anatomic pathology you know machines can score slides you know maybe better than pathologist but the unit atomic pathologist buy it back radiologists you know I mean little nodules I mean what do we have to do to convince the physician to have trust in this yeah so there's a few things I think they just have to get used to it and no I mean seriously and so like I think frankly that you know medical students who are training now as compared to current practicing "
    },
    {
        "start": 3197.46,
        "text": "physicians are going to generally be more comfortable with this idea I think over time as they use it in practice and they recognize model works really well they're going to become more comfortable with it I think sort of a key thing that you know people are working about working on right now is sort of comprehension or understandability of models that's a key piece and how to display that to position so they sort of understand how the model is getting to a particular decision it's sort of important because you know then the the clinician can can know if in the given situation that he's in right now can is the model appropriate can it be applied to that situation and then I think it's sort of like like if it makes questions life easier like that's really going to be a thing like like ultimately if a pathologist doesn't have to look at every single cell on a slide if they only have to look at you know 1/3 of the cells because an algorithm screamed the "
    },
    {
        "start": 3258.78,
        "text": "first 2/3 you know I think like that's better and I think clinicians like it or for example on you know if you're reading cardiac on the rise in the algorithm can calculate the ejection fraction for you so you don't have to like take the time to do that I mean I think I think the clinician will like that but this is sort of an inherit problem and I don't think like we're gonna we're going to be successful with all we give them as black boxes from my experience as a clinician I don't and I don't and I think you're right i mean i think that the fda is gonna have to demand that there's some sort of way to look inside the black box so we can understand how the models generating a decision and brian i also wonder how much this is why it's a trust you do this at the university of michigan rather than somewhere else right part of what you're needing to do right is you drive the money part of why the pathologist are hesitant to not look at the slides is right now they get paid 100 bucks long i "
    },
    {
        "start": 3319.8,
        "text": "get that it turns out once we stop paying them to do a cell count they will really the sudden with how the culture counter could just count those cells just fine right and part of what Mike's doing that's so interesting is dealing in places where the human beings who do this are too slow yeah to do this right Mike believes this stuff is 3:00 in the morning radiologists aren't there at 3:00 in the morning so yeah I think yeah I think we're making our hope is to make lives easier and actually focus the clinicians so they spend their effort on the things they do best right like presumably interacting with patients helping patients make decisions and leave some of these sort of more technical stuff well I think one thing that we should think about and you know when dr. Tsao was here with the Institute of Medicine and gave his talk and we've had several talks at the Health System level but it's a national problem he is positioned burnout yeah you know and this could "
    },
    {
        "start": 3382.14,
        "text": "really help you know especially into the intensive care unit settings and things like that we're in in addition to the burnout we're seeing one one none piece of data that was fascinating was 27 percent of the positions in the ICU they're suffering from PTSD and that's I was told that that was really an underestimate and so you know in these high-stress situations you know having these kinds of tools that we could rely on you know could really help with this you know I think we should be thinking about this because physician burnout is really a national problem I also think like pretty soon we're gonna have to think about medical curriculum changes so that we actually train physicians to think about these things and learn how to use these things because know the sort of the curriculum that I was exposed to as a medical student on sensitivity and specificity and all these other diagnostic test evaluation three or so "
    },
    {
        "start": 3442.89,
        "text": "is this a new era so physician training is going to change [Applause] "
    }
]