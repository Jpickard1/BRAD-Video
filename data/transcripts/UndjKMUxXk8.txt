last but definitely not least i'm very happy to introduce dr judy gachoya if you want to go ahead and share your slides now i'm dr jakoy ken she is an assistant professor at emory university school of medicine in georgia and the department of radiology and imaging sciences she's a multi-disciplinary researcher trained as both an informatician and a clinically active radiologist she also works with nih on the open data science platform or odsp component of the dsi africa initiative to harness data science for health in africa her career focus is on validating machine learning models for health in real clinical settings exploring explainability fairness and a specific focus on how algorithms fail she's worked on the curation of data sets for the society for imaging informatics in the medicine hackathon and machine learning committee and today she'll be talking on bias and medical ai imaging a data centric approach so thank you very much dr kachoy for joining us today and i'll hand it over to you now thanks thank you thank you so much for the invitation and also i mean very good talks i was busy taking notes too uh so this is a great symposium i can't wait to catch up with some of the parts that i made so i'm just going uh straight into it uh these are my disclosures nothing that's going to affect the quality of this talk and you know i sort of decided to narrow down on some of the work that we've looked at because the data in in medical imaging is really there are images and we have reports and for a 20-minute talk i think that may be a little more difficult to try and achieve and so i'm going to be talking a lot about uh reading race paper how ai recognizes patient's racial identity in medical images and i think that's going to be relevant even from the previous speakers about some of this phenomenon we're seeing from either lung function testing which we also correlate with a lot of imaging and also uh as we start to even look at uh you know social deprivation index and other uh population metrics that tell us where these patients come from and so i'm not going to spend too much time on the common language because it was actually described just even in the last talk and what we we refer to when we're reporting you know race uh ethnicity and specifically for this talk we defined it as just the social and legal construct how you see yourself and how other people see you and usually i find myself that i have to justify like what's in a name or label and i'm an interventional radiologist so unfortunately we do get involved with moms when they're delivering it's always not a very exciting time for some people because postpartum hemorrhage is actually a leading killer of black women and if you look at this data from for severe maternal mobility from the new york area it doesn't matter on your federal poverty level it turns out that uh even if you reach us just by being black and we can they've corrected it for education level all these other factors just by being black your chances of death in hospital when you're delivering are very very high and so we know that uh the other things um around like for example pain is another example where we know there's quite a lot of disparities uh i'm i do procedures and even anecdotally you may realize that sometimes your black patients may not get the correct like adequate pain control because there's the perception that black people don't feel as much pain and so quite a lot of work to be done there and then on a bigger scale you know not just in radiology we see bias in ophthalmology and rheumatology and even in sort of just the clinical medicine for example this work done by ziad which is uh you know uh they're setting an algorithm that is being used to refer who are the most sick patients and you can imagine um you know if let's say you've said these are the patients that have identified from a pulmonary function test and these are the ones i'm going to send more uh resources for and you're already biased in your initial decision or who's represented in that population then you can easily pop it but you know propagate the bias and i encourage you if you've not read this paper to spend time looking at it i think it was fantastic work and here they said you know we don't even include the rest of the patient but when when they look at the results they see that black patients are referred for additional care when they're much more sicker and in later stages of disease and the reason is because they were using health costs and yet if you had sort of understood that the health economics you'd know that the blood patients usually income or emergency room cost more than primary care costs so the metric that you use even uh to tune or what you're going to target for your model is super important in terms of thinking about outcomes for this patient and so why did we do this study and and this is something that i got asked a lot you know why would you even ever look uh that the images have race information you know and it turns out it was really a happy accident we were looking at a paper just as a special issue and we're looking at this work that had been done before to exclusion which looks at 14 labels of chest x-ray and tries to figure out using true positive rate which is a metric for bias measurements in computer vision for um when you're looking at images and we're just looking at how it would vary then really um you know the rates the ranges here usually are from you know minus 0.8 to a 1.2 is a you know it's thought to be the band that's appropriate but when you start to go lower for example here cardiomegaly then the white patients are less favored you know for this subgroup and we know we know you know this group had already done this work and you know ultimately they say that you know if you're hispanic or black if you're on medicaid which is the insurance that uh you know people who have low income uh or are offered then these models do not seem to work very well for you and so this group also expanded their work and looking at just normal imaging you know where there's no finding a normal x-ray and starting to see you know how does it perform across groups and they bring this new intersection to of around for example i'm a black woman and so if the model is not working well for women and it's also not working well for blacks you have this double burden of you know sort of unfairness from from a model and they still form the same you know uh you know hispanics blacks medicaid you know younger patients but you have to be careful about the data set for chest x-rays because most patients when you're less than 18 years you get imaged in a pediatric hospital and we know that the public data sets do not really include that cohort so you have to be very cautious about how you interpret that but they again showed that this intersection you know again like black female black you know hispanic and you know black male maybe even in a different category that shows that there's this rate of in under diagnosis and so here in emory uh which i think the people in michigan would also appreciate we have a big black population and um you know we said well we could repeat this experiment and we were very sure that once you just show the model that a more diverse data set this you know these problems are going to be resolved i'm just showing here the sort of three big public data sets you're either using the mimic data set from that israel the checks expert data set from stanford or the nih data set but the nih data set that lacks the risk information and so uh you know when when we came here and we we pulled our own data you can see we almost have an equal population of blacks and whites and we said we we found the solution we just need to show these models more diverse data and voila it turns out that we were wrong that we we observed this if i can convince you that this is a higher peak compared to this one we found out that yeah maybe for cardiomegaly you'd see that the amplitude of the peak went down but the pattern persisted and so this was really baffling and when we started investigating why we found out that it's because this model start to learn these underlying features even on medical imaging it's not just because which is really extremely concerning as we start to even build these models that merge clinical information and medical images for example in the case of breast cancer where you can look at the mammogram but you can also look at the demographics and other clinical information and so we then refined our study to look at three things to first really quantify was this real were we making a mistake probably we are but um you know and how did it work across multiple imaging modalities and what could be causing this uh finding and what were the confounding and atomic and phenotypic population features and so uh we collected more data sets we had the public ones we were able to use our friends to get some rest labels because those are overall missing so we have very varied amount of risk information between from six percent blacks to forty four percent blacks uh you can see these are just like three data sets there's a ct chess data set a digital hand atlas data set a breast imaging uh mammographic data set and a cervical spine data set and then we set out you know let's put in all the models that we could think about and let's train these uh models and see uh what we find and it turns out that the the models perform very well so i'm talking about more of deep learning models resnets that type of architectures dense net and efficient and with aucs of like you know above 94 percent and this was very baffling and when without even training the model again when we took it to a new institution we've had several institutions try this is that the model performed well this is what when you're building algorithms you want in your life that you train a model from emory send it to michigan and it works great and that's like amazing and so this this was even more concerning so then we said you know what maybe so it's not that the the la the the model itself is one can't say that it's maybe resnet only and uh and you know maybe we said well maybe it's something about the chest how it's interacting with with the images so we looked at you know the hand radiographs the mammograms and the cervical spine and the mammograms true they are probably the least performing here in the 78 percent but we see quite a very high performance now we do have six radiologists in our group and none of us could perform beyond random which would be around 50 percent uh in terms of identifying if this is a black uh white you know asian uh patient and so we found that the performance was persistent across all groups across multiple uh architectures and you know saliency maps are really commonly used in medicine and radiology to tell us where the model is looking this way haphazard we spend a lot of time trying to audit them when they're normal disease abnormal disease and even uh ended up working on this gibbs planation which are looking at the latent encoding space and trying to see when you stack all the images what the difference is and you know maybe getting a sense that it could be something about the shoulders um but still we know that we haven't unfortunately really found out what these models are looking at in terms of this imaging and being able to tell you where the race of the patient is the same thing with the breast mammograms and the cervical imaging now our initial hypothesis was that you know that it may be something that black skin interacts with the with the x-ray and so what if we brought non-radiation type of imaging for example um you know mri or or ultrasound and so this group uh worked on uh doing these experiments using two brain mri studies with as good performance above 95 percent uh for uh you know 0.95 ucs for uh predicting race in you know you can see that this is a uk data bank uh population so that theory went out of the room and so then we started to figure out well maybe these models are looking at um maybe the black patients are bigger bodied or we know that breast density changes across races and sometimes maybe the black patients just have you know cardiac disease and this is what they're looking at and unfortunately we were unable to ever find out that any correlates for bmi or breast density or age and even when we combine them all together we still have um you know it's not slightly more than random performance around 0.64 but it's definitely doesn't account for the 0.9 force that we see and when we look at the disease distribution and just say tell me there is just based on the label it's still in the 0.6 percent 0.61 and we start to start to pluck out some features now the bone density experiments do need a little more work uh just to do the image manipulation but that didn't really make a difference and we also didn't notice any differences for example if could you maybe it predicts better for 40 years versus 50 years versus 60 years or impact of patient sex and so uh we then started to change the images add noise make them blurry and in this case this is our test set where we were sort of um blocking where the you know the model is looking at the most and for this we found a drop when we added noisy and blood images but like i mentioned the radiology's performance is around 0.5 you know and so it still doesn't explain um you know the high performance but it's better than the radiologist and so this means that there is some resignal uh persistent in these images and when we block where the model was looking at the most uh we still see a slight drop in performance but we see that even in other parts of the image that they may not be contributing the most to the model prediction that they still have um high aucs we started segmenting the lungs uh into the in you know into the soft tissues and the lung tissues and seeing how would that perform and we see lower performance when you just look at the lungs so it could be something still around the composition maybe not just how we look but what makes up the soft tissues uh because we see much more performance but still the performance in the lung is much better than random and then we started to do this experiment we would break the chest x-ray into components and then block each component or just look at that component for prediction and when we look at that component for prediction we find 0.91 as the percentage so for example when you use c using one patch but when we block and then use the prediction we see the most significant drop when we look at this area above the stomach which has a performance of 0.6 and now i mentioned no difference between age or gender this is probably the most concerning now now when you have an algorithm let's say a dermatology algorithm and it's not performing well then you you could show me all your cases and they could say you know those those cases seem to be all on dark colored skin but we applied these uh low and high pass filters that allow us to look at the you know the grass things and the fine contours uh you know at every end of the spectrum so this is a chest x-ray after applying a high-pass filter you know 100 centered at a center of 100 and you can see this is barely perceptible as a chest x-ray but in this x-ray you still have a performance of you know close to 0.85 to tell you what the race of the patient is i mean what are the models looking at when they look at something like this that we just called grayscale and when we change their resolution we start to come down you know for example this pixelated image there that you couldn't even really tell was an image uh at four by four you still have the performance of better than radiologists it's true it's not 0.9 but you can see even up to 40 by 40 this is when you are still getting at this type of performance we try to make sure that there's some sanity check what if we shuffle the labels and we start to find now this more random expected uh performance and we look at the equipment maybe black patients and that could happen for example if you're a big body you may be scanned only on one scanner maybe the clinic is close to one scanner you know those things that that you as a data scientist have to think about as you walk around um could be affecting our prediction but we don't really see much of a difference even when we filter on one image one scanner uh uh at that level and we find quite a difference in where patients are scanned uh for example this is a mammogram study and you know we also look at uh this is two mammogram studies that are acquired across different equipment and we don't see a difference in the performance and again this is just sort of like the location of the hospital this is a more older population this is a more academic institution so seeing a mix of patients and this is more of a cancer population and we still see a good performance sustained across and so again what about the pixels right these images and we know that we've been changing the pixels in radiology you can use uh this uh organs to tell you whether this is normal air fat soft tissue these are some pictures examples uh usually it looks much prettier on ct scans and we don't find that as a contribution although we've subsequently uh analyzed this uh in a in a different way that shows us uh that it still is not explaining why uh these models are able to pick up the the patient's race then this group in stanford had done something interesting on their data set where they had curated photos right so you have uh the the digital image which is what we were working with but then they started to do this experiment as a way to show for example like home in kenya where i come from uh what if you took a picture of the image does the quality change and so they had uh processed these images on iphone and nokia around 6000 images were available to us and we were going to sort of predict so this is the same image you can see this is after cell phone manipulation this is the original image and does that image still have race information yes even if it's a little lower so this is even again was very against our melanin hypotheses but also like how how do you still take persist this information across a picture of a picture and so we found that we have superhuman performance for risk detection on imaging multiple modalities beyond chest x-rays and this mechanism unfortunately this is close to 18 months of doing this work cannot we cannot figure out why and what we would think would be obvious clinical confounders and anatomical explanations were not helpful for our case and we still need to explore this phenomenon especially as we start to embed it in the context and i'll show you some of the things that are coming down the pipeline and also understand what this means for policy and fda regulation and so this is some work uh done again by this emma timber emma and azad who start to look at knee radiographs and osteoarthritis i mentioned that pain is a big disparity uh for you know especially for low income earners low education and blacks and you know they developed this code that is usually used to grade your steel arthritis was trained hundreds i mean many many years ago uh predominantly in a white population it's called the kelgrain lawrence core and they train an algorithm to generate this new score and find that the algorithmic um severity they also don't just use the imaging they use the patient's reported pain because you know that there's actually a discrepancy in that and they find that when they use this algorithmic prediction score which considers the pain uh factors from the patient that they can mitigate and reduce the gap um you know by almost five times uh based on race two times one income and three point six times on education and you know it's not just doing this work it's to see why it matters and one of them is eligibility for surgery and we find uh that they when they use this algorithmic prediction score it you know improves the amount of people who become um eligible for surgery and so uh but when we start to unpack this and magic to our work we were collaborating with them and creating an msk data set here at emory is that we find that the modern actually lands red so now we don't know is it because that the model narrows the gap between blacks uh and other races because it's planning the race better and it's not bad if it's using it well but we know that this ai model models are black boxes and if they have confounders that we cannot really determine then we will be when i take it back to my home village and you know we have a heterogeneous variation of the rest and they're not going to work as well and so this is an open area of research and we have tried to do the same again with this mirai model that also again just looks at the breast and tells you you know in five years this is the risk of breast cancer again performing much better than the terra cruza model which is predominantly very poorly performing for blacks one my last point here is to also remind that this this coding that we still see for race even if it buffers us and we need to figure out why it matters we don't need to remove it we just need to understand why it matters is this idea of um that missing codes missing patient codes like icd-9 or icd-10 codes is actually an air health equity issue and we know this is some work that we had done uh looking at value-based care looking at hierarchical codes we know that they are overall complete incomplete but it turns out that you can actually train your ai and just looking at the chest x-ray it's going to generate a problem list for you so this is looking at chest you can see these are more much better you know ablation studies to show where the model is looking at this is congestive heart failure he had you know atrial fibrillation you can see with obesity it's looking more towards also the soft tissues and you can see vascular disease it's looking at the aortic area and diabetes also in the soft tissues and around the aortic area and it turns out when we look at these patients who do not speak english and if you are a physician and you know you have to use those uh cats that you have to translate um someone you know swahili to you're not going to spend all the time trying to figure out every single problem this patient has you want to focus on why they're in the hospital and get it done and so we can start to even see this potential of completeness of the medical record as we move along and so if you're a geek and you're interested in trying out some of these models we've released our code we've worked a lot with the community uh to make sure the data sets have some of the features that would be important for you and i think that we need to work more on explanation mitigation and we are participating in a federated learning to improve the diversity of the labels that we have this was done by a tremendous tremendous amazing group of people across three continents six universities doctors computer scientists students attend you know uh professors and it's just been amazing and i encourage you to review how we do it using this village mentoring and hive learning experience thank you so much for the opportunity and great symposium thank you very much dr kachoy that was a really interesting talk um can you hear me can you hear me now you can hear me okay um are there any questions i'll look in chat um uh someone asked where will this recording be posted so we need to obtain permission from the the speakers and um we'll determine that yeah we can email all of the registered participants next week [Music] okay any questions for dr kachoy welcome to unmute yourselves right in chat or uh we also have several participants here in the room um hi so uh my question was regarding the methodology um so when trying to figure out what are the confounding variables that cause this kind of performance i was just thinking what if uh what if one use like saliency maps that allow want to understand where the model is actually looking and then maybe compare those values with the confounding variables would that be like a good approach uh no president maps we put them in because all the reviewers on your paper will not accept your paper without looking at them in imaging but they are terrible there's quite a lot of work that has shown that if you even just flip the labels that the selling team ups i i mean even if you change the order that you feed in your little do the testing we need better explanatory tools for medical imaging unfortunately um that's a gap but we still use them and you know and sometimes they'll make sense but overall they're not very useful and uh we spent quite a bit of time on it and other people have but we we don't know the why and and you know quite uh a lot of you have even been involved in debates like does it matter if it works well for a certain minority population and it doesn't but you need to know that that this is working because it's working not because it's relying on a confounder but i think salient maps would be not how i think about it the first time okay yeah thank you for answering that question and i really enjoyed your talk thank you thank you any other questions okay well um i guess let's thank all of our speakers um very much um for all of your wonderful talks this is a really great symposium um but it's not quite over yet um so before we we move on um well first of all i want to remind everyone that following lunch at one o'clock est so um in a little less than an hour um we'll be coming back here for a panel discussion on you know what we can do moving forward and how data science can be incorporated more and racial health disparities research so please join us again after lunch for this discussion but right now before lunch we have some student awards to announce