[
    {
        "start": 15.26,
        "text": "your view okay everybody it's my pleasure to introduce today's speaker Marcie is not here so it's falling into my honor to introduce Chantal with assistant professor in biostatistics in school Public Health his background include bachelor's degree in biology from university's master's in statistics PhD in neurobiology from Duke University he did PhD kako with Matthew Stephens Jones research is in statistical genetics large-scale data sets including how to analyze structure of the population making proper adjustments in extracting data so he's an expert in methodological development is in collaborating broadly in your medical school today he will talk with us about differential expression and estimating gene expression heritability using this "
    },
    {
        "start": 76.92,
        "text": "data set tell us Macau I don't know what that means welcome Shawn okay thank you very much Jim for the very nice introduction and so today I'm going to talk about software to that away recently available it's called a macaw that can be used to analyze differentially expression that perform differential expression analysis and also to estimate in gene expression heritability so as we all know that transcriptome and on a expressions they are really placed a kilo in the biological system so it and it's RNA is that and transcription coming from this DNA and it's translation that's in the protein so anger is essentially plays a central role in the system here and it's variation can be linked to many disease causes and the disease propagation and so therefore many studies have nowadays performing trying to perform those transcriptome analysis by collecting and you spreche measurements are every single gene in the biological system and when "
    },
    {
        "start": 138.209,
        "text": "they perform this type of expression analysis when they collect this data they usually generate a couple of interesting important questions to ask and they're one of those key questions they usually are interested in ask and in performing is this differential expression analysis and so in the differential expression analysis or they people usually do is to look at one gene at a time and it's try to ask whether they can look at the association between this gene and the predictors of interest and those predictors of interest can be quite a journal also for example could it be disease status so you can crack a case can do data and you try to figure out which gene are elevated or are reduced in the cases where this control it could it be risk factors or all your elemental covariance so those are all can consider as a vector and try to look at association between expression and those vectors and they're also more recent today people are also interested in looking at the association between gene expression and genotype and also I putting this into the differential expression category this is more "
    },
    {
        "start": 200.07,
        "text": "referred to commonly referred to as say an acute your mapping or expression quantitative trait or lower mapping and so by looking at the Association pattern between the genes and the bunch of those predictors or genotypes and then we can understand what a gene expression levels are changed and during say between cases and controls and by understanding that this provides as sensual first step towards understanding the molecular basis of say disease suspected ability and so this is one key important tasks that's a commonly performed whenever you correct the expression data and the second type of task perhaps slightly less common and perform is to estimate the gene expression heritability so the expression heritability is defined as the proportion of the gene expression variation that's due to genetics so what you do is to you if you can can have that you look at one gene at a time and then best young individual relatedness all their genetic similarity "
    },
    {
        "start": 260.91,
        "text": "due to the genotypes they can trust to estimate how much proportion of this expression variable are due to genetics versus due to environmental factors and a perpetually this gene expression had abilities then you can gain a better understanding of the genetic basis of all those gene expression evaluations so for example in this particular study they can estimate the gene expression heritability for every single gene and you can Prada this heritability values and you can see that in this bladder tissue the gene expression heritability has a run has the media of around 20% and although there are many genes have a high heritability and a human some are about 50% or even 7080 percent and by measuring the gene expression heritability now you can get a better understanding of how those genes are influenced by those genetic effectors was this environment effectors and so this is sort of true of those quite commonly applied analysis and to sand the common ghosts wants to identify "
    },
    {
        "start": 320.97,
        "text": "differentially expressed genes as is try to estimate the gene expression heritability and and nowadays certainly always the development of those sequencing technologies when people collect the gene expression data they usually perform just on a sequence II so the onion sequencing experiment array is conceptually pretty straightforward so you initially have your mRNA and then you will transcribe it into a cDNA you prepare library with adapters and then you venturi sequence other reads and once after you sequence all the reads you try to map those tweets back to the G and so intuitively if a gene is highly expressed then you tend to measure a lot more reads for the energy and so for this gene you will be able to see more read accounts mapper to the particular gene transcript and so that's a general procedure of anga sequency and it it becomes more and more common in modern studies and in this process you can see that on a sequencing has actually two very important features so the first "
    },
    {
        "start": 383.1,
        "text": "important feature is very clear is that RNA sequencing data are complex and that's because you sequences the data and you macro reads back and now you get a essentially an integer value to measure the gene expression level and therefore has the demanda undeveloped into the message to model this type will come to pass the data and there's an previously experimental race that is have shown and say from meriones paper they shown that the gene expression variation across technical replicates and can be accurately described by a Poisson distribution so essentially you collect your sample you use the same sample you / - Mouse processor differently measures a gene expression value you can see that for the same genes as count will roughly behave like a Poisson distribution centered along its gene expression level and however now if we are more interested in looking at more complex designs where say you are going to crack and mountable biological samples instead of technical replicates say if we "
    },
    {
        "start": 444.51,
        "text": "perform a case case can choose that you are going to crackers 3 samples for the cases to resemble some other controls and then all those three cases and so you can choose their biological replicates and if you look at and just count the expression data across those biological replicates then you will find that a Poisson distribution is no longer adequate because they're extra variations across those biological replicates and this is so-called the over dispersion so variation cannot be accounted for by the standard opossum regression model and so as a result of that all the modern tools and relying on this negative binomial model try to model this on a sequence income data so commonly use a package for those people who are familiar with expression analysis this a HR and a t6 they're all essentially right and they're very similar negative binomial models so compared with the Poisson model negative binomial model will have two different parameters one model than mean so as a models of variation so it's able to account for the extra variability that are you seeing that on the sequencing "
    },
    {
        "start": 505.35,
        "text": "data and and certainly and also there are strong and training developing comdata they're still an uncountable methods are used and so for example in large relatively large scale on a sequencing studies for example if you want to map eqtl so you typically just do a compound normalization to energy inspection levels and then perform Association tests and so those ones are also you however by comparing those kana best methods versus non-count best methods people recently know or realize that is the kind of person that sits I usually gives you more accurate and that's expected because the Sanger sequencing data I indeed a common based and so that's the first feature so that Sanger sequencing data counter based zelos is trend in developing cannabis methods to do with so that it can get more power more accuracy in measuring gene expression performing gene expression analysis now the second problem that's "
    },
    {
        "start": 569.009,
        "text": "just a start to realize in this field is that many samples in the Sanger sequencing studies are usually and non independent so for example in Somalia sequencing studies this are family based so they recruit family members and family members genetically related to each other so you would expect that gene expression levels it also would it be correlated with each other and the some of the sequencing studies maybe they were contain samples that they display population stratification so one example we encountered before with this Finland a sequencing study with a cracker samples from three townships in Finland and the street on ship samples there are all unrelated so they are not genetically related to each other however there because they are located in three different townships they tend to have different the environmental inference and also they are more or less genetically similar right all those are not family members within each Township and so that will causes population stratification and at the end of the day "
    },
    {
        "start": 631.049,
        "text": "you will find that for the two people in the same townships they tend to have similar gene expression levels compared with and people from two different townships and so that's the population stratification and so those are a relatively small proportion so only a small proportion studies a family based small proportion of studies will be or have those population stratification but almost all on a sequencing studies are influenced by those batch effects so the batch effects notion has been well recognized in earlier microarray studies and it's being recent Angus sequencing studies and have also identified that a batch will you in for instance the expression level quite a strongly so for example if you for the two samples in the same batch you will see that the gene expression levels measuring that a batch will tend to be similar more similar compared with two samples that are carried out different batches and certainly if you there's only batch effects and if we know the batch level this is relatively easy to control for but we also know "
    },
    {
        "start": 691.889,
        "text": "that there are many other factors those are hidden confounding factors that appear on the batch effects so in the winter some samples correctly in the summer then you will expect their sequences correctly in the winter they are more similar to each other compared with between winter and summer right so those are many confounding factors and many of them are hidden as though those are difficult just for and those confounding factors will also cause similarity between gene expression measurements and so because of all those reasons and thus imposing on a sequencing are often non independent and so the expression levels are often were systematically covary with the sample structure and so therefore you want to direct message somehow to account for the sample now independence and structure and a pharoah to account of others and could lead to spurious associations are reduced power and this has been demonstrated over and "
    },
    {
        "start": 752.759,
        "text": "over again in genome-wide Association studies but less so among a sequencing studies and so this are the two very important features for next week so first is its count data secondly they are asked and Fonda independence in the AMA sequencing studies so therefore one who controls and however recent previous approaches they essentially are only able to account for one feature of the Zionistic data so for example some of those approaches there were normalize accounts to continuous values and after normalization now they can use a linear mixed model to account for sample nine independence that you choose a family structure population stratification or human company factors but this approach clearly because of this normalization they ignore the count based nature of sequencing study so it may not have the idea and the other hand most approaches there were simple uses negative binomial model to model this raw counts and over dispersion caused by biological sample "
    },
    {
        "start": 812.95,
        "text": "replicates and but this type approach is because they're using a simple negative binomial model they fail to account for example knowing dependence and so therefore it may be susceptible to higher force discoveries and so those commonly used messes at D CK nature and so motivated by this lack of messes that can account for both this two very important features in Ganga sequencing we decide to develop a pasta mix the effects model so the main idea is that were going to model the law comes directly and welcome to a counter for the sample nunya so what we do here is we are going to look at one gene at a time and like all the other methods and then for this gene for IC individual would have our reader come this Y sub I and will model this Y sub I as the Poisson distribution and that's depends on this total read depth sets and sub I so you can expect that if we individual have higher reader depth then tend to have a high rate account so this and sub I essentially try to normalize this and "
    },
    {
        "start": 874.78,
        "text": "then another opossum normalized read factor this lambda I and so by using the Poisson distribution will model the continuity of this data on the other hand for this lock as a possum a parameter will moderate at the log scale and we modeled as a linear combination of these four different terms so the first term under standard coverage that you want to control for say age and sex and then the second term is one predictable interest that you really care so say case versus control binary indicator or could it be a genotyping via in gene mapping performing in couch or maybe in studies and so this two terms are generally so any packages you will be able to find those two different terms however very different for any other packages now we have two other different terms one is so-called the genetic effects the other is so-called environmental effects and so the GI is one value for ice individually is one value for the eyes individually as well and we or the juice together and a group or the "
    },
    {
        "start": 936.48,
        "text": "east together and we assume that behave have a different behave so the you will assume that they are independently normally distributed alike any one would assume in standard linear regression model so the yeah I hear you know way it's try to model the variations that are independent and this could it be just environmental noise so the new capitalist independent environmental noise and on the other hand now we're also grouping G together but now a models as G as correct so we use the multivariate normal distribution to model their coloration and Noah's particular K matrix this covariance matrix so the K matrix cares and by n with any of the sample size so for example if we have Family Studies and educate route to use is the occasional matrix if you have a data with population stratification you can use genotypes and linear function of genotypes to construct a matrix button and if we have hidden confounding factors you can also using all the expression data to infer this K by K "
    },
    {
        "start": 996.93,
        "text": "some host and polarization in this matrix so intuitively this image isn't important here so if I energies samples or individuals they are kind of related to each other whether due to confounding factors so genetically then this K idea will be tend to be a positive and large value and because k IJ a positive in and large the inner will introduce correlation between gr and Gigi's term and therefore by controlling this G terms you will be able to remove this company vectors so therefore by modeling this G term now we are able to control for sample now independence so all the beta coefficients you eventually estimate will be free of those sample independence correlations so that's essentially the main idea and the certainly post GN area were modelled over dispersion that extra variance that cannot be accounted for by the Poisson distribution and so you know we're also models over this version that's very similar to negative binomial models you generally see so the key feature now is that it will use the possum model so we "
    },
    {
        "start": 1058.64,
        "text": "can account for the condemnation we can are able to model the over dispersion and we are also able to model the individual correlation expressions morality do too and those hidden confounders by incorporating this so that's the main ideas they're using the passo makes a fast model to control and to account for there's two important features on a sequencing data and so that's the modern set up now the model may sounds like straightforward a model by transology inference of this model is extremely difficult and the reason for that is this is the post mixed effects model and it belongs to this generalized linear mixed model family or the Geo mm family and they are unfortunately is the N dimensional integration inside this model and solving an n-dimensional integration is computation a very challenge and so there are many different methods a variable for example the commonly known methods at Costco might quadrature order penalize the "
    },
    {
        "start": 1119.42,
        "text": "courses like ricotta PQ R approaches suppose of those two works reasonably well if you have a small number of rendering effectively integrate integrals of low dimension however here our particular model has an N dimensional in the closer in the world color with a sample size so those methods are not a particular scale up with the same with the sample size so faith in this problem we decide to do and develop another alternate approach is through this MCMC based basing methods so it's a McCullar McCullar now this type of message that attractive because they can naturally account for uncertainty in the parameter estimates and and and also can achieve arbitrary accuracy of the chains allowed to run enough time but there are also problems with this MGM's item mess it's so one for one thing is that it's computationally heavy so it's a cubic so every iteration is cubic in the sample size so that it will make the algorithm very slow to rob and the second problem is also this is a basin type approach "
    },
    {
        "start": 1182.12,
        "text": "right so if you rhyme zmz at the end of the day you will carry or posterior summary but you cannot generate get actuators and by origins are generally in favor of p-values to summarize their results and so therefore we decide to develop our alternative MCMC approach that can solve these two problems and so what we did it was to divide this new MCMC algorithm which we call it to them a coy algorithm stands for the mixed model Association for Candida by data element ation so the main idea is we consider some light and effectors and by introducing this threatened effectors it will make the MZ m\u00e9xico strata forward and in fact now we are able to reduce a computational cost from cubic to only quadratic to make it intense faster and also in addition to that we're also we're running the MCMC although we're working on the posterior how interest is not on the posterior but a rather the likelihood so with best and rely on ascetics we essentially get our posterior samples but we're combined "
    },
    {
        "start": 1244.35,
        "text": "with the price and post you know to get extract our like record now with extract the like clicker and now you can perform any standard frequentist test say like little ratio test or water test so eventually you will end up with a p-value so this is how we can solve this why is computational issue as the other is at the end of the day we are going to provide you sheer aerospace down there like instead of the posterior so this has a two normal parts of this new algorithm and so if we look at this a computation time right you can see that computation time is really fast so the standard approach is MC MC g or MN it's our package and you can record a computation time on the y-axis and you can vary your sample size on the x axis you can see that this is a cubic scale so you're essentially with your sample size increase meant it's really goes up quicker is this standard approach now compared with the standard approach our approach our McCaw software is much much faster so even with one sudden "
    },
    {
        "start": 1305.04,
        "text": "individuals it's much faster compared with the older type approach and on the right hand side is just showing the computation time on the logs they also you can really the more detail so for example 1,000 sample size it takes going to be take about five hundred five thousand maybe 1000 and hours to run on the standard approach while it only takes about 10 hours to the our approach so it's going to be a hundred to one thousand times faster so it's sighs its n times faster than the old approaches and the second enormity of our parties we eventually computed p-value and let's realize an asymptotic so we get our posterior we know our price with essentially subtract this posterior over prior from this post you know to get it like so that's really bad sounded like a herd you go through the posterior divided by the price so we take this relationship eventually we get our beta hat the ML estimate instead of the posterior head and also the standard "
    },
    {
        "start": 1366.19,
        "text": "error of beta hat instead of posterior virus so with the beta hat and a standard error now we can compute and the p-values are extremely stable also we are using MCMC type of approaches so we're showing here is the minus log 10 p-value from one rung of the MCMC versus a minus log 10 p value from another one of MCMC so you can see that it's really almost anti ignore and you can change all those heritability values h square where those inside and you can see that this where those are all very consistent across different round so that our previous are extremely stable and so now we have our method model and also the algorithm now we can combine them together and though we can ask a question whether it really works well or not so to do that we'll perform the first bunch of simulations so one type of simulations and now simulation just to check the type one error data set up simulations power simulations so therefore you can compare with other methods and their power to "
    },
    {
        "start": 1428.26,
        "text": "detect the differentially expressed genes and so in this type of simulation what we did there was to simulate tens out of the genes that are not differentially expressed so we call those are now genes so under the now your PC virus should be well behaved hopefully should it be under the uniform distribution and then and because and then we use the kinship matches to introduce some sample now independence and we use the real data together this kinship matrix so that's a paparangi sequencing data and then we also estimated as over dispersion parameter Sigma Square from this buffoon data so therefore our simulations will be as realistic as possible that's close to real data we see and in our model recall that we have a chi-square parameter and so that's essentially habitability parameter and we also choose a bunch of realistic genetic heritability models say 30% in this baboon data that's roughly the media value in this data and "
    },
    {
        "start": 1488.55,
        "text": "we also varied as a bunch of parameters and then we'll compare several different methods so on one hand would compare with the linear model and a linear mix effects model that are implemented in gemma so in this case since now we have a count data so we have to normalize them take the we actually perform a compound or more as a shoe so we compare different normalization in front of the contour one works really the water the best one so therefore we use upon a normalization and to supply those values to the linear model and dynamics effects model and to run differential expression analysis so the difference between linear and linear mix effects model is that linear does not account for sample now independence was a linear mixed effects model I about to account for the sample non independence however it's walking on the normalized data so it makes the feature of modeling the count data so it may be less powerful and then we also compared with the standard Poisson equation model negative binomial "
    },
    {
        "start": 1548.56,
        "text": "model and the commonly used package say D seek to and AJ so do you think to AJ and actual binomial models they're roughly based on similar models negative binomial so they are not they are modern account data but they are not able to take into account those sample non independence and so therefore with all those methods what if what I expected is that the linear model posso negative binomial and d seek to a nature they are going to provide a going to give you very inflated p-values because we have some point an independent in the data and those methods are not able to can show for that and the other hand of the linear mixed model and our macro model are going to control the spree better Square and compared with the linear mixed model with a possum mixed model we are hoping to see in the next set of simulations that our model is more powerful also in the now is the optimal should be all well behaved and here shows the results so what I'm plotting here and the x-axis is expecting a minus log 10 p-value under "
    },
    {
        "start": 1609.9,
        "text": "the now while the provider will follow a standard not a uniform distribution between 0 and 1 and then on the y-axis where she deserved minus 10 minus log temp she borrows for this data so if this people is how will behavior then you would expect all the propellers are going to sit on this diagonal line so you can see that our mock master does sits on the diagonal line and an even slightly conservative based on computing this lamb that you see the genomic control factor on the other hand as you would expect for the negative binomial model it's really inflated it's under the notes Javaris am frighted house only it's much worse it's huge impression and if we look at H and D 6 these are all huge infractions and so in this data so and as you would expect and on the other hand when you use gem as a linear mixed model then you also have a calibrated she values but the difference between German and makalah dynamical model in the count data or Gemma modeling the normalize the data so it might be less "
    },
    {
        "start": 1669.9,
        "text": "powerful so this is a now simulations and all those infractions it's actually getting worse with the increasing sample size so what I'm plotting here is on the x axis is increasing sample size say with 63 individuals as in the real purple and the sequencing data 100 individual 500 up to 1000 and on the y axis we are plotting those genomic control factors so if I draw my control factor close to 1 it means that there's not much inflation while if it's bigger than 1 then says a huge inflation and so you can see that across the sample sizes two lines one is the Gemma is as is Marko they tend to control the genomic control the previous pretty well so that have one error are generally well behaved however in contrast with this 2 and the negative binomial model or the linear regression models they all have inflated one and the HR de sica even above that so we are not able to show this in the same scale and so you can see that's rude now simulation you can see that "
    },
    {
        "start": 1731.1,
        "text": "standard software package to have this issue of giving you in fright you'll have one and besides this now simulation would also perform the second set of simulations to compare powers and so here what we did it was too similar to 110 thousand genes again by this time my son and I are now genes says I'm not differentially expressed by one thousand are differentially expressed the our goal is now to identify those differentially expressed genes and we also similarly to the data as similar to this before except that for this one sound and differentially expressed genes now our faders are now there and then we ask apply all those methods and ask which one works best so plotting on the y-axis is the area under the curve which measures the power of this method and on the x-axis it shows you a bunch of variables and that determines the power so for example on the left hand side it shows you the H square which is heritability so you would expect that if "
    },
    {
        "start": 1791.89,
        "text": "the highly heritable right and the same sample now independence could it be a more severe problem so you would expect our message performs even better so if heritability is exactly zero then you can see that our method but collogues works very similarly to the negative binomials maybe slightly that's powerful right because over here we don't have sample 9 independence issue at all and however watching now you have a moderate moderate heritability say a gene have a 30% heritability that's roughly the media gene our data you can see that now our method works better than the negative binomial and also works better than others and the 1 heritability is given hires and the difference becomes even higher as well as you would expect and so this are essentially the power comparison and you can also add in the Asia and a de seek and in our experience the DC can edge at and walk at all when you have a decent sample size so they work probably ok if "
    },
    {
        "start": 1852.7,
        "text": "you only have three samples versus three samples or five versus five but once you it's beyond a file then the performance of hnt seekers really extremely was so in this case we have a power of 63 some house you can see that you seek a nature they're much worse then say that even a standard Poisson equation model the negative binomial model and certainly it's wasn't and so this has a power comparisons in that simulation data and we will see the similar patterns in the real data too you will see that HN do you seek them to really perform very well when you have a larger sample size and so now we have all the simulation so we decide to move on to look at real data and we look at three different real data and they are representative of three different types of and sample nine dependence so the first type is first data is this poppin data so in the papen data what we have is related individuals so it's a family basis today so they're individuals are related to each other so you would see standpoint on independence in the data so the data that corrected are the whole "
    },
    {
        "start": 1914.169,
        "text": "blood samples say about sixty three individuals and twelve thousand the genes and there were the variable a variable trustee sex so we try to identify sex associated genes and we use a microsatellite to estimate the kinship matrix so that it gives us the K matrix and to perform analysis and so here we choose on the left access left panel issues a power versus a false discovery rate so the high better and right hand side which shows the enrichment of X chromosome students so let's look at the left hand side of us so here on the x-axis we're showing the first discovery rate this is estimated by permuting their individual labels and the y-axis which shows the number of the sex associated genes and so therefore conditional on a fixed force discovery rate say 5 percent or 10 percent you can see that our mess looks better than the other methods and trip by quite a large margin and then below that it's mostly "
    },
    {
        "start": 1975.58,
        "text": "it's usually the linear equation of the GEMA but sometimes that this negative binomial can catch up and across all those data sets we can see that HR and do you seek then to really perform very well and that's commonly seen when you have a large sample and in order to validate also we validate roughly by permuting those genes we're computing the FDR however we still want to more perform a additional level validation to check whether we are identifying sex associate genes so intuitively we're thinking that your sex associate genes should be primarily sitting on the X chromosome or the Y chromosome so therefore we use the number of genes at our hands on those sex chromosomes as a good indicator on whether methods are performing well so here shows their enrichment you can see that if you look at a top 400 genes or 300 genes you can see that our methods that had more genes on the sex chromosomes than their other methods so that's the first data analysis the second data analysis we'll look at the fusion data where they "
    },
    {
        "start": 2037.65,
        "text": "performed an angle sequencing study on the skeletal muscles and so this data they have this population structure because they acquire data from the stree finland count townships and those individuals within each town although they are not belong to the family that genetics all relative to each other than the more related to each other than the between townships and so in this data will have 267 individuals Chinese are the genes and we look at two different variables one is the og TT which is essentially type 2 diabetes status so you can think of where the binary indicates where the individual habbit have 2 diabetes are not then the other one is the glucose levels which is a continuous quantity and the glucose levels and type 2 diabetes are generally correlated with each other and there probably have a similar genetic cost so at the end we'll also look at the enriched the enrichment of differentially expressed genes between these two variables in order to validate some of our results and they have genotype data so we use do not have data "
    },
    {
        "start": 2099.69,
        "text": "to estimate relatedness matrix and so here shows the results so on the left to panel shows the power versus FDR so on toppest have two diabetes and the body is glucose levels and there on the right hand two panels we show the overlap so essentially look at how many of those t to teach differentially expressed genes are also GL associate group has associate genes and also the vice versa so for all those four panels essentially from metal perform well you would expect them to have a higher values in the other lines and the consistent with the purple data in the fusion data we also see that our mess and macaws or performs much better compared with other competing methods and in fact in this data the second and cinema is a linear equation and GEMA and very similar to the poppin day there was a negative binomial model possum or attempt to perform well so and also hid seek then the perform whether it looks very similarly at the bottom so "
    },
    {
        "start": 2161.61,
        "text": "this is our second data analysis where we look at how our method can be applied to control for population stratification and finally we also look at asserted data and so this data attractor are independently individual so this is a Yoruba data this are collected a Moses lymphoblastoid cell lines l co-sign lights and those samples Browns two part of this international hapmap project and so in this data there have 69 individuals 13,000 genes and we try to identify again since it's the only variable but about for us was male versus female so we try to identify sex associate genes and in this sample we know it's known that that hidden confounding effect so we try to and the skinning confounding effect will cause the sample now independence across the samples so therefore we try to use our method to control for this and in this data we also do the sub sampling and so the reasoning is that standard in many different sequencing study the sample "
    },
    {
        "start": 2221.85,
        "text": "size is usually small so it's usually on which 3 versus 3 5 versus 5 so therefore we down sample this data to create only 6 samples 6th and individuals to individuals or 14 individuals and then we kind of perform analysis into subset of data you know to see whether our method can do well you know small sample size and so here shows the results now you can see that in so here should n equal to 6 and y equals to 10 and n equal to 14 so the first thing you can see is that when sample size is really small right an equal to 6 many men says they look very similar to each other because there's only that much information you can extract for so all the methods will look very performer is similarly and on the other hand now is increasing sample size in equal to 10 or an equal to 14 now you can see that our now start to show more boundary to compare atom assets although the advantages here still relatively small because of the relatively small sample size so that you can see that all the "
    },
    {
        "start": 2283.049,
        "text": "messes tend to come stand together to each other and party in any case you can see that our method still works pretty well and compared and the second one is say the linear mixed models of the Gemma while compared with standard methods the HR and do you seek you can see the DC killed here H is around here suppose I was in our methods so this is assertive data analysis and so then finally we also look at our computation and so since we are fitting a complicated model using a complicated algorithm so in terms of computation time where I added a SATA bandage so but it's a computation time it's not that bad so if we look at it's a linear regression if you apply our package it will take just seconds to finish the whole analysis way if you coded a correctly and if you are piling image it's also less than a minute or in a minute for the papen data or for the t to t human data and if you use a post or "
    },
    {
        "start": 2343.92,
        "text": "negative binomial model takes about that's for the human data but now if we apply our algorithm takes about 19 hours so you can see that computation rate is much more demanding however 19 hours it's not really that bad can considering that when you correct the sample size of 200 individuals right that alone will take half a year to collect all those sequencing data so a day of analysis is not that a big deal and more important today now we are now implementing this macaw into our version and we are applying several computational tricks to make it a much much faster so now we have a much efficient instantaneous ox emissions so the approximation ensures that the p-value from this our new version makalah SEC 2.0 version looks very similar with the older version from our code so the coloration is almost one does - locked empty marrows however in terms of computation it's about us an order of magnitude faster than the older "
    },
    {
        "start": 2404.49,
        "text": "version so or the version takes 19 hours to analyze a human day that nights only takes about two hours to analyze them so this is just the older version computation time again on the local scales and our new version is about here so it's part of him more than ten times faster than the older version and the so far we have been mostly focused on the application of McCowen identifying differentially expressed genes however the Makah model is quite a verse at how to so you can actually apply to do many different sins so once you can do is you can use it to estimate gene expression heritability and that's the important task because typically when you ask me gene expression heritability you feel the linear equation models you have to normalize the data and turns out this normalization will bias your estimate so here should serve without show some simulations so what do we on the x-axis essentially shows the mean gene expression level say you have average of 50 reads 100 rates so on and so forth and then you apply either our mass method directly model the count "
    },
    {
        "start": 2466.5,
        "text": "data apply the GEMA the standard linear mixed model method under normalize the data and what you see is that the our method essentially have a means that centered around the choose which the choose is about 30% we do this our simulations so you can see that all those red bars are centered around the choose points 30% however when the mean rate accounts is more saying normal line ism well under estimate so here in this case we have a 30% heritability but on average we are only estimated to be about 20 mark and in two percent so there's about 20 percent reduction bias down water biasing the heritability estimation rules normalize data and that's a very important area now we are trying to focus on looking to a real data and in couple real data we did also find that the linear mixed model tend to give you much lower heritability estimates they're modeling the counter properly and the besides dealing with on a sequencing data Amoco can also be "
    },
    {
        "start": 2527.85,
        "text": "extended to model our sub sequencing data so in the past several sequencing data you can get a maceration data and you can perform differential mass ratio analysis and standard approach in the differential methylation analysis in the field is to use the beta binomial model and which you can assume sample independence and when we apply this Maho data into this into this data with sample nine dependence you can see that amico will provide you calibrated p-values under the now as you would expect while the standard approach of using the beta binomial will gives you freighted a few errors so it will fail to control for type 1 errors and we can also apply to a real positive sequence in data so here we have about 50 individuals and think and we try to identify sites they're associated with H and then we'll commuted those individual label so we can consider now distribution and we can calculate the first discovery rate in this data and so we're prodding the power it century was the first discovery "
    },
    {
        "start": 2589.92,
        "text": "rate and you can see that our message that Macau works better than the other ones including the beta binomial including the linear mixed model and the linear regression model and the power differences can be a quite a substantial so when at 10 percent FDI 10 to the power is about 60 percent of power gain so it's a quite a substantial being also in this passivity sequencing studies so now you can see that we have introduced the Makah software under the method and algorithm and we showed many examples where the macaque can be used to analyze any sequencing data and also pass average sequencing data so as a shorter summary some macaw is a very flexible modern framework kind of software package to analyze post and sequencing data and a parser for the sequencing data and the in terms of identifying differentially expressed genes are differentially methylated asides it can produce kirbridge C values so therefore it has advantage over the other methods and it can also "
    },
    {
        "start": 2650.609,
        "text": "provide unbiased estimates of gene expression heritability was as amazed as once you normalize they tend to underestimate to the expression heritability and they can naturally handle the sample name independence due to is the individual related illnesses are due to family studies or population stratification where cracker samples from different say townships and also human company effects those objective facts are on record and batch effects and can be much more powerful compared with several other existing approaches and here I just chew our references and this mokou software and the software is freely available on my website and there's also we're also posted a package and short today so right now it's a seed package but it's quite easy to use with the DPR manual on the website and the so finally I want to mention that the most of work are done by a suit ran and also on the anger sequencing data and also Amanda Lee on the far side for the sequencing data and the judge Han recently are also focusing "
    },
    {
        "start": 2711.839,
        "text": "on idolizing gene expression heritability and will have various collaborators including Laura Scott hander mukbang key in providing this fusion data and also share an attorney from Duke University in providing those so that's why RNA seek data right so that's a very good upon so let's just imagine a setting right so in the setting where I say you have say 100 total rates for one individual let's make it very simple right and then saying there's a hundred total total rib depths you say Clara fifty rates so if you take the ratio between these two "
    },
    {
        "start": 2772.799,
        "text": "normalize it it's quantified right and then you look at another individual which individual has a thousand traits and this individual for this gene had mapped 500 reads so if we take the ratio again it's quantified so if you normalize the data post individual worker point of five arrows right and the essentially contains some information however right since you have different relapse a call this to individual they tend to have different there's a quantified really measured at different accuracy so with the solid reads it's much more accurate estimate so by modern comdata directory century model rangers accuracy you that happens it's normalize the data and so therefore by doing that you would be able to give more power and this accuracy certainly depends on you know the reader depth variation across individuals and also how much reads your map around this genes are you would expect if you have a large evaluation costs read adapt variation across individuals the condom allah will perform better and also if you have a lower read accounts right then you also "
    },
    {
        "start": 2834.42,
        "text": "expect the kind of model to perform better right so that's kind of the general intuition why the kind of model in those cases would be how better than normalize it yeah yeah so that's yeah very good Epona so we try to we actually as a extension project we try to group all the genes together try to borrow information so our idea was kind of follow this DC can age as the over dispersion parameter if we motor ology and jointly together you perhaps can get more power to estimate more accurately reporting information across genes it turns out that you don't see much power gained by modeling that so the reason i guess because we are located data with relatively large sample size with above ten samples essentially and in those cases border information across genes and this over the explosion path parameter perhaps tend to really help "
    },
    {
        "start": 2895.859,
        "text": "much and we're also following information across genes inherited estimation that done to improve much power user so yes yeah yes that's right yeah yes exactly yeah yeah so you can run parallel so you can perform you better faster right so here's a so standard approaches right yeah you know that I use a typically putting in the species in the data in the model to control for say some population stratification but here you don't really need to do that because we have the key metrics so by incorporating the camera it's automatically shows for the pieces it's like a genome-wide Association studies one approaches you doing the PCA you put the pieces and that approaches the linear mixed effects model we just put it okay magic Berman coupons match for the "
    },
    {
        "start": 2956.099,
        "text": "patches we didn't put a few either so in comes a patch we also computer k matrix based on a gene expression I was so you know where were using the expression level to estimate the example relatedness and without computing the pieces but yeah I think you're referring to a very good opponent where sometimes you know if we include if the patch really strong right if the top two pieces are really fraction patchy effects of perhaps ideal to just include us to pieces inside and we encounter one case that's thinking including PC in addition to the camera just provides you more better power and right in fact Lee in one of those data analysis maybe it's in diffusion data for God we would put a five pcs following their previous paper yeah yeah but other ones we just directly put in a key matrix "
    }
]