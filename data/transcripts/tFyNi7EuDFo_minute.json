[
    {
        "start": 0.399,
        "text": "he good afternoon uh it's my pleasure to introduce our speaker today uh Dr Jin y Jessica Lee professor of statistics and data science from ucra Jessica's research lies in the at the interface of statistics and biology with a focus on developing statistical methods to understand large scale genomic and transomics data Jessica earned her bachelor's degree in biological sciences at the chinua University and PhD in biost statistics from UC Berkeley she joined ucra in 2013 at The Faculty since then she had made significant contributions to computational biology especially in creating tools for single cell R sequency analysis she's a leader in guarding rigor and transparency in toour development a strong advocate for using "
    },
    {
        "start": 60.559,
        "text": "synthetic data to Benchmark computational algorithms and a creative tireless scientist that we come to know and appreciate she collaborates broadly to pursue topics uh such as classification problems and multispecies Joint analysis her record of innovation has earned her numerous prestigious award other than ones you know such as SF career HR 35 and those from SLO K or c C she has also received just bear with me here received the Overton prize from the international Society for computational biology the emerging leader award from the committee of President of statistical societies MIT Technology reviews award for 35 innovators under 35 Johnson Johnson women in stem scholar award and I've got a chance to he to hear her radically Fellowship from Harvard that gave her whole year of "
    },
    {
        "start": 120.759,
        "text": "visiting professorship to connect with Scholars from a exceptionally wide range of uh disciplines in Natural Sciences and Humanities so it's again my pleasure and I want to thank the seminar committee led by Ami king and the King for inviting Jessica now please join me in welcoming Jessica Lee thank you so much Professor Jun Le for the very generous introduction and and also thanks to Kimi and Mii for inviting me to give this talk and visit this very vibrant department and it's my great pleasure to talk about some of our recent work just from the perspective of permutation it's just it's a new talk actually I reorganize my old stuff into this new talk to provide I will say hopefully a simple but useful perspective and which I hope everyone can integrate into your method development or data analysis so just as "
    },
    {
        "start": 181.08,
        "text": "Professor Le introduced my research has been we try to sit at a junction of statistics and biology so that's how I designed the logo if you pay attention to it GSB and the two colors one for statistics one for biology so I also use the same two colors to highlight the two concepts my title permutation and genomics so just to G your interest from the beginning yesterday you might have seen the big news for the Nobel Prize in physics and I noticed this very interesting figure in Chinese news media and I kindly asked my post of hand to help me translate this to English so basically if you don't look at everything right it's just two Dimensions one is the object we study so we can go from fundamentalism to liberalism so it can be very broad or very specific and we can also have the method going from fundamental to Liberal so what it means is that we will have less requirement in "
    },
    {
        "start": 243.64,
        "text": "defining what is physics so this is where we are neuron networks is also physics but this liberalism made me wonder the connection to our research right if we think about genomics I would call it a liberal discipline in four ways first of all it's interdisciplinary nature second it has a very data driven Focus instead of theory driven or hypothesis driven driven if it's data driven you may end up getting many interesting findings from data third we have a very rapid evolution of methods as you can see which I will show very soon for single cell data analysis fourth it has allow flexible analytical approaches so we have them if you have real day experience you will know how many steps are involved in your data analysis and different people may do data analysis in a different way so this can be SE in this website the Single Cell RNA tools. "
    },
    {
        "start": 304.44,
        "text": "org so you can see the quick Evolution right of methods we ended up getting more than a thousand or 1500 methods for just single cell data analysis so facing this success huge success in AI right Nobel pricing physics even and also this data analysis liberalism in our field so the very fundamental question I would say this is the question I had even when I was a PhD student at Berkeley so is statistics still relevant because statistics itself is a such an old discipline more than 100 years old given we have so many tools so many flexible approaches and we been so liberal in doing genomics is it still relevant in my opinion statistics is a discipline that ensures rigor in data analysis so to me as long as we care about rigor it still matters so how do I why do I say this let's let's go back to the history to the origin so one of the most famous and fundamental "
    },
    {
        "start": 364.96,
        "text": "method you guys learn from intro to stack course is two sample te test I happen to had the opportunity to visit the the origin of this method is actually in the Guinness bu Buy in Dublin in Ireland so this guy whose name is goette he actually work at this buy and he was interested in comparing the quality of their be here in two batches to see whether the new technique improve the overall quality so to answer this question he needed to compare two sets of numbers from each batch so because he was so humble he named himself using the studo name of student that's why it's called students T Test but basically what this is about is that we want to compare the two the two averages or two means of the two sets of numbers so even without statistics we could do the comparison right as an elementary school student and there should always be some nonzero difference but the question is "
    },
    {
        "start": 427.28,
        "text": "is the difference big enough to say yes we have a better quality with a new technique this is the question statistics help answer so you can see even back then it's about the rigor in data analysis how do we interpret data how do we make conclusions continue from here I really like this article and I strongly recommended to people here it's about this summary of important statistical ideas in the past 50 years and Andrew Gilman is a renounced station at Columbia and there's an even I'll say simpler summary on medium about this article so I will draw my talk from a connection in this article because it mentioned I think it listed eight important ideas and the second of it is called bootstrapping and simulation based inference under this idea there's a quote in permutation testing resample data sets are generated by breaking possible dependency between the "
    },
    {
        "start": 489.12,
        "text": "predictors and Target by randomly shuffling the target values so permutation is the theme of my talk and I will show you three examples of how it can help with my research so to begin I will first show you two scenarios about permutation so Bri briefly right in many me machine learning problems data analysis problems we have a supervised learning setting in which you have a matrix and here I'm just denoting the rows as samples columns as features and you have a company response or outcome Vector called y that's called supervis setting and for unsupervised learning you just have this Matrix x no y and in my talk I'll give you some respective examples for B for B data which I'll talk about it's actually under this setting so here the features are genes samples are just biological samples and why are some sample condition labels binary for the unsupervised learning "
    },
    {
        "start": 551.079,
        "text": "setting my single set examples will be under this setting here the samples are cells and the features are genes so the three examples I will give you the first one is for bulk differential expression analysis second for single cell data visualization and the third the most recent ongoing one is the aggregation of single cells into meta cells so the first two Works were published so this B arnc de analysis is one of the most commonly performed D analysis in our single sorry in our Arn data analysis I should say so differential expression is defined as oh if a gene has different expression levels under between two conditions I call the gene a de Gene differential Express Gene so this cartoon shows a typical scenario in which you have collected experimental replicates under each condition because RN in those days was quite expensive so people typically collected three replicates per condition "
    },
    {
        "start": 614.24,
        "text": "and this comparison is done Gene by Gene you look at one gene at a time so intuitively we want to call the left Gene which shows greater differences between the two conditions a de Gene the right Gene maybe not so much so the statistically it was done as a hypothesis testing problem one statistical test per Gene with no hypothesis that the gene has equal expression under the two conditions so if we decide to reject the no hypothesis we call a g a DG that's a setup for this analysis if you want to learn how to do it the two popular methods you will definitely encounter are Edge R and dc2 they were both developed more than 10 years ago and they were designed for small sample sizes like the three versus three scenario so it's a small sample size and we know that it will be a big challenge to get some reasonable statistical power because your information is so little so therefore they have to compensate the small sample "
    },
    {
        "start": 675.72,
        "text": "size by assuming a distribution of data R data are counts if you summarize the data right one gen get one count in one sample so they assume both methods assume this negative binomial distribution for count data which is more General than the passon distribution because you get an additional variance parameter which could be bigger than the pong mean if it's pong you just have one parameter and furthermore both methods will count for the fact that different R examples have different sequence in depth and Sample Library sizes so in this formulation essentially we assume one negative binomial per condition and we assume that the condition has its mean parameter me one and mu2 multiply by the sample specific size Factor so given that then you have some Independence assumption for the counts of the samples under each condition so for these methods both methods the no hypothesis is that mu1 is equal to mu2 that is "
    },
    {
        "start": 738.519,
        "text": "subject to sample size differences or size factors which believe that the two negative binomials should share the same mean parameter I will say this is a reasonable hypothesis only if the negative binomial assumption holds because it's what you assume for the data so our surprising discovery was by coincidence in a collaboration project with Professor way at us Irvine with his former postto yum who is now a faculty herself in China and my former student shinjo who is now a faculty himself at Oran state so this collaboration Started From The Journey where you may applied dc2 HR to this data set which is about immunotherapy of patients blood samples before therapy which is 51 patients and 58 patients on therapy and it was published in the cell paper the blood R examples from patients so her first finding was that D2 and HR actually give "
    },
    {
        "start": 799.16,
        "text": "quite different numbers of De gen at the same false Discovery threshold say 5% so Edge are found almost twice as many as theic to and they're over lab is not so big so the intersection is not so big so this motivate you may to look into this question and ask this question why do dc2 and HR identify just drastically different D gen because here the sample sizes are 51 and 58 then I had this question so in this cases can we do some permutation because there are many ways we can permute the data so just imagine that if you are just want to randomly assign a total of nine samples into two groups of 51 and 58 you can do 109 choose 51 right that can give you a large number of permutations so let's just try that and I call this condition label permutation because once we permute the labels there's no more relationship between the condition label and the J expression then we get a more surprising "
    },
    {
        "start": 861.12,
        "text": "result that is if you look at here so from the permedia data each permutation we can apply a method and get a d DG number and we can do many many permutations and then get a distribution of DG numbers whose average is the height here and whose standard deviation is half of the arrow bar here and you can see that from many permitted data we get even more D genes than we get from the original data which is more surprising and how could that be possible the reason is the negative B binomial assumption does not hold for this data set so just as a mention the Assumption was first assumed for replicate data right experimental replicate data under a control condition so we can have some assumption about the sample variance due to pong and then we can assume that maybe there's some biological variance in the three replicates subtle rep variance and then POS give us the count but and together "
    },
    {
        "start": 921.88,
        "text": "like for example gamma for biological variance and pone for sample variance together we get a negative binomial but these people these samples are from patients and patients could be more heterogeneous than they should be than experimental replicates so that's why our first check is to use the 51 data points and 58 data points to check whether the negative binomial fit well to data and these are based on the edge RS fited negative binomial so these are called Quant Quant QQ plot so if the model fits well we should expect to have some like the dots should lie on the straight line but that's not the case we have some clear outliers that's for HR and for dc2 another Gene we also see the similar pattern so basically what's indicated here is that the negative bomal assumptions not reasonable for this data from Human patients not replicate and furthermore we divided the genes into "
    },
    {
        "start": 983.72,
        "text": "two groups two extreme groups one group includes the genes that are rarely identified from per data the okay genes and the second group contains the genes that are frequently identified from per data the bad or problematic genes so for between the two groups we check whether the fitness or goodness of fit for negative binomial for genes in each group have some differences and this shows that okay let me just explain the y axis is the negative lock P value for goodness of fit test the smaller the P value the worse the fit and because we take negative log we have larger value to indicate worse fitting so in this case it confirms our guess that is for the problematic group negative binomial fitting was poorer and I want to give a credit to my current Master student who did her undergrad research in Chinese University of Hong Kong after she read our paper published she pointed out an issue in our goodness "
    },
    {
        "start": 1045.839,
        "text": "of P value calculation and we fixed that so the I'm we I'm presenting here is the uh post correction figure the good thing is that our conclusion qualitatively still holds but we have to fix the scale here to make it more accurate so coming from this conclusion like negative binomial assumption doesn't hold for this data set then the next try will be for us to use a test that doesn't assume negative binomial assumption and we use the will coxon rankon test which is a so-called nonparametric test it has a different n hypothesis basically I'm making it simple by assuming my data are normalized so I don't need to worry about the sample size differences with the normalized counts the no hypothesis of will coxen right some test is that if you just randomly pick one value one count normaliz count from condition one and you random pick another one from condition two then which of the two numbers is bigger is equally likely so "
    },
    {
        "start": 1106.12,
        "text": "you have 0% 0.5% chance that one is bigger than the other so it's based on the relative ranking of values instead of saying negative binomial and the same mean so this doesn't have the negative binomial assumption on this data set we look at even though the result may be disappointing because will coxen didn't give us any discoveries however it's not self self-contradictory because it also didn't produce anything from perm data so it's consistent so I just want to say that this is not a benchmark study because we didn't include all possible methods I know there are there's maybe like two dozen methods out there it's not Benchmark study we just want to say that the Assumption matters and if we just use popular methods without knowing their assumptions we may get false discoveries so surprisingly this me this analysis paper got a lot of attention especially on internet after publication and some people even tell me I mean someone from usel Medical School tell me "
    },
    {
        "start": 1166.28,
        "text": "oh this is your most famous work and I was very shocked because it's not a new method it's analysis but what tell what it tell me was that wow something we computational people took for granted like we need to check a methods assumption it's not as obvious to people who analyze data so that's why I feel like permutation is something people can Implement in their practice but the condition the discussion continues I would say so this is like how to say I shouldn't complain but basically our correspondence should have been published half a year ago as you mology but we're still waiting so this correspondence is actually well we're responding to two correspondences that pointed out either an issue in our analysis or a suggestion so one correspondent saying that we neced normalization in our analysis or in our simulation part so we definitely so I should say I shouldn't go to too much detail but we did some correction to analysis and said that we "
    },
    {
        "start": 1228.84,
        "text": "want to argue that if the semisynthetic data based on simulation does not assume any batch effects in its definition of true D gen then no normalization should be applied so this is something we should also pay attention to because real data analysis involves normalization but our simulations based on our assumed ground truth so in that case we have to be careful about whether normalization should or should not be used in our simulation second correspondents pointed out that if we app apply a procedure called winsorization it's just a fancy way to say outlier removal basically you remove the outliers from your data and then you can run the2 EDR so this correspondence shows that without the outliers the false discoveries we reported were not severe however we want to argue that even though this is true but in real data analysis throwing away outlier is not as straightforward as it is right because we need to set a threshold how much to throw away so basically in this "
    },
    {
        "start": 1290.799,
        "text": "correspondence we just showed that world coin remains to be robust even though we consider normalization or rization but I just want to say that there are many choices in real data analysis and whether you want to go with a robust method or whether you want to go with the delicate method specific for the STA set you need to make this Choice that's the first example second example single cell visualization so this is an example where I will show under the unsupervised learning setting we just have the Matrix X so the motivation is about the common use of tne and umap for visualizing single cell data this is a very well- written block for tne so in short when you use it you need to pay attention to the hyperparameter because both methods are nonlinear redu Dimension reduction methods for tney there is a there's an important parameter called perplexity perplexity roughly measures the neighborhood size so basically if you "
    },
    {
        "start": 1351.72,
        "text": "said the perplexity to be small you're assuming every cell's very close neighbors are neighbors and you want to preserve the neighborhood when you make the perplexity very big then you have a very large neighborhood size so both T and umap are good at emphasizing clusters so you can see clear clusters and this is a simulation example in which the real data the the the truth has just two Dimensions so this is the true visualization but you can see that when you vary perplexity sometimes you're closer to the truth sometimes you are very far away from the truth this issue was also reported recently by a nature method journalist saying how do we see data as tne and umap so basically there's one sentence in the abstract sometimes these methods take a second thought we have to use them carefully so the question my collaborator Professor Lucy sha who is a statistician in Kong and my student christe Lee the problem we try to tackle "
    },
    {
        "start": 1412.4,
        "text": "is to answer this question can we tell whether a cell's embedding is dubious or trustworthy in a Tey or umap plot our idea is to decide on this trustworthiness by looking at the sales neighbors before we run te or umap and after we run te and umap this is our intuition so basically we look at two sets of neighbors and this cell one is one good example I want to show using this cartoon that is before we do the embedding this we can Define some neighbors for the cell one after we do the tney embedding we can Define the neighbors so on both sides of this slide I'm showing the same tne embedding or umap embedding doesn't matter it's an illustration same embedding but I'm labeling two sets of neighbors one based on the pre-embedding space one based on the after embedding space so to just clarify because we hope to make some "
    },
    {
        "start": 1474.919,
        "text": "reliable conclusion about the relative locations of cell clusters to each other in this 2D plot we said the neighborhood size to be half of all cells because we want to capture or check the preservation of mid-range distances furthermore for the pre-embedding Neighbors we actually defined it in the principal component space because that's the standard practice in the Single Cell field that is you in you first do some lock transformation then PCA on your data then you keep about 20 to 50 pieces a number of your choice you then input those pieces as the input into T near map so in other words you're not using T near map to reduce the original thousands of Dimensions as gen but the PCS and also in both tne and umap they operate on the UK median distances of cells in the input space the PC space so therefore when we Define pre-embedded "
    },
    {
        "start": 1535.48,
        "text": "neighbors based on ukan distances in the PC space and we Define 2D embedding neighbors based on ukan distance in the 2D space so then what do we do we want to check the consistency of two sets of neighbors by consistency we mean that whether the ordering of neighbors are consistent and also whether the distances between the neighbor neb to the cell one in the 2D spaces is consistent so here we just order these neighbors ukian distances in the 2D space to cell one by following their order in the respective space the reason is that we think when we look at this plot we are visualizing the 2D ukan distances so that's why that's what matters and then with these two ordered ukian distance vectors in the 2D space we check their consistency and we use the Pearson's correlation to measure the consistency because we need to care about the absolute values the magnitude "
    },
    {
        "start": 1597.84,
        "text": "of distances not just their ranks so this is a good trustworthy self embedding example this is a bad example I just want to show that the sales neighbors change a lot from pre-embedding space to the postembedding space okay so that's the intuition but we need to formalize this intuition and our question question is what is the no hypothesis in this problem so I would say it is that a sales neighbors half of all cells are just random after embedding so before and after embedding you have a random selection for half of the sales as neighbors but how do how do we obtain such a no hypothesis how do we realize that we can use permutation so in this case I'm just thinking about one case in which I can permute every Gene in dependently okay so then the cells became permuted cells because they're not no longer the real cells but every Gene still has its marginal distribution "
    },
    {
        "start": 1659.76,
        "text": "preserved because I'm just shuffling the numbers for each gene so I do preserve every Gene's marginal distribution but I didn't preserve Gene Gene correlations or cell cell relationships since I do the independent permutation for every Gene essentially I get a new population of this per cells and in this population the permuted cells have exchangeable relationships they're equally likely to be close to each other so that give me the no hypothesis and then we Implement that so the permuted data is just the permuted cells I talk about and I just need to do this permutation for once for my whole data set because every cell becomes a permuted cell and then I can calculate my score for every perer cell and these scores together give me a n distribution so this reflects what my score will behave if there's no real relationship among my C so under using this s distribution I can calibrate my "
    },
    {
        "start": 1721.32,
        "text": "score for the real sales whether I believe the score is high enough so I can call this real sales embedding trustworthy it must be much higher than the no scores or if the score for Real is so bad that is even worse than most of the no scores I call that dubious just one message is that here the percentages of dubious and trustworthy embeddings Are Not Just the Two Tales because I'm using the null distribution not the real sales distribution so in fact on a real cell population or real cell data set we found that majority of the real celles can be called trustworthy which is good and minority dubious and we don't have so much in the middle the gray zone so looking at one example in our paper and have multiple examples but this is the most striking one this is the first Hydro single aristic data set publishing science in 2019 so this is the original visualization we were able to reproduce the "
    },
    {
        "start": 1781.559,
        "text": "visualization and we run our algorithm to detect dubious embeddings and they are labeled as red so see what's interesting here is that many dubious embeddings are those small clusters and if you don't know that if you just look at tne you might say oh these small clusters might be cell types or cell subtypes but if you know that their locations are dubious relative to other cells then we should be careful when we draw conclusions this is one use of s the method to detect dubious embeddings given one perplexity given one parameter and another use is that we can change the perplexity parameter and then for each parameter we detect dubious embeddings and count the number so we can minimize the number of embeddings and as a way to optimize the hyper parameter perplexity so for this data set we ended up getting the perplexity to be 230 instead of 40 so you see comparing this new visualization to the "
    },
    {
        "start": 1842.08,
        "text": "previous one we see some big differences in particular if we focus on these three colored cell clusters previously the blue and purple one green and blue ones were were like surrounding the orange one the orange one is a big cluster of stem cells so you see that these neuronal ectoderm cells were surrounding them the orange one in similar distances but after our visualization optimization these three clusters are unified down here very far away from the orange one so if you look at the datas how to say the Jinx bression data in heat map you can see that our visualization is more consistent with the data so these small clusters are are indeed not so different and they're very different from the orange cluster so that's my example Two showing that when you use permutation you may help you decide whether visualization is reasonable or not the third example is the news recent work which hasn't been "
    },
    {
        "start": 1903.48,
        "text": "published yet but we hope to post the preprint version so this is called The Meta cell concept which is very interesting because you change your data you no longer work with single cells but you aggregate them into meta cells for the purpose of having less sparse data so this in this cartoon if you just have the Single Cell Matrix this is from this paper and you basically can so basically you have the cells as the columns genes as the rows this is different from my original X Matrix because I regard celles as samples but anyway if you aggregate certain cells into one meta cell you get a smaller Matrix so here your columns are meta cells and you get fewer zeros because of the aggregation from 90% of zeros to 50% of zeros but from a statistical perspective this is interesting because the question is is there a reasonable definition for aggregating cells into meta cells "
    },
    {
        "start": 1965.559,
        "text": "obviously we cannot aggregate too much right if you aggregate everything into one meta cell there's no information or if you don't aggregate you should just work with single cell data so there must be some tradeoff there and this is the literature review so so far there are four meta cell methods published from the first one meta cell meta cell 2 they were from the same authors there's one called supercell and the most recent one is from Dana Pierce group called C cells on nature B technology so they all use the meta cell concept and aggregating cells into meta cells in different ways we also have many prominent application papers they are all nature papers right so basically this paper implement the meta cell concept even though they may or may not use the method here so it has been used in practice but if we read those papers there's no consensus on meta cell definition so this motivate us to think about this problem can we Define a meta cell in a "
    },
    {
        "start": 2026.919,
        "text": "rigorous way and this is the first meta cell paper I'm just mentioning the quote here it it defines a meta cell as a homogeneous collection of single cell profiles that could have been resampled from the same original cell so essentially we translate the sentence to say that we think the variation of celles within a meta cell should be attributed exclusively to measurement or technical error not biological differences and this concept or this question motivated me to look into or actually I read this paper before so I draw the connection here it was from Matthew stens group it's a very interesting paper very well-written paper on nature genetics about separating measurement and expression models in singles rna6 beta so this paper was originally for about the zero problem or imputation problem but it's very conceptual and it's related to this "
    },
    {
        "start": 2089.079,
        "text": "metasol concept so in this paper the authors proposed two layer models one for expression model that's the distribution of the true expression levels of genes in cells and the measurement model on top of that that is the distribution of The observed counts given the true expression levels so this is probably the most technical slide in my talk which is how to describe this two layer model so again we consider a cell to be an observation from one distribution cell I from one to n n cells we consider a gene to be a feature J from 1 to P so this two layer observation model has the first layer about the biological true expression so here I denot this to be Lambda I for cell I it is a multi-dimensional vector because we have all genes here and P dimensional Vector so expression model describes how the cells true expression levels for the P gen are distributed so in "
    },
    {
        "start": 2149.16,
        "text": "particular there's something to pay attention to Lambda i j is defined as the relative expression level of feature J like g j in cell I so it's the proportion and all the Lambda I JS across J sum up to one okay so it's the proportion why do we care about proportion because sequencing data only give us the relative proportion we don't have the absolute abundance level so that's why it's more reasonable to talk about Lambda i j than the absolute number of features in Celli because we don't observe this and this F is the distribution model which is p dimensional then given the Lambda i j the measurement model is used to describe how our observe count is distributed so Yi J is the count of feature J in cell I and Yi plus denotes the library size of cell I the sum of all accounts so basically the measurement model is like given the true proportions of features "
    },
    {
        "start": 2209.76,
        "text": "given the cell library size how would you allocate the total into individual features and this is a one-dimensional distribution just for the measurement so together if we want to have a statistical definition for meta cell we want to say that it is it should be a group of single cells that share the same Lambda because Lambda is a biological signal okay so we just want to have approach to check this definition if we think a meta cell satisfies this definition we want to call it a trustworthy meta cell otherwise we want to call it a dubious metas cell so we're borrowing the names from the previous project and we want to formulate this as a statistical problem so why would dubious meta cells matter so this is again from this review paper for meta cell at MSB molecular systems biology you can check out this paper so it has this nice cartoon showing that if your meta cell happens "
    },
    {
        "start": 2270.52,
        "text": "to include cells of two different types here then it will bias your ging Jing correlation analysis because this is what you get using single cells this is what you get using meta cells and if you have a dubious meta cell here you will get a different correlation calculation so how do we tackle this problem to check whether some meta cells are dubious some are trustworthy how can we screen out the trustworthy ones so my post. pan leted this project and our first question is can we achieve these goals so we hope to give you a statistical Criterion based on which we can identify dubious meta cells consisting of single cells from different cell States these are the problem ones we shouldn't use and we should also we also hope to nominate the top performing meta cell method out of the four and optimize hyperparameter so every method has a hyperparameter which can be translated "
    },
    {
        "start": 2331.64,
        "text": "into the granularity level that is on average how many single cells do you aggregate into a meta cell we help to choose this in a data specific way so our intuition here is that within a trustworthy meta cell features are approximately uncorrelated this is the most important part because we think the differences of these single cells in this meta cell are just due to the measurement error and the measurement error should be roughly independent for different features so they should just give us a rig correlation and I have to say it's not zero correlation but weak correlation the reason is that because we have the cell library size as a total so that will still induce some weak correlation but that's pretty much it so this is from like a real data we can show that for a trustworthy cell we found meta cell we found the gingin correlations does not show a block structure but from a dubious meta cell we do see some coration structure and "
    },
    {
        "start": 2393.359,
        "text": "our strategy is to propose a per meta statistic we call MCD standing for meta cell divergence and also we hope to get some reasonable null distribution by permutation so this is the third permutation I will introduce today first question previously for as did I use a within Gene permutation is it enough here so we permute every Gene independently right for that purpose so we can get a permuted sales with exchangeable relationship no real relationship but here the problem is that the cell library sizes are disrupted because once you permute Aver independently each permitted cell doesn't carry the original cells Library size but in the meta cell definition for the measurement model if you remember it involves the cell library size because we're working with counts not normalized counts so the cell library size should be preserved so then this is an interesting strategy Han "
    },
    {
        "start": 2454.079,
        "text": "came up with I call it double permutation which is like for this top row we Premier averaging independently just as a way to serve as the how to say negative case where genes are uncorrelated so based on the original Matrix and this permuted Matrix permuted Gene Matrix we can get our statistic MCD and I ignore the detail here because our work is still ongoing but also I want to give you the idea here so MCD is calculated from the top row but how do we get a reasonable no distribution for MCD so pan had this idea that we can permute the genes within each cell we do the within cell permutation so if I do it this way then every cell keeps the same Library size as the original cells but here the genes are permuted genes they're not the real genes so therefore to we need to give this Matrix its own negative control that is given the permuted genes we perform within Gene "
    },
    {
        "start": 2516.0,
        "text": "permutation again at in the top then these two matrices will give me the same statistic but under the N so the idea is that for the bottom row we are getting it from the case where the genes are indeed uncorrelated but the key thing why we need double permutation is because for each Matrix we need to keep it genes but disrupt their correlations that's what we did in the top row right within Gene permutation preserves every Gene's marginal distribution removes G Gene correlations it removes cell Library sizes what when we do within cell permutation we preserve the cell library sizes but we remove the genes marginal distributions so these genes the per genes are no longer the original genes so that's why they need their own negative control as here we have here so that's the double permutation strategy and I can show you that it worked so this is a simulation in which we know the ground truth so these are the meta cells that are not pure so they are "
    },
    {
        "start": 2576.4,
        "text": "composed of different cell types and these are the meta cells that are pure so this is our statistic MCD so you can see that we do we are able to separate the two group using the statistic but how do we determine a threshold using permutation if you just do within gym permutation that is you just do the top row without this within sell permutation that's the problem you are being too how to say come too aggressive or actually should say too conservative so so basically you will call too many cells meta cells as dubious because the threshold is just too low but with that double permutation by preserving cell library sizes we get a much more reasonable threshold here and we can also see that here this left one shows how many dubious meta cells we detected using double permutation not so many but if we use within gy permutation half of the meta cells are dubious which is not possible and this also shows that using "
    },
    {
        "start": 2637.24,
        "text": "our double permutation approach there's a much better separation between the pure meta cells the trustworthy ones and the dubious ones then we use just within G peration due to time constraint I'll just quickly show you two real data applications to demonstrate how this method we propos MCU riger can be helpful the first one is to help with Gene coexpression analysis so this is a a real data application from this from this let me see from this nature medicine paper and it was first implemented in the using the meta cell method paper so we reproduce the result so the left column shows the Gan Gene correlation Matrix you get from single cells okay so it's from patient data healthy patient or covid-19 patient we hope to see some gen Co expression differences between the two groups of patients but you can see that with single cell data the Cor ations are very "
    },
    {
        "start": 2698.0,
        "text": "sparse because data are sparse and if we use all meta cells and this is from the original paper we can see that for this group of genes block of genes so these genes are basically adaptive immune response genes we see some strong correlations in both groups of patients both healthy and covid-19 but using our selected trustworthy meta cells using MC riger we can see some big difference so the same group of G have weak correlation in healthy or weak coexpression healthy patients but strong coexpression in covid-19 patient this is consistent with the the nature of these genes being adaptive immune response they should be active in covid patients but not healthy patients and looking at the data we can also see why we have those fake positive correlations using all meta cells so here the blue meta cells are the questionable dubious ones the red that are the trustworthy ones so "
    },
    {
        "start": 2759.079,
        "text": "you can see this blue one here here here drove the correlation to be very high but they should be removed and if you look at a single cell data original data you don't see such strong correlation there so that's one example the second example is for single so multi-owned data we have RNA seek and attack seek so basically we are trying to draw some associations between enhancers and G expression for inferring G regulation so we can actually see that using our trustworthy meta cell versus using all meta cells we see some differences and this is a data analysis from the C cell paper and nature bch so we see that there's one Edge used to be found by all meta cells that disappeared so if you look at the raw single cell data by cell type we do not see a peak here so which suggest that this Association was indeed spous but if we look at this additional one we found using just trustworthy meta "
    },
    {
        "start": 2819.92,
        "text": "cells but missed by the previous analysis we see that this is indeed a small peak in all the cell types here by aggregating cells into meta cells we can discover this but if you don't use just trustworthy meta cells You by using all meta cells you will miss this one so that's just a two of the main examples we hope to show in this work so in summary I talk about three ways of doing peration the first one is a simplest one for the de analysis we just permute the labels so we can get a negative case the second one within G permutation is for us to get some permuted cells whose cell cell relationships are disrupted and here we don't need to preserve cell library sizes because in this visualization thing our score is defined as pearon correlation whose range from negative 1 to positive one so we have the same range that's comparable between the cells and per cells and for the third one is the most tricky one because we had to do this double permutation to "
    },
    {
        "start": 2882.079,
        "text": "preserve cell library sizes when we need a negative control so I just want to say that even the permutation may seem as a easy technique but when you want to implement in your analysis it's very important to think what you want to preserve what you want to disrupt and then design the permutation accordingly so finally I want to thank my collaborators and my students and post off for the three project project in particular the first project which is a surprise finding has actually led to many how to say feedback and interesting discussion about how to do differential expression analysis an analysis of 10 years old and for the visualization we hope that we can help people visualize or have a better more reliable TN and um plot by optimizing the hyp parameter finally for the MC riger we're hoping to submit the pre-print very soon so you can see the whole idea there thank you very "
    },
    {
        "start": 2953.92,
        "text": "much hi Dr thank you for your inspiring talk so uh because I see some people doing like a per perturbation based method like introduce noises to the data and to see whether what differential CH I remain the same so what's real difference of the Coe The Code idea between this one and permutation so you said other paper use permutation to introduce noise peration mean perturbation yeah adding some noises to the data you have good question actually that's related to another project in my group I would say if you have heard about this bias variance tradeoff thing in statistics that's it so basically here our peration is to provide provide a negative control for removing the bias and when you add noise to your data you are evaluating the variance of a result how robust or how stable your result is the riger is about you know the bias "
    },
    {
        "start": 3015.48,
        "text": "bias yeah so what I'm introducing here is all about the bias we're getting a negative control case but I totally agree I think variance is another thing to consider yeah thank you it's very interesting talk and um um I'm curious uh for the B de uh differential expression analysis one um your observation is for some data sets that like NE to binomial distribution doesn't hold but currently on single cell it's like plus and netive distribution on their main distributions I wonder have you look at them and how good are they and how does that like I don't know just effect I mean gener yeah very good question question I would say that um um for single cell data and we have abundant evidence we have a genome biology paper called statistics or biology the zero inflation controversy it's a review paper so in that paper we actually found and I think Shan Jo at Michigan here he also had another paper "
    },
    {
        "start": 3076.72,
        "text": "showing that negative bomia is a good distribution if your cells are homogeneous in one cell type so with in that case you can safely say oh no Al the Single Cell technology has the Umi unique molecular identifier for removing PCR bias so for that case negative binomial is reasonable which is also related to that two layer model right the measurement model can be reasonable assume AS Plus on and if the expression model is gamma the negative binomial would be good and because you know gamma is a very flexible distribution for describing positive real values it has two parameters anyway so I think for the question we report here for both AR examples I think here just the the patients are very heterogeneous and they may have different genetic backgrounds right so when you pull them together you don't use other patient coars just simply do the two condition de test you might have false positives but in here we can discover that negative binomial doesn't fit well because of the large "
    },
    {
        "start": 3138.319,
        "text": "sample sizes 51 58 but previously if you only have three values there's no way to check you just have to assume that negative binomial I think that's another difference but for single cell data I wouldn't be too worried because we usually have at least maybe aund cells to look at right so we have a large number to check the distribution yeah any other questions thank you hi uh I'm very curious about the time complexity of your methods usually permutation based methods are question I I would like to say that the good thing about all the permutation I introduced here is that except the first one where we did 1,000 permutation just to get to show the distribution of the deeg number but for the second and the third one we only permuted once yeah the reason is that we actually after the permutation every permuted cell will give you one n "
    },
    {
        "start": 3201.2,
        "text": "value and they together give me a distribution so I don't have to repeat this again and again and the reason is because for both s and MC riger we're looking at a per sale statistic so we're not looking at the STA for ourselves so that's why we don't need to get many many rounds of permutation which is a good thing to me yeah so that's why it's not complicated you're just doing permutation for once yeah thank you Dr so my question is about like for the like some single cell R uh data analysis if we want to focus focus on like some Downstream tasks like earning velocity something like this how important do you think we need to do the like the laboring uh inviting just trustworthy check definitely that is something we are definitely interested in yeah and I would say that's why I feel like probably this talk can inspire you to think about how it can be in "
    },
    {
        "start": 3262.24,
        "text": "covered in your analysis right some negative control where you know that nothing should be found as a as a sanity check to see because I think for single cell data analysis the reality is that it has a lot of steps involved right we are not just doing a one-step analysis so we need to be sure that some signals we found are not due to the previous steps but are should be really be in the data yeah so I think Professor Jun Lee when he introduced me he mentioned that I'm also advocating the synthetic no data idea to be added to data analysis and I also permit just one approach to implement the idea yeah thank you do we have online questions online question I'll ask couple still have a few minutes yeah uh um so for the meta cell right uh you mentioned the hyper "
    },
    {
        "start": 3323.039,
        "text": "parameter yeah um have you thought about making it different depending on whether it's a dense area or the sparse area very good question I have to say that yes so right now for this work alone we're just proposing a Criterion and we're trying to optimize each of the four methods in terms of their hyperparameter and then we can compare these four methods to see which one is better for this data set there are some results I'm not showing but I would say ultimately we are dividing we are devising our own algorithm so when we devise an algorithm for defining meta cell I think it should consider density so maybe the neighborhood size shouldn't be the same thing so this gamma is just a way to summarize existing methods hyper parameter maybe it's not directly gamma but basically have another parameter that controls the gamma we are just using for Simplicity but how do we divide single cells into trustworthy "
    },
    {
        "start": 3384.4,
        "text": "meta cells by optimizing our Criterion that is something we're working separately as a new meta algorithm yeah thank you thank you for your nice talk um I'm just curious um whether the SD is applicable to even image based special transcript to mix which is also single resolution yeah for sure because as did is just a visualization thing right it's just a it's an add-on it's an add-on to T near umap as long as you can use T near umap to visualize your data you can tryd great yeah thank you another one so on your second part about um embedding credible versus dupus embedding uh there is a parameter U 50% of the cells yeah yes if you dial that down to 20% would you get more of the rare clusters to be credible good question yes in this paper we did try "
    },
    {
        "start": 3445.52,
        "text": "varying that neighborhood percentage right from 50 down to like 10% or 20% up to 80% 90% exactly because tney and umap are designed to preserve local neighborhood if this sale neighborhood size is small then you will find more sales to be trustworthy yeah more embedding to be trustworthy because it's that's what it's designed for but we chose this 50% because we want to talk about mid-range relationship like here oh sorry I should go back where is it yeah so I should go here so basically want to say whether the relationship between these clusters are reliable so that's why we go larger 50% but the good thing is that we in our how to say stability analysis or sensitivity analysis we found that if you change the percentage to 40% to 60% it doesn't change that much yeah so that's what we did yeah "
    },
    {
        "start": 3506.44,
        "text": "last one okay I'll ask last one um so the in the final part yeah there the assumption that Library size is purely technical yeah and then when the size is bigger or smaller the features are un correlated yeah biologically I think it's a profound question whether different cell types genuinely have original size been different and something propagated into exactly yeah that's a very good question um I I think here we are in a simpler situation because we hope that the meta cell definition should just include cells of the same type right every meta cell if it's trustworthy it should only include cells in the same type so that's why the cell type differences shouldn't be biological that's what we hope yeah thank you yeah thank you "
    }
]