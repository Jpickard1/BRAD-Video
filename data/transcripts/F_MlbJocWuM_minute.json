[
    {
        "start": 2.46,
        "text": "foreign which was an N polymer realigner for improving pileup based variant calling for nanopore sequencing um so before I start talking about my projects I'm gonna do about 15 minutes of background because I'm sure everyone has different levels of experience um talking a little bit about old genome sequencing then nanopore sequencing and finally some breed alignment and variant calling so whole genome sequencing just an overview of the whole pipeline um and in this example it will be viral genome sequencing although it's the exact same pipeline um for human genome sequencing you'd start with the nasal swab your sample for humans it might be a tissue sample and then you do some kind of laboratory prep to isolate the DNA with a virus you might be starting with RNA and reverse transcribe its DNA but in the end you have your dni and then you use that as input to a sequencer and I'll be talking about "
    },
    {
        "start": 63.12,
        "text": "nanopore sequencing so this is their small handheld device the minion the output of that is a raw current signal which then using a machine learning tool called a base collar gets converted to a string of acgt so you're going to get strings of acgt anywhere length from 100 to a million bases long and basically nanopore sequencing can read fragments of DNA that you'll need to use to figure out the actual genome sequence so the next step is aligning these reads to your reference genome in order to figure out where they came from and the last step is going to be variant following where you identify differences from the expected reference genome with the the measured actual reads so here we can see this a was mutated into a c and we are going to to mention that in our our final variant calling "
    },
    {
        "start": 124.439,
        "text": "so my project in particular is focusing on this alignment stage and it's trying to align reads more consistently in order to help the the downstream variant calling step um a little bit farther back in the pipeline with sequencing there's kind of a few major players there's even more in recent years but kind of the industry standard has been Illumina short read sequencing where each read is a hundred to a thousand bases they've always had really good accuracy and they're currently one of the most cost efficient options more recently Oxford nanopore and Pac bio have become major players the main benefit of Oxford nanopore reads is that they can get incredibly long um and they've done a great job improving their accuracy in recent years from like 90 to 99 to 99.9 percent and the main benefit of having such long reads is that they can unambiguously map to the reference genome and even span "
    },
    {
        "start": 185.459,
        "text": "long and credibly repetitive regions and once you have long reads you can also use that to phase variants on each of the two haplotypes and figure out which variants came from the mother father in the case of germline variants and Pac bio reads are a little bit shorter than nanopore reads but still pretty long and they're great for their accuracy and repetitive regions of the genome where nanopore sequencing sometimes struggles so for this project I focus on nanopore sequencing as the technology and now I want to mention that when you're doing the alignment there's actually different options for doing this you can either use a known reference genome and align your reads to that reference or you can start from scratch and identify the prefix and suffix overlaps within your read and construct a genome from scratch and I'll be focusing on reference-based alignment um so now in terms of applications "
    },
    {
        "start": 246.12,
        "text": "there's a whole load of things you can do with this you can figure out the exact strain of our viruses and bacteria for contact tracing figuring out antibiotic resistance um you can do any kind of patient diagnoses for rare genetic diseases or cancer and there's also existing work on predicting drug response to certain treatments based on the unique genetic profile of each patient so now on to how nanopore sequencing Works basically you start with an ionic solution and you take a membrane that splits it into two compartments the next thing you do is you apply a voltage across this membrane so any ions on one side of the membrane are going to want to move to the other side of the membrane because they're charged at the moment they can't because the membranes in the way so what you do is you embed a small protein Channel or a nanoport into that membrane and that allows ions to move from one side of the solution to the other through the nanopore now if "
    },
    {
        "start": 306.12,
        "text": "you put DNA on one side it's also going to want to move to the other side because it's charged as well and as this DNA moves through the pore the bases of the DNA end up altering the rate of ion movement through the through the pore because they're both trying to move through simultaneously so what you can do is you can measure the current over time and you can figure out which bases of the DNA are currently obstructing the port based on your measured current so in a very simplistic view there might be four different current levels for each of the four possible bases and based on the current over time you can figure out the DNA sequence and read your your whole read now in reality um this signals a lot a lot Messier you can see it's a little bit noisier and in reality instead of the current level being affected by a single base it's "
    },
    {
        "start": 368.1,
        "text": "actually affected by around five to six adjacent faces because that's how large the constriction in the nanopore is so instead of having four possible current levels you actually have four to the fifth or four to the sixth so you need a lot of machine learning in order to figure out what the actual sequence was from your raw current trace and I'd like to mention right now that one to issue that Nano core sequencing runs into is if you have a repetitive DNA sequence because the current level is affected by only four to five bases if all those bases are the same as you shift over one base at a time the current level is going to stay pretty much the same so if you have a long repeated region it's really hard to identify the transitions from one base to another and know how long that repeated region actually is so here we can see some example reads aligned to the reference genome and we can see that it gets the length of this repeated sequence of A's incorrectly a "
    },
    {
        "start": 430.319,
        "text": "lot uh relatively frequently so now on to the bioinformatics part the read alignment um the simplest way to align reads is with edit distance edit distance is the minimum number of edits required to transform one string to another and in this case the two strings are our expected reference genome and the read that we just sequenced and edits can take the form of substitutions where one base is changed to another insertions where we gain an additional read base that was not in the reference and deletions where we lose a base that was in the reference but is not in our sequence to read and now it's important to note that a lot of times there's there's many possible alignments for the same two sequences even with the same minimum at a distance so in this example the insertion could have happened here where it could have happened one base prior or even this insertion and "
    },
    {
        "start": 491.4,
        "text": "deletion could have just been two substitutions and what we want to do with our alignment is actually end up with the alignment that best represents what probably happened and we want that to be consistent across breeds so that we can identify which mutations are actually present in the sequence so now in order to calculate this edit distance and find the optimal amount alignment imagine that you're aligning your your reference to your read and you've already aligned some prefix of your reference to some prefix of your read and you have some remaining reference spaces and some remaining read bases and a pointer to the first of each of those now when you're trying to align the next few bases you have three different options so ignoring um what you've previously aligned the next base could either be just the base in your read which would be an insertion or it could be just the base in your "
    },
    {
        "start": 552.899,
        "text": "reference which would be a deletion or it could be both bases in both your read and reference and if they happen to be the same it'll be a match if they're not the same it'll end up being a substitution so at each position as you walk through the reference and read bases you have three possible options so there's three to the r possible alignments where R is the minimum length of your read and reference so this is a lot and in order to find the minimum at a distance we don't want to have to walk through all possible alignments so how can we do this more efficiently fortunately the edit distance is independent of the prefixes that you've already aligned so if we've already aligned the first four bases of the the reference in the read then if you you have an insertion it's just going to be that at a distance plus one if we have a deletion that's going to be that at a distance plus one we have a match the edit distance will stay the same and if you have a substitution the edit distance again increases by one so we "
    },
    {
        "start": 614.339,
        "text": "can use dynamic programming to store the best possible alignment um of our read to our reference for each of the possible prefix lengths of both the the read and the reference so what we end up with is with is a matrix that contains all the possible prefixes of our read and our reference so at this highlighted position we've aligned the first four bases of our read and the first four bases of our reference and as I showed in the previous slide from here we have three possible options we can align one more base in the reference which would be a deletion we can align one more base in the read which would be an insertion or we can align the next base in each of the two sequences which would either not your substitution in this case insertions and deletions increase the score and a match does not actually it's a little bit easier to flip this around and looking at this position look at the three possible "
    },
    {
        "start": 674.94,
        "text": "positions we could have come from and again considering the the score increases for each of those movements as long as we take the minimum then we can get the score for the best possible edit distance up to that point for this prefix of the read and this prefix of the reference and at the end after walking through the entire Matrix the spinal cell will be the minimum at a distance for aligning the read to the reference now the only problem with edit distance is that it doesn't account for the fact that one insertion or deletion is going to be much more likely than the summation of multiple smaller insertions or deletions so in these two examples here here there was an insertion of three bases here there was an insertion of two bases and an insertion of one base and according to this the two edit distances are the same they're both three however the one on the left is "
    },
    {
        "start": 735.18,
        "text": "much more likely so in order to account for this and to prefer this alignment on the left when we align the reads um affine Gap scoring was invented and this is known as the Smith Waterman or needleman Bunch algorithm and basically the idea is that instead of having one state Matrix you end up with three so the first state Matrix is you're in the normal State your aligning bases to one another and then you have an inserting state where you're currently inserting bases or a deleting state where you're currently deleting bases now if we look back to the transitions I mentioned earlier if you're starting in this cell you can perform either a match or substitution moving diagonally and there's no penalty if the next two bases match but there is a penalty if the base is mismatch um but we are also allowed to start an insertion or a deletion and here we "
    },
    {
        "start": 796.079,
        "text": "raise this to be a relatively large penalty so inserting a base might be a penalty of five and the same thing if we're starting a deletion where as a substitution will only cost three however once you're already in the insertion State you can continue making further insertions for a lesser lesser penalty and then you can move back to the normal State and end the insertion for no penalty at all the same thing in the deletion State once you've started deleting bases you can continue deleting bases at a lower penalty or you can transition back to to the normal state so what this does is basically there's an increased penalty for each time you start an insertion or a deletion so because you started two different insertions here you're going to get penalized twice for entering that insertion State whereas here you're only going to get penalized once for entering that insertion state so the alignment on the left is going to be preferred "
    },
    {
        "start": 858.48,
        "text": "so now on to variant calling um for nanopore sequencing this is typically done in both steps so the first step is known as pile up calling whereas a a pile up is the alignment of all your reads to the reference um all piled up against one another and this first stage is just to identify candidate variance So within a small window it looks at each reference position what bases are aligned to those positions um a c g or T and using those summary statistics it figures out whether or not there's likely a variant relative to the reference and in this step which is where current nanopore sequencing variant calling struggles it's important that the alignments are consistent so that all of the the changes to the reference sequence are reported in pretty much the same place otherwise you're not going to realize that there was actually a mutation there and the second step is full alignment calling "
    },
    {
        "start": 920.339,
        "text": "where basically instead of just looking at a narrow window you look at a much wider window and you have higher dimensional information about the reads in order to do the the final variant calling and we're focusing on improving the alignment so that we can identify a better set of candidate variants during this pile up calling stage this is kind of what the input looks like um in one of the the current tools called the pepper it's encoded the reference genome in these four colors and then forward mapped acgt and reverse mapped reads um the uh the color basically represents the proportion of reads which support that base at each position and in general they agree with the reference but for example at position 23 there's a little bit of disagreement on whether um the the bases green or red so in a or T and then using a sliding window "
    },
    {
        "start": 982.98,
        "text": "approach we decide at each position whether or not there's a variant using a convolutional neural network so now on to what my alignment yep what'd you asked um how they basically like determine um this particular protein yeah so they're just a general proportional Department of nucleotide yeah so this does um so this input here is like an image which is going to be an input to the network which basically decides for each position in a sliding window so this purple window here will refer to this one position on the reference um and it's trying to decide at this position is there a variant or not "
    },
    {
        "start": 1045.919,
        "text": "so these these color values um these are the encoded reference spaces and then each of the other rows are the proportion of read bases once you've aligned all the reads um to the reference so here we've aligned all the reads you count basically the number of A's the number of C's G's G's and deletions at each position and then you increment the values in each of these correspondingly and it uses this small small image basically to try and make a binary classification of whether or not um the the reads support a different base than than what the reference says is contained at that position did that answer your question or no "
    },
    {
        "start": 1110.059,
        "text": "um this is done across the entire genome okay okay so now on to um the motivation um for my algorithm um but before talking about the motivation I want to uh clarify a definition so in the title of my talk I referred to n polymers but more typically repeated sequences are referred to as homopolymers or simple tandem repeats I referred to them as n polymers just to uh to simplify it so it's more succinct so if you have a one nucleotide repeating sequence I call it a one polymer instead of a homopolymer if you have a two nucleotide repeated sequence I call it a two polymer instead of a simple tandem repeat with repeat length unit two and so forth if we have "
    },
    {
        "start": 1171.799,
        "text": "a three nucleotide repeating sequence and so forth um and the next important definition to know is copy number so I've already defined n as the uh the length of the repeat unit so it's two bases that get repeated um and then the copy number the expected number L is the number of times that this repeat unit length is repeated so L is the expected length from the reference L Prime is the measured uh length from our read and here we can see that GA was was only repeated twice okay so now the the main motivation for this whole project was the fact that for nanopore sequencing um basically they split the evaluation into uh Snips and indels so snip is a single nucleotide polymorphism or basically a substitution mutation where one base is changed to another and an "
    },
    {
        "start": 1234.26,
        "text": "indel is either an insertion or deletion where bases are either added or missing from the reference and now for nanopore sequencing the accuracy performs very well on Snips with high precision and recall but for insertions or deletions it has much lower precision and pretty much abysmal recall um and as I mentioned earlier this is largely due to the fact that it struggles in in repetitive regions um so here we can see as I mentioned earlier um the current level is affected by multiple bases at a time and so when you have a lot of bases that are the same repeated after one another your current Trace is going to stay relatively flat and it's hard to detect transitions from one base to another and know how long this the sequence actually is and here are some statistics for how frequently it gets it wrong so these are confusion matrices for one polymers so "
    },
    {
        "start": 1294.62,
        "text": "it would be a single base that gets repeated and then the percentage of time that in each Square the actual length was called the predicted line so if we have three A's in a row we'll get it right 96 percent of the time but if you have 10 A's in a row we'll only get that right like 58 of the time and if you look at repeated sequences with a larger repeat unit say three bases it does a little bit better starting at 98 but as we move on to longer repeat lengths um we see the same Trend it it progressively gets worse um with around 87 accuracy um and further I want to point out um that these insertions and deletions in N polymer regions are very important um and they're a large fraction of both the the calls that we get wrong when variant calling and also the true insertions and deletions "
    },
    {
        "start": 1355.1,
        "text": "so on the very left we can see the different variant call types there's mostly substitutions a few insertions a few deletions and then very few complex variants and we can see that almost all the substitutions we got correct um but if we look at the false positive and false negative variance the majority of them stem from incorrectly calling insertions and deletions and if we then blow up these errors to look at all the mistakes we made then I categorized the the genome into different regions so other is any kind of non-repeating region in the genome one polymer is when you have the same base repeating and so forth so we can see that in the genome the vast majority majority of it is non-repetitive um so even though more than 50 of the genome is non-repetitive that only accounts for less than 10 percent of the total error "
    },
    {
        "start": 1416.419,
        "text": "um and any of these repetitive regions with repeat unit length one to six they're vastly over represented in terms of the the errors that we end up making and then if you look at the true insertions and deletions so this is based on some form of variant calling ground truth against a reference genome we can see that again the repetitive regions are overrepresented so actual insertions and deletions are fairly likely to be copy number variants um of these repetitive regions and general indels are are much less likely in general so the main motivation for this project is um the fact that alignments end up being inconsistent if you use the Gap I find scoring that I mentioned previously because that doesn't account for the fact um that getting the lengths of repeated "
    },
    {
        "start": 1477.62,
        "text": "regions wrong is very common for nanopore sequencing so here we see a bunch of A's and a bunch of T's and if we say there was an extra to a and one fewer T's then it's going to choose to place it as a substitution at this position whereas if we say there was an extra a but no expertise it's going to choose to left a line an insertion at the start of all the A's and then if we have two extra A's and two fewer T's then we're going to see substitutions at position seven and eight so the issue comes later when you try and do the variant polling and you have these pile up summary statistics or reference position um and even though there was an extra a in all three of these cases when you look at position two you're only going to see one out of three had an extra a and then you're going to see two out of three substitutions at position seven so because the alignment isn't really that "
    },
    {
        "start": 1539.299,
        "text": "consistent there's no one position where there's clearly an insertion or a deletion and that's why to some extent the variant calling struggles with insertions and deletions and as I mentioned the alignment doesn't really reflect the probability um that substitutions and insertions and deletions are actually happening because your scoring Matrix and functions are totally static everywhere on the genome and they don't account for the fact that substitution sorry insertions and deletions are orbs of magnitude more likely in repetitive regions so at the moment you have this one substitution penalty Matrix where if you mess up any of the calls you have this penalty score and for insertions and deletions the cost is always going to be the same whether you're adding an a to a sequence of 20 A's or whether you're adding an A and a non-repetitive region and that that's kind of the issue with with the "
    },
    {
        "start": 1601.159,
        "text": "current alignment so now I'm going to talk a little bit about my algorithm what I changed to improve this alignment um and basically I instead calculated um the actual probabilities so on the left over here I have a confusion Matrix and this is the count in millions of all the reads that I align to a genome and I counted every time that the reference said there was an A and our read said there was an A and the same thing our reference that there was an A but our read said there was a c um and then I have the measured probabilities of all of these different errors and now on the right I converted these to penalty scores um using the negative log probability of one base being called another so basically I just divide um the number of times and a was called a c by the summation of that entire row so given an a what's the probability "
    },
    {
        "start": 1663.38,
        "text": "that it was called a c and then I take the negative log probability of that and the nice thing of about this is that when you're doing the alignment and you have multiple substitution errors um as you do the alignment you sum the penalty scores um and if you're looking at statistics the probability of two independent events both happening you take the two probabilities and you multiply them but by shifting this over to log space multiplication is the same as addition so by calculating the actual probabilities in log space space and then adding them as we do the alignment what we're actually doing is calculating the most probable alignment or sequence of edits that actually happened given the measured profile of these errors in our data set so if you change the substitution scoring Matrix a little bit and same with the insertion and deletion penalty "
    },
    {
        "start": 1725.299,
        "text": "in general but the main Improvement was adding a lookup table um based on the actual rates of certain length and polymers getting called other lengths so again we did the similar um error probability calculation where if a repeat unit is of length n um and it was expected to be length I but it got called length J um then we calculate for each of those possible repeat unit lengths um what our penalty score should be for accidentally adding an A or dropping an A and so forth so we end up with these score matrices for each of the different and polymer lines so homopolymers if you have one repeated sequence if you have four A's in a row actually but you call Three A's then you have a slight penalty for doing so but if you had 10 A's in a "
    },
    {
        "start": 1786.32,
        "text": "row and you called Nine this time you still dropped 1A it's the same change but the score is lower because it was measured to be much more likely to occur um based on the the statistics in your data set and then if you have a longer repeat unit length um again things are similar but the scores are a little bit higher because it's less likely to lose a copy of AAC than to just lose a copy of a single base and we can see interestingly that that losing a base so bottom left of the diagonal generally has much lower scores than the upper right or adding bases because as I mentioned the nanopore signal stays relatively constant um in repetitive regions so it's difficult to detect transitions so it generally ends up underestimating the number of bases that there are so this does end up complicating the "
    },
    {
        "start": 1847.039,
        "text": "alignment algorithm so instead of having one table with edit distance or three tables with affine gap scoring now we end up with five different state tables so we have um M for when you're in the matching State IND for inserting a deletion but now we also have tables L and S for lengthening and shortening these repetitive regions so for each Matrix um I've I've marked with a symbol according to that Matrix so if we're in the match State and we're Computing this cell with the star then the star superscripts represent possible predecessors so if we're at position I and J in our read and reference and we're trying to calculate the score at this state then we move one previous base in the read and reference and we take our substitution penalties for converting this reference space to this read base and that's one possible "
    },
    {
        "start": 1909.679,
        "text": "predecessor the other possible predecessors are leaving the deletion insertion shortening or lengthening States each with penalty zero and then as I mentioned earlier for The affine Gap scoring um if we are Computing cell IJ in the insertion State you can either be extending a previous insertion for a lower penalty or starting an insertion penalty starting an insertion with a higher penalty from the match State and so forth and then for lengthening and shortening things are complicated because um you have to look a number of cells back depending on what your repeat unit length is but basically it allows jumping there from that state based on looking up the the score for that error in the penalty Matrix and these transitions are only allowed if certain conditions are met and those conditions are basically enforcing that we're only "
    },
    {
        "start": 1970.34,
        "text": "going to allow these transitions when we are in an unpolymer at the start of a repeat unit so this allows adding and dropping individual repeat units based on on the scoring in that lookup Matrix and then there's some nice properties of of having this additional possibility for transitions um so first the penalty will depend on what your expected and measured length was going to be so here we can see if the expected length is Three A's versus five A's versus a days the penalty for dropping 1A if you only started with three is is higher because it was less likely but as the length of the formal polymer increases your score for dropping in a decreases because that's that's more likely um and then I'd like to mention that the penalty depends on the length of the repeat unit so losing a base "
    },
    {
        "start": 2031.899,
        "text": "um from a homopolymer is more likely than than losing three bases um from uh and then polymer with with n equals three and I only allow inserting or deleting uh like one entire unit at a time um and another thing to note is that the actual sequence isn't really considered so for a given repeat unit length so here the repeat unit length is three and there's three copies I'm not actually looking at what the underlying sequence is I'm just saying um this is kind of the the repeat pattern and assigning penalty scores based on that instead of breaking it down by each possible sequence because for unit length n there's 4 to the N possible sequences and quickly that kind of grows out of hand and then this alignment does allow considering overlaps so if there's kind "
    },
    {
        "start": 2091.96,
        "text": "of nesting with one repeated sequence within another repeated sequence we can handle that uh last thing to mention was that I did do some banded alignment for efficiency so if we're aligning a read to the reference nanopore reads can be a hundred thousand or even a million bases long so instead of doing o of N squared A Million by a million Matrix uh we already have the alignment we're just trying to modify the fine grained alignment around these repetitive regions so if the Red Band shows the previous alignment um we can actually only look within 30 or 50 bases of that original alignment and now we're doing 60 or 100 by 100 000 instead of 100 000 by a hundred thousand and there there we gain a lot of a lot of efficiency during the computation um and now so the main goal of introducing these lower Gap penalties "
    },
    {
        "start": 2152.619,
        "text": "for repeated sequences is that now if we look at the aligned reads it should realize that getting the length of this chunk of A's and the length of this chunk of T's incorrect that was probably two separate events so we probably added some extra A's here and dropped some T's here instead of preferring substitutions because it doesn't have a kind of Gap penalty for looking at repeated regions so now if we look at the pileup we can see that all three reads agree that there is an insertion of an a here thanks for coming um so now on to the results um we measured alignment consistency using Genie Purity which is basically a squared summation of the probabilities for each of the categories so if we're looking at a particular reference position um we split it up into insertions and "
    },
    {
        "start": 2213.64,
        "text": "then everything else because basically the five other categories are acgt or deletion but insertions are kind of a special case because there's an infinite number of possible insertions at a position so we had to calculate that a little bit separately but if we first look at acgt and the deletion then to kind of give you an intuitive understanding of Genie Purity if all reads say that there was a c then probability of C is one one squared is one so your Purity is one if ninety percent of the reads said there was a c and ten percent said it was something else 0.9 squared is 0.81 so regardless of where the other 10 percent went your your Purity is going to be between 0.8 and 1 um if the REITs are evenly split between C and G then that's 0.5 squared equals 0.5 squared which is 0.5 um so that's going to be 50 Purity um and anything less than that your "
    },
    {
        "start": 2274.96,
        "text": "reads are very discordant and not really agreeing what happened um so down here on the bottom we show this is a logarithmic y-axis so we can see that it's very common for reads to agree or does magnitude less common for for REITs to disagree that terribly um and green and blue are after applying our realignment um and red and yellow are before applying our realignment on the two haplotypes um it's hard to make out the uh the amount of improvement that we've gotten so on the top I show the ratio um per um her position of the Reeds that had that Purity after realignment divided by prior to alignment realignment so we can see that um from a baseline ratio of one we decrease the amount of reads with High discordance um by a little bit more than 50 percent so this this realignment did improve how "
    },
    {
        "start": 2337.48,
        "text": "much the reads agreed at different positions um and then for insertion specifically it didn't really make that much of a difference so here's kind of an example of what the uh the alignments looks like prior to and after to the realignment um and here this is a very compressed um like 50 to 80 reads that were aligned to our reference um and there was a deletion at this position so purple shows insertions the black lines show deletions and then anything colored is a substitution to that colored base we can see that there were a bunch of A's and then a bunch of T's and RV aligner figured out that okay changing the length of a was independent from changing the length of T so it aligns insertions and deletions to the start of those two regions respectively whereas minimap using the affine Gap scoring what it does is if only A's were inserted or deleted then it puts them "
    },
    {
        "start": 2399.099,
        "text": "all left aligned but if both A's and T's were deleted then it prefers one Gap over two like I mentioned because of the gap about fine scoring and as a result it puts that deletion somewhere in the middle of the a'smts and then if there were substitutions sorry if there were extra A's and fewer T's then it will be represented as substitutions from T to a and same thing vice versa so basically instead of having a lot of different representations it chooses to to align them to either the start of the A's or the start of the t's because statistically that's most likely what happened um because we're doing nanopore sequencing and then if you look at the actual accuracy improvements for the pile up variant calling typically they evaluate substitutions and indel separately and they do Precision recall curves um here we kind of have two baselines because the initial variant color layer 3 didn't do free sorting by hablow type "
    },
    {
        "start": 2462.3,
        "text": "so I added that to claire3 in order to get a more realistic red Baseline and then we can see that with the realignment in green we improved substitution precision and we improved indel precision and recall a little bit um and then this project is on GitHub if you want to check it out um I was funded by the NSF um and thanks everyone for listening yeah well if anyone in the room has a question again feel free to raise your hand and talk to questions and if anyone online and the question uh you can put it in the chat and you want to read it through with him uh if you're in the room if you can speak up and and if you remember yeah yeah um so you you use this in the context of all genome sequencing and if I understood correctly you train this what actually happened Matrix on the alignment of the whole genome you're trying to sequence yeah have you started "
    },
    {
        "start": 2524.079,
        "text": "with changing what you've made it if you're trying to focus on realigning things inside repeated regions have you tried feeding an information specifically from those regions and seeing how that changes the output um I don't know if that would help at all but it doesn't let the benefit is not necessarily in the SNP rate like at some point of the change the idea is to get those the ingredients to be fixed it could just be the information from there try that yeah so the question was um basically when here when I calculate both the substitution penalties and later um the uh the repeated region Gap penalties right now I'm Computing it based on the alignments to the entire sequence um the question was would it help to compute those scores based on just the alignments to specific subsections of the genome even like yeah yeah so the the reason that um basically I aligned it to the whole "
    },
    {
        "start": 2584.68,
        "text": "genome is so that basically you can get both the general substitution penalties and the general insertion and deletion penalties so if you're in non-repeated regions um and then the N polymer lookup table which is these matrices they're only being so this Matrix even though it's like aligned to the whole genome um the number of counts in each of these is only going to be incremented if there was actually um like a repeat of of Chunk three um of a certain line so it's kind of all this Matrix is kind of already only looking at 10 repeat regions were you wondering like if there's it's looking at 10 repeats like as they occur but planning is their region the G phone the reason regions of the genome in which that is a lot more common than it is yeah that's a good question um so basically right now it's kind of implemented um so that basically like you can provide a bed file to mass whatever regions of the genome you want to "
    },
    {
        "start": 2645.94,
        "text": "compute the statistics on uh basically because like I was providing this as like a general case tool I just did it for the for the entire genome um but yeah you're right like if you're interested in like specific regions um then the really nice thing about um like calculating all these like statistics and setting your scores based on what your data set actually contains yeah is that you can just redo that if your data set or regions change so it would totally be feasible to recalculate this for a specific subset of regions that you're you're interested in yeah clear the question online Damian is wondering what is the basic input for your software is IT band file DCF files yeah so the basic input um is just a bam and if you want to recalculate the statistics like that's an option and you can provide a vet file for like where in the band you're going to recalculate the "
    },
    {
        "start": 2707.02,
        "text": "statistics from you can also just take the pre-computed statistics and you just give it a bam and it gives you a new bam um just with like the small realignment changes yeah you're saying basically like these these matrices especially like for longer likes if you have like a smaller subset or something it could be completely unstable um and how do you kind of make sure that um the scores still makes sense um so basically I kind of glossed over it but um I do apply some amount of smoothing and imputation to this Matrix so basically I enforce certain properties to make sure that all the scores still make sense um so basically um I enforce like a few different "
    },
    {
        "start": 2767.5,
        "text": "properties one is that um if you have um say a one base deletion that's always going to be more likely from a longer homopolymer than a shorter one um and then a few different things so that basically um at lower repeat lengths as long as you have like a reasonable region that you're sampling you're going to get plenty of examples and good statistics but you might not for example ever get an example of a four polymer that was repeated 57 times so basically some of those values are imputed and there's some amount of smoothing to make sure that all the values are reasonable yeah um I was also thinking about um kind of like this affine Gap scoring people use like different functions for that some people instead of just having like an insertion score and then a like sorry an opening an extension score "
    },
    {
        "start": 2829.839,
        "text": "um they either like Smooth that out into like something actually like logarithmic um or whatever so basically instead of doing my own kind of like custom smoothing I could also do some kind of best fit um that would be like Tire dimensional but would also work yeah the problem I hope reset men for this employers were yes I think the preset start at 50 by 50 or 100 by 100 um and then if it's longer then I would just take the value in like the last row so like 50 by 50 and then just say okay the same same penalty for the same amount of deletions as that questions don't see anything else online either so thank you again "
    },
    {
        "start": 2893.31,
        "text": "[Music] oh can you bring back a little bit traffic again oh yeah yeah um oh my God I'm gonna grab some pizza I mean that's what I'm gonna do for um [Music] you know yes "
    },
    {
        "start": 2976.98,
        "text": "how nice yeah [Music] yeah I know I know yeah "
    }
]