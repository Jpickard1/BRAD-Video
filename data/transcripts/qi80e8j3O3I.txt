appreciate everyone coming this is the last seminar for the spring we'll be breaking for the summer starting up again the fall if anyone is interested in presenting in the fall I have started scheduling people so please let me know if you were anyone your lab is interested I'm always looking for speakers so today I'm pleased to present Huey Jane from biostatistics who's going to talk to us about unit free and robust detection of differential expression from RNA seek data and there's a sign in sheet going around so if you can sign that that were you pursued thank you thank you all for coming today I really like this seminar series and if it's a lot of promise so very glad that I'm contributing back to it so so today I'm you know even though the title looks like fancy but today I'm just going to talk about a very simple I'll put it this way very basic problem all of you know very very very well about it so the and is a very short only one page so I'm going to basically I I like to have sort of like a discussion to really learn from both of both of us so even though this is a very basic problem and people propose many different approach to it surprisingly to my understanding there's not a satisfactory approach even all day so I'm going to put away it my understanding on my approach but still it's not perfect so discussion welcome okay so I'm sure all of you know are any sick very well by this time of death you know development so I go through very briefly this is re sick we take our MRI from the sample break it down into small fragments due to the limitation of the sequencing technology and then you know prepare the library and go to sequences in the end we gather the reefs right and is our ASIC is popular because a lot of advantage is high throughput you know getting cheaper and cheaper and give supposedly better measurements from gene expression than microarray wide diamond a dynamic range you know accurate digital counts and also you know is capable of doing things are novel like novel transcripts novel splicing gene fusion kind of thing but even though the talk today is we're going to talk not going to focus on that so we're going to talk about this basically basic problem how do you qualify gene expression from our isolate so a typical pipeline you know processing or a series like this which first mapping reads to the genome or transcripts then we summarize the read you know each gene which acts out how many reads you got and then we quantified the expression there are a number of software out there because you can use whatever the results are more or less similar and then you summarize your data to some sort of novelizations and then the panel your study design i do differential expression analysis i mean this is a very general term it could be like a two sample comparison could be multiple sample comparison but if you have the outcome it's like a continuous you could run a linear model here basically by differential expression analysis i mean you do the statistical analysis trying to find a gene which is of interest to your biological study okay so today I'm going to focus on this single thing normalization but turnout is to be critical because it slides between quantification and you know follow-up study and turn out that my understanding is that normalizing it's so critical that it should be incorporated gather with this study okay so this is a card home demonstrating what kind of data we get from rising right so we prepare several biological samples or we might think that they might come from different condition one could be control one could be treated you also have replicates you know trying to control for the logical variation then you do the sample product label sequence in the end you know after you map the reads you can't you got a table right like this so basically for each gene suppose we are only interested in the gene level for either gene we got number of reads from that sample so this is a table suppose it would be has like 25,000 lines because there are 25,000 jeez we have one table per sample another people another sample right these are the data we got with our so this is analysis start from here so this is the data analysis we want to basically based on this data from the rnc experiment we want to go back trying to make inference on the original sample what's going on in the original you know sample you know what gene has changed and things like that okay and this was the among the very first paper forensic in 2008 and they basic deal with the problem how do you quantify gene expression from IIT an idea is very simple right so after you align the reach back to the gene so it said in this gene gene wine experiment one has six weeks and so on so forth and different experiments could have different total number of risks due to a lot of technical issues of you know cost so basically how do you use this read count to quantify gene expression and these people they proposed this rpkm in their very first paper forensic but it's still the most widely used approach to quantify gene expression it's as simple as this you take the number of reads for that gene you divide by the length of the gene some people correct for the boundary in fact things like that but basically divided by the length of the gene then you divide by the total number of reads for that experiment some people use the total number of Reese you know mapped some people use the total number Reese you know regardless map it or not no matter what you divide by this - you got an RT care and they show that our weekly arm works pretty well as the gene expression level you can use rpkm for downstream downstream analysis like differential expression you know things work pretty well for this thing to approach but if you look you know this will never mention the paper but if you think it you know harder you think it through you'll see that the rpkm approach actually made lots of assumptions first of all it assumes that each gene has a well-defined lens to divine right it's not true because each gene we know that they have many alternative splice transcripts they have different lands right so people develop you know approaches trying to take into consideration that be composed of all the reads from the gene in two different isoforms you know there are software doing that which is not a focus today but you know there are some improvement over this assumption also the number two assumptions by demanding the lens of the gene these people are assuming that you know basically you got a uniform sampling from every transcript at every position right this turn out to be again not true technically but close to be truth but there again a lot of kind of bias like three-point bias like all the non uniformity there are approaches and software trying to do with that again this is not the focus of that and assumption number three only happens when you use when you directly use the RPK on for downstream analysis if you use that for example you you qualify rpkm from a sample and use number as the gene expression level from that sample then you compare that gene across many different sample directly without doing additional normalization then something you're making is that actually the total amount of nucleotides is constant across samples so this is very actually tricky because there are other thing besides our became they make different assumptions this assumption you know is is a Paris not advisor essential basic close because most of time the sample we compare come from same cell type you know you imagine the south have similar size similar a molecular weight so they're a physical mass it's about the same so it's not too far away so there are many approach parallel to rpkm trying to improve on that some people directly use the read counts for differential expression they build all sorts of a count model like poisson or negative binomial final model some people normalize the count by the sequencing library depth but now by the Dreamlands some people you know this fpkm is essentially arctic air and just taken care of the read pairs and some people argue that our weekly on is not good because the are became value is not essentially what you want for gene expression they further normalize the are became value within the library within a sample say karttikeya so basically it's normalized are became within the sample so all of these apps are i call them different units for data summarization but other people actually some people Harlem normalization we can call them with the in-sample normalization because when you do this you look at one time of the time okay so they argue that ppm is better than rpkm because ppm tell you the relative percentage of the transcript basically if you sample one meeting transcript in that particular sample how many copy that you assemble a particular transcript there are go this is about it but - sunny is that this is just making a slightly different assumption than our PKA if you remember the RBA I'm assumes that the total molecular weight of the two side of the same well the TPM assumed the total number of transcripts of whose are the same and then you may argue one of something is better than the other but to me they all sort of arbitrary assumptions okay now I talked about the major point you know triggers all this called kind of work is that our ASIC measurements are intrinsically relevant not only are I say actually my call rate coupies are all the same they are intrinsically relative in the sense that you can only imagine expression relatives within a sample so if you got these freshman value for wandering is to the other genes one you say that the other you know the first gene is twice as abundant as a second one within that sample when you compare the gene expression across sample you have to make assumption because it's this are the intrinsic relative Ness here I will demonstrate suppose this is what happened in the in the biological sample right so this gene is 2.5 times abundant as the first one and so on so forth and this is your data after your sequencing here I use rpkm but you could use any other Union I mentioned earlier basically you know you gather here I just want the same number to illustrate I okay you could get a good estimation on the relative abundance so what years in another sample due to some treatment we operate regulate one gene so this gene go from two to four point five five about to fold up other gene are more or less the same not affected by the treatment okay so that's the ideal of in the hypothetical idea of this treatment then after your sequence and no matter how deep your sequence no matter how you summarize the data using whatever approach you I mentioned earlier this is about what you get so this gene will be higher than the other one but not as high as it is what it really was in the biological sample and all other dream will go lower and the reason is very simple because all of these transcripts that compete for the sequencing reads right if one transcript goes more abundant it will get more reach and relatively all other times you get fewer read so in the end no matter how you summarize the data this is what you get if you compare this to this you will see all the genes are changing even though you in reality only the last one is changing and this one this problem won't be solved even though you have replicated replicate performed similarly that's why actually this is a problem not only to alright see when you do a qpcr going to do micro you also have to normalize or something you have to assume something it's not changing across conditions like the housekeeping game that's why women do cuties are just how CDD but you're assuming the housekeeping gene are not changing right you really realize how assumption so here is another view of this problem so this is the truth world change of origin say most of the genes didn't change before and after the treatment there kupo didn't change center around one and a small group of gene say they go up by two fold so that's the underlying truth all change before and after the treatment this is what you get by observing the fold observe the full chain in the data so you know the the group of gene really go out they still go up but not by two fold but less than 2 fo and the majority of gene will go down a little bit because this relative missed and the normalization is basically trying to find a baseline trying to correct for that otherwise if you don't do normal if you don't do between sample normalization you will you will think all the gene on changing basically and that's wrong and this is why the between sample normalization is essential and there are so many a protest evallo for that for example the most popular approach for microarray this card of normalization on how normalization it assumes you're basically the the gene expression distribution in the bar along with the sample across different conditions are the same it's a very very strong assumption so that there is always final boss but also you lost a lot of signals because the full change will be skewed biased and so on and so forth but they have to do that because if you see here are a sick you know even though it's relative but it's relative it is relative within a sample right but for microarray a lot of a problem happens because a very low abundant gene of a very high abundant gene they are either background noise or saturation so it's not like you can shift everything easily so con halogen performed really well microarray data lets artistic data starting from rpkm you can't think rpkm and some sort of normalization right then ppm is another kind of normalization and then people find those are not robust because in RB k i'm a ppm if one transcript or small group of transcripts for some reason of source a lot of wreaths in one sample now all other gene in that sam who will go down and that's not good so people want to correct for that so instead of using the total count of that library they use for example the upper quartile the 75% hotel basically remove all the gene which are very very abundant for whatever reason use the 75 book called help but you could also use the meeting whatever you know result will be different but the same similar then you know these two are the very popular approach for differential expression and they propose different kind of normalization approach it's fairly ad hoc and you know sophisticated but idea is basically to make the normalization robust so examine your edge are they use the tree in the mean we know that the mean normalized by the means the same as normalized by the total sum it's not robust but if you trim the mean basically throw away everything you know change a lot on the two extreme used the middle part it's more reversible and then this one use the median the meaning is more robust than the mean so they basically they propose this kind of normalization why because you probably agree with me by now is that normalization is very essential for downstream differential expression if you don't normalize well the downstream analysis no matter what method you use will not be reliable and the up to here of some people actually brought up this idea this normalization and differential expression are actually intrinsically the same problem because ideally if you know what genes are changing what they are not changed you should only use the genes are not changing to normalize right but if you don't normalize well you cannot get the second problem solved so actually they are three sides of the same problem so why not model them the same way so Li at all they propose this iterative normalization basically they use whatever method to start with and to do differential expression and then use the gene which are recognized as non differentially expressed than to the normalization then III until converge okay this method works pretty well and it was actually also proposed even back to ten years ago for microarray study but this kind of method never got really popular and the reason is first of all in most of data set it doesn't really make much difference okay the difference is very subtle and also typically when people using iterative methods you are trying to optimize on sort of function but it's unclear what function is this metroid matter is optimizing without knowing the function it is optimizing it's very hard to understand how it works and in particular this method even though it doesn't make much difference it actually depends on the initial point so if you start from a different initial point you end up with different solution so because you know this interest just because we don't really know understand what this is doing so that you know we don't really want to approach which depend on an initial guess right and then of course you can use housekeeping gene but there are also arguments what gene to use right so basically there are so many methods of proposed for this problem but it's not fully solved so to understand the limitation of between sample normalization let's imagine this simple scenario where in this sample 50% of genes are not affected by the treatment the other 50% are affected by the treatment and they go up by twofold okay so this is the truth of change and the accordingly this will be all observed the full change by in another study supposedly 50% of gene are not affected by the treatment but the other 50% are affected by the treatment and going down by tooth or so this is the true folk change if your computer observed function you will see that they have the exact the same distribution on observable change basically these two sample are the same except that their gene expression level are off by two time things are Asetek only give you relative measurements from your data they seem equally likely but in these two case here this 50% gene are non differentially expressed here the other this is the opposite the other half of the gene are not changed so this is the limit of RAC or you call it between some normal normalization if this is what happening no one can really know what you're not changing because this is exactly the opposite and their data they give you exact the same days okay so one simple way to get rid of this leaf act is basically to assume no matter what you do last time 50% of genes are changing right that's essentially the assumption that is meaning or me approach they are making okay if you have more than 50% you cannot do trim the meat right you cannot trim off like 50% from one end okay so if you go back to this so if you look at this the only thing we need to do for normalization is to find this baseline if we can know that this was the baseline then problem solved so all of these different ports just trying to find the baseline the meme might be here the media might be closed by the trim Denis might be close by but now them actually give you this this this true baseline the only thing give you the true baselines the mold but no one really tried that and that all of these are hypothetical this is what's happening in real data if you have real artistic data you compute a full change based on our reach this is the kind of fold train track so how to pick the baseline because if you change the baseline all the be around the gene will be changing right so shall we use the trim the mean the meeting or perhaps the most no one really knows you know which one is better it depends on underline because we are making different assumptions but it seems like the mode works better than the other the rest so that's all that's the essential idea of my approach and it's just you know in practice you cannot directly compute a full change and take the mode you could do that but it's not ideal because the full change has uncertainty in it right because there's a sampling there's randomness if you do the experiment twice the for change will change a little bit of mode might shift a little bit you want to incorporate everything in the same physical framework but the essential idea you just use the mole as the as the baseline and use mode I the baseline doesn't solve the problem of the limitation here right because at this case you have two modes are equally high you don't know which one to pick however this is only hypothetical in reality is I think it's never happening that you have 50% of gene are changing caused by the treatment and they all go up by exactly twofold you can never make that happen right so even though you have more than 50 percentage in are changing in reality they probably go up or down at different fold so basically this peak will be flattened down and the remaining peak will stay high that's why the mode actually will work so in practice the mold approach actually can works even though you have more than 50% Aegina changing and it doesn't matter whether they are going up or down symmetrically or asymmetrically it doesn't affect but also affects other approached quite a bit okay so this is assembly our idea we just pull up this model which solve both the normalization and the differential expression simultaneously okay and if quite robust doesn't rely on assumption you know the other methods are making and as a side product as a byproduct this approach is independent of the unit remember that all the data summarization including read counts CPM are PK and PD on this model doesn't really care whatever Union is summarize your data you put in this model the inference are exactly the same identical which it I think it's nice because nowadays you know sometimes you don't get the raw data you got some summarization from other people and it's really hard to transform or convert between different data summarization and if different data summarize didn't give you slightly different results that's annoying but this one doesn't it's not a fact it by the unit so you can give me basic rpkm ppm comp whatever I just plug it please all the same so this is a simple model here let's for simplicity just talk about a two sample comparison problem just the essential the peep has kind of bad study so we assume that you know in the first group the log transformed the gene special value is normally distributed with the gene specific varies it was a gene specific mean and Wizards library adapts of you know basically baseline which we don't know all of these are unknown in the second group we have the same Varys and the baselines shifted by a gamma so if gamma is zero which means the two group have the same baseline for the gene which means the gene is not non differentially expressed if gamma is nonzero which means the genes differentially expressed so in the end we just want to know what gamma is zero or nonzero or we can rank them by their value of gamma which is the fault change because this is on a large scale so gamma is a lot fold change exactly so you can we can easily put together a model of it by turnout we just try to put this model the problem is not solvable because it's over parametrized too many parameters so we have to add a constraint basically to peak among all the possible model explained the data equally well pick the most parsimonious model basically with the smaller number of genes are differentially expressed that's why we put a penalty here it's a l0 penalty basically we want to pick the model which has the smallest number of differentially expressed genes and then we just maximize this function is it the log lapi penalize log function we match my list function we get everything okay the difficulty is that if you know this penalized model optimization you know that l1 pal l0 penalty is very hard to optimize because this whole function is it's non-combat and it's actually high dimensional non convex but turn out that this specific model has this specialty in in the sense that we can actually transform it into a one dimensional non convex function which can be easily solved using an exhaustive search ok you cannot solve none how much problem in high dimensional in general but one dimensional is not a problem so this is how we solve it basically you you first normalize within group they all normalize between group you don't determine the threshold remember here is alpha is a threshold peaking the threshold is always hard in this kind of regression by eternal that alpha is linked back to the key statistic for each gene across two samples so we just used for example that the typical T statistic alpha level we can gather corresponding alpha so we don't have to choose the cutoff and in the end once we have this we have essentially this one-dimensional normalization problem to solve one dimensional non convex problem to solve and once we solve it we can go back get all the gamma and the MU and so on everything so we got okay so first we do a simple simulation so we simulate from our model we have 1,000 gene you know we peak 100 out of the 1,000 gene and we let them change basically they either go up go down random change and this is the gamma soft just ran out of battery this is a gamma soft by our method basic Harris 900 gene are not changing the remaining 100 most of them are changing if their change are too small we consider them not changing okay and then we push the boundary here we last 400 gene are changing no problem and the change go asymmetric so all the jingo on at this moment actually other matters start with it because they somehow if you consider the meeting or me know cause it's on whom the full change are symmetric but here they go all all of them go up our method still works find the baseline find the differential expansion then we push it further we left 700 gene go up we still find the baseline so this is very very hypothetical is that are happening in reality but all the methods fail at this moment and if we push further we left 900 gene change our matter fail as well so basically you know this is an unsolvable problem at some degree but we push the boundary slightly further so this simulator is so this time we simulated from our model the lognormal model and trying to simulate realistic all sorts of a scenario for example this figure is like this we have 30 percent of genes are differentially expressed in the simulation and we have no outlier to it because a lot of times I'll lie for example gene give a very very big cop to see whether one of the matter will fail here we all are yet and amount of 30% gene are changing we left 50% go up 50% go down randomly ok and these are basically the number of gene Pataki has differentially expressed and the number of you know false discoveries so you want a flat line which perfect so basically thank you this is too small to see but we compared to all other existing popular matters basically this scenario is to single right 30% change symmetric no lawyer so all the method performs similarly and similarly for other cases so even though we have 70% gene are changing if the change is fully somatic other masses actually works if you find a Meno meeting works by in the case that we have a symmetric fold change or we have more than 50% gene are changing 70% you know 70% asymmetric the Green Line is our method or the method perform like this because if you fail to pick up the base line you'll fail to do the further expression and this this is the summary as a table so we have different percentage of gene are changing at different percent of gene going up or down add a different percent of outliers and these are a you see of all four methods edge are dec Lima and B so the tema was developed form or microwave but recently people find it performs actually equally well as other methods for financing so the others underline they you see are the the best perform the one so we'll see that in the easy case all the methods perform very well so in this case for example edge are performed the best but all the methods perform almost the same right but in cases that is very difficult our methods still get 80% AUC but all other method get less than 50% a you'll see you know that we have less than 50% a you'll see you basically you perform worse than random cast right so this is the simulation from our model the log normal model you probably know that this one assume also assume of a normal model if you apply this one on a log transforming expression is basically log normal these two assumes negative binomial so we also similar not negative binomial data and the result is similar so basically no matter how you model the data the the problem is still so finally we do a real data experiment with peek to sample from a CQC project one I think is the brain sort of a tissue the other one is a sixteen human tissue combined so if you compare these two sample almost all the gene are changing you know there's very small percentage in which I may not be changing your column hostb gene or water so that's a lot so to solve the problem we we pick 20,000 gene out of this 25,000 gene from this to sample each one has four replicate and we consider them since they come from totally different tissue we consider them as different expressed then we take the remaining gene and as now the private Express and we take the read counts from the replicates from the same tissue so that they are from the same tissue we know they are not changing so with now we know the grounds five 55,000 are changing 25 not changing you see that this is the extreme case and we compare all the methods so this is a result it's at the AUC of our method this is a you see all three other matters so we basically we know that all there are actually several comparisons of different expressing software recently and they basically the conclusion is that all of them performs similarly some are battery in some occasions some are not and this is the same conclusion here and this is the AUC power is better the false positive rate power is much better the false negative rate are all similar because there are genes which are changing but we don't we didn't pick it up okay random so distributed over express range randomly pick from the 25,000 gene so you can think the the full change probably be small as symmetry but still it's not perfectly symmetric so that's that's about it and this is only for to group comparison but you can push it further for multiple group comparison you can do like a linear model kind of thing but for the linear model case the the optimization turned out to be very difficult but you can replace L 0 by L 1 then that leads to convex optimization then you can solve it numerically but it's less robust than the ll0 penalty so this is the joint work with my student here new journey in about that who is also here today and we have a manuscript available online and archive and tearing is also develop mark our package so if you're interested we can send you the source code for you to practice thank you all any questions what do you do with genes that have zero spread zero comes oh we just zero in one sample that both yeah it doesn't matter I mean to take Lobby you have to add something to it so example you add one to it and take loss if you like pseudocode is yeah basically yes we don't do anything special and now the other software do anything special with them estimation that they used things to use they don't use the counts of whether your different method use different approach right say this one I think it's using the are rpm this one is its using accounts but also adjusted by the library size estimated using ad hoc approach this one also use the large transformers expression value same as here but we think the law transform expression value but expression value can be anything you can take you can put in the block transform comes here as an expression value we gave you the same result so when the performance is better because you simulated asymmetric signals that's where the performance is going because so to consider that could the user just the kissy-face we had distribution of the two groups a steam engine starting alarmed about it yeah this is about what typical your gap you can call it a symmetric and how an asymmetric is it's never perfectly symmetric and the the we also compare for this kind of typical routine case basically all the masses perform similarly even though that the hop genes are not exact the same list but basically you know the signal is basically there if you know the baseline you normalize against the baseline you computed heat has resulted it's already there the Lima works as well as all of them happy so in our case actually all the software are similar that's what I can see now you are absolutely right you can you can do in people look at typical what you get I think is if the user is a very experienced to use it always look at the distribution of the tributes and what they are grossly different this user should be quite alarmed I'm very hesitant to do probably want to advise the biologists to shift its baseline yeah yeah in practice for example the project that Steve we work on on the muscle sequencing RAC we got different percentage of mitochondrial grease in different muscle sample runs from like 10 percent to 50 percent that actually buys all other reason it's very hard to decide at that moment what to do you you ignore them you include them change the results so why do you think the parsimonious assumption is best real situation if the real case is one you do have a higher number of that yeah yeah you will fail at that moment I think is that if you know for sure a group of gene are not changing like you know for sure the housekeeping gene you should go with the house structure this approach is basically trying to artificially or algorithmic very pick up a set of genes smaller set of actually larger set of gene has housekeeping done so try to find the largest size of gene which I'm not changing between the groups have you tested housekeeping genes specific from real data just cuz you know like how much change is a really in that we didn't press but from some paper I read is that you know the most of the house to begin thinking can easily changed by two fold between different samples and the to fold is a lot in differential expression so one gene is never reliable you need a set of housekeeping genes how do you kill the turnout that this this alpha will basically determine the lambda the lambda will determine whether you use truncate this one to zero or not and in the end if you look at this cutoff it turned out to be exactly the same form as a key statistic of that gene in a two sample so basically is the normalized the difference in the expression average expression level in the two sample so in the end we use the T statistic use that you know 0.05 0.05 depend on how you pick it we use point on five when point nine five alpha level for the T statistic to determine alpha so we're not doing anything like a cross-validation kind of thing but you have to pick out yes so stretch coding is actually in tickler abalones yeah basically basically if the normalized difference of you know to sample are small enough you consider it's non differentially expressed at zero right so the alpha basic value of the threshold in that this that's another idea as well yeah there are also other other paper I mean this normalization issue as as I mentioned was there you know microarray data as well right some people pick the largest side of gene which they keep their rank between the two condition right this works also okay I think basically for the easy case all the matters for work similarly right for the difficult case I haven't compared to that because there are too many matters out there but I think for the difficult case this one is at least as good as any other matter that could help with asymmetric tails but that's also not apologies Murray I think it's very smart and but it's still a little bit I would say ad hoc in the sense that you don't know what function you're maximizing I like to put a framework at least this is a likelihood function right so if you if your assumption sort of is true then this give you the best solution but it all depends on the assumption another question they're in it can heal from a solution called business a non-convex problem yeah we do an exhaustive search so yeah we are getting Emily guaranteed so you use another Barrett yeah we know that we are maximizing the objective function because we do a exactly search on the one dimensional non-combat function so we basically can find the maximum and quite fast yeah this is negative log likelihood so yeah we are minimizing this how long does it take for example how long does it take for example to run it runs once you pull all the sample together right it runs as a matrix of samples and take almost no time it's like 13 seconds so it's very fast no more questions thank you