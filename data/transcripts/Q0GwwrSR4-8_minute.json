[
    {
        "start": 11.269,
        "text": "that's a great turnout so I do see a few things so comment endless seminar series before of level spiel it's just basically an opportunity to start to learn about new tools technologies methodologies and new tools good they might be new they might currently in development there may be something that's been around for a long time we've just been using that way so there's a sign in G I think most people sign in at sitting it's product I'll just stick it back um but you have to sign in please do so or just also specified just as long as you're good things they want to continue offering so that I was introduce our speakers today speaker is and she got her PhD in computer science with that focus informatics from Wayne State and it's currently a lecturer you're in DC Fe and today I'm gonna talk about something a bit different which is "
    },
    {
        "start": 72.84,
        "text": "qualitative change detection approach or therapy so this is a paradigm shift from most approaches and to find three points and look for treatment my approach is trying to find ways to prevent the disease and defensive steps words not what's keeping the healthy state after this this typically happens when people are trying to look at data from disease and try to find treatment for it we propose a time shift for keeping the healthy state we're still going to use data that has this one and control but in this case we'll trying to find when the ships happens from normal to disease to be able to keep the normal the framework is that "
    },
    {
        "start": 133.4,
        "text": "we have a lot of data that's coming in there right there are high-throughput technologies that are being developed and more and more data in collected and more and more high dimensional data is being collected so we have a lot of complex data available and there is a challenge because technology is not picking up extract the knowledge from the data and a typical high-throughput experiment contains two different phenotypes contains control and condition disease and healthy and it extracts either a set of genes for a set of pathways so we have parts of parts of a part of a set of genes that have interactions between them so we have gene parts inside we have sensor reads which is the expression data and this is more about engineering technical look at how how we can interpret expression data "
    },
    {
        "start": 196.159,
        "text": "in doing search and we have another dimension that typically is not included when you do pattern analysis which is time series when you when you are trying to identify systems that don't work if you carry don't look at the time so methods that look at part set usually find patterns methods that look at the data that comes from the sensor from gene expression is different they do enrichment analysis they find what these different what elements are different we can find patterns we can find new blueprints new pathways new networks and we can find differences using those blueprints and network which is a nice mathematics class topology which is pattern analysis so we can account for the interaction between those parts which typically the part set against its don't and my my research my interest was like lying in between finding "
    },
    {
        "start": 256.609,
        "text": "differences using those blueprints using the pathology and also using time in this analogy and this is this is called the systems approach because it looks at the whole system instead of looking at just one gene at a time and saying is this different than different typically changed act in in tandem they know type things isolation so that's why a systems systems approach is a good approach it looks also at emergent properties of the system that might not be available if you just look at one part the example that I that I only leave is we have a car the car has the different parts unless they're put together the car doesn't move so that's an emergent property that you can have only if the system is put together so looking at the system's view like a group blueprinted engine of a car we can we can identify what's being what's wrong in the system and those new "
    },
    {
        "start": 318.02,
        "text": "prints by the map of an engine of a car are biological pathways and we have different types of pathways that are available that have been constructed using knowledge we have thing any batteries will change interact with each other we have metabolic pathways where we have a compound that gets transformed in another compound and that reaction is is being catalyzed is being sped up by an enzyme we have different types of pathways where we have different types of nodes like component nodes and process nodes which are in the ad the NCIC 230 interaction database and we have this type of structures because we want to store this knowledge that we gain from doing experiments and identifying interactions between we also "
    },
    {
        "start": 378.259,
        "text": "have direct lead interactions and under direct interactions in undirected interactions so we have a lot of complexity in the pathways - and also paternalism portent telling us which part of the system doesn't work participatory describes specific processes that might be disrupted in a condition so use this to be side into the disease or to gain insight into that and how we do that we do an analysis that ranks the pathway identifying which are the most disrupted ones in that condition so typically a path analysis tools takes takes information like molecular interaction all these phases pathways and biological experiment data it's an expression methylation protein protein expression it does a few processing of the input data it doesn't matter metrical statistical modeling to scored the genes "
    },
    {
        "start": 438.409,
        "text": "and then this called the pathways or score the proteins and then score the pathways and then after the purpose coding is is done we have either a significance assigned to that score or just the scored and we rank those side effects that's the speaker process for a path analysis but now we're here we are using time and I think time is a very important dimension in to integrate in section analysis there there have been a lot of this is a bit of an overview of the methods that have been developed so far for path analysis and they're being methods developed for metabolic fat analysis much fewer than the ones before seeing any fat reanalysis and none of them are using time so here there is a different view of the same method they are categorized by the type of scoring that they do aggregate score in linear nonlinear that the scoring for the fatty level and we have note scoring level "
    },
    {
        "start": 499.429,
        "text": "which is the gene for the protein scoring which can be used using graph measures using similarity correlation using probability approaches or using normalize in value or node valley so we have a lot of approaches that have been developed and yet none of them include time that's why this is a even better overview where we have path analysis methods that are more than thirty that have been developed gene set analysis methods where the topology the interactions between genes as you see here are not considered we have those we have methods that that inferred networks that discover new relations in England and we have only four methods available that actually try go see d'Artagnan in in a way or another so that that's the scope of this research is to develop a good method for integrating this this type of data and "
    },
    {
        "start": 562.149,
        "text": "extracting knowledge and answering the question when does that change from normal to this happen when does when does that change from the effects of the drug happens in the organs and I named I could take the positive changes in biological system and it's QCD that's an abbreviation of it so the motivation was to identify systemic transitions between states of the system oh we have an organism doesn't go from normal to disease in a rapid progression called diseases like Alzheimer disease it can take a long time until the disease manifests but changes happen behind the scenes and changes are accumulating until until any potentially irreversible change occurs for instance I have an example here for liver damage we have 30 liver damage liver and then we can get "
    },
    {
        "start": 623.929,
        "text": "to liver cancer if no intervention is being done and this is similar to the idea of wearing a fitness tracker for instance that can track your heart rate that can track different different inputs from from your body they can track how well you sleep they can at some point maybe even track swept content and they can identify if changes happen in a certain direction and they might actually allow YouTube to take steps in to preventing those changes to progress to the disease but as the overarching goal the wealth method that can be used to detect the systemic changes and this changes if you look from one single time point for another single time point they might not be readily available because the is progressive so either you you see the change when it's already at the DC state or do you see that or you don't see the change "
    },
    {
        "start": 684.009,
        "text": "because it's between two states that have to close to each other so the application is identifying disease before the fan site is present then Isis contact is to the detection of field intervals uses use this practice and sequential data is those blueprints the gene gene networks and use this time for data because if I stood enta phi an interval of time and why is this it can use different types of sequential data you can use neuron signals you can use this is progression and that's not fine course but it still sequential data if you progress from one stage of the disease to another it output change intervals and is designed to identify when that gene had an existing method so far and not suitable for this and that's a bit complicated work so that you know there is a need for this this method to provide machine learning they have no "
    },
    {
        "start": 744.399,
        "text": "labels supervised machine learning there are no labels in this case we don't have the disease and know this is labels for this kind of data to use a Richemont analysis there is no let's finish team self in this case because we have this time force to do pattern analysis regular pattern analysis there is no final type of paracin again there are no labels to compare so how would we do this existing methods are not suitable as they are to answer this question so what we thought of using is one of the existing methods and incorporating that that's time course information into it so how we did that we take experiment data which are different times or different sequences and we take the system information and for most organisms we do "
    },
    {
        "start": 804.89,
        "text": "have a lot of batteries available we have pathways that describe some of the processes in in that organism in those cells and we use that as a guideline to the interactions that happen between between the components of the system we compared every two time points so we still have to do comparison but we don't have a reference we can't compare just with time zero because we don't know if that's where the disease starts you can compare with n zero because we don't even know if that's the normal thing so we just make all the comparisons because the goal is to detect if a change happened during the time that we captured this data and after we do all the comparisons we compute we evaluate if the system our system that's given by a pathway by the network has changed during those two time points and we can "
    },
    {
        "start": 866.42,
        "text": "do that with any pattern analysis method because it does compared to two phenotypes from time zero and ten ones from time zero and times six times six and time - so it compares every two time points exhaustively and computes that that perturbation that change the value is the change in the system and they are assigned a perturbation value all arts or or changes between every two time points are evaluated then we select time intervals with large perturbation that are changed and those will be the red ones when we mark them and once we and we select that is inca gamma gamma mixture model and I'll detail that later and once we selected those changing those intervals that changed we have to narrow it down it might it might be that we have overlapping intervals that change so we need to identify the "
    },
    {
        "start": 926.87,
        "text": "narrowest interval where the change might have happened and this these intervals reinforce each other - we can reinforce each other and that means the change happened between here and the very next point where where we have a red Park and going to more more details yet so once we we narrow it down we have the list of change interval and that's the goal of the matter the the novelty of the method is this part to compute that narrowest interval because or any-any pattern analysis method could be used to actually compare every two time points but how do we identify the change interval you will end up with a lot of these kind of values which you won't know what to do that I weight which is the interval that easily interest to you so we run this method on multiple data sets we run it on simulation simulation "
    },
    {
        "start": 987.18,
        "text": "data we run it on the Elgin expression data we wanted to see how it how would we say behaves anyone we develop a method we want to to evaluate it then we want to compare it with other matters to see if it does so we can send it on different data for e.coli building we used a simple organism model organism because it's very well studied it has a lot of networks that we know um if the system means has all the parameters and worked out so it was good evaluation valuation candidate so when in an environment slacking it is all I do the fragile room to help it go to an environment that has nutrients and that's the that's the phenomenon that will simulate in here we have the Fletcher building Network which is our system and that's that's been presented "
    },
    {
        "start": 1047.87,
        "text": "in literature previously and we generate the data depend on stuff of the pelvic ten minutes for a total of 21 time points and this was implemented in art so we wrote a note code to actually generate this data and the non change interval because the phenomenon is so well studied we already have that answer now we're going to run or method and see if that matches the expected expected answer is the network from literature and these are thresholds while reading expression of the next gene touch touch accumulating when I'm actually in starts being expressed mixed groups group um change nothing Express have a master regulator you have another regulator and then we have I six groups of genes that are being that are being turned on by those regulations so we we "
    },
    {
        "start": 1109.79,
        "text": "chose a simple network so we can control how the how it behaves and we can properly evaluate our method in a controlled environment so to simulate the data we use the functions for protein accumulation and nd cake and we we use the parameters as I mentioned here from premature to to actually generate that data we display the data and we saw because it's a control environment it goes very clearly that there is a change here there is a shift expression to high expression okay if you think about it this first first start heart starts activating this this gene first and then these proteins and enlisting and together they regulate more genes and Morgan's and mornings so "
    },
    {
        "start": 1170.81,
        "text": "the expression accumulate for all beings and here on the y axis we have the genes from the network and on the x axis we have the time in minutes and here I wrote I display the depiction under federal law named Bo yes and he 30 minions generation so this data suggests that the colonies if I acquired some colorful columns in its stable generations probably this is this is scale data but he scaled the date I scaled so we can we reduced the time scale mister simulation as your your research suggests that the colonies "
    },
    {
        "start": 1231.63,
        "text": "devote generations no is presentation so here we were trying to show at a different scale or the phenomenon is happening did it simulated data that's happening should not be that long think about it this way if the you collided girls at like 37 degree in alpha is about the endings of 30 minutes like I said but the thing the phenotype Pristina is looking at now is in a nutrient depleted condition so I don't think they would be able to grow 20 to 30 minutes that fast that means in such London people much slower yeah and that's a date that carries around I guess it depends on the exact "
    },
    {
        "start": 1292.2,
        "text": "environment I think a 30 minutes time interval is pretty reasonable its scale data it's not it's not we use the parameters from literature and we generated the data we didn't take data generated from experiments in this case so this is not it doesn't replicate exactly the data so what we do in this case we compare all the system states using a patterning pact analysis we chose this method to do the comparison because it takes into consideration the interactions between genes for instance if a gene that is changed affect a lot of other genes that's more of a regulator master regulator so we want to give that gene a higher weight so we propagate the change that happened "
    },
    {
        "start": 1354.24,
        "text": "to that gene to the downstream genes and in the case where a gene is lower on the here--he on the pathway here he then it gets this change gets propagated only to the to the to lower levels so it doesn't affect as many genes as it would be a affect be any NF and so that it affects the NC as well but in this case because B and F are affected only be F D and C are affected and this is a toy an example of the perturbation pressure computed in the two cases so when we have this change and the changes are are similar we have a actually here the change is greater we have a seven perturbation factor and here even the change is smaller yeah smaller we have a higher perturbation factor so the weight the weight of the "
    },
    {
        "start": 1415.59,
        "text": "genes the way the genes are considered and the way the change of the genus person is projected or stand downstream it helps us in in evaluating better the fat and the change that happens in the fat in step two and there are as I showed before in the timeline and as I described you the path analysis method there are there are a lot of methods that do tank path analysis but this is one of the few that actually considers topology in this way and that's why we chose this method to select intervals with a large system perturbation we come on we take the distribution of those perturbation factors those changes between every time point and we feed to "
    },
    {
        "start": 1476.21,
        "text": "gamma distributions to it because we expect to be comparisons with no change and we expect there to be comparisons with high change and that's how we want to distinguish between the comparisons with low and with high change and once you identify those two distributions mixture of Sudama distribution at the intersection we consider that the threshold between the high and low changes and we put a 5% threshold if one of the the distributions contributes less than 5% the mixture then we don't have a change and that's how we evaluate that mean and typically the high distribution contributes less in the case when there is no change because you won't have high perturbation factors to to actually account for that I can go "
    },
    {
        "start": 1538.97,
        "text": "back to the perturbation factor and explain a little more so this is a positive value it is computed computed from the difference in expression for each team in absolute value that it's it's a propagating down stream so it's the perturbation factor it's always a positive value and the higher it is the higher the change in the pathway and that is why when we are modeling the distributions we are looking for the high participation factors that whatever studies systemic change and the low participation factors will tell us the reason so once we do this we want you to know if that 5th of the distributions is good that was that was a concern of ours and we use different different measures like backpaper divergence is a measure of how a probe a distribution is different from one another one we use the comma bottom spin of test that quantifies the distance between the "
    },
    {
        "start": 1599.12,
        "text": "empirical distribution functions to up to samples do you use different measures that that are complementary to see if our distribution was rotated out to distributions or didn't and as a result those gave us confidence when we looked at the results to either move on with the method or adjusted if needed to to perform better so after we identified the high and low changes like in this case we have large participation and this is the example for the e.coli that you're building for our simulation data we have large participation and we have small perturbations and we can see that our large participation being for perturbations reinforce each other to the point that they can identify states that are before our changing teen "
    },
    {
        "start": 1661.97,
        "text": "interval emerges as the narrowest interval that has overlapping large perturbations intuitive so there is there is a more formal description of this I think today we are looking for the interval that has has no changes in it and many changes that come before it and go after it any questions open it I lost everybody or you all got it I think it to become a bit clearer as we go through so this is this is the formal description it basically tells you that that we are looking at everything that's before touching interval and everything else that's after and we shouldn't have any change in the interval and so it says is the maximum value to satisfy that condition meaning this is the maximum "
    },
    {
        "start": 1722.149,
        "text": "value for 4 or this part and this should be the they this should be the maximum value of these states and this should be the minimum value of this state so nothing nothing those before this so for s six is eight or a seven I think goes and nothing comes from there all non significant changes this algorithm was implemented in our and once we run our algorithm we identify the change and excited those those closely with the real change interval so our interval was detected between one hundred and eighteen minutes and three hundred minutes and the vo change interval that was known was between two hundred and forty minutes and two hundred and seventy minutes so that means our interval should be "
    },
    {
        "start": 1782.63,
        "text": "broader but it contains the real change may be with if we if we had more sampling or if we use the maybe if we had those are sampling with a pattern closer to the to the chain interval so after we identify the change interval we see here these states and a group together and that gave us an idea and the thought of a meta state what if this is a meta state of non disease or non effect of the treatment and what if this is an at a state of the disease or the effect at the treaty we try to see to evaluate if these are real meta States if this happens just by chance or these are real "
    },
    {
        "start": 1843.98,
        "text": "occurring events in this state and to do that we took those groups of corsica takes place before and after the change interval from that and all comparisons between states Latino meta States have a small perturbation and all comparisons between states of the meta state-to-state subsided have a large system participation so again we can see this here everything that's before everything that's between these fates has a low participation everything does between state this state has a low perturbation and everything goes from here to here has a high one but that's how we define the meta state because that gave us the opportunity to see if this is ideal in the ideal case we would have exactly this situation "
    },
    {
        "start": 1906.789,
        "text": "directly the situation and this is the ideal case everything it's perfectly in our in the case of our data it didn't sit perfectly there were some changes that in the ideal case when everything fits the definition of the meta state would have had to be large perturbations and they were which is expected all matter is not perfect and we can't expect it to be perfect we expect it to give us good results that can help us identify that interval and help us identify mental state to bury this mental state we we did a comparison with ideal case and we assigned to each of these comparisons a label is it consistent with ideal is what is not consistent so if they are if that number of consistently inconsistent states "
    },
    {
        "start": 1966.94,
        "text": "under the null hypothesis when there is no matter state should follow a binomial distribution and we use that we did we did a test to get a p-value and to identify if those are your meta States and that's the test we did we compared it with the idea and we evaluated if they if that happens just by chance or if it's something that it's specific to the data and for each of the letters that I used or for each of the part not only scrubbing that's a computational part I also pulled the package or the the way that we implemented that that algorithm so we have our ideal case we have our absurd case and for each of the meta state or meta state 1 we compute how many are consistent and how many are inconsistent inconsistent presence this that are "
    },
    {
        "start": 2026.98,
        "text": "blind are inconsistent with ideally and we expect that many to be inconsistent more or less in the in the in the random case in the random is we should know the probability does a comparison is consistent or inconsistent should be 0.5 should be 60% right rolling a dice Okoye not a nice I think flipping a coin so that's what what in the random case they're sendin they should have the same amount of consistent inconsistent arcs or comparisons and this is not the case and from that aspect to we did the same and all this is an overall view of all the evaluation that we need so we in the distribution we had the measures echoed by paper divergence we had a contest we had overlap which tells us how good the "
    },
    {
        "start": 2087.16,
        "text": "area between the black decline overlaps with a with a purple line so that was a measure to see how the two distributions compare the black one is the one from our measured perturbation factors the purple one is the combination of our our mixture model that we fitted to that distribution and then for each potential interstate we got a p-value based on the binomial series so taken all this together for the coli final building we have a relatively high overlap we have a low for by the whole white paper divergence which we want it to below that tells us the distributions are similar we have a high p value or the common room you notice that tells us again that they're similar "
    },
    {
        "start": 2149.5,
        "text": "we have more than 5% on our number on our distribution of large perturbations that also gave us confidence that that that is a do change so overall in this evaluation we concluded that there are your method States and we do have a change that overlaps with the gyoji we did this for your data too so that was the case of simulation they tend to have more cases I'll toast summary of the results so we used biological experiment data or first time metamorphosis so the flu tribe was the three major the preventive stages can be a larva pupa and before becoming regular time before becoming an adult fly and that's a depiction of it so we use that type that the data from the development of stages "
    },
    {
        "start": 2209.84,
        "text": "and then we used a hedgehog scene and in fact we from a fatty database which is called Kyoto feed our genes and genomes and we chose this pathway because it has a crucial role in organized you know then I think the body plan for the fruit fly during development so that is a process that is relevant for our phenomenon we took the gene expression from a public repository a virgin expression they tie gene expression omnibus and we have the idea of the dataset here and also here we have we had the non change intervals so spy is again a very well studied organism so we knew when the change happen and this study was published it has a precise treat here's the picture at the Hedgehog Hedgehog signaling pathway and the genes in marked in red are the ones that were changed so quite a few of them changed "
    },
    {
        "start": 2270.23,
        "text": "during the process which was expected and should we have the data in a heat map similar to the one that we had for the simulated data we can see that this is more noisy this is real expression they try it's not the control simulated nice-looking heat map that we had before so we try to about we wanted to evaluate the power method actually is useful in a real case and in this case the the DA chimney interval goes from minus four hours to zero hours and hour method detected the interval from minus eighteen to zero hours again it was overlapping with the chain interval and it included it and even more so the center of our change interval was lower than the change the center of the change in 30 our change interval so we detected it earlier we can say but this evaluation this "
    },
    {
        "start": 2334.38,
        "text": "evaluation gave us confidence we wanted to see if there another method out there that's doing something similar and we should compare with that method and see if we do better or if we at least to as good as they do there is a method that we abbreviate Indy and BM method developed to detect natural biomarkers and the PDG state notably this method doesn't identify change interval it identifies only a pre busy state only one time point as the input high-throughput data in London and a large network of protein-protein interaction that's another downside of this method because processing such a high amount of data it's very time consuming and resource consuming so we can we can if we can do just as good or better with a smaller network that would be that would be beneficial and the hypothesis for this method is that the leading network so the network that they "
    },
    {
        "start": 2395.82,
        "text": "they identify small networks or the large network of protein-protein interaction and that network is the first to change towards the disease state so that's the network biomarker that they're looking though what the NBN does is one of the change the change in gene expression over time as a Markov process it doesn't do the impact analysis that we do where we propagate the change of the gene it approach and uses a straight traditional based local network and so Peter quantifies the change in state so it basically evaluates the network again a different state but it compares always with the state zero it doesn't compare all the state it compares it stays zero so they do have a reference they do set a reference state also it identifies one single PDC statuses this is what the inflection point I know is it doesn't end if I change interval and we "
    },
    {
        "start": 2457.079,
        "text": "hypothesized that in our change interval you could still and keep the healthy state you could still reverse the changes that started the evaluation of the nvm was done so functional analysis using the experimental rate and pre-existing knowledge so they used gene ontology analysis they used now process known processes and marker functions that were related to their results to to evaluate to see if if there is all made sense and and they did and also they use the existing knowledge relates to the condition under study which is also available information in could use them they they did the study for two phenomena misfortune exposure and human hepatitis C virus induced episode a Sonoma the HTV to ACC transition and we decided to do the analysis of the same datasets to see how we compare in the "
    },
    {
        "start": 2520.169,
        "text": "human liver cancer progression eye infection we have tried to see only two major major complications and need to cancer and we were liver failure so in order to study this phenomena we chose the Genisys pathway and this describes the signaling mechanism of an inflammatory response so that that it is relevant to see if that system changes and then we we use you an expression data for hepatitis C virus induced or the server carcinoma for a disease pages and this is again publicly available in the in the expression and here is the picture of how you would go to those eight stages from healthy DeRosa is to hi great this plastic knowledge goes to very early personally up to very advanced which are the stages "
    },
    {
        "start": 2580.17,
        "text": "that they had in the dataset in the end the previous data set and now in this case we don't know when when the the systemic change actually happens because this is a complex phenomenon and we have no reference we don't know the ground truth for that reason we used the comparison with the other with existing method state is very early about the cell carcinoma which is very good you want to cut the cancer early this is a depiction of the the pathway from Ken and the red jeans again and the teams that changed in that experiment and here they detected them very earlier ACC as a DC stage our method the QC we detected the change interval that we from control to high-grade is plastic so it was an "
    },
    {
        "start": 2641.55,
        "text": "earlier interval Oh quite wide one at that but it was an earlier interval that allowed us to detect the transition earlier so we we had more phenomena that we went to and that we did the validation on we didn't wanna just limit ourselves to three or four phenomena we want you to make sure that this works on multiple data set and before I go to the results any questions that you have about the method than the validation yes yes because the this is the start of the interval so it actually starts from the normal state yeah Oh captors and even though it was normalcy yes and probably that control and cirrhosis what's your clothes so the change wasn't big enough to go to the next one so it "
    },
    {
        "start": 2701.97,
        "text": "it's conservative in the sense that it finds a wider interval thank you any other questions but the method the intuition is we were trying to do an analysis that is not biased by reference and an analysis that is exhaustive in the sense that it compares all states and the challenge was how to identify that change interval and we told that by overlapping those comparisons that have have large perturbations and narrowing them so that the results so here that we held two three four five six turbine model organism and we try we included no validation to two phenomena from the "
    },
    {
        "start": 2762.69,
        "text": "reversing model organism for fruit priming to this palliation in ethanol exposure we had the wide range range of phenomena from tell me that you don't have a big change like the voidance reflex but our method even captured that because all the other data or they are the data points allowed for forty to capture that we had more complex phenomena like a cape depend on exposure like foregin exposure in mouse and the liver disease stages for human we try to have a wide range of validation to make sure that our method performs well and its general purpose and for four inch they doesn't wait we mark this data sources or if the data source is a mathematical model that means it was synthetic data we generated that to have controlled data that we can "
    },
    {
        "start": 2823.47,
        "text": "we can actually by our own method on and then we had five data set where it was a more noisy data set coming from real gene expression data this here we have the detected change interval the vo change interval in the case of the bacillus subtilis correlation they actually match perfectly in most of the cases they overlap and the detected chain interval included a dirty needle in the case in these two cases we didn't have what we're seeing interval and this was what I thought that was already published and did something towards identifying a disease state identified okay this that's why they have done we also validated or mental states for each of these cases we wanted to see do we really have the transition from one big fantastic to another for for "
    },
    {
        "start": 2883.589,
        "text": "instance in the e.coli case we had from Nigel Nigel in the case we had no this is the disease in the last time when you read exposure and this is this portion gas that's very toxic so in that in that case I want you to see when the first changes in the mouse happens before even the the symptoms or the physical symptom that you could see happen oh the same thing for each correlation so we have a wide range of a passive in the case of the meta States for for our console data for our simulated simulation data they were all significant maybe significant and they identified the United States which was expected we had our our date modeled using parameters for this phenomena and no noise so for the real "
    },
    {
        "start": 2948.58,
        "text": "datasets where we took the data system from a public repository we had to three cases in which that particular method state did not that was not significant in the validation for the East's correlation if that was the case and for most companies provide exposure in this case we think that the change happened very quickly because that guess acts quickly in the East correlation again there was a wide ring wide phenomenon so again this bear this is close to significance but it was a significant so we think that this this might be a case of releasing the data that didn't allow us to identify that and the food by alcohol exposure in this case we actually have fruit flies that are exposed to ethanol and then that state "
    },
    {
        "start": 3010.89,
        "text": "reverse it's kind of that you get wrong and then they get sober we we have they go from Tober to drunk the so bored so it kind of reverses the mat that's it and that's a special case to our method on so overall we with this resource I felt pretty confident that we we have a good method and we can use it on data sets that don't have an on change interval to identify it there are the limitations to it and that's that's a future work that can be done for different diseases there are limitations implementing this is implemented only for signaling networks this is apparently the only which in expression data and it needs that system that looked it so to address this limitation for reading a blue thing most of the organisms do have quite a few roofing it's available for them but this is "
    },
    {
        "start": 3072.66,
        "text": "stealing patient because tying it so blooping might introduce bias evaluated only with an expression data we use in the simulation neuron neuron sympathy and it performed well we didn't do that on a real date aesthetic we don't find one that that that has this kind of beta so we could evaluate it on protein measures we could evaluate in methylation they tell you the value is any kind of measure as I said even measures coming from from trackers that that collect that type of sequential data and implemented only for signaling networks so that's that's something I was working on and that's another method that I developed in the mr. Bhalla craft analysis run so that's how I would think was trying to address that so these are "
    },
    {
        "start": 3134.43,
        "text": "the limitations I'm going to go quickly through the implementation details of the approach the singles now so this was implemented all in our there is a package processing was done using the a PLL our package function for that gene IDs on map the gene symbols using the respective annotation packages and we use that for every real data set that we had this was for for East this was for a fruit fly from our cell for human and you know expression of the specific standpoint was thought it was the average of the replicate so it typically in the real data sets we have multiple multiple measurements for a time point so how we use that in our data we already and that's something we can work on to maybe we can get more information if we look at the time wise data of the "
    },
    {
        "start": 3197.37,
        "text": "samples that might have this knowledge and now I'm gonna take questions I want to go to this method too but I think this is this is enough for now now take questions I'm really passionate about the topic English as preventive medicine I like I like the idea of not allowing ourselves to get to the disease state and using every method that we can to prevent that and these these kind of detection methods I think help with that in the sense that my dream is to have a method like this that collects data from the trackers that we have and tells you you need to sleep more you need to keep an apple you need to know something and that can help you stay healthier longer "
    },
    {
        "start": 3260.23,
        "text": "so that that's that's what this work is about proxy for time intensive logic progressions yes and also so yes so that would be the the use case for cancer and specifically I have at the end so by looking in the change interval so we also looked in the change interval the genes that change there and I thought in that are on the pathway that we we actually used for our analysis the genes "
    },
    {
        "start": 3323.48,
        "text": "that were included in our analysis and genes that change during the change interval we would think those are early markers of the progression towards disease and we found these that are of interest for celery carcinoma they're not new genes new candidate but it again gives us confidence that the method the method is going in the right direction so that that's a use case of the method you would analyze the cancer stages the data from cancer stages and then when you identify the interval you would look in that interval to see the genes that change because if you look from the beginning from the first time point to the last time point those genes might not change there because they might eat the plateau or revert after a thorough point and those might actually give you some insight that is it do you have any idea about the computational "
    },
    {
        "start": 3383.54,
        "text": "requirements that your trackage would need given for example for every time point you have for example one full gene expression set on the ninth and you have for example ten I'm so how long and how resource intensive your process would be all these analysis I've done on my laptop yeah I need two minutes to ten that means the most in it yeah because someone else is the ones for the for the simulation they tower there were fewer genes and simple networks like they were almost instant so the reason I'm asking about this is although doing a regular time point sampling is not and doing for example to the core and a sequencing is not possible at the moment just because it takes a long time but moving forward "
    },
    {
        "start": 3443.56,
        "text": "long green technology with Oxford nanopore they're actually inventing and then a lot of people are actually working on this but moving forward Drake RNA sequencing is actually very very likely and that do within minutes of getting the patient sample so I usually a program like this is integrated with acting a sequencing method it can actually predict the famous right powers as the patient state is changing from disease and something like this can be very effective for example in ICU yeah and I'm thinking also not there but it helped us devise better treatments so we don't treat disease when it's not there and we don't use toxic preventative measures so I "
    },
    {
        "start": 3504.03,
        "text": "think it goes to more targeted personalized medicine and what you described is exactly that like catering to the patients right there right so I think the fruit fly the drunken drunken fruit flies example shows us that they could so maybe not as good as the other cases but it's good it's actually identified that there were the words physician back to that stage and that's why we didn't have to metastasis [Applause] "
    }
]