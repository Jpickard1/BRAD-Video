[
    {
        "start": 0.03,
        "text": "then you go in already I usually know what I wanted at least roughly in some cases I burn specific designs and I've like purchased and modified there's a West Coast this is head to her that specializes in composition so a lot of my pieces like this is one first hisses whimpers yeah shows that so I would bring a lot of things in and say I believe this ain't right now since I started I believe Ronnie Garrett but my last couple pieces with him I've been just saying because he's got big eyes I had been going to more more life in new and Americana traditional thing so like my they were good artists but I didn't they wanted to do this I formed a sound like in that stuff right a little bit more freestyle alright Jared is a little "
    },
    {
        "start": 71.57,
        "text": "when I want to go straight down Blair's not no shading just straight outlines all the way right that was the most painful go back "
    },
    {
        "start": 133.08,
        "text": "[Music] thank you my back pain because it's a there's a very few open spots I'm not willing because I know how much but it's "
    },
    {
        "start": 194.099,
        "text": "shoulder yeah I've got one that's essentially based on Celtic Cross so basically I gave I gave the guy that did the back piece because I got that done my calf and I gave me neither did my back hurts "
    },
    {
        "start": 329.95,
        "text": "[Applause] "
    },
    {
        "start": 420.36,
        "text": "[Laughter] [Music] [Music] no no it's on right that's better okay so today we're going to talk about "
    },
    {
        "start": 484.27,
        "text": "dinner I'm going to explain what this is all about and I just want to start by introducing a few people so Marcy introduced you know honey Bill and myself and there are two more people here my engagement and she's right there and John Wigington who are very much part of that team so it has been a great collaboration I think between bioinformatics and metabolomics core and so some other people are also were also involved and peter was working in helping marine and he helped us develop some of these features of the tools that we will be talking about and of course Chuck rent and have an ongoing collaboration with George Michael ideas okay so here's a brief outline what I'm going to sort of you know what we are going to try and do today right so I will try to talk about background and motivation and "
    },
    {
        "start": 544.28,
        "text": "then honey will take over and talk about workflow and Bill will jump in and we'll talk about some features of the software and performance and so what we're hoping to do if we can get through all the slides virtually going yeah what you're going to try and sort of demo the vinner output but our primary goal is to just you know first to set the stage to talk about why we need this tool in the first place and then basically you know to show you some neat computational solutions that were developed for this so we don't have we don't talk about metabolomics here often in this department I think you know we're mostly talking about genome and ap genome and so this is just basically to kind of you know remind everybody that yes or a metabolites and so there are many things that are very cool about metabolites and you know one of them is that there are perhaps the closest to the phenotype and "
    },
    {
        "start": 606.26,
        "text": "to the function and so core here that is doing a beautiful job at measuring these metabolites and biological samples and so I think that's sort of you know the extent of my introduction right so metabolomics kind of emerged in recent years as a high solute onyx science right and so usually the sort of you know common approach to analyzing the omics data be an RNA seek data or proteomics data or whatever I mean the usual approach that sort of you know it seems pretty illogical is to basically to make biological sense out of the data is to map the you know your RNAs metabolites whatever biomolecules to biological pathways and metabolomics has no exceptions are of different tools which I hear some "
    },
    {
        "start": 666.38,
        "text": "open-source some commercial tools that allow you to do that right so why am I talking about pathway tools and what does it have to do with sort of you know the tools that we're talking about today so the tools that we develop dinner is actually a tool for cleaning up the data why is it required and why am I talking about the possible losses okay so well first of all it turns out that these possibly approach I mean although it works so metabolomics data and we believe that it works because you know we have our own tools that we devolve many of you heard of it it's called Netscape it works but it only goes that far it don't answer you know works for the non metabolites that map neatly to known metabolic pathways non metabolic pathways primarily covered what is called primary metabolism okay if you go beyond that for example lipid lipid "
    },
    {
        "start": 726.77,
        "text": "metabolism is not covered by any of the pathway databases that you might know about like you know it can't be react on you name it lipid metabolism we just don't know enough about lipid pathways okay so the other reason why and to talk about is because high throughput metabolomic studies in particular on targeted LCMS studies they generate a lot of data and not all the metabolites and not all the features as they're usually called in a data set our regular identified okay there are a lot of unknown compounds unknown features in these data sets and if you don't know what it is given rise to an alternative approach which is data driven and so this is again very well known in the literature "
    },
    {
        "start": 787.7,
        "text": "people have tried building their is sort of you know derive the information about networks from the data directly from the data and usually this is done through you know using some kind of you know correlation based approaches so what I'm showing here as Pearson correlation network as you can see is this is very dense because I mean these are marginal correlations this is a partial correlation networks this is notable works this is from paper and so part of the problem diseases large sample sizes with partial correlation approaches and so we have developed in the past two and more module for Medscape for handling this correlation networks but the one the reason I'm talking about this is because when we develop this you know this is going to be really great and aren't so little cluster no metabolites "
    },
    {
        "start": 852.87,
        "text": "hopefully help us to identify what this unknown metabolites are right so what I'm showing you here as you can see these are all I mean assets and you know there are a bunch of these things this empty kind of you know notes empty boxes these are all unknowns so for nature is wanted to know what they are right so and when we started looking closer it turns out that well I mean they were really highly correlated to the known compounds when I it turns out that one of them were chemically related so basically you know I'm showing you here prolene and so all these chemical formulas you don't have to understand them but just take my word for it this were either various sort of you know fragments or you know otherwise chemically related substances that are related to the respective feminists now "
    },
    {
        "start": 913.69,
        "text": "where does this come from okay so here's this sort of you know quick primer on metabolomics the so basically you know what people are using and what we're using here in the metabolomics core we're using so-called hyphenated techniques such as lc/ms and gc/ms and so basically you know first you do for example liquid chromatography gas chromatography and that serves two molecules right and so chromatography serves to separate these and mass mass spec allows you to basically you know identify the determine determine the mass and identify what they are so this is just sort of another useful kind of you know slide yeah this you know illustrates the sort of you know point of sort of you know how chromatographic column so you "
    },
    {
        "start": 973.74,
        "text": "know hopes to separate this and what we kept at the end I'm rushing through this because I really wanted to get the winner but it's but the things that is really important is that the output of chromatography from at a ground which basically you know you know y-axis we have intensity which is related to amount of particular you know peak here and retention types we've done some time is related to chemical properties of the compound how long it takes to it is a little from the coal right and then the others about mass spectrometry so basically mass mass spectrometer can only see charged molecules right and it cannot if they don't if they don't charge it doesn't see have charged that it doesn't see them right so it has to "
    },
    {
        "start": 1035.449,
        "text": "be able to form an ion right and so what we are even though what we are doing is called soft ionization nonetheless sometimes molecules break apart okay various fragments and other things that they were so it's you know even though it's a soft ionization techniques it still artifacts great so now this is the essentially mass spectrum and so from this we get mass-to-charge ratio or MZ you're going to see that a lot and again we haven't tested okay so this is like my quick crash course on the lcms or liquid chromatography mass spectrum okay so so basically what we just talked about is the fact that one metabolite so as a result of this one in the double "
    },
    {
        "start": 1096.71,
        "text": "can be represented by multiple features right and this makes compound identification more difficult and creates the implications and one of the next slides may be right so these are various you know sources of redundancy or you know degeneracy that's another term for it and so basically you know the breakdown it's good I mean it depends on specific data set but in general I mean you have you know fraction of known metabolite so for example human plasma it can be 400 known metabolites or something like this Marine is it realistic number three four hundred something like that or whatever you know I mean sometimes more but you know so you know this you know unknown compounds as it turns out it is highly redundant right and so alright so to "
    },
    {
        "start": 1162.14,
        "text": "really every statement point our goal for the software is to to the greatest extent possible minimize the redundancy in the data set so as Allah mentioned a single metabolite can give rise to multiple different features that we detect across samples and so our goal is to really reduce this to a single most representative peak or feature for each individual metabolites and as a secondary but also important goal we want to also sort of get at what exact chemical adduct is represented by the applying feature so to just give a brief example this is just a mass spectrum for one metabolite tyrosine so what we see is about six different Peaks that all represent one metabolite but we only want say this one the program to keep "
    },
    {
        "start": 1225.7,
        "text": "for and then just eliminate the others that are contributing to greater redundancy so the main removable czar your isotopes such as this carbon 13 variant of the protonated tyrosine we want to get rid of this additional atoms these are sodium additions to tyrosine and then as Allah mentioned before lc/ms is imperfectly soft and we do sometimes lose certain groups such as an amine group and a carboxyl group so these six Peaks we want to reduce it to just one individual feature which is generally the most abundant one so which so what properties are we going to use to sort of screen out these redundancies so the first as mentioned before is that these chemicals that come from the same parent coldly so as Ella mentioned "
    },
    {
        "start": 1286.19,
        "text": "before come chemicals that Allu within close to each other are chemically related so the ones that are from the same parent are pretty much the closest in retention time so then I also mentioned before is that there's a very very high correlation between these redundancies so that we can use and exploit to find and then there's some kind of readily observed mass relationship and by that I mean difference between masses that we can use to sort of recognize certain advocates such as the earlier show dealing with the loss of a mean or other mass masses that correspond to certain events so that's sort of the motivation and this is kind of the order in which we sort of detect certain properties of these redundant features so this is the overall workflow so from "
    },
    {
        "start": 1349.04,
        "text": "the input data we have a retention time grouping step a correlation clustering step and then we try to use the mass differences between features to try to get at the underlying annotation for the chemicals for the that are represented by the features so I'll just go through each and every aspect so starting from the input data so what we require is basically a table like this where each and every row represents a feature and what we just require is a name for each feature it can be a chemical formula a name that we suspect to be for the feature it can be mass at retention time or compound one compound to however it's named it will be treated the same we just require that each compound each feature should be given a name then we require mass an MZ format retention time "
    },
    {
        "start": 1410.81,
        "text": "we generally expect it to be in minutes and we also require at least three samples with measurements across these are the sample intensities which correspond to the abundance there's no requirement on what order these columns go and you can include additional columns so minner is pretty flexible in terms of what data tables we can use just as long as you don't use special symbols because we do have to read in the table so just a brief quality control step before actually processing the data so since we're gonna be doing a lot of correlation analysis it kind of helps to limit at this stage some of the missingness in the data so as a brief first step we sort of exclude those that "
    },
    {
        "start": 1472.33,
        "text": "have excess missing this by default over 30 percent so that's a first reduction step and so with those that remain behind we just do a little bit of median imputation and then also we have a recommended step of telling log transformation to just our intensity values closer to some normal distribution so then we get to our main retention time grouping step which is called pinning and so what we do is we basically segment the entire chromatogram into these self-contained units or bins and we do this by first detecting first we sort the features by the retention time and we detect gaps in consecutive features and if they exceed "
    },
    {
        "start": 1535.37,
        "text": "a certain gap we end one bin and start a new bin and so really this default gap right here which is one zero five minutes or three seconds it's really perhaps the most important parameter that you supply binner because it really is a way of controlling how big these bins eventually are and so we chose zero five point zero five minutes as a because we suspect that features that are more than that distance apart are generally different metabolites and so using that we sort of get different groups of features and put them into different bin in order to have a bit more efficient visualization a bit more efficient downstream clustering but you can "
    },
    {
        "start": 1596.95,
        "text": "already see from this stat where we then take the pairwise correlation already we see groups of metabolites that are highly related to each other and we suspect eventually that these will be all part of the same parent eventually we'll be doing correlation clustering to sort of detect these smaller cluster modules but before we do that we decided eventually during development that we had to treat one class of features a bit differently from the others and those are our isotopes so basically we do carbon 13 basis scheme for isotope detection so for each and every carbon that's carbon 13 it adds an additional Dalton to or massing it to the underlying compound so if this is the "
    },
    {
        "start": 1657.76,
        "text": "minor isotopic mass that we want to keep each and every carbon 13 adds one Dalton to the mass so about 1% of all natural carbon are carbon 13 so generally tends to decrease for the total and abundance of the compound with carbon 13 decreases for each carbon so very simple rules that we used to detect and this is all within one bin I should mention so they loop within a small retention time gap and then they meet some correlation threshold by default 0.6 they meet the assumption of dwindling intensity meaning just that the intensity of each subsequent isotope is smaller than the one that came previous and that works generally English because we have a small compound so a limited number of carbon so this "
    },
    {
        "start": 1719.71,
        "text": "assumption generally holds for satellites but unless there are isotopes of other molecules such as sulfur or chlorine in which case it might affect these second abundance and then one other very important thing that we exploit in this isotope detection stage is that when we find the difference in mass between two suspected isotopes it can really be helpful for detecting the charge state on the compound so for example if it's a point five Dalton's apart we have a double charged compound 0.33 Dalton's apart it's a triple charged compound and that's going to be really helpful later down the road when we start to assign chemical for masses and chemical addict's to these individual features so after isotopes are detected they sort "
    },
    {
        "start": 1782.47,
        "text": "of pulled aside and they become sort of bystanders which then helps us do our clustering that happened before bidding no this happens this bins width was different than the isotope the RT gap I'm sorry final five no the the point zero five is just to help us find the difference between want the gap between one bin and the next after I understand but this is point one which is bigger which means you throw them in two different you can throw them in two different bids I mean in the end you know it's still with it one's been I mean regardless of whether we detect it at this stage is it just a default 0.1 but yeah I think 1:05 or 0.1 would be sufficient to detect these isotopes okay so with that I handed over briefly to "
    },
    {
        "start": 1843.609,
        "text": "Bill I could just clarify that real quick the gap is just a gap we're looking for what a bid may contain a much wider right yeah then could be anything from one minute to 1.5 minutes because there would be no gap between one minute 1.5 we have a continuous retention time of features they are twice looking fusing some pins can be quite quite large yes I understand that you could theoretically throw one of those into a different bin no it cause it's done after day after day yes and they didn't yeah right isotope detection is okay so clustering thing is that said good Alex are we good good enough try again try again okay all "
    },
    {
        "start": 1916.13,
        "text": "right so um yeah so Hani mentioned that we first separate things and separate all the features into bins and at that point there is an underlying correlation structure that we want to elucidate and so basically what we're trying to do here is look at how these cook these compounds are related if we have a bin with it has multiple different features but then we assume what if you were done in features as well and ideally what we'll do is is find some some natural groupings so the natural groupings we hope will will be groups of redundant features that we can annotate and eliminate redundancies so when we started this whole discussion just a while ago what a little over a year ago I guess it was when we first started thinking about these things we just thought well we'll get started we have to do something to start with we first try we'll just do simple hierarchical clustering and it seems to work pretty well probably most of you run into that I'll go and I have a slide just to show "
    },
    {
        "start": 1977.87,
        "text": "what it does but I think most people have seen dendrograms of hierarchical clustering where you start with everything in a different cluster take the two closest features move them into one cluster then you can Sharyl distances the next two closest things you can find you just keep combining all the way up the tree so it's really easy to implement because it's already been done in our by probably a lot of different people in different libraries and we happened dinners written in Java but we I'm doing this a little more from a developer's perspective since its tools in tech there's written in Java and we already have a nice framework for calling our from Java so we thought that was an easy way just plop it in there and it works and it gave pretty good results but then we started looking into it well here's just a quick little stray SHINee and of the dendogram I finally really have to cover that I think you all understand what hierarchical clustering does we decided we needed something a little bit better the problem with hierarchical clustering and for us is that you have to say ah "
    },
    {
        "start": 2039.73,
        "text": "priori what the number of clusters is and we don't know that we prefer to have the data tell us what the number of clusters is we came up with just an ad hoc way of specifying that number but it wasn't really getting at the full structure so we decided well we need something a little bit better well we talked to our colleague George McFly tiss I'll mention he's a very good statistician and he said I will just do a lot clustering we thought oh why didn't we think of that because we're not statisticians but but um so we decided we had to look into this so the white clustering actually uses the data to determine the number of clusters empirically so it seemed much more appropriate for what we're doing so just very quickly I won't bother to explain the math behind it that is okay so when I look it up for reference or have a reference to it what a silhouette value is essentially it's taking for each feature how well does it fit into the cluster so we're we what syllable clustering does it starts with two clusters and does the full hierarchical "
    },
    {
        "start": 2102.49,
        "text": "cluster than that sees how well everything fits those three clusters and sees how well everything fits and so on it chooses the best number of clusters based on those results so what this formula is saying is that for each feature I does it fit better into its own cluster or we would have done should we put it somewhere else is it where it belongs essentially and that overall gives a measure when you start accumulating this over the entire over all the features it gives a measure of how well you're clustering is gone I hope that makes some sense obviously we don't have time to go into a lot of depth but keep in mind just in general this is already a lot more complicated because it's doing multiple rounds of hierarchical clustering and keeping track of it all the hierarchical clustering methods I know do the whole thing yes and and so it seems like you should be able to do the clustering once and then move down the tree and say slice here slice here that's effectively what silhouettes what silhouette clustering does okay it's "
    },
    {
        "start": 2162.84,
        "text": "nothing it's not actually doing the cost for a corner it does the clustering it does it it does it at each and in each stage of the dendrogram nice to find out oh yeah that's not that the clustering has already happened because it's implicit in the whole tree that's right yeah that's right then you don't have to do you don't repeat the process every time you just look okay that's right so as an example here I'm not sure if this is maybe the the one of the bins that honey showed before but you can see from the pairwise correlation structure of the yeah this is something that binner puts out automatically and in the output the red rectangles indicate very highly correlated regions so you count them up it looks like six is a good number of clusters for that for that bin so what we do here k is the number of clusters we calculate the average silhouette value which was based on the formula I showed before for each for all the clusters take the average for each cluster for each well for the whole data set given K equals two three four and so "
    },
    {
        "start": 2224.97,
        "text": "on all the way up and you can see that in fact in this case cake has the highest average silhouette value so that's what we choose we decided okay well k equal six looks like the best thing and there's the plot that sort of illustrates here the singletons numbers three five and six here don't have anything to compare themselves to so they automatically get a zero value but everything else gets compared and it looks here like six is indeed the best fit so that's a nice clean example well they're not all that clean clustering the rows are you testing rows or custody it doesn't matter because it's it's actually feature by feature so it's a it's a symmetric it's a square even though it doesn't look like it orient a little bit better here what we start out with with the input data is correlation yes is features in the rows and then intensities across the columns so what we do is take each pair of "
    },
    {
        "start": 2285.62,
        "text": "features and correlate all their intensities for all across all the samples so for each pair features we get one single correlation value and that's what each cell of this of this matrix represents yes right so a bin is going to be a stretch of retention fairly now or we hope stretch over attention time no so they don't necessarily that's it's a good point they don't necessarily end up continuous in retention time because now what we're doing is taking everything in the bin and just looking at the correlation structure so whatever the clustering algorithm tells us it actually does tend to shuffle things around a little bit and so so it's no longer necessarily sorted by retention time at that point but we're looking for now so we've already sort of used retention time as one measure of similarity and now what we're doing is "
    },
    {
        "start": 2345.95,
        "text": "looking at intensity as another measure and correlations between intensities and using that measure of similarity to further identify redundancies so we're sort of going at it one dimension at a time and this is the second dimension that we use so I hope that answered to some extent so I'll have to keep moving on here but um anyway so they're they're always catches it's never as simple as it seems so we had to do a little bit better than just the straightforward silhouette clustering problem is well this is when Jan was the co developer on the project started looking to this and we realized that the unweighted silhouette clustering that I showed tends to choose K equals to just the two cluster K way too often or doesn't really get out structure and there's a weighted silhouette clustering in the literature as well but that the idea is to down weight the value of large clusters to avoid this K equals two problem and problem is it tends to do too much and it over splits and you end up doing the whole thing in two at least for our type of data into a whole bunch of different "
    },
    {
        "start": 2407.46,
        "text": "clusters way more than you actually want so Jan came up with a sort of parametric model that allowed the weight in the in the clustering to vary from completely unweighted all the way to weighted and then what we do is calculate across a grid of weights we do the entire silhouette clustering and then it finds what the best one of those weights is and so we think that really is doing a nice job now of losing the correlation structure but implementation is always a question so it's obviously from what I described way more computationally intensive than just simple hierarchical clustering could have used our to do it but it's way too slow and this is a good point for anybody who's tried to struggle with our and thinking there's got to be a faster way well higher level languages are a lot faster I was surprised to find out this actually that Java is is actually typically 50 100 times as fast as our if you do it carefully I I'm an old guy I started out "
    },
    {
        "start": 2470.009,
        "text": "and see hate to admit it but I actually had to learn Fortran at one point too those are really fast so there's still valuable C++ and Fortran probably blew it better than Java so if you're really in a critical application they might even be better but for what we're talking about we're talking about minutes not hours or days so Java was what we were already writing in and it made a lot of sense so we implemented in Java and now we've taken what we're trying to do is this is supposed to be an interactive tool for exploration so if it took two hours to do the clustering it's not the end of the world but going out getting a cup of coffee you know walking the dog you come back it's not really interactive but we implemented it so it takes maybe two minutes and that way you can really get your results resets and parameters if you want to run it again and it is much more interactive so that's a sort of a a little bit of insight into some of the challenges and actually a competitive stuff and so now that we've grouped it by retention time grouped by correlation "
    },
    {
        "start": 2531.66,
        "text": "so now we have a step to finally try to get at what the underlying chemical addicts are both before that we we calculate the differences in masses between features that are within the same cluster and we have basically a way of just showing this in the spreadsheet where differences in masses are shown and very commonly recognized differences are highlighted in their own color and those are based on mass differences that are very commonly seen for example the difference between a sodium addict and protonated addict is twenty one point nine eight an amine group is about seventeen point zero two so over time using this tool and you know looking at the chromatography sort of recognize a lot of these very common mass "
    },
    {
        "start": 2593.88,
        "text": "differences and that is the first step to helping you understand what addict's or chemical groups are in the data so this used to be the only way for which we used to screen out different addicts but it turned out to be not enough so for our annotation for more complex for example multiply charged addicts or multimers it became apparent that we not only had to find differences between different features but we also had to find it with respect to some hypothesized nutrient s and this is sort of the solution that we that we are trying to implement and we came up with so first we divided our entity file into a set of what are called "
    },
    {
        "start": 2655.509,
        "text": "charge hearing addicts and what I call neutral additions or losses so what these charge carrying advocates do is they help you form a hypothesis for what the underlying addicts are in the data and then these mass differences are sort of used as a way of complimenting the advocates that you previously found so I'll just walk through a brief example of how that works so just the annotation procedure so to begin we dis cheese the most abundant feature in the cluster and that generally is going to be the feature that best represents the underlying metabolites and from that we obtain a hypothesis on what the neutral mass is for that addict so just to show this this is the most abundant feature right here and here are just two "
    },
    {
        "start": 2716.92,
        "text": "isotopes that are kind of bystanders we're not going to do any further annotation on them but first we say okay could this be a protonated addict if it is this is the neutral mass that we detect and from that we get only one addict it which is basically just this same M plus h and so we doing this for other hypothesized charge carrying addicts until we get the one with the best hypothesis the one that gets us the most addicts it's so for example that we next try this as a sodium addict with a base mass of this - sodium s and we get a total of three charge carrier hits which just so happens to be the best "
    },
    {
        "start": 2779.469,
        "text": "hypothesis for this particular cluster and then once we come up with what we think is the best hypothesis we use the mass differences that we previously computed to come up with the remaining hits so we previously detected this this and this from the best hypothesis and then from those that remain the differences in mass between this and this and this and this help us find the remaining based on the neutral the second class of annotations which were in the second part of the file so that's generally a very simple example how we do this detection and the reason we we did this is to be able to detect more complex edits such as the multiple charged when we have multimers and just to get the best hypothesis for what the most abundant feature is okay now I "
    },
    {
        "start": 2843.069,
        "text": "handed over to bill a bit more building our annotation file on right so so the whole purpose of well one of the main things in being stands out about being here compared to some of the other tools that doing similar things that we're trying to be really interactive and we're trying to let expert chemists like marine is just basically driving this whole the whole process of sort of each subsequent step that we take is in response to something that marine finds in the actual data for the metabolomic score and so that's been partially possible because we have an annotation file that allows someone to start with well we started with a fairly simple file just a few basic annotations and I'm sure marine quickly brought it up to a lot more than that based on what she knows but then she's even admitted that there were few that she didn't know about ahead of time that she found through using in our analysis and so "
    },
    {
        "start": 2903.46,
        "text": "then that the idea is that over time for your own platform if it's an individual platform you can keep adding more things to the annotation file and get a more complete set and more more accurate set of annotations as you go so you don't have to be really knowledgeable starting out but ideally if you're using the program you will become more knowledgeable about your own data as you go and so that's that's what we have we hope that's a really useful feature but there's sort of a question if you're using our program with your annotations in it and we're just outputting based on those annotations then how are you actually going to learn anything more you're just getting what you put in well so there's this is again something that Jan wrote put came up with but there's an output tab that we have that basically does a distribution of the entire data set what are the common mass differences that are found in the data set what mass differences by just happened to show a lot more frequently than you'd expect by chance some of those may have already known about and may have annotated it or put in your annotation file but others "
    },
    {
        "start": 2964.71,
        "text": "may be completely new to you so then here's a an example of what that can look like it's probably a little small and I'm not sure how easy it is for everyone to read that but the idea is that basically there's a distribution on the side where each little cross mark indicates a certain number of of that number of that mass difference in the data set and only things that are much more likely than chance are displayed here and what you can see then in each color represents sort of a continuous mass little range of mass you can see it's done down to thousands of adult and so it's it's pretty fine grid mass or grid you know fine grid so basically from forty six point nine eight five down to nine nine four there total of two hundred eighty five hits as you might call it that are very close to that mass well that happens to be something that was already in the annotation file it's hydrogen plus two sodium's then down at the bottom another "
    },
    {
        "start": 3026.33,
        "text": "one that's based on them but it's not I'm saying a neutral mass difference again it's annotated but then in the middle there's this fifty point oh one five a whole lot no idea what it is it's not annotated but if you find that if you see that you probably want to look into it and decide is that something I want to put in my annotation file or is that maybe something that is commonly occurring and because of chemical properties and is not something that's a redundancy but at least it gives the investigator a chance to explore based on what's actually in the data so it's just an example of how we try to uh to give people a chance to grow as they and I think well we may or may not have time for a demo but we may have a little bit of a chance but you might be asking how well does this whole thing work overall so before showing output I said a quick comment about the performance and this is we are still in development so we very vague numbers and "
    },
    {
        "start": 3088.309,
        "text": "obviously does depend on the data set but the isotope being we typically get from 10 to 20% reduction and we're not the only ones doing the isotopy a lot of people do that so that's sort of cheating in a way but it's important to do that first before doing the annotations and then after that we get anywhere from probably of the remaining features probably 10 to 40% it really depends a lot again we're being interactive here we have a lot of parameter settings you could be either more aggressive or less aggressive in your annotations the more aggressive you are the more annotations you get but the more false positives you'll get if you're less aggressive than you probably were or more toward the 10% range but you can rely on those annotations more strongly and we're still still going so we may get further we're still a lot of structure that we haven't really elucidated if you start looking at all the patterns its we're all going not just looking everywhere it's it's really fun to look at it but um we think we're doing pretty well compared to what's out "
    },
    {
        "start": 3150.049,
        "text": "there right now and so we're putting together manuscripts and hope that it will be reviewed soon but that's so we that's that's a lot of reduction basically there's another just a quick story I want to share that's not in the slide but something that Maureen shared recently was another group that had was it 15 or so significant metabolite so so when people do these analyses is usually one a disease versus a well group or something like that and so T tests or something are performed and what they really are interested in scientifically or what are these significant metabolites what might be driving the difference between the sick and the well people well so we give the the core gives them a list of or wherever they got their samples done they get a list of the most significant features in this case I think there were 15 top hits as they're called 15 metabolites one of which is glutamate well turns out actually ten of them really make they didn't know that so they were basically thinking they had 15 different compounds that they had to investigate in fact when we annotated "
    },
    {
        "start": 3210.83,
        "text": "and we found out that one of them was glutamate two of them were isotopes of glutamate and then seven more were pretty easily annotated addicts or or variants of glutamate we actually but um so that just shows you that you can't really trust your your hits in this kind of data to be independent of each other and it's really important to look at the redundancies to try to decide whether you actually are looking at what you think you're looking at in this case they really only had probably six independent hits it maybe they wanted to go a little bit lower and their p-value more so we hope that's one of the other values besides just reducing the overall number of metabolites and so right now the output of binner is an excel file I think has been mentioned throughout the talk it's not just an excel file I mean "
    },
    {
        "start": 3271.91,
        "text": "it's a pretty complex excel file at that I mean it has multiple tabs and all the heat maps and all the sort of you know math differences and you know all these things are sort of you know already pre formatted or you know coordinate and so on and so forth and so I mean I guess on one hand this is great because I mean I guess you know as we all know scientists like to work with spreadsheets it's convenient it's familiar interface and so on and so forth I mean there are some things that we think would be potentially you know if we create sort of you know fully interactive environment like in Java for example then you know potentially we could provide access to spectral data or you know basically look at the peaks and so one example here as I mean we didn't really talk much about outliers but I mean so we're looking at sort of you know multiple samples right and so before it ever gets to be no the "
    },
    {
        "start": 3333.77,
        "text": "program basically you know has to go and sort of you know align all this sort of you know peaks in different samples right and so here is an example of well integrated peak and it's fine but on the other hand on the right on the right there is an example of a poorly integrated peak there it just sort of you know cannot decide the program cannot decide which one is the right peak and as a result you can key I mean it affects correlations it affects retention times right and so I mean if you look at it and if you especially if you're trained to look at this data so you know marine can look at them and say well you know what I don't believe that integration I'm going to do it manually or whatever but I guess you know it affects your I mean in general I guess you never have some examples where you looked at outliers and it affected so so this is just you know some examples of the things that we think could be handled in a more sort of into our integrative sort of interactive "
    },
    {
        "start": 3394.43,
        "text": "environment so that's you know at least one idea so just stop here and just want to emphasize again that none of this would be really possible without you know working side by side with metabolomics core and a lot of ideas really you know came from marine and all we had to do is basically bill her to do and Jen had to do to try and you know figure out a way to code the certain sort of you know logic into it how we would want to reproduce the analysis that you know people would don't know who would do sort of you know some I mean unit which is not a trivial exercise but it's not the worst way to develop software so I think I'll just stop at that Thanks yes I have to reveal my ignorance here in my question but so in the example that bill gave where you identify ten variants of glutamate that actually to "
    },
    {
        "start": 3456.93,
        "text": "mate not ten different things Maureen do you go back to the bench then invalidate in completely independent of thinner to say yep sure enough I got generations of glutamate so what we're doing right now there's a few ways we're trying to do that some of it is about leaking we're gonna die chopping patterns because certainly addicts that are forming are going to have a very distinct isotopic pattern compared to the parent ion glutamate there are some other really obvious and use that have obvious if you are mess of atomic shows that are happening when you look at trends and sort of the what's called the man s defect you know the the number after the after the decimal place after the integer to kind of show that it's very unlikely that a metabolite of what that mass defect would occur at the same time is glutamate so there's some other "
    },
    {
        "start": 3517.05,
        "text": "things that we're doing to sort of validate that but it becomes very difficult to do that on the bench because first of all these are transient things it may occur an addict maker on glutamate today Iran and tomorrow it's not because it happens in this source and is very high pressure and high voltage environment so if they're always changing but so those would be a little difficult and and and then the other thing is for example I mean you could do em SMI sensing like inter loss Peaks where you have a fragment and prove that that fragment is that or if you have an instrument do Emmet's to that's there for example so there's definitely ways of doing it but I was thinking of a technology that's independent of mass spec also for for gene expression studies I find a gene that's differentially expressed using sequencing technology you know you come back to it with qpcr and because it's an independent technology or at least quite a bit different is there at a conference called before this where we're looking in Airlines data and what "
    },
    {
        "start": 3578.28,
        "text": "we think is some some fragment ions of something like felony for example and one of them that comes out has exactly the same mass as benzene and so the question was like how do you know if that benzene isn't part of an environmental exposure and that's really there instead of part of phenylalanine and so you could think of a way that you could you know do some sort of an exposure study where you could say look you know this one that has a benzene exposure and it doesn't very compared to this one so that's really I mean that would be an expensive way of doing it but that would be one with money and then another thing I was going to mention something else so just because we happen to be part of several large consortium marine spent all the time I mean they were like you know reap trial data from couple of them she can tell you about it I think it's also valuable I mean basically what it is is different weapons that have different instruments so I don't know if so you know passes for independent validation over there but they can have completely different platform but they're running the same set of samples right and so I don't know "
    },
    {
        "start": 3641.91,
        "text": "absolutely I mean so basic marine was able to show that you know it's it's extremely unlikely but if you're using completely different chromatography and a different instrument they're seeing the same highly correlated teachers and so I think it sort of adds confidence to the identification here you know so and so it's that example where they had annotated something as benzene and it was cool leading with phenylalanine that was seen in one mass effect platform where they were doing a pallet chromatography and we are using reverse phase chromatography so proton would be so different the fact that we both saw both of those Peaks and exactly the same time it's very unlikely I mean it not was just - it was like six because the chromatographic Li benzene should separate from phenol I mean in my platform or theirs I mean you have one of the two it you know I don't know which one would be more likely to separate in but it would in one of them actually if I understand this correctly benzene as benzene and you can look "
    },
    {
        "start": 3702.36,
        "text": "though if half of that benzene came in fact from environment and the other half came from like it's a breakdown product of saying on it's not the situation where you have you know you can tell that you have some bacterial sort of DNA or RNA present in your sample become analyst this is not human DNA right so there's no such thing anyway you can tell it is your chromatography if I injected an authentic benzene standard an extra two minutes but I'm getting a peak that says it's benzene that's coming out of three minutes and the thing I three but it's not benzene so it's just we can validated in certain ways do you report at all the very early in the process if it wasn't in thirty percent of the samples chocolate do you report those somewhere there's never a report tab at the beginning to report those might be interesting if they were abundant depending on number so my future better version that would be to be able to "
    },
    {
        "start": 3764.55,
        "text": "specify groups as well and say we want to only do the reduction using any symptom of a absence or presence cut off with it well therefore because something may be present entirely in one type and not in the other and you don't want to get rid of that so thirty percent can you know if you examples I could get rid of it but you know it's what software does only that yet still even then through software they don't do that effectively so they're programs don't think on those downstream analysis programs that allow you to do that and I mean so it's show me yes so so this is a summary tab that we did we provide basically you can see that you report how many odd in this case that weren't even seen values but it would work neither were we don't actually list them it's a fair question might be interested in seeing that information we're still expanding what we have in somewhere especially thin reasonable signal strength right and so yeah there's always the question of is that biological what's even more alarming "
    },
    {
        "start": 3827.97,
        "text": "notes if you don't look at a database like the Kevin Lewis workbench that very few people report any pre-processing we do so and not know what it has done in magic yeah but but I think it's useful because I mean cannot remember like you know two weeks from now nothing to remember what it was yeah sure too much there yes so sure so we like showed the mass difference distribution so that is actually it was a screenshot from the output we did generate that automatically we have here compounds mass and retention time that the master tension time and intensity are the three things that we use to do all this and then it's not II mentioned there are some other columns that you can bring along is Kendrick mass effect that's used in other to look at contaminants I guess mostly it's probably the biggest use right now other things as well anyway then we show our annotations and "
    },
    {
        "start": 3890.59,
        "text": "sort of the math of how that what those what those masses correspond to the clustering arrangements and then the correlation Maps basically for each cluster and there's the visual of the beam here if you're looking at a big data set the green price it's over to green or the base isotope or the base of the isotope roots and the blue or DVI supposed to be fine and then similarly and the annotation part green means the base ocular ion and then the yellows are the sort of derivative secondary annotations so we try to make it a little easier to scan all this enormous amount of data and it's a lot easier okay and then what's another let's see trying to think if there's something else that's interesting nationally right well the mesh differences no way though the making sure yeah yes the arrows not "
    },
    {
        "start": 3958.62,
        "text": "unfortunately first because of the resolution you know I'm unable to get from one to all the tabs once here we should probably wrap up anyway since we're an hour maybe take the rest offline sounds like a plan you "
    }
]