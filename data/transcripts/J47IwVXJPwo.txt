the other gang set up just want to mention there should be a sign-in sheet floating around these caps around if you already signed it at those and is the next person and he didn't keep your name so welcome our wrongful tech nakshatras you're sorry to the pizza arrived a little bit late so getting a fight earlier than women hurt again saying she is passing around to sign it and so today's speaker is why I am and here they research investigator it to CMB and just about multi-omics integration alright hello everyone welcome thank you much for introduction so let's start okay so procedural medicine the because of the precision medicine is not newest from how intuitive the fact that one treatment doesn't doesn't fit all the patient suffering from disease we can see it from two different angles one is that the patient's made respond differently to the same treatment or we need different treatments to get the same outcome the same response will be differently from from from from different patients so this idea may come from the fact that traditionally we categorize diseases based on symptoms based on what we can measure but of course the the very same disease may present different symptoms and the truth is that is probably some molecular mechanism that is evident from the norm and that can be different in they can be in a spectrum of diseases these are also relapse with for example there are drug repurposing drug repositioning in terms of shared will occur mechanisms or aberrations this is just an example this is a people show you that people taking or any medical reason antagonist of these beta to Adreno receptor have you found to be less at risk yet the Parkinson's disease because there is a shared there's a common molecular mechanism now problem is that we don't we don't know the molecular mechanism maybe we cannot even measure them anyway starting with that said starting from this we could think at seton oncology as the perfect arena to play precision magazine because cancers are categorize are classified maybe on what we can measure what we know about them for example classified based on histology but we know that deep down maybe the same kind of cancer classified in the same way can be in truth different because of different underlying molecular mechanisms due to mutations so how does particular oncology applies well it depends here I am bringing the data from this trial we're about 200 patients were enrolled and warm last secretly they were split between precision oncology and traditional approach and the results were apparently not spectacular it was not a very big difference between the precision medicine application and the traditional treatment also only less than half of the patients could be enrolled could be how can I say they could be destined to having a precision oncology treatment this is another trial where we start with about 700 patients for then when we look to find these normally core aberrations that can trigger the precision oncology treatment the number of patient that can have the treatment drops down considerably now why is this well it's because if we look for something we already know from literature if we look for we identify molecular signatures in cancer we are not assured that they are common okay maybe we can pin down a specific aberration which we know but respond to a specific treatment but it's very rare and capacity the the outcome so for precision medicine treatment starting from these premises we will need large cohorts alright so here I'm talking about a different approach so instead of looking through the literature and find this this molecular signatures we can try to find Scylla patient the the Assumption here is that similar patient will respond similarly to the same treatment we choose in terms of data science clustering with cluster patients that are similar and this is the base of precision medicine in order to cluster patients that are similar we need to define a similarity okay and so we will maybe discover signatures instead of using three identified ones now why not you use video mix so we are finding similarities between among patients and we are using all the data we can get that now we have a lot of different at your genius data you can measure on patience we have genomics establish transcriptome proteomics we have blood test clinical variables that our electronic health record we have imaging okay so we can put all together and cross the stream of patients to see if these clusters correspond to really two groups that are responding in the same way to the treatment we're not so the way we do that is using the react use direct information something that we measure from the patients is just a toy example so I have a certain patient in this patient represent represents a sample now I measure that we're talking about cancer so we let's say the sequence sample from the patient and we can pinpoint which genes are mutated is it again this is like oversimplifying but just to give you an idea so each station can then be represented by a vector write a binary vector is each each element of the vector corresponds to a specific gene and I'm just checking if their gene is mutated or not I'm not considering how many mutations are not waiting the mutation just really putting a 1 if I find a motility all right so this is direct if I had 10 patients I would have terrorized 10 binary vectors I can't cross to them well I could apply for example intuitively euclidean distance and find the similarity between the patient find the matrix that you squared patient number two by patient number and find each how a patient is close to another it could be this is just an example okay sequence all the genes I could look at all the genes I could you die I could but well that's that's really general okay it's not important so far what our interesting thing is that usually we we don't know just what we measure on the on the patient we have other forms of knowledge other information which are relevant but not direct I'm not directly measured on the patient I mean what I mean by indirect information I mean but what can I find the literature what can you find on the databases which kind of information I can drag out of decades of biology and medicine we have audiologist gene ontology the disease ontology we are databases connecting genes and diseases like this Gina we have coding databases for interaction databases gene interaction databases and so forth now all this information is usually ignored right a cluster only based on what I found in my patient these indirect information represents the relations it can be seen as representing the relations between the features that they measure on a patient practical example you have the sorry you have the patient okay and here are the mutated genes now I take the first gene and I call it Jean J and from literature I can check out which genes here which of the Jews other genes here are interacting with the gene and okay the genies interactive Williams with itself and with this other now I can he not intuitively plug in this information here right because it will be if I just just opposed this vector to this other vector I will find that the features are repeated equally for all the patients that information is useless so what I'm proposing to do the presenting here is a method that will transfer the patient that will measure the similarity among the patients and will be based on both the direct information that I traditionally get and the indirect information that I can that I can line out databases and knowledge bases and so forth I want to plant all together in a model that will output a measure of patient similarity out of that I can get the clusters and of super of patients okay so genes they can be considered interacting by a different set of rules for example co-expression Eugene Co Express and then we say the Internet another way is Eugene Scott reporting's proteins interact they bye to each other then I explained that I defy the two genes has interacted now there is a plenty of variety now can i define interacting genes and this is reflected in a variety of databases and rules and the different databases to define interacting genes might even provide you a score or a measure of the certainty of the strong of the strength of the interaction that can go from for example oh this interaction has been predicted by an algorithm to beautiful experience by independent teams targeted experiments prove that this Court's traction is present and we're pretty sure with a meeting in the middle there is a cool expression that is the zoomed out of high-throughput experts okay so countries doesn't string have interactions as a spoken zero to one point that depend very much depends on how you're measuring it then you can rescale it between 0 to 1 if you want one whatever yeah interaction its final the confidence of the interaction the fact that the interaction is real is not for us what if I measure co-expression okay then I'm not necessarily forced to binarize it the cool expression can be strong or can be weak that can be a little bit of it that we are lot of it same goes for putting bindings you can binarize playing bindings and save it for things bind or not or you can measure for an affinity okay we're diverging guys can can I can I go on and later we can't how you just say what you're showing was just a toy example yes okay thank you we can discuss more later let me go on now I can do supervised learning if I am our label attached to each sample which can be a class or it can be a continuous value in that case I would do regression with a class of the classification but that is not what I'm presenting here okay whatever sending here is unsupervised learning so clustering of course we made the wrong route of validation to prove that the classes that we find are meaningful but if we are going to find the clusters we don't know from the beginning we don't know how they are distributed we're discovering stuff we're discovering saying that here we are discovering groups of similar patients so this is not supervisor now from one example let's go to a practical one now I can get off database data set patients from TCGA a patient suffering from acute myeloid leukemia and you have a direct information that having married on these patients so I have clinical data patient the samples from the patients they have divisions in the I can also miss of the gene expression profile now I could measure similarity just with this and I could measure similarity just running a similarity measure on on all the possible pairs of this patient that could use any distance I could use Euclidean this then I could use stock or distance I just have to set the rule but what about the indirect information well here is the direct information that I'm use I'm using here so the information this is yeah then I designed these algorithm which is a way to weight the deleterious ness of the single nucleotide variants so instead of saying ha this patient has this single loop that variant I can say ha this patient have this single ethnic variant which I suppose to be this much or this much or very much dangerous just a way I could have not use it in case but and then I can plug in G interactions pathways and so forth from other databases information that I can find freely online they have the API you can write scripts download filter and you go up now will it be nice to put everything there act and in the information one invade matrix yes this is exactly what I'm doing here so this metric is pretty big here is the number of columns I'm protists square matrix it is a block matrix in each block here is representing a data source it is symmetrical so for example I have the clinical data and the patient from TCGA and I put them here it's a relational matrix so it's filled with the relations between the rows and the columns so the relation between mutations and patient is for each patient I have if it's by night for example I would have a run if the mutation is present in the patient but in these cases that there's a way to apply or if a gene is interacting with another gene then I would have one or a weighted number in the corresponding element of this block connecting genes and genes and so forth this matrix is extremely sparse these parts not just because who walks of or entity but even the boxes are not are sparse okay put some numbers here but this partners is maybe sometimes over 90% in some blocks ok now once one sweet we we have this matrix put together here is a result the proposed approach to integrate direct and indirect information and extract the latent features so we go for matrix factorization 3 factorization so here we have the initial matrix and well before you let the Machine crunch the numbers for a while I can't come up with three other matrices actually two other maps G s and G transpose that multiplied together would you be back there's initial matches so you can you're not here that the Diagon is empty dues for technical reasons so compared so these this Matthew this big matrix has been split into two and the blocks of the diagonal are filling here another matrix which has the same dimension as this one and this one is the mention I showed before but the Amazon diagonal gone okay that you've moved here so to put it simply these mattresses are smaller okay so the size on the rows is the same this the the number of columns is way smaller right so if I can factorize the information I have here into three different mattresses that are smaller it means that I am I am in principle filtering out all the noise and keeping only the useful information if I multiply this by this by this and they are smaller than than this one and if I multiply them together and I get emeritus which is almost exactly identical to this one that means that I captured capture the information in the latent features that are here so why I am doing all this is because I want the latent features and I will come back to this in a second is this sorry it's non-negative matrix factorization it's not based on you mean eigen values but I I didn't get excited the word well anyway it's non-negative matrix factorization now this factorization is driven also by constraints from these methods so I'm trying to reconstruct the a matrix that is representing relations between different elements because the diagonal is empty so under the Yaga and I have the relation between the same kind of element like gene and gene pathway pathway mutation a mutation and so forth hey I am reconstructing these these math equations without without them but they are true and they drive the their drivers constraint the factorization now I'm doing this because in the latent features here and we find theoretically embedded all the information of all these different data types and data sources that I cram together so this is the difference between this method and for example the normal non-negative matrix factorization which would have only two mantises here in or and should not have this okay then hopefully the latent features will will have embed that with you for besides the info besides embedding information that I extracted from the patients will carry patterns from the databases in the external indirect information that they put into so how can I be so sure how can I see if it works or not but one way is to simulate so if we simulate we know the ground truth something that of course doesn't happen in reality we know that not truth so for example here I take by patients out of the 200 the 200 patients and I use them to simulate 40 patients pseudo patients each so in other words I create five artificial clusters made of 40 patients inch how do i simulate well I just up put dirt on the data so I can use noise I can make data more noisy or I can remove some data I can't add missingness so to speak so I can take five patients that for each of the patient I add some noise I remove some some data and and and so forth indeed this way I create five five clusters now I don't tell my algorithm the ground truth this is a supervisor I run it and I check how it goes so for example we have two hundred simulated patients right so this is a similarity matrix this matrix is telling me which patient is similar to each other and this is the ground truth so we have five patient high clusters out of five patients one two three four five then I knew from the beginning that they are coming from the same source so they should be all similar they should be your closer together and the others should have a similarity of zero it should be different now let's say I can I can run my method not necessarily with all the matrix factorization that I described before I can run any method that will output a similarity matrix once I have the similarity matrix from the method I can compare it with the ground truth and see how good the classroom was argued the similarity measure did work so in this case if this is my result I'm kind of happy because the distance measure reconstructed the classes but I'm less happy here because here are some false positives that there are similarities which shouldn't be okay and I can subtract I mean I can measure the mean absolute error out of all the elements of this matrix and average measure of how the mattresses are similar this is exactly what I did not just with the with the method for data integration that I described before but also with other similarity methods so we can run clean distance for example on the raw data we Trent we can run a PCA on the raw data and then calculate a Lydian distance on the PCA element we can use the ability networks which are deep learning algorithm in we can run group factor analysis which is another factorization technique very similar to the one that I discussed but not considering the external the indirect information okay so here we are here we have the five methods applied and for each method there are is 25 little squares so each square is a scenario obtained by adding noise and removing data so so here we have a growing percentage of missing data to simulate the patient and here we have a growing percentage of going to protect growing amount of noise this may be the person that you will not bother you with human details anyway here is very simple here is very hard and as we can see the these two I can I say no fancy method are not working very well the color represents the mean mean absolute error and methods that are both simple such as calculating euclidean list on the PCA or or directly on the date now are working but still the method that I showed is consistently working back the simulation the mean absolute error is in will not spoil the noise and missing data scenarios or ask them panel 5 also we can see that other methods are for example the deep belief Network is suffering a lot from missing us but not much from noise okay so very good apparently integrating indirect information is is useful to find the real distance of find patient distances and crosses but another way to to show that it that the the clustering makes sense is to do the clustering and then use some information that was not included in the clustering process to validate the clusters so what it's simple with in the in this data we don't have information about the treatment ok for this patient but we have information about their survival so we know which patients were still alive after 5 years we know the ones that died and when they died in the survival is basically the main descriptor of let's put it brutally to the council of bad the cancer is the favorable or unfavorable outcome of the cancer can be seen as the survival time now here let's remove the survival let's remove this information let's run the algorithm let's find the cost and let's see if the clusters are different in terms of survival if they're if they are and of course they are otherwise and within presentative Lots they are meaningful okay so this is these are the survival curves for the five mountains apply to the same data without the survival time to clusters for each method in the proposed emitter that I'm proposing here is the only one where the survival curves of the two clusters or this thing so without knowing it just by grouping together similar patients this algorithm found a cluster without more favorable outcome and across and without more negative outcome it's important to see that all the other methods fail all the other methods they present curves that are more or less superimposed I can measure the p-value the only p-value really significant p-value is here with the method that I presented in is about point O one so okay Robina I presented a patient similarity calculation approach that exploits both direct and indirect information the data structure is conserved what I mean by saying so is that remember it is Mavericks when I put my data inside his matrix decide some basic linear scaling and not talk about normalization but just linear scaling I am NOT altering the data as I found them in the databases the the data structure is respected nothing is compressed nothing is particularly elaborated I put together and it built for space spice or Express if your genius data and he can be exploited in tariffs or other products which I will illustrate in a second another thing that I want to point out and I mentioned this when I explained this method to medical doctors vision group we are doing here finding the patient similarity is the machine learning or carburation equivalent of a very human thought process which happens every time a patient steps into the doctor's office showing the blood test result describing the symptoms and the doctor goes back in your mind and even unconsciously Paris this new patient with what she already sold would she already know with the patient she had treated in the past as it is that this works for diagnosis this works for like finding the right treatment it's and of course if it's a human thought process process is subjected biases we know for example that more recent experience are affecting our decisions more than experiences that we had in the past but this method doesn't he doesn't have this kind of human biases or all the patients are like considered equally there's no timeline although of course there are other problems okay there are may be other biases but not the human ones okay so but what is fine there's approach a little bit we can get a lot of we can apply it to all those different problems here is another application or basically the same principles of putting together external not directly related information with which you measure so in this case is Brutus 40 protease protein target pairs prediction now proteases are protein proteins and they have some specific targets and when they find the target they cleave it they cut it okay this problem by informatics is very well know the first algorithms trying to predict for days and for these targets were deployed 15 years ago more or less and and yet what happens it happens that some proteases are very well studied while others are not there is not an equal distribution of the human effort in studying 40 ages because some are more interesting than others so so for example you are your we're aware of CRISPR technology CRISPR is crisper cast 9 Caston i stands for caspase-9 caspase is proteases proteins okay so caspase families very well know that there are others that are not and if this creates a sort of imbalance in the predicting algorithm which are tilted towards predicting targets for the superstar proteases in by apply this method elaborated algorithm that can predict a range of targets for proteases for a range of proteases that is way wider than state of other state-of-the-art algorithms most of the state-of-the-art algorithms are predicting maybe they have a different model to be trained for each protease or maybe they work for police families or maybe they work for target families but with this approach I could have a truly general model working predicting targets for any protease and and most of all most importantly it works better than other state-of-the-art algorithms even only their pacific target cells another thing that is possible to do by expanding the similarity / clustering method that i presented here is here calculating predicting gene-gene interactions cool expressions in this case predicting GG in co expressions without know the expression data so this is personally I think this is cool because when you wanna the zoom gene-gene interactions of course you need gene gene expression if you have a measure G gene co-expression how could you do it without it without the expression data well with this method so in this case we have our data set of patients suffering from mild dysplastic symptoms in for them we have the single nucleotide variants measured but not the gene expression okay but we have gene gene we know that there are gene-gene interactions that there are domain domain interactions that are put that there are protein for the interactions and we know that domains belong to proteins the proteins are encoded by genes and so forth so we can brand again or together find latent features the scrap this describing genes and using these features to find the candidate gene interactions so in this case I don't I don't I'm not looking for the studio of a GE with all the other genes I'm just looking the single gene gene pair and I take the the ones that I've top similarity and I claim that they probably are Co expressive okay I claim that how can I prove it well of course they can use for example this database string de riches which was not used in in the process and I can see that these independent source of information is confirming it well most hours house of the kid that interaction are indeed interacting and was then more than off ever score that the same database define as high confidence another way another way is to take external data sets okay let's go to G row and look for MDS data sets that have controls and gene expression now since these 320 Kenya interactions come from Jesus only I expect some of them to be interactions that happens in any bone marrow cell including the the healthy ones but some of them I thought should be present they should be typical they should be a signature or bio dysplastic cetera they should remind them out of the mutations so they should be there are there are they well yes set them okay so I can see if the coup expressions are significantly different different between cases and controls and I find in the first dataset which is as Morris 24 thirty four in the second liters of them 14 the third dataset and you total there are 83 gene-gene interactions that are differentially expressed and so does indicate that they might it probably really belong to aberrations of the mild explanting syndromes 31 of them are present in two sets and 13 of them are present in present in all the three sets okay you're a conclude so this algorithm is there is a get lab you can download you can use it you have me here so bright me for any question if you want to apply to your data I'll be happy to talk with you to tailor it to your needs and of course I cannot conclude without thanking the several people behind all these projects my colleagues and friends spective university of pavia annually university of arizona arizona here you meet you the ECI F ECE sorry here the University of informatics center and of course the Lee lab which is my lab for the support net so I can criticism and thank sighs I wonder sighs that's an excellent question no unfortunately I cannot answer right now because we when we reach the results we were in a rush to publish it but I am working on it and it won't be interesting to measure not on one data set and one problem but over you know this right spectrum of problems how adding each data source is influencing the result and I will definitely do it in a new time and also the like even the same size information absolutely so one one and one question without correct answer is how do you decide the size of the latent features so for some of this work we use just a fix it sighs it was reasonable for other for example for the protease protein prediction problem since it was supervised in that case we did the cross-validation on a training set to find the optimal size there are some indexes to describe the sparsity of the matrix compared to the rank that you can measure in literature you can find a lot of different approaches to define the size of the latent features but unless you go for an empirical approach which implied implies supervised learning there's no sorry to answer my take as an engineer is if it works you're happy if it doesn't try with with other sizes but again we we didn't I don't want to be misunderstood when we didn't for example for the patient similarity 1 we used a specific matter to define the size of the latent feature that is based on the rank of the Barack's divided by a number which is related to the nonzero element so the sparseness what whereas we decide to go with it we went with it we didn't try different different sizes and then keep the best okay we just choose one and also because these calculations are kept can take time it can be very big mattresses so that was the strategy that would yes if you said early on that you did not have any treatment information we even we didn't plant them in yeah buddy why you excluded that um because with that couldn't you use this forward they should dose absolutely absolutely this part this has been done with a similar method a bird was not exactly this one but I was not in charge or cabinets so I basically work with what I've been given and honestly I should go and check if the treatment information is present or not I don't know but it's a good question I will definitely work on a problem which is to cluster and then check the treatment later maybe instead of survival for example to find similarities and then see how they match with the treatments or support yes how do you actually assign the indirect information oh the final sign like put up you've got to be putting in your table you see yeah so how do you decide which indirect information goes to which patients oh I don't decide which direct information goes to which patient endurance which indirect information goes to each patient for example I cannot measure pathway interactions of a pathways in patients or maybe I can but they should not do and so it's empty so anyway you have to make arbitrary decision to how to represent data so for example the gene interactions are not commuted to patients they have their own walk they are not put into patients but which patient has regenerated this year so but that's the wreck I hope I answer your question [Music] so the data sources can be both direct and indirect okay so if I'm talking about path what it's like pathways here we didn't put anything for the pathways but we could have we could have for example choose to say look we check the mutated genes for each patient and we sum the number of mutated genes that the patient has in a specific pathway and put that information so information highly correlated yes and it would sound something so we did well we didn't do that but we could have yeah Chris so you could just have it acting like weighting factors for which they come yes yes that could be and and this is one more reason why it would be very interesting to try to take away this blocks one by one and see how the result is is modified or try different combination the point is it will take time and I'm working on several other projects but I will do it thank you for any last questions right right right so we made sure that the Euclidean distance was higher between all we know the possible communication of them was higher than the average distance of that patient from all that but but in this way we actually helped any similarity measure based on including distance and in fact Euclidean distance on the data and on the PCA they've worked better than deep learning okay I think that that's why that's one of the reasons what well s is not symmetric yes yes is not how comes in vitro system entering the dot product is no up it was isometric okay wait this is symmetric G is no it's not symmetric G is the area it's brought a yellow you've set the number of columns is smaller than R then yes it still does it still grow the N so G square no no use not square but it's not enough it's okay yes as is yes you can you can if you have a matrix which is a ten by five okay can you see what they are the first five rows bold that brought the yellow it's different from the arrow right of course it couldn't be the arrow but can be blocked that ya go I maybe later I can't making sure you the data or we can discuss this all right