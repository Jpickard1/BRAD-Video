think that's actually the only announcement I have today so with that I'm pleased to present today's speaker we have again Lee who is an associate professor and biostatistics welcome to mine we're just getting started all right again uh feel free to start when you're ready okay thank you thank you everyone for coming and thanks for having me present online [Music] she'll probably have to leave a few minutes early but if you have any questions feel free to shoot me an email or if I cannot answer the question during the talk so uh without further Ado I'm gonna introduce some new methods that we developed recently for microbiome data analysis not only based on this uh idea of amalgamation to deal with this microbiome compositional data and this work is a joint work with my postdoc Dr Yen Lee and my collaborator Clinton at the University of Connecticut so microbiome is an important component of our human body and it consists of archaea bacteria viruses fungi Etc and Studies have actually conjectured that the number of microbial cells or genes would even outnumber the human cells and genes so to some extent we are more of the microbiome Al and in US rather than ourselves and the microbiome is not static it's very Dynamic and it changed with time with different health conditions and as a result it's a good indicator of many health problems for the human body so there has been a lot of study trying to associate the microbiome profile with various health conditions in human and in fact there has been a lot of successful stories there and more recently there has been a lot of studies about how to use microbiome as a treatment Target for various diseases so it's really a very burdening field with a lot of opportunities as well as challenges so I'm a statistician so I'm mostly focused on in analyzing the microbiome data say what the microbiome kind of really tells us so to start with I want to give you a very brief overview of how microbond data are typically acquired so this is a figure stowed from a paper published in 2013 so some information might be a little bit outdated but you get the idea so generally samples were extracted from the human body if you are interested in got microbond typically people take samples from the from fecal samples and if you are interested in low microbiome for example you can take samples from flung Etc once the samples are extracted it's gone through this PCR procedures again magnified and then people use sequencing technology to sequence the micro the microbes in these samples there are two main Technologies one one is the apple consequencing or 16s rra which is the most commonly used one and pretty cheap and the other one is the shotgun sequencing or metagenomics which give you more granularity but rather than speaking more expensive and once you get the sequences you can apply some existing analytic pipelines in by by informatics such as the mother pipeline which is I believe developed in Michigan by the uh by by a research group in Michigan and also there are other pipelines that I can use to pre-process the data so you can get some more well pre-processed data that can be directly used for statistical analysis and after this bioinformatics procedure we got what we call the otu data table so essentially you can consider that as a data Matrix where um rows are otus or in other words microbes at different species uh different taxonomic levels and the columns are different samples and typically we also have some additional information such as the taxonomy and the phylogeny of this different microbes and those can be used for further analysis you can collect additional clinical variables trying to associate it with the Oto table and do various statistical analysis so as a statistician we basically take data from this otu table and conduct a further analysis from there so that's also going to be in the focus of uh today's talk but I would like to acknowledge that there are definitely a lot of uh interesting and important statistical questions in this pre-processing of the microbiome data so that's something that needs to be further looked into so once the Oto table is available there are various statistical analysis that can be conducted on this data set to name a few you can quantify the diversity of different samples of a diversity beta diversity Etc we can also visualize the data try to say particular patterns in your data set or you can conduct a differential balance analysis assuming that you have several different conditions you want to really identify which tax or which microbes really is differentially abundant between its different conditions for example house healthy versus disease what is the driving factor or what is associated with that differentiate between the two groups you can conduct this differential abundance analysis or you may be interested in performing network analysis or regression analysis so there are all kinds of analysis that can be performed on the Oto data table so you may Wonder then what's the big fuss about microbiome data why don't we just use the existing statistical method to achieve this goals well they're not so straightforward in the sense that microbiome data has its own unique features so that's why um in many cases the existing method may not be appropriate or sufficient to analyze this type of data to be more specific due to the technology limitation microbiome data although we want to get the absolute abundance of different microbes but in reality typically we cannot obtain that and what we can obtain is really the propulsion of different microbes in the sample or in other words the data is compositional and so on the right figure for each sample you may get a composition of different microbes at a particular level and the the actual value for different microbes is just the propulsion of that microbe in the sample so it's sum up to one then that we call it the compositional data and this compositional feature creates a lot of challenges for statistical analysis because most existing methods is only developed for um you know continuous regular data in the including space but this compositional data does not reside in the euclidean space it's in a Simplex so many methods or Notions do not directly extend to this type of data so that's one major challenge another challenge is that the microbiome data typically they have this additional information um that it can be leveraged in analysis for example we do know the taxonomy of different microbes and sometimes we also know the phylogeny of different microbes so those are all tree structure that capture the relationship between different microbes so how to take into account that additional information in your analysis to make the results more interpretable and more scientifically reasonable is a challenge and in addition the data is typically zero inflated meaning there are a lot of zeros in the data and it depends on which level that you analyze your microbiome's ad um if you look at the spacious level you may get excessive zeros in your data set meaning a lot of species actually do not exist or maybe just their bonus is just too too low to be detected in a particular sample resulting in a bunch of zeros again those zeros also going to create a lot of challenges for using existing statistical methods and on top of that we also have the measurement errors these propulsions or this relative bonuses they are not measured very accurately so there are a bunch of measurement error and also the data is typically of high dimension so these are all challenging features for statistical analysis of microbiome data to address this challenge the most common approach is first transform the data try to leave the data from the Simplex to the Ukrainian space using some transformation and then apply existing methods that's still the most common approach in the literature for microbiome analysis and the most commonly used transformation is called the log ratio Transformations for example you can use the center log ratio transformation once you log transform this composition divided by the geometric mean of this log Transformations then the data is gonna is going to take in the range from negative Infinity to infinity and you do not have this uh sum to one constraint anymore and the log transformation cannot directly deal with zero so what people do is to add some pseudo count to these zero counts so to make it non-zero but those pseudo counts as sort of arbitrary and it definitely introduced bias and noise in the subsequent analysis but that's sort of the state of the art uh methods for dealing with microbiome data namely first adding serial counter deal with zeros and then apply this uh log ratio transformation and then apply existing statistical methods so we thought we thought that this transformation based methods although they are common but may not be most appropriate for analyzing microbiome data especially given the dominant features of conversationality zero inflation as well as this additional tree structure that govern the relation between different microbes maybe there are some other uh there's a room for method development that can be better tailored towards the analysis of this microbiome data so that's sort of the motivation for our analysis or our method development so next I'm gonna focus on two aspects um one is dimension reduction and one is regression analysis so demonstrating how we can further uh libraries in this compositionality of the data to come up with more appropriate statistical methods for analysis so if you have any question feel free to interrupt and happy to take any questions along the way so to start with uh we first look at this very fundamental question of dementia reduction and come up with a new amalgamation based reduction method called principal amalgamation analysis or PA for short so the dimension reduction is a very common commonly used tool as a preliminary analysis of the of the data set it facilities visualization interpretation and downstream analysis so the main idea is we want to reduce the dimension of the data while still being able to capture the main information or major information in the data set and in for microbiome data there are several uh ideas for dimensional reduction uh one is the uh you know extreme reduction using some indexing method for example we can use the diversity or complexity indices to quantify each sample and we can Summarize each sample using just the alpha diversity or some other complex complexity indices that's an extreme reduction of the data right you can reduce the original High dimensional data to just a single digit summary so that's one that is one reduction method for microbond data and it's it's commonly used but it also have apparent limitation so you just reduce the information too much and only give you a little um a limited perspective of the data so another way of doing dimensional reduction for microbond data is using selection namely we only restrict ourselves to a subset of the microbes and a subset of dominant compositions and that is also very common in real analysis so people tend to ignore taxa with lower balance levels some places to drop them and then just focus on the small side of textile with sufficient abundance so that is another way of doing reduction and finally there are transformation based methods namely your first transforming the data using this logarithm transformation and then using existing dimensional reduction methods such as principal component analysis or principal coordinate analysis to get a low low dimensional representation of the original data so the third method this transformation path based method suffered the same limitation as other transformation-based methods right so it doesn't really accommodate this special feature of the data uh including the compositionality and zero inflation so looking at looking at all these existing methods um we sort of felt like it's not sufficient to really capture the what is uh what is needed for this field so we can we ask the question of what constitutes an interpretable and in fact a reduction of this microbiome compositional data and after some thoughts we feel like the amalgamation may be the right way to go for reducing the dimension of compositional data in particular amalgamation means aggregate compositional components for example here is a toy example assuming that it starts with a five-dimensional data Vector of compositional data meaning that X1 plus X2 plus all the way to X5 equal to one okay so amalgamation basically is simply means you combine some of these components together lower dimensional data Vector for example we can combine the first two and combine the last two then you reduce the original five-dimensional Vector to a three-dimensional vector but the the the good thing is this three-dimensional Vector is still a compositional vector it's still sum up to one so that's uh what amalgamation is about and we can extend this idea to high dimensional compositional data under some constraints to basically what we want to do is to reduce the data and maintain the information in the data set as much as possible so that's the fundamental idea so that leads to this uh principal malformation analysis idea which essentially is a optimization framework um if you think about this amalgamation procedure let's say if we start from P dimensional data we want to reduce this to a much lower K dimensional space then each amalgamation operation essentially can be represented as a K by P Matrix where the Matrix is a binary Matrix which each row being with each column essentially only having one entry being one and all the other entries being zero as long as the Matrix satisfied that con condition it is a valid amalgamation Matrix then what we can do is we can search over all possible amalgamation operations from P Dimension to K Dimension that minimize some loss criteria and in particular this loss function can be very general it can be user defined it can be for example it can be the um it can be defined based on the alpha diversity in particular we have shown that if you use the Simpsons diversity index as your loss function then with any type of amalgamation The Simpsons index will always decrease so that is to say we can Define the loss based on the decrease of The Simpsons index so what we want to do is we want to find a low dimensional representation of the data that minimize the diff the decrease in The Simpsons index so that could be your loss function and once we have a loss function defined we can search over all this automation operation space and find the best one that gave us the smallest loss and that's going to be the the amalgamation that we're going to use in the end to reduce the data as I mentioned this loss function can be quite General it could be based on this decrease in the alpha diversity or it could be based on the difference in the beta diversity or based on the change in the beta diversity you can quantify the beta diversity for the original data right you can also quantify the better diversity for this reduced data so you can use the difference between these two as your loss function and the idea is to minimize the difference as much as possible while reducing the dimension so that could also be a valid loss function or more generally you can use some entropy based or model base or even transformation based measures to quantify the loss at the end of the day this loss can be user defined once you have a loss you can always refer to this optimization framework to find the best amalgamation operation that reduce the data from P Dimension to K Dimension so that's the fundamental idea behind this uh this proposal so I want to go into the detail of how to actually choose the loss function and and solve this optimization but just want to give you a highlight overview of of how we actually solve this and how it performs in practice so as you can imagine searching over this space could be an MP heart problem so for most for some loss function you maybe will able to explicitly solve this optimization but in most cases it won't be solved in the in a reasonable time so what we propose is a heuristic approach to approximately solve this optimization uh problem in particular we consider three different scenarios or three different uh variants of this heuristic one is unconstrained one is the weak taxonomic taxonomic hierarchy and one is a strong taxonomic hierarchy so the latter two will take advantage of this tree structure among different taxes to guide this search and the first one does not use any additional information so to start with the first one the unconstrained approach the basic idea is instead of directly finding the optimal amalgamation we can search it step by step each time we just search over all possible Pairs and decide which pair if you combine them will give you the smallest information loss and then you can keep doing this procedure until you reach the desired dimension or desired information loss so that's the unconstrained approach alternatively if you have this tree structure between different taxes you can further enhance that by leveraging the trade structure for this uh amalgamation procedure in particular the weak taxonomic hierarchy means given this additional tree structure also we call it taxonomic tree but it doesn't have to be a taxonomic tree it can be a thousand NX tray as well so given the tree structure each step we do not search over all pairs but only search over the pairs with the same parent that way once we combine them we can still maintain this tree structure so that along the way we can gradually reduce the dimension but still maintain the same tree structure so that's the way taxonom taxonomic hierarchy approach in comparison the strong taxonomic hierarchy further an additional constraint that is each step we only search the leaf nodes at the bottom level of the tree we only proceed to the next level of the tree if all the lowest level nodes has been combined so that's the strong version of this taxonomic hierarchy so to give you a example so here is actually a tree from a real data example so as you can see this is the tree structure among the 62 different taxa so the unconstrained approach basically in the first step we search over all possible Pairs and say whether we should combine them or not based on whether that combination gave us the smallest information loss and in that case 2 and 3 is a possible it's an eligible pair 12 and 13 is also an eligible pair 26 and 27 is also algebra pair basically any pair is an eligible pair for that unconstrained approach and you just search over all possible Pairs and say which one give you the smallest loss smallest loss in the information and then you proceed with the combination and proceed to the to the next step in comparison the weight hierarchy um two and three since they shared the same parent they are still a valid pair to be considered in the first step and 12 and 13 they also share the same parent so it's still a valid pair but now 26 and 27 since they belongs to different they have different parents they are not on eligible player anymore so if you go with the weak hierarchy then 26 and 27 is not as popular to be considered in the first step maybe after some combination there's they will become an eligible pair but at least in the first step they are not and the strong hierarchy approach uh only two and three five and six Etc these are eligible pairs 12 and 13 are no longer agriculture because they are not both on the lowest level so we only proceed to the next level once we can uh finish the combination on the lowest level so that's the strongest uh strongest version of this uh amalgamation procedure so here are some uh potential outputs that we can we can draw from this procedure and in particular this is a real data example of 62 taxa and one thing that we can do is we can draw this dendrogram to show how the amalgamation actually happens in this data set in particular this example we use the Simpsons index with strong taxonomic hierarchy so as you can see the top part of this figure is a dendrogram which shows you how this uh different texts are actually combined along the way so for example the 20 33 and 35 they are first combined and the next one is probably this 52 and 53 and then so on so forth since we use the strong taxonomic hierarchy so we won't touch any family level nodes until we have aggregated all the genus level nose so that the this uh the height of the tree actually gave us the percentage change in the loss function in The Simpsons index as you can see once we finish combination of all the bottom levels the genus level that will lead to 12 decrease in The Simpsons index so namely if we aggregate the original 62 genus Genera to 41 families then it resulted in the 12 decrease in The Simpsons index and we can further proceed to the next level to the other level which only end up with 22 taxa and that leads to 21.1 percent decrease in The Simpsons index and so on so forth and the bottom part is it says color coding of the taxonomy taxonomic tree for the 62 taxa so since we use this strong taxonomic hierarchy so you can say the aggregation actually followed quite closely with this uh taxonomic uh tree structure for the 6a2 taxa we can also use the same information loss function by using different taxonomic constraints and as you can imagine it would lead to different amalgamation path so the last one is the unconstrained one the middle one is the weak constraint and the right panel is what we just saw the strong constraint and they corresponds to different type of amalgamation depending on which one you believe is the most reasonable dimensional reduction to proceed another way to visualize the the outcome is by drawing this information loss along with the info the dimension reduction so here the three panels corresponds to diff three different information loss criteria and The Simpsons index is what we saw before and the y-axis is the percentage difference the percentage change in the sentence index along Dimension reduction and x-axis is the number of uh president dimension of the reduced data so we start from here where without any reduction then of course there's no reduction in The Simpsons index and along the way as we gradually reduce the the dimension of the data we still we start to have information loss and the three different line types corresponds to the three different constraints this black curve is the unconstrained one and that's the lowest since you don't have any constraints on how you combine the data so uh conceptually it will give you them it would be able to maintain this information as much as possible and this uh grain curve this grain curve corresponds to the strongest constraint a strong hierarchy so you can see the step change right so that happens when you end up with one level of the taxonomy and jump to the other level and then you all of a sudden you have a big jump you have a big decrease in the information and that's the green line and the red curve uh is sort of in between that's the weak constraint it's sort of have this uh nice property uh with the unconstrained while at the same time still use the information from the tree structure so that gave you how the information is uh is decreased in this awake constraint amalgamation procedure and the middle panel corresponds to another information loss criteria which is the Shannon index have very similar patterns and the last one is using the beta diversity the brake Curtis index so we measure the information Lost based on the difference in the beta diversity from the original data and this reduced data again the three lines corresponds to the three different uh procedures so that way you can also use these figures if you have a desired cutoff for the percentage change in the information that can also give you a way to select the number of dimensions for the reduced data for example you want to keep keep your uh if you care about the symptoms index and you want to make sure that the the information loss is not Beyond 20 then you can you know use the 20 as our cutoff and decide what what is the smallest Dimension that it can reduce to um but still maintain the data with the reasonable amount of symptoms index so this script Cloud could be used for selecting the num uh the dimension for uh this reduction here's another visualization uh using this brake Curtis as the information loss criteria so again here uh we actually use brick Curtis as our information loss criteria and conduct this weak taxonomic hierarchy and the three panels corresponds to three different reduced data the first panel corresponds to the reduced to the dimension of 40 second panel reduced to the dimension of 20 and last one reduced to the dimension of 10. and here is the pcoe plot for the original for the original data as well as the reduced data while the remain points are the original data the blue points are the uh reduce data and we use a circle to to just Mark which one corresponds to which in this uh pcoa plot as you can say when we reduce the data from 60 Dimension to 40 Dimension really there's minimal difference in this uh two-dimensional plot in terms of how the samples are scattered how the samples are how their Mutual relations uh change in this two-dimensional space once we reduced it to 20 Dimension we start to see some some difference in their layout so some data points may start to move and only when we reduce to the 10 Dimension we can say uh more dramatic difference in the layout in this two-dimensional space that is to say this procedure maintained the beta diversity of this data set pretty well if you just reduce it you know even reduces to 20 Dimension so which is a quite promising result from this analysis so that's uh that's pretty much it for the dimensional reduction and for dimensional reduction again it's a it's a tool for preliminary analysis or exposure analysis of the microbiome data and these new procedure basically just provides a more flexible way of reducing the microbiome compositional data and you can flexibly decide what your information loss function is and how you want to proceed whether you want to use the tree information to guide the dimensional reduction or not so it's in general it's a very flexible approach that can be uh can be can be useful in reducing the complexity of the microbiome data to start with so next I'm gonna shift here and talk about the regression analysis uh and say how this idea this amalgamation idea can also help us with regression with the association of clinical outcomes and the microbiome data as well as selecting important microbiome features at different taxonomic levels so we call our method in the relative shift and I'll explain a second why we give it that name so to start with I just want to give a very brief background about the regression analysis for microbiome data in the literature the most commonly used approach for a regression of microbiome compositional data is called the log contrast model it's a model developed back in the 80s to associate the response a universal response with this compositional predictors and the idea is instead of directly associating the response with the compositions you first transform the conversation taking one taxa as your reference and then take all this log contrast uh as your true predictors and then fit the standard linear regression model so this logarithial transformation again the main purpose is to leave the composition from the Simplex to the euclidean space later on um people find that it also has a symmetric form which is essentially regressing why on all this log transformed compositional data with additional linear constraint that the sum of this coefficient is equal to zero so these two models are exactly equivalent and of course uh in practice you can also use other Transformations such as the center log original transformation instead of this um you know particular taking one particular text on as a reference you can use the geometric mean of this text as the as the reference then that corresponds to the center of operational transformation but still it has this equivalent form so this model is fine but it has definitely has some limitations but one thing it cannot accommodate zeros in the data we call that you have to do this loud transformation if you have zero then it's definitely not going to work so what you do is you increase or you you change the zero values by adding some small pseudo account to change it but as you can imagine once you change the value you introduce bias to the analysis so that is one big drawback of that and also it lacks straightforward biological interpretation if you think about this uh coefficient in this model um since this XJ this compositional data they are not individually they are not independent meaning if you change the value of one XJ you know for sure some other xjs would also change accordingly because they are compositions so the betas cannot be interpreted in the same way as in the standard linear regression model namely you cannot interpret beta J as how does change increasing one unit of this predictor affect the response so there's no straightforward interpretation of what this beta Gene really means in this uh log contrast model especially given that you also have to um you know acknowledge that the betas actually sum to zero this additional constraint and also it's not a very straightforward how to incorporate additional information such as the tree structure among the microbes so that is another limitation and finally if you conduct an analysis at different level of the taxonomy taxonomic tree you may get a very different result you can consider if you do the analysis on the species level maybe you can find a couple of spaces to be imported but once you start aggregating the data to the next level to the genus level then you may find a totally different setup Janus General to be important so there is no way to to guarantee that the analysis conducted at different levels are actually consistent so that's another big limitation which uh really constraints the reproducibility of this uh analysis so then we ask a question can we directly model compositions without transformation and use that in the regression model so that's sort of the motivation for this work and the answer is yes and the way to do it is actually surprisingly simple and we just know it's a particular fact that if you directly regress this response y on this compositional predictor X1 to XP the model itself is not going to be identifiable because you have this X1 Plus all the way to XP equal to one constraint so the model is not identifiable but if you reduce the degree of freedom for this regression model by one the model will become identifiable and to reduce the degree of Freedom by one you can simply get rid of this intercept term forget about the intercept and only focus on this this regression without the intercept the model is fully identifiable meaning all this beta 1 to Beta P they are uniquely defined once we have the data y X1 to XP and I'm moreover we have Superior interpretabilities for this regression coefficient by noticing the the relation that for any pair beta J X G Plus beta k x k you can always re uh rewrite this relation in the right hand side which is beta K times the sum of these two compositions plus the difference of the coefficient times x j so what that means is that if you can if you maintain this x j plus x k constant this difference will tell you how moving concentration from x k to x j would affect the response namely this difference of the coefficient carry very important meaning that is how shifting concentration between taxa would affect the response so that actually gave us give it its name the relative shift because the sh this uh this difference in the coefficient really capture the shift effect of the concentration and that also leads to a nice combinatorial property of the model that is if two coefficients are equal then what we know is that you can arbitrarily shift concentrations between these two taxes without really changing the model so that just gave us another way of uh connecting to this amalgamation idea if the coefficients are equal then it just means these two texts are from a prediction perspective they can be treated at the same entity and all we care about is the total composition of these two taxa we don't care about each individual composition you can arbitrullate change the composition change the composition between these two taxes as long as the total composition is the same that would have the same prediction effect on the response and moreover the model is a scale and shift in Balance meaning that you can you can change your response to the scale and um and you can change the scale and me of the response all you need to do is to change this beta 1 to Beta P by the same uh scale and shift uh uh skimming and shifting so I won't go into detail so a quick how to add a comparison with the standard law contrast model we can say there are some major differences between these two two ideas first in terms of predictors the relative ship model do not need to conduct any transformation and in terms of coefficient there is no constraint well well only one constraint you do not add The Intercept term in this regression model and in terms of the interpretation the relative shape model has nice interpretation where the difference of coefficient of coefficients can be interpreted as a random effect of Shifting concentration between taxes and that can also be generalized to more than two taxa for any number of taxes you can always construct a contrast of this coefficient as long as this contrast the coefficient they can always be interpreted as how shifting concentration between this subset of taxa would affect to our outcome and in addition if we are dealing with high dimensional data uh we can use this uh this combinatorial property to achieve feature aggregation in relative shift model we can combine some text because there are individual composition does not really matter in prediction we can combine them to reduce the dimension of the predictor while the law contrast you may need to refer to the feature selection and recall that we argue the aggregation or amalgamation is more appropriate for compositional data in addition if we have additional tree structure that can be easily Incorporated which I'm going to talk about in a second and it's also quite robust against measurement error because you can combine taxa to higher levels and the higher the level is the lower the measurement error going to be effective so that's just the nature of the microbiome data so to incorporate this additional tree structure here is a toy example of how we're going to do it uh essentially the basic idea is we we want to aggregate taxa with similar taxonomic path so assume this is the your taxonomic tree structure these two nodes comma one and comma two they share a lot of this uh taxonomy along the way from the root node all the way to the leaf node the only differ at the bottom level and starting from their parent they become the same node so they share a lot of a taxonomic path in that way they are more likely to be combined compared to say node comma 1 versus no comma four which only share between uh from from the root node to the second level so comma 1 and Gamma 4 are less likely to be combined compared to Gamma 1 and Gamma 2. so that's sort of the fundamental idea and in order to um realize that idea where it can actually use some technical um some technical tricks to convert the original regression coefficient beta 1 to Beta 7 to this intermediate coefficient and leverage the existing methods of zero sparsity in the statistical literature to encourage this aggregation of the coefficient because recall that in the proposed model equating the coefficients is the same as combining taxon once the coefficient of this two taxes are the same naturally they will be combined because individual conversation does not matter anymore so that's the idea and with some technical technicalities we can essentially rewrite the original problem at this optimization problem where the loss function is just a standard release Square loss and with the penalty of zero sparsity penalty on this intermediate coefficient that we introduce on the tree and subject to this constraint that the original coefficient is a linear transformation of the is intermediate coefficient so I won't go into the details of this uh um of this approach so you can log into our paper if you are interested so just want to use the last couple minutes to quickly talk through a real data example to show you what would be the output of this uh new approach so we apply this method to this Origins study the oral microbiome study trying to study the association between our microbial and biota and um some diabetes as well as periodontal diseases so in in the first attempt we try to associate this r microbiota with periodontal disease and we have over a thousand diabetes free individuals we collect the microbiome data from their sub changeable plug samples uh using the 16s ra sequencing and after pre-processing we end up with over 500 taxa at the spacious level with known taxonomic structure the response we use here is the periodontal status which is essentially a ordinal categorical outcome but here for Simplicity we just treat it as a continuous outcome and use this approach so zero represented healthy one represented moderate paranormal status and two means that we are severe periodontal status and we also adjust for additional covaries such as sex age race Etc so that's the basic setup so the Paranormal status is the outcome tax is raised as covaries and microbiome is the mem High dimensional predictor so we fit the model uh use the cross validation to select the tuning parameter and this is a major result of the model fitting so in particular the out circle is the spacious level of the original input data of five over 500 taxes at the spacious level and this solid dots this black dots are the feature aggregation result as you can see some texts they are aggregated to their immediate parent these bunch of species are aggregated to the genus meaning individual composition of the species does not really matter in prediction but rather their genus the composition of the genus for these species actually matters and some other attacks are maybe aggregated to uh to higher levels so at the end of the day we can aggregate the data at different levels adaptively we do not need to um to restrict which level we want to aggregate the data but the model will adapt limit aggregate the data to a property levels and here eventually we end up with a from the original 530 taxa at the specialist level the model essentially speed out 31 groups to be significantly meaningful in prediction that includes five spaces for General 11 families or others four classes and a three file and moreover we can look at the coefficient estimated for these 31 groups and try to interpret how does the composition actually matters in this uh in the prediction of the this periodontal outcome in particular we can look at the coefficient and uh these here are a few examples the trypanogenous has a coefficient of 0.7455 which if you look at all the coefficient estimate is relatively High compared to other coefficients which means if you shift concentration from other taxa to trepanoma that would increase the outcome and recall in practice uh in this setting a higher value of the outcome means worse periodontal status so that is to say a higher concentration of trapnoma essentially is uh is is harmful for the periodontal status similarly for the peptococcus you also have a pretty large coefficient that also explains higher concentration of this uh of this texture linked with worst periodontal status which is consistent with what people have found in the literature and on the other hand we can also look at tax how is specifically low coefficient values for example this uh acting actino Messiah tallies so which have a negative coefficient and that one essentially there has been studies in the literature showing the high concentration is linked with health with our health so we cannot have actually some nice interpretation of the coefficient so more [Music] more in-depth analysis can be conducted on this coefficient as well which is ongoing work so just want to Briefly summarize uh what I've talked about today so we proposed this amalgamation based methods for microbiome compositional data in the first part well I introduce a new dimensional reduction approach called principal malformation analysis the idea is really trying to get a interpretable dimensional reduction at the same time being flexible to accommodate different needs from the practitioners so the idea you can minimize some user-defined laws to get a lower dimensional representation of the data while still maintain the uh the information in the data set it's suitable for visualization and subsequent analysis of this microbiome data and for the second part I introduce a relative shift regression which is a multiple regression framework that Associates the universe unitary response with high dimensional zero inflated compositions and because of the the way the model is set up the coefficient actually capture the effect of Shifting concentration on the outcome and it naturally both methods can accommodate guidance from the taxonomic tree structure so it could be made more General in practice so with that I'd like to thank you and also happy to take any questions if you have and then the tools for both Masters are posted on GitHub so if you are interested you can try it out or uh if you have any questions or interested in collaboration um definitely feel free to email me so I'll be happy to explore um all various applications using this uh this approach thank you so I think we only have a few minutes for questions because I know that needs to be in pretty soon to go teach but if anyone has a question online you can put it in chat or either Zoom hand if anyone in the root another question feel free to ask therefore there are no questions but let's give it a couple minutes in case yeah I'm just interested uh you know is there do you have a way to detect and see different populations of different microbiometrics uh constituents with different severities of the periodontal disease I mean this is pretty pretty important stuff I mean you know it my understanding is that you know the microbiome even like varies depending on what tube it is so you know it's just really pretty amazing thing that's that's true so uh again this is only the first attempt and I believe a more granular study you know the definitely can reveal more information about that and so far the method we only use uh you know the cross-sectional data at a single tooth um to study this relation since the original goal of this study is really trying to study the r microbiome the association of our microbiome with diabetes so this is sort of a a secondary outcome that people uh use in this in the study but I agree that you know a more granular study would reveal more interesting information from it but potentially this method can also be generalized to accommodate that goal yeah I can definitely see that it's a very promising area thanks so it's very interesting um I actually have a question about um when you end up posting the results for different economic levels uh can you speak a little louder so I have a trouble here yeah it will hurt yeah okay um can you hear me clearly now very good yeah okay I had a question about um the um pineal aggregation example were there any um predictors or repentance you found for what determines um for your principal components will be at the species level versus The Genius level versus family level the um so you mean the dimensional reduction part right yes uh can you repeat your question I don't quite got your question um when you were going to the final real example and showing when you further dimensional reduction steps for which one you select so can you go to the regression analysis um like slide 22 I think okay 21. this one this one yes um I was curious um if you found it there were any patterns or predictors which um um after you did the computations for minimizing the loss functions which you could use to figure out um the patterns for if this is genius level family level or domain level which ends up as the final dimensional component yeah that's a very good question so we have actually haven't died for this example uh trying the the paas the dimensional reduction approach the the main difference between these two is one is supervised one is unsupervised right so um the dimension reduction if you you can also apply the dimensional reaction today and say you know whether you can replicate to some extent this reduction uh structure right without using the response that would also give you some alternative perspective of whether you know the the supervised reduction result is mainly because of the data structure or because they have association with the outcome but uh that we haven't done yet but that's a good point yes we do have a question online uh I know we're about out of time Nick if you want to unmute yourself and ask your questions and then we can decide if you've got time to answer it or if you want to take it out there well yeah I was sort of asking the same question I guess like what what influenced your choice to uh uh or what should influence your choice between doing the uh pre-aggregation or using regularization to do to do the aggregation yeah so it really depends on your goal right so if you know you have a particular response that you want to associate with definitely you can directly go with this uh you know supervised approach there's no need to do this uh dimensional reduction in the first place but the dimensional reduction is sort of a more General tool that it can use to simplify the data so you may be able you may be interested in learning this uh you know Network structure among the taxa but the the data is just a way too high dimensional to directly be applied so then you can try this dimensional reduction tool first right to simplify our data while still maintaining the majority of the information in the data and then apply the network analysis so it's really two different tools that it can be can be used in practice but short short answer is if you are interested in regression then you can directly use this without using the dimensional reduction because it does build into the official aggregation you in it is that a question yes okay thank you I think we probably need to wrap up yeah so I probably have to run but thank everyone for your interest and if you have any further questions feel free to email me and I'm happy to discuss in more depth great all right thank you