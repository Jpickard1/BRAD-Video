I rap I hear me oh okay those trouble clock so let's start okay don't thank you for Tamina talk my name is fan and I'm a second year PhD student from dr. Gira's lab today I will talk about my current research which is an accurate and interpretable computational model for predicting nucleus of resolution from the contact maps and I'm currently preparing the manuscript for this research and hopefully it will come out at the end of this month so during my presentation you can ask questions at any time you can just leave a comment in the chat box and I will try to answer his questions during my presentation so first let's talk about some background we are familiar that gene transcription is a key step of genetic information flow within biological systems it requires the collaboration of enhancers promoters and a lot of DNA a lot of binding proteins namely transcriptional factors and there's a lot of evidence showing that the initialization of gene transcription is related to that direct communication of enhancers and promoters in 3d space they might directly contact with each other like this or with the help of other transcriptional factors like CDs therefore cocaine and a recently published ABC model even showed that a gene should an element's effect on a gene the depends on posts is enhancer activity and then it's contact frequency with a gene in 3d space to sum up in transcription is not only regulated by the properties of the regulatory element but also regulated by 3d comms and structures therefore people are paying more and more attention to 3d from their structures which also requires technologies for capturing accurate chromatid confirmation there are two types of technology the first time is imaging technologies including like fish bone or chromium peep but today we'll only focus on the ligation based on extensive cause they are also the most common and can efficiently capture 3d structure and a genome-wide scale so in case that some of you are unfamiliar with technologies like hi-c I will briefly go through the workflow of the ligation based method so at first the DNA is cross linked to his family hide though if frappin fragment a and fragment B is close in 3d space they are likely to be cross-linked then this cross-linked DNA will be cut with restriction enzyme and label the ligated then the hybrid sequence will be cut again and peppers too is paired and sequencing technology and aligned to the wrath of genome though if we find that this part comes from fragment a and this part comes from fragment B we can conclude that fragment a and B are close in 3d space so with high-throughput sequencing we can observe millions to peel yourself contact peers like this which is called high C technology and this result are visualized in contact maps but then more evil if there is a dot here in the content maps it indicated position a and position B has a have a in contact and darker colors means more contacts are detected between with the move of this part other colors means more context that detected between this tool cosine so during the past decades I say technology has already helped us uncover the 3d common structures at hierarchical levels the first level is called a B compartments so if we check the conquered maps of one chromosome we may notice some place structure like this it indicated the crumbs of the entire entire chromosome can be split into two parts we call two compartments a and B and a tends to interact with a like this and beat hands to interact to SB that will result in the plates patterns in the content content maps this is this is out a mega base skill then if we resuming the contact up a little bit to hundreds of Kara bases to several mega bases level oh no will will find the most squares along the diagonal it even Kate data low sigh inside a square like this only tends to interact within the ways dollar sign within the same square that is called topological associating domains or tab it is believed that at most times Tattler boundaries of transcriptional regulation one evidence is that when we change the distance between s hhg and its regulatory element within at hat we only observe little effect on the expression but if we disrupt it has structure look observer total loss of its expression then if we continue zooming this contact Marvel will notice them will find some highlighted thoughts like this is called chromatin loops on this thought indicate data anchor of the two anchors of this dot interacting with each other for more structure like this this is in correction building the loop and loop most loops are related with CDC F and coherent binding so although a lot of 3d compton structures like this are discovered with high technology it is still really utilized in analyzing transcriptional regulation the reason is quite intuitive because the typical length of regulatory elements like enhancers promoters and maybe the binding motifs of transcriptional factors are URI between tons of base pairs to hundreds of these pairs which way say is that nuclear some kill but even the highest even the best hi-c technology only reaches kilobases resolution or even tons of kilobase the resolution so there are still a big gap between these two skills the reason is in NC technology we can only include that a fragment a and fragment B these two fragments have content but we don't know which part of retin-a and which part of writing B has contacts so if red million from BH hoola we can only get really like low resolution however this challenge can be overcome the new technology micro C and DNA size they use different enzymes to cut the DNA into shorter fragments even into mono neutral zones which will result in a much higher resolution like a nucleus of resolution so this technology micros in the micro z outperforms hi say in detecting short range interactions let's see this is a term ^ or a5 is like within tons kilobases or we see hundreds of kilobases this other IC contact map and microsecond segment from the same region we can notice that the fine skill structures can only be detected by mitosis technology and that's why micros it can also help us to discover a lot of fine skill structures even we seen like tons of Kara bases and this Technic and it can finally match his a resolution with some regulatory elements like enhancer promoters and TF binding motifs so that will provide us a great data source for you know joint analyzing with other datasets this is a typical microsecond tag nerve and the resolution is 200 base pair we can notice some even smaller squares which is called micro test and some stripes and loops formed between enhancers and promoters so this kind of high resolution content Maps has a lot of applications for example we can identify TF collaborations because the binding motifs of TFS is owning like tons of base pairs long so you can only be observed by microsecond time Maps then of course we can also explain eqtls because the Quixote ELLs are the pairs of parents and jeans and we can try to find whether the Barrett and Jean really have a contact between each other and that's like that can help us to interpret the eqtl but however high resolution micro state contact maps are only available in a few cell lines so that's a question we want to ask that is can we use computational approaches to enhance a resolution for like many human tissues or many different cell lines and but especially for those tissues which we do not have really high resolution containment and if yes if we can so what they had Whitney of course wish we can use low resolution data and try to enhance it but then fine scale structures can only cannot be detected by a low resolution IC technology we still need some additional information some additional high resolution information for example but you have gypsy or histone chip seat so there's really a really large amount of data the deep learning method might be the best approaches for us to use but you know deep learning methods really work as black boxes we know it works but we don't know what so we want to act well and we interpret a model as much as we can and I to like find out why this model can make accurate predictions or how it like no where did the model get the information of this like high-resolution fine scale structures and that's a second part of my research the interpretation of the models so this two parts was he has seen another it percolates the model actually there are some previous methods fine - he has a resolution of I say content maps the first type of approaches just regard hi say contact met as pure images enhance them with image super resolution approaches no some classic method for deal for the only waste images like convolutional neural networks or generative adversary networks can be applied to enhance the resolution we just thing with low and high resolution and then the model can he has low resolution content maps to the high resolution contact maps the second type of methods assume that the contact between too low between two locus a and B and be predicted with the epigenetic marks near locus a and the marks near local speak as well as a PD stance so to sum up the second type of process data the contact maps can be predicted with Owen with with epigenetic marks like this high C red and the previous method cost high C plans will use this to measure as baseline but they ask you some limitations of the previous method poor thing they're trimmed with IC data so they are enhancing low resolution high C to a high resolution high C so you know some structures cannot be detected by high C - there's the upper limit some skills and fine skill structures can not be predicted by their model the second thing is now of them use the information from both low resolution content maps and ebay genetic marks actually the reason is the aggregation of cutie contact maps like this and one definitive actually it's quite challenging because we don't know how to properly aggregate thing and that's exactly the model we are going to or we developed which is low resolution content nerves and epigenetic features to make predictions of high resolution content nerves so how did we ever get a higher content the 2d content maps and one de genève marks the answer is that we use graphs to represent hi-c data so this is a typical graph structure was born out and for ages of course this ages can be weighted then we can use adjacency matrix to denote the graph for example this point five correspond to this point five is weight age this age weight and correspond to the age between no 2 and node 1 so let's go back to high state technology let's go back to hi-c contact maps will notice that I said contact maps actually it's also a is also an adjacency matrix each node correspond to each contact beam at a given resolution and the age between two bins with a contact between these two pins and the ages are also weighted because we detect different number of contacts between different pair of nodes though after denoting the after representing the I say can't a map as really large graph let's focus on obli let's focus on one node for example for this node let's assume this node correspond to this being contact map and let's say if now we have eight epigenetic marks ataxic cities Europe and some histone modifications and for this node for this may be one KB region temple we can find out all the signal streams for this eight originating of marks at this at this be like this one this one this one this one this we have 80 GB genetic marks will get a 8 dimensional vectors will get a 8 mm will get a a dimensional vector like this so in our graph we have nodes we have ages and each node we have an 8 dimensional vectors attached to that node this is called an attributed graph so since we can aggregate to the content maps and when the epigenetic marks ways the attributed graph we can use some graph methods like graph convolutional networks to predict our high resolution content maps so I will give you a very brief introduction of the graph convolution pollution networks so I think most of us are familiar with traditional 2d convolutions like using a convolutional kernel or we say a sliding window to slice through the original image or original 2d input and remember if the window reaches this pixel it's like is aggregating the information from all eight neighboring pixels like this and it's similar or graph convolution networks you can assume there's also an illusion of kernel or sliding window in the graph so if the window reaches for example this node it's also aggregating all the information from its neighboring node from oils neighboring nodes it's like the information is diffuse the information is diffusing in the graph and at every common craft convolutional layer the information will diffuse into its neighboring nodes that's why when we go to this node the neighboring nodes information will flow into this node is like it's aggregating the information for all the neighboring nodes so that's commerce graph convolutional layers does though this is our the structure of our model or suite we still have some traditional one deconvolution for example if we focus on this be this pixel the traditional one deconvolution tries to aggregate the information of its neighboring piece allows the genomic fiber this is just a neighboring nodes like this when neighboring nodes this neighbors trying to aggregate the information from his neighbors and we'll also have graph convolutional years this graph comes from highs a Content map so it's trying to aggregate information from the nodes which come enacting with this internal in icy contact maps but then who if we have a contact here what contact here about compact here we're aggregating the information from this node this node and this node so by combining there's two different type of convolution we got a hidden representation for each node and you will use this kind of with this hidden representation to predict our high resolution contact maps and this model have this model has total but only 2.3 million parameters which is much less than the input dimensions this is also a way for avoid or other avoiding overfitting and as we mentioned before micro see outperforms high say in detecting short-range interaction so our model only predict da trench interactions within 200 KB so if let's say if this is a if they say the full from some are listed a full contact map of one browser but only predicts this region both ranch interactions our input is high our inputs are high C contact maps at 5 KB or 1 KB resolution and EB genetic data from maybe from encode or a pizza or road map of course you can use any epigenetic Marx but to make this model like more universal like can be applied for more cell lines where only use the most common eight marks a taxi city CF and sub histone methylation and histone acetylation of course you can also think model with other optional marks any marks and the output will be the predicted contact maps at two hundred base pair resolution then during training we use micro see as ground truth though all validating our model first we use cost on some validation with great human morning stem cell chromosomes into pinning validation and test that between the moto is a training set and evaluate a model is a tested using high C or using embryonic stem cell microSD as strong shoes so who is a common way to evaluate we evaluates a similarity between contact maps it is called distance stratified Pearson's correlation with them hope this opera agonal is a predictive content map and this lower diagonal is a real contentment if we want to calculate their similarities will pick up those three terms will pick up the street huh the first street how will be the diagonal and the second straighter will be the line next to the dead no and continue like the cert one probably unhealed 1001 and will calculate so Pearson's correlation between with them between this data from predicting map and this data from the real contact map and will calculate the from the first straight on to probably well the 1000 strata and this little correlation we get the blue line is the correlation between our predicted contact Maps and real microstate contentment and the red the Green Line and orange the yellow lines the yellow line are two baseline as a high C record type as we mentioned before the red lights are correlation between I say contact - any my closer contact max who can say that our model outperforms also baselines another issue is that hi say content maps of human bonding stem cells were used before and three-sevenths who is almost 1 billion content but for other issues or concerns their lives we might not have that good I say data but we want to check what is a minimum seconds in depth with an input of that or the input high cycle tetanus so we randomly remove some context of the input human warning stem cell hi-c contact maps and try to evaluate a model with also it's a variation coefficient and we found that when we drop out 99.5% of contacts we still go somehow good results but if we talk about 99.9% of on that race from the original high signal Denmark the result would be much worse though are so we can prove that probably only point 5 percent of the content rate from the original has a condom that's about 8 million content peers the second terms is about 8 million this disciplines in deaths will be enough for a model to make like somehow good predictions and more important things like more important thing the more the same more important than getting high correlation coefficient is that we want to make sure that our model can capture the fine scale structures which cannot be detected by high say technology then here are some examples this part is a input high C contact map in the upper dad know it's a real magazine content map and the lower diagnose our prediction result this input some of the input epigenetic marks and we can see some fine scale structures like loops torque wrench looks like this kind of Polycom regions this kind of anchor center scribe can only be detected by microstate tech maps but our predicted connect Maps can capture and predict these kind of fine skill structures quite well that also proved that our model can predict bicycles skill structures now let's go to some details the first type of high skill structures we're talking about is our loops we already had a question we had a question yeah um what causes the peaks and valleys of HIC + predictions at those resolutions uh I'm sorry um it is what causes the peaks and valleys of hi-c plus predictions at those resolution oh I see I see plus um oh that's a really good question actually I'm still very confusing but I'm also a little confused by this because I say plus only use convolutional neural network so it's training a universal convolution kernel it's true this method means come Universal I mean no story in some commercial kernels chewy has a resolution this kernel will slice through the full contact Maps it will not like the currently the same in okay thank you for that question I think probably I'm not very clear about this can we like talk offline about this us I also do to check the data like why this got some Peaks and that is here but I assume that I suppose only means some Universal kernels for the full content map it will not like distinguish diagonal regions and the regions far from diagonal so that will cause a as a pass maybe that's a reason why has it pass is not like a good addicting method like that yeah no let's continue to the loops so as I mentioned before this kind of highlighted thoughts are loose first we identify all the loops from micro state content maps and you will use that as ground truth then we pick up all the regions like which is supposed to have a loop from both our imputed content Maps and the original has a contact Maps then will pile up all the regions we collected this is called parallel analysis which is quite common analyzing the typical structures in icy contact maps when we noted that in our imputed content Maps this region the contacts are much more enriched in loop regions which indicate that our model can exist fully enhance the loop regions and make accurate predictions for loops and here are another tool find skill structures first while insist the first ones are spliced types are this kind of horizontal or vertical lines anchor and the diagonal is really relatives kind of one-sided institution and we we cost at 1 KB resolution by checking the horizontal and vertical lines which are difficult Lee Dugard and his neighbors and finally our impurity content map captures 75% 71% of scribes microsecond time maps while high C content maps only capture 5% and the third type of tortured Tad's thus important method for all intents is to calculate the iterator into installation scores to calculate the insulation score we use a square a square window to slice through the diagonal and we calculate the summation of all the contacts within square at each step at each step so if the square Reach is a boundary which is the boundary between two ads you will got a minimum value because there are really pure contacts then probably them this window but there's still some debates about like whether Tad's overlaps where it has a continuous and because we are doing fine skill structures our windows are are smaller because we want to capture them or it has like micro tests people even don't know whether smaller heads like whether they are continuous or whether leaks is it exists everywhere or the only exists in some positions so to avoid this kind of debate with direct to use as the correlations the correlation between ISIL as as insulation scores though the installation score between my cozy and unimpeded map is between 9 to 4 and the correlation between micros and high phase when they double all fives are to--to which means our model can give accurate predictions of tag boundaries and have distributions and of course our automate goal is to predict the content maps for many cell lines and many tissues which we do not have high resolution micro second technology so we train our model with down lines which we have high resolution color maps and try to imputed content maps of other human tissues but now we don't have high resolution magazine content maps but like we don't have a brown shoes so we have to validate our model views them like indirectly validate our model we used to use some other data set then eqtl will be a great data source actually there are some previous papers concluding that the contacts are enriched between eqtl pairs like the pairs between the variants and transcriptional start site TSS but they say the contact are enriched in these pairs but the previous research only used low resolution contact I see contact maps for human tissues the resolution is only for T KB and if we check the eqtl pairs from GTX database will notice that half of the peers for half of the pairs the distance the distance is always in 100 KB it means that in 40 KB resolution and MF the pairs the devera 10 TSS they are within the adjacent pins or even at the same beans so this kind of jointly joint analysis cannot provide result which are reliable enough but this now we have this now we have we can impute a contain maps at condo base pair resolution we are able to check the results at a really high resolution check the eqt appears here we use tank reset example we imputed the content map of pancreas at 200 base pair resolution then we collect pancreas eqtl data from GTS database and find all the baron PSS Pierce example if this is a parent disappear we pick up its connecting regions as well as his son neighboring and neighboring regions and for this Pierce for this pair we pick up this region then we pile up all the hundreds of thousands of regions and this is also kind of pile up analysis and this is this is a result for the pattern of energy we know T that even add two hundred base pair resolution and the region's the length of this region is only 4 KB will notice that the eqtl pairs are contacts as a eqtl pairs are still enriched which can bring the joint analysis of hi-c than equal here to a new level but we'll also need to prove that our imputed contact maps are cell type-specific though it means like if we check penguins eqtl pairs in penguins contact maps but between the pairs the contacts will be enriched but if we check penguins equal to your pairs in the content map of another tissue we should expect the context unless he reached of the contact maps should be different for different tissues especially for this don't have specific regions so we also check the temperature to do regions at our predicted content in our predict contact maps of human burning stem cells we notice that the center is less enriched which proves our predictions our cell type-specific and um and it matches well with the acuity of hairs and in following days were going to impute contact map for more cell lines for about more than 10 human tissues and try to use quantitative method to quantify the enrichment now let's go to the last part of modern interpretation with integrated gradient us both deep learning more for deep learning models they are basically like black boxes because we know it works but we don't know it why well but we don't know what the tempo is the model come it's a model say like this image is a reflex camera but we can somehow also try to interpret this conclusion by asking which pixels are the most important reasons for the model to make this kind of decisions for example well this image probably Pamela pixels are more important than what some of the pixels are more important for the model to make this decision and this is called this kind of this kind of contribution is called attribution and attribution can be calculated with gradients from the model and here we use inner gradient gradient to calculate the attributions for per-capita attribution to further interpret our model and here we I don't want to go too much detail about this method so if we are interested with the talk of line for this so this our typical result example if we are interested in this loop region we can pick up this region and try to calculate the attribution on this region which is like asking which epigenetic marks that which regions are the most important for a model to make the decision that there is a loop and this is a result for this loop we have noted that city CF all the red color means positive attribution and blue color means a negative attribution and we noted that city staff contribute the most to the protection which might indicate that this loop is caused by did you sell finding the sum of course there might be some other type of loops for example this one this loop the city self is not an important to probably there's some other mechanism for this loop and actually the good thing is that if you are if you are interested with any region with Emily for interest in this line this stripe you can pick up the region and ask like wish and calculate the attribution for arbitrary region and we even build a web server for this but continue ideon upload the full data because after imputing the content for all the tons of like human tissues and other cancer cell lines will also upload to that server here we also want to support calculating the attributions online in our server so he has a demo like this loads of content map is a demo so probably this is not a final version but what works like this remember we can go to a region and let's see if you are interested in we don't know this type we can draw the square here and of course but we cannot do real-time calculation for example if we draw the square the attribution will be calculated immediately we didn't do that because the calculation of the attributes take some time maybe tens of seconds though another thing a supporting real time calculation is like really user-friendly we can drag a region and submit a job and it's finished ok so then we can load the attributes here these are attributes from a taxing city self and other histone modifications awesome Honda here and the attributions are calculated here so we can say for this regions we are interested probably h3k seventies ok Tony's ever the methylation contributes more and h3k4 trimethylation contributes less like this so this kind of attributions can be calculated online with our server and this nucleus nucleus on browser is developed by a doctor gmask group in the nature Mellon University and we're collaborating with him to publish our results and in the following months after we imputing all the content maps for like pancreas liver all the human tissues will upload our result to the server that and then everybody can everyone access to the data and everyone can calculate the attributions or there are no all the regions they are interested in and so I think that's it and this is summary of my this is a summary of the presentation I'm sorry wait I don't have a like like a manuscript but it will be available soon today so the face I think the most important thing that I want to introduce an idea that graph convolution networks can come by I see with the vision etic marks and this is might be the graph representation might be a really great way for jointly analyzing I see with other datasets and then with our model the fine scale structures can be accurately predicted by the per ml and they in then we can impute a Content map for like tons of cell lines and tissues and the predict result can be attributed to different improve features which could help you to analyze the Phi skill structures and I would like to thank my research advisor dr. G Leo and our collaborators dr. Ryan dr. Joe and dr. Jung and all the fellow students being Germany Joe helped me to collect all the data and you can help me to test the baseline models and you can build a web server and also I will also like to send all the field students in my lab though that's it and no way I will answer any questions you have about this research about the Isaac glass that's a really great question actually honestly I can't say I don't know why but I will check the predictions of hazard glass okay so that's it okay don't have a great day and thank you everyone for attending