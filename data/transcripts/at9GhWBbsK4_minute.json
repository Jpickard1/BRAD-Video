[
    {
        "start": 2.1,
        "text": "um I'm it's my absolute um honor and privilege to um introduce Andrew White um uh he's coming from the University of Rochester um he did his undergrad at Rose Holmen Institute of Technology and uh graduate work in University of Washington Seattle and then a postdoc at the University of Chicago um but perhaps maybe the one thing that you may want to know him for and he'll talk about at the end is he's the champion of the protein Emoji so um look forward to that towards the end so um and we're really excited to have you here and hear your talk thank you thank you um I'm excited to be here is only a 45 minute flight it's actually quite nice so one of the Privileges of uh my university is that it's five minute drive from the airport I've actually biked to the airport from my office I haven't walked but I think it's possible yeah so you can just hop on a flight you know 45 minutes to Detroit or maybe an hour to Chicago and then be back and then go back to your office that's quite nice so I want to start by "
    },
    {
        "start": 62.76,
        "text": "acknowledging the people who did a lot of the work here um so I'm going to talk a lot about this work by ziwa who's sitting standing next to me here um and then later on I'll talk a little bit about um some natural language work that involved the entire group um and then Murad uh helped I think the first project I'll present is um a little bit about producing peptide properties um so I'm going to talk about deep learning so I'm going to teach you all deep learning now quite quickly um so deep learning is a process of connecting um two types of data that we've gathered X which is called the features and the features are things which in principles should be easy to obtain like the molecular structure or like the sequence of a protein um or maybe like um some text and then why the labels this is the things which we think are difficult to obtain and so we want to automate the process and the underlying hypothesis is that there's some functions some f of x that connects to the X and the Y and so we call these things we call X the features why the "
    },
    {
        "start": 124.32,
        "text": "labels in f of x is the function that we're trying to learn and we're going to approximate it with a f with a hat on top of it and that is our approximate function so this is kind of like regression except for we don't use a line we use something called a neural network so the kind of grand equations of deep learning are to first take your features and do a matrix algebra operation just WX plus b and then put it through a non-linearity the whole point of this non-linearity is just to make it so if you do further Matrix operations they don't all collapse to one big matrix multiplication that non-linearity is literally a function which is nonlinear it could be like something which is only positive on one side of the the quadrants so like positive X values and then zero for Negative X values it could be a logistic curve it could be a tangent a lot of choices here you know over years over the years it's like an empirical decision what to use so there have emerged some important nonlinearities but it's there's nothing really deep about the nonlinearity and then what happens is that we learn H "
    },
    {
        "start": 186.3,
        "text": "H is some hidden some new representation of our features and then at the end we can put H into a linear regression and it will turn out to be uh an easy regression problem at the end so deep learning is like taking our original features and learning some new Richter deeper hidden set of features but what makes it actually work is that these W's and B's are trainable um so I wrote a book on deep learning available here you can read about it it's targeted to people like chemistry and material scientists and people who don't biochemistry who want to move from their knowledge of chemistry uh to learn about deep learning so what you do is you stack that operation over and over again and you get these things this is like what a real neural network looks like um this is a convolutional network so on one end the picture comes in you know which is a very big Matrix of of colors and then on the other end what comes out is um predictions of things which are in the picture and it's just a series of reshaping so you can choose in the matrix multiplication what the shape of your output is going to be because W is "
    },
    {
        "start": 247.56,
        "text": "your choice you get to choose what these weights are so it's a lot of reshaping putting it all together and what comes out is a a big model that has all of these degrees of freedom and nowadays what we call a large neural network is something which would have a billion or above three parameters in it so these are quite large matrices we're multiplying here um although of course you know anything above 100 you can characterize as a deep learning I think it's reasonable so this sounds all very abstract let's see what are some examples right like you know this this looks like a matrix multiplication right what can you do that's interesting matrix multiplication here's one where X is the sentence a robot doing chemistry and why is this image so we have a function which takes in pictures and outputs text and we have a function which takes in text and outputs pictures it's quite remarkable but if you think about it language is definitely representable um as a vector or as a matrix and "
    },
    {
        "start": 308.34,
        "text": "pictures are certainly matrices so as long as there is some underlying relationship between text and images we can learn it um here's one this is the funeral of molecular Dynamics by Rembrandt so I don't know if any of you guys do have D but MD is not so popular anymore these days so I like to think fondly of my days of doing lots of MD so there's the funeral from like the Dynamics by Rembrandt um and here's this one I didn't I wanted to learn about the university so I just do what I do now and have ai tell me what's interesting so the University of Michigan's Department of computational Medicine and bioinformatics is most famous for any predictions what the AI thinks let's see this one's a text output the development of the Michigan micro modes I have no idea what those are I have no idea I Googled it there were some news articles from 2015. I guess that's the most famous thing this department has done I don't know if I agree with it but that's what we came up with "
    },
    {
        "start": 368.34,
        "text": "there you go wow there we go that's maybe that's what likes it thanks all right so um deep learning is fun there's exciting things to do with it but what's really brought it to the domain I think of medicine and chemistry and materials is um we've gone from sort of uh uh just kind of going hog wild with deeper neural networks more parameters to having some principles and understanding of how to apply it so I'm going to talk you know just really briefly about um uh some sort of rationale for doing deep learning so what goes into the decisions so one of the terminal one of the terms we use a lot is called inductive biases how do I hide this heightened meeting controls everyone thank you um an inductive bias is like in computer science word it's some fancy thing that just says what is what do we know about that f of x that function which goes from X to Y um a lot of times it's just like the physics or something we know about "
    },
    {
        "start": 428.88,
        "text": "um uh the the way our data is generate like if it's chemistry like we know that energy can only be negative when we're looking at molecules or something or for energy is only negative for the things we're interested in it's not true but or we could say like if you rotate a protein its function shouldn't change like if you take a protein structure and you rotate it 90 degrees it should still behave the same uh because the orientation doesn't matter these are examples of of the inductive bias as we know about our system um a lot of these inductive biases show up in how we represent molecules so we're thinking about drug Discovery um we may represent a molecule like on the left which is called I think it's called Lewis Lewis Dot Structure I don't even know what that's what is it called anybody know I looked up on Wikipedia once I think it's a Lewis Dot Structure but anyway the way we all draw chemical structures okay and the reason I look that up is because I like how to a poll on Twitter to see whether people think molecular structure means that I show you a three-dimensional image of the compound or I show you the line drawing of it there's two-thirds thought I'd show them "
    },
    {
        "start": 489.96,
        "text": "the 3D 3D rendering of it and one-third thought I'd give them the line structure I thought it was fascinating like what do we think the word structure means but anyway so the Lewis Dot Structure on the left the right is the point Cloud so XYZ coordinates it turns out when you write down compounds in this representation you're actually putting some implicit inductive biases like by writing it down as a graph you're basically saying that the uh there's atom permutation equivalence if you change two carbons which are bond to the same things you shouldn't change anything in the molecule you know this from like NMR for example you know equivalent hydrogens equivalent carbon should show up um so let's see you know some examples of this so what this means for deep learning is that I may represent all of the atoms in this drawing here um using what are called one hot vectors so like my first row in this Matrix is my carbon one of the carbons and my second one this Matrix is one of the carbons my third row is one of the carbons etcetera down here or this one is a nitrogen because there's a one in the position or sorry an oxygen because "
    },
    {
        "start": 550.68,
        "text": "there's a one here now if I'm predicting a property per atom like how electronegative the atom is or what its chemical shift is if I were to swap the rows of the atoms on the input then the output row should swap really simple idea like I think my my toddler can figure this out but surprisingly it's hard to implement that in the functions themselves to be permutation equivalent that's the name for if you swap e input the output swaps same thing as if I'm predicting properties of a of a point Cloud so I have here a simulation of a polymer just moving along and I'm predicting those red lines those red lines are coming out of my neural network and I want it so that if the polymer rotates my predictions rotate and as a polymer translates I want my predictions to translate so these are things which are translation equivalence and rotation equivalence so these are things which I want my model to do because I know it's important now if I don't put that into my model my model needs to learn itself that things are translational so by picking these representations it's really important to pick a model that matches them so from that there's sort "
    },
    {
        "start": 612.3,
        "text": "of a set of deep learning models that we choose like if we're working with text so we can represent many things as text we use sequence networks like convolutional nodes for Cardinal networks we're using points we use things like equivalent neural networks and in principle we want to like basically go as simple as we can to solve our problems and only move up to higher complexity if we need to so if we work with text there's a lot of really cool things we can do with it um so sure I think I have one little example here so um I made this one because like my son really likes to bang on the keyboard he just likes to you know go to my desk and pretend like he's working so I thought it'd be fun to have like an actual thing you could do so if I made this website where if you just type on the keyboard it will draw molecular structures using a neural network so you know if you want to do a drug Discovery screening campaign here you go here's some compounds for you this is what some docking programs do all right so it's it's a remarkable that this is just like a website you can load up and it has a neural network that will run in "
    },
    {
        "start": 672.36,
        "text": "your phone or your web browser the URL is mold.1 mol.one if you want to try it and so it's incredible How Far We've gone because you know deep learning used to be like you had to have a GPU you know it's like a specialized piece of hardware on your computer and now you can run it on your phone and and just have fun so it it's really specifically in text models it's really incredible now what is possible with sequence well we all know that proteins and DNA are representable as sequence um molecules can be represented as sequence as well like in Smiles or selfies you can also represent chemical reactions as um as strings so a lot of reaction chemistry including what catalysts to use or what reagents to use or what solvent to do it in so um really the sky's the limit as far as using sequences what are sequences not good at I won't talk about my talk but um what they're not good at is uh if you need to predict a property per atom if you need something to predict per atom sequences are not permutation Echo variant but a lot of times we're predicting a whole molecule property or a whole protein property or something like that all "
    },
    {
        "start": 733.019,
        "text": "right so let's now talk a little bit about amino acid sequences so in this project um we wanted to see how far can we go with a machine learning model so we made a machine learning model that predicts properties of proteins and peptides we wanted to see what is sort of like the not what is the perfect model what is the perfect way to present the model so we wanted to make it easy to use we want it to be explainable we wanted to give you uncertainty um so I'm just going to show you you know a little bit of reasons why we did this okay so um this is an example of a model that's what 2017 seems like 2017 is not that long ago that's about five years ago now and if you go to try this tool out this is the best one I could find so if you go to the tool you can put in a sequence um and it uses what is called like a server model you submit a job to the server sometimes the server is overloaded it takes a little while you wait in line and eventually you get back "
    },
    {
        "start": 793.98,
        "text": "your result and the result is like a web page you can't really extract the information that well so that's kind of what is sort of I would say the an example of one of the best done models that are in the past for bioinformatics um so let's go and see what is a more typical example so this is uh an average one this is a paper from a few years ago this was done um at the National Institute of Health and they had a server set up on the National Institutes of Health for this project and if you go visit it now this is what you get when you go to try to do predictions so it's four years old and it's already gone and um I think that's really typical of uh of a lot of machine learning work or bioinformatics work is that it's really hard to either do it on a server because you have to pay for it you have to maintain it you have raw like this where it just disappears one day because I think rajeshi moved uh groups or something I don't know um and then or you have to pay for "
    },
    {
        "start": 854.1,
        "text": "Server time and you know it's a lot of work so or you have to install packages so what I want to talk about is um our page so this is what our page looks like it's like that same kind of approach where it all the computation is done on your device so there's no server so if you log in with a phone you log in with a laptop it's all done there and you put in a peptide or a protein sequence and then we'll predict a few properties and what it will give you is confidence like this one is zero percent probability it's hemolytic it'll give you you know this is weak predicted to be weakly soluble um if it resists non-specific interactions things like that and how easy would be to synthesize with a solid phase peptide synthesizer and in these notes Here you probably can't read them from here but it says how the model was developed what are the factors for its use how good how accurate it is what are the caveats things like that and citations so we think this is really you know the best we can do in presenting a model and this is a static web page it doesn't cost anything to host you could host it you know at home we use GitHub but it's "
    },
    {
        "start": 915.779,
        "text": "free you could use the University website there's no server anywhere there's no computation it's all done on the person's device who goes to the website there's no dependencies so hopefully this will last you know my life I don't know my let's say 10 years that's my goal this will be around for 10 years and we have a few websites like this now I showed you 1.1 we have one for predicting solubility and I think this is kind of the future is that if we can get to a point where models are trivial to use and I didn't show you but you can like do a whole bunch and save them and compare them and download them I think this is really you know getting to the point where uh uh it's as easy to use and we're starting to think about user interface you know not just like put the code up somewhere so I'll talk a little bit about how the model was developed I won't spend too long on it um one of the things we did in this project was pretty things some things hemolytic um how do you predict if something's hemolytic you have like um a dose response curve or something maybe there's a better name for this but basically you increase the concentration and see how many cell red blood cells were liked and then you have to pick "
    },
    {
        "start": 976.98,
        "text": "some arbitrary cutoff we picked 100 micrograms per mil if you're above that so if you're right at that point if half the cells are lysed you're hemolytic if you have to have a higher concentration you're not Humanity it's just an arbitrary data point that we've picked um this one is very fascinating so predicting if a protein is soluble so this is a this is a there's a data set out there basically where somebody did electronic lab notebook collection and they would say like what sequence they're working on purifying in E coli today and they would go through discrete steps they would say this sequence was selected to be expressed to purified then they would clone it express it then they would see if it came out of the cells and then they would purify it and so what they did is they looked at the lab notebooks in like December of I don't know 2009 and then they looked back in 2010 December and if a sequence had made it all the way from up to the soluble stage or it had stopped that is the decider of "
    },
    {
        "start": 1036.98,
        "text": "if it's soluble or not so a sequence could be insoluble because they didn't care about the sequence anymore it could be insoluble because they lost their samples and they give up it could be insoluble because it's actually insoluble it could be insoluble because it couldn't be expressed right so it's a very nebulous term but basically it's if the protein sequence is likely to fail in the system so as you can imagine it's hard to get a very good score on this probably because there's a lot of other details that go into it but if anybody knows of a data set above like protein solubility that's not this convoluted I would love to know about it um the last data set we did for this model is predicting if something is anti-fouling I'm going to just really touch on this very quickly it's kind of a deeper problem but what we did is we looked at like on proteins are there regions that have like um a low sequence conservation so regions which mutate often that are often that are on the surface of proteins and that are found in basically higher and higher environments like going from the extracellular space to the cytoplasm to like thermophilic cytocytoplasm so we're "
    },
    {
        "start": 1098.6,
        "text": "just trying to find out what amino acids on the surface of proteins and aggregation prone environments and we looked at a bunch of fragments from things in the setting um and that's where we got this data set so the idea is that if your peptide has a lot of like sweater ionic residues or a lot of hydrophilic ones it's likely to resist non-specific interactions because it tries to stay with water that's my PhD thesis was about so if anybody wants to talk about that you know let me know again I'm not going to talk too much about the model it's not that important but we put it all together into the model and um our results so what's incredible to me is that this is in JavaScript so like I don't know if you guys know what JavaScript is but it's like the programming language that like you know we use to play videos on YouTube or like your banking uses to like show you messages or it's like what Facebook is written in and we're taking that and hacking not really hacking but we're using it for deep learning which is not usually what JavaScript is meant to do but we still get results that are equal to or about the same as the best you know models out there so um on he almost his task we actually had 84 accuracy and the best one out there "
    },
    {
        "start": 1159.679,
        "text": "is 85 you know 86 so we're very close and this is running you know on it can run on uh on uh your car screen and your if you have a web browser in your car you know or something you can run it on your smart watch and this one is on the non-failing task there's no other models out there like that but we did some basis lines and this is the best um and then for solubility it's 70 accurate the best ones out there are 75 or 80 but this one's on a slightly different data set so it's not really direct Carousel but so it's incredible that you can get like this high accuracy without needing a server or GPU or anything um I won't mention this but we did some work to see like why the model Works um and uh you know we put this out there and we pre-printed it maybe I don't know two months ago it was already cited in a paper where somebody was synthesizing peptides and wanted to see if they were hemolytic and they used the model and they found out that what is that you can't read it 26 out of 33 were correctly predicted and they were trying to make hemolytic peptides so it was it wasn't like you know it was a little "
    },
    {
        "start": 1219.679,
        "text": "bit of a hard problem so I think this is kind of like the future we see for a lot of the machine learning work um and and uh hopefully it's uh uh we'll get it more experimentalist to use it all right so I'm gonna now talk about on the second project here and that is what if you don't have 10 000 data points right you don't have 20 000 proteins that you've already tested what if you have one protein that you tested and so this is a a topic um called um basically online design of experiments like how do you use deep learning from experiment one to guide what decisions you make on what systems to test um so I have like a little little table here to help us keep our method straight so these are a few ways you can learn a few data points in machine learning or AI um if you want to do experiments so that at the end you have a model which is very accurate at describing a phenomena you would choose Active Learning if you "
    },
    {
        "start": 1280.94,
        "text": "want to maximize something like you want to find the optimum sequence of the optimum compound of the optimum and setting for a system then you would want to use Bayesian optimization and if you want to do a series of experiments or a series of examples and you want each of them to be good rather than just the last one to be good or just one of them to be good then you would call this a bandit problem so these are the kinds of um you know if you are in your own setting maybe you don't want to do basic optimization like this paper maybe you want to make a model that's good then you would use some like Active Learning or if like you're doing something I don't know and I don't know anything about medicine like let's say you're in the clinic so each time you try something you don't want it to you don't want to try random things because you think it'll be interesting you want to make sure each time you try something that's meaningful then you would try to treat it as a bandit problem okay I know a little bit of math um so basically I just want to get the terminology straight so um we still have this f of x and y things but here x f of x means the Black Box experiment that is that is you going into the lab and trying something and doing it that's "
    },
    {
        "start": 1341.539,
        "text": "this data generation process okay so um think about f of x as like a very expensive to do experiment you don't want to you don't want to call that function very often um and then we have what's called a Circa model is that we're going to build a model that approximates the experiment right this is kind of the dream of like a lot of machine learning stuff we want to we want to not have to go into lab we can have an approximate model for it so that's the surrogate model here and then in Bayesian optimization we want to find um an X which maximizes this f of x so that's kind of the goal is we want to find what would maximize our laboratory experiment so binding Affinity uh something like that um so here's an example so the red curve is that f of x something we don't have access to the orange curve is our best guess of what the red curve looks like and then each blue point is a experiment that we've done and so the goal is to maximize the functions we're trying to get up to this maximum red line here and asks you a few examples we can find our way to the top and you'll see that it provides some things over here and "
    },
    {
        "start": 1402.86,
        "text": "there's some things over here a tribe that are off the plot but that's you know example of Bayesian optimization we're trying to sort of explore this this unknown red curve and reach the top using this Orange uh model which is a representation of what we've learned so far okay now completely unrelated to Bayesian optimization there are these things called pre-trained models and I'll talk a lot more about protein models in this in this talk but um preaching models are where you basically take a bunch of you know all known protein sequences and you delete pieces of it and then you ask a model to fill in the missing pieces like you know maybe there was like a uh uh I don't know an RG X and you want to know what's the thing that should go in there if you know your your uh cell service interactions you know D would go there right so that's like an example of what you would these models are trained to do these things worked really well at new tasks like if you if it knows all about sequences then if you want to use a different soluble "
    },
    {
        "start": 1463.76,
        "text": "it only requires a few training examples so that's a pre-trained model so the goal of this project is can we use pre-trained models with Bayesian optimization so can we bootstrap our learning process so that we take these excellent models that are pre-trained and then use them in our own experiments so that we can adapt them to work well with just a few data points so to do this there's like a two big question marks the first question mark is that Bayesian optimization requires you to have an uncertainty like you notice how my orange curve had these bands I need to have a probability of what we think the outcome will be and that's because we need to have a confidence like if we go in the lab to try something I need to know if I'm confident it's going to work or if it's not confident and that's because we need to balance exploring new ideas with exploiting what we already know that's the kind of golden trade-off and optimization is that you need to both explore and exploit and so to do that you need to have a measure of confidence the second challenge for using pre-trained models in based conversation they have to be they have to work with low data like I told you these pre-show "
    },
    {
        "start": 1524.419,
        "text": "models are trained on thousands millions and billions of sequences will they work well after training with just a few examples um so there is like a classical solution to this called Bayesian neural networks um uh Bayesian neural networks are really popular in like the 80s and 90s and stuff because people realize that we could just make neural networks Bayesian which means they would have probabilities and then people realize it's really really hard to do anything but they're very difficult to work with I won't talk about this paper too much but there was a paper in 2021 and they basically showed that there is a way to approximate these neural networks with something called Deep ensembles it's really simple is that rather than having one neural network have five neural networks and feed them all different data and then whenever you want to predict something to see if they disagree or not if they disagree you have low confidence if they agree of high confidence really simple idea there's more to it but that's basically the idea of a deep Ensemble and this paper showed that this these people they consumed you know millions and millions of CPU hours to do "
    },
    {
        "start": 1584.419,
        "text": "real Bayesian neural networks something which you know is like sci-fi basically because it requires so much compute and they went and they made some real bayesianal networks on small data sets and they found out that they're worse and less stable and not as good as these dumb deep ensembles where you just make five copies of the neural network it's kind of remarkable paper I mean the guy who did the paper Andrew Gordon Wilson his conclusion on the paper was that basal networks were a little bit better so we'll keep doing research on it which may have been a self-serving conclusion but I think that's great for him um but so anyway so we took Bayesian neural networks um and then we took Bayesian opposition I won't talk sorry not Basin we took deep ensembles so I want to talk too much about the model in here but basically um we propose a sequence to try then we go into the lab and test the sequence then we train our deep Ensemble and then we do this Bayesian optimization framework to choose which sequence to do next and the goal here is to be looking at the problems we only can do 10 experiments for 20 experiments that's what we want to be targeting "
    },
    {
        "start": 1646.48,
        "text": "all right so these deep ensembles give us the uncertainty um so now we need to test this so here's one of the things about um deep learning is that we want to test this we don't want to just try it in the lab ourselves because if we just do it once like or twice or even three times like we're not sure if it's actually working well so we want to be able to do this like a thousand times or a million times to make sure that what we're proposed is actually robust so we had to come up with ideas for what would be a challenging task um uh that we can actually generate data for so not going to lab but ways we can generate our data so we came up with three ideas one of them is like guess the sequence kind of Wordle style I'm not going to talk about that today but you can read the paper so it's like you know you basically there's an unknown Target sequence and you propose a sequence and it doesn't tell you which position or right it just tells you how many you got right you have to go down and propose another sequence so that's kind of peptide Wordle another thing we do is use a previously train models experiment so my hemolytic predictor I pretended like that's doing an experiment the model works well it's accurate you can pretend like it's doing "
    },
    {
        "start": 1707.84,
        "text": "an experiment and the reason we do it that way instead of doing an actual Source because we can call that model a million times and and find out something deep about our work the last thing we do is we use Alpha fold as an experiment so we want to use peptide protein docking as our experiments we want to find a peptide that will bind to a protein really well with Alpha fold and this is actually a real use case because um in outputfold you put in the Pro the peptide and the protein and it tells you if it docks or not but we want to go and find what is the best peptide for docking for a specific program Protein that's an example of the Bayesian optimization all right so the first thing is does this work at low data so I'm remember we're calculating um probabilities right so at the beginning with only 10 examples we're very poor correlation but as we add experiments we start to see a good correlation here but what's more important is that how often do things fall outside of our confidence interval but as if I propose an experiment and I say there's like a 10 chance that it's within this region is it in that region 10 of the time so that's measured with something called "
    },
    {
        "start": 1768.559,
        "text": "calibration and so basically we want this bottom line here it's integrating how often we fell out of our confidence interval and we see that it's improving so it means that our models are calibrating as well as training so that's a good sign that we're basically able to measure the confidence as well I think in this task you know this is we would like to be better at 10 examples but but it's okay all right so we now have calibrated pre-trained models with Bayesian optimization so let's see what we can do so the first one I'll show you is the hemolytic task so what we do is we pretend like we've never measured any hemolytic peptides before then we start with one random peptide and then we put it to the model asks it to try which peptide do we try next let me try a second peptide let me try third peptide and after say five examples we now can predict we we've found a peptide that is 50 likely to be hemolytic then after 10 experiments we have something that's 60 and then after say you know 40 experiments or so we've reached the most "
    },
    {
        "start": 1829.34,
        "text": "hemolytic almost the most hemolytic peptide we've seen before so like the the dashed red line is the most hemolytic peptide that we have in our data set so we're getting quite close you know after trying 50 peptides but even after just doing you know 20 peptides to get something this hemolytic is pretty pretty successful and then these other two lines here are just showing you know if I remove the Bayesian optimization part how well does it work or if I remove the pre-training how well does it work and we see in both cases both of these parts of the model contribute okay so um the then the second example I'll talk about is um something called a Ras gtpas so this is um an oncogenic protein and we want to inhibit its binding with the sum of seven sons of seven son of seven uh which is just another binding protein which uh uh we want to inhibit the um the protein protein interaction so we're going to try to design a peptide that will bind in the exact same spot that the son of sevenless protein binds in and so this is an example where we know the protein "
    },
    {
        "start": 1890.48,
        "text": "and we know where on the protein we want to bind now can we design a new peptide that binds tightly to that region so this is a Bayesian oxidization problem we have a sequence we want to find we don't want to do a lot of alcohol calculations because in this case the alpha hold calculations take maybe three minutes per run to five minutes so we can't really do a thousand of them we just want to do maybe 100 of them um so we do this basic optimization so we keep trying different peptides out uh and trying to bind them into this site and then we can basically see we look at two measures of how good our binding is the first one is are we in The Binding site so I'm showing them purple like distance to The Binding site and angstrom so obviously we start off at the top with a pretty poor binder but we quickly get to the right site um and then the second one is confidence so confidence measure is basically how stable the peptide is um and then we see that as we start doing this process we're getting more and more stable and then these dashed lines are known peptide binders "
    },
    {
        "start": 1951.679,
        "text": "um so you can see that the binding it doesn't quite bind exactly in the right spot um and then the confidence is kind of low so we're actually trying to find better comparisons but we definitely beat this known binder um to the peptide and by the way the reason why we beat the non-binders that we think that the um this from this Pat Gary paper is that it sits not quite on the target side it sits off to the side so it's not a bad binder it's just that the site that was being trying to be targeted is not quite there and this is kind of the thing is that this is not like a it's not like a binding pocket on like a protein you would Target with a drug this is like a big fat um site that's I think 15 residues long um that's a protein protein interface but so we see we can basically get very good confidence and we can get to the right site so now we have a method to Target you know any uh protein and what's cool about this method is that we didn't need to do the Co Crystal we didn't even need the crystal structure of the protein right it's Alpha folds so we didn't need to know any details about what the protein is we just need to know sequence and we can fit a peptide to "
    },
    {
        "start": 2012.22,
        "text": "bind to a specific spot so we've actually packed this up into a collab so if you go to our um the the code for this paper you can run on collab you just put it you literally enter in the protein you enter in which residency you want to bind to and then it will design a peptide that will bind specifically to those residues okay so um this is like a general way to do peptide designs so if any of you are designing proteins or peptides and you want to see like what experiment should I do next this is a strategy to propose new experiments um and it works well and it's kind of targeting when you don't want to do more than 100 experiments right so it's just not like if you have you know something which is high throughput or you're doing screening this is more like if you have something more difficult like um like one of our collaborators does nanoparticles conjugated with peptides so it's you know you have to make nanoparticles you have to to do the assays it's a little bit of work uh here's an example of using the code so uh like the first five lines are just "
    },
    {
        "start": 2072.639,
        "text": "basically getting it set up then you do you tell it the outcomes you tell what sequence you did and you tell it what the outcome was from the experiment so in this case I said two it's just in this case I'm counting the number of alanines I think in the sequence obviously your experiment is not counting nominees something more complicated then I tell it again and then I say I can ask and say ask what sequence to try next and it tells try this sequence so then I tell it okay that sequence has three alanines and then it asks and says to try this one and then I tell okay it has this many alanines right and then um when you're done like you want it so remember the things that proposes are things which balance exploring and exploiting then what you can do is say like Max and say can you just tell me what the best one you think would be and it tells me the best one it thinks is g a AAG and then I you know can also predict things as well so I can make predictions so this is how you use it um you can upload a spreadsheet if you'd like yeah by the way I think it's called way easy "
    },
    {
        "start": 2134.03,
        "text": "[Music] um no okay this is like supposed to show you the page that's fine okay so um I want to talk to you now a little bit about language models so this is Glenn hockey from NYU and Glenn one day told me like he didn't like using vmd how many people know what vmd is just one of you okay so vmd is like if you ever want to look at a protein you know like make a nice picture of it rotate it you know you've probably used Camara there's another package called vmd that is very good if you're doing lipodynamics and so um open the structure of an antibody came up with a voice control system select everything no selected actually before we play the video I just want to describe so what we did we took a language model that had been trained on like all documents on the internet and then we hooked it up to a voice control system and then we told the language model we literally put the text in there like "
    },
    {
        "start": 2194.38,
        "text": "turn this audio command into command server vmd and then it it did so then we just hooked it all up and then we can do like I think was which is really cool so show too open the structure of an antibody select everything no select the protein switch the representation to new cartoon actually change style to ribbon rotate it 90 degrees along the Z axis select the alanines show them as licorice select the seventh residue zoom in on it render it all right so um what was really cool "
    },
    {
        "start": 2255.64,
        "text": "about this is it it was really like to make the commands work it was really like 10 lines we like just said this is vnd commands and we showed it what format we wanted them in and it knew how to work with bmd um and the voice and what was incredible about it is like the voice control was not very good like we'd say select the Alan means and it would say select Allen's like the name Allen but the the language model knew that in this context of proteins it must mean Allen right it was incredible it could like correct uh transcription hairs like that um and then we kind of wrote a paper on this is um Glenn and I wrote a paper about like these language models and how they could be used in chemistry so this one here this plot we said to a language model which can write code compute the hydrogen dissociation energy curve sorry plot the hydrogen dissociation energy curve with DFT and it wrote a Python program to set up hydrogen I wrote a Python program to pull it apart and do a bunch of single point DFT calculations that then wrote "
    },
    {
        "start": 2318.16,
        "text": "the code to plot it it wrote the code to label the axes and put the right units in it and it was like one line of text it was incredible um and so we basically wrote about like what are some of the consequences like what about teaching a class right like this is actually like a class project for one of his classes Glenn's classes and so like well you know if a student can write that project one one you know line of English then we have some some trouble here and uh it this is like now this is out of date what is this from 2022 God now that was a long time ago back then what year is it it's 2022 yeah so since then this is in like visual studio if anybody who knows a visual studio is it's like a way that people write code now you can have in your Visual Studio these completions coming to you in real time what's incredible about is that it works in all computer languages like before there were things like this that would work in some computer languages this works and vmd I can tell you there's not much code out there for vmd so there's not a big training database but it just knows "
    },
    {
        "start": 2378.28,
        "text": "about computer programming so then we ask the question like do they actually know chemistry like we know that they seems to know what an alanine is it seems to know what residue is it seems to be able to like compute hydrogen dissociation energy but really do they know anything else so we set up some example questions like here's an example question is that it says um this function performs some tropolis Monte Carlo sampling of the 1D icing model temperature T in Returns the trajectory so like that's the English language and then this other stuff is we just said you know what is the input to it and then what do we want returned and we put insert there and then the I don't have the code so don't get too excited but then it actually wrote completely correct code it wrote the entire Metropolis Hasting algorithm with sampling with saving the the state trajectories like we indicated it used the temperature we gave it um here's another more interesting one we said to it make a scatter plot where y weekly depends on X can be the correlation coefficient put r squared in "
    },
    {
        "start": 2439.3,
        "text": "a text box at the top left and save it to correlation.png we did it all it's actually I probably should have the examples up here but it's really quite remarkable it knows what why weekly depends on X so we put together a data set of I think like I think there's 84 questions here across bioinformatics chemical informatics General chemistry molecodynamics plotting quantum mechanics simulations spectroscopy statistics thermodynamics they got 82 percent accuracy so these are questions like write down I don't know um uh like uh for a two for a two atom vibrational system with um you know no translational motion what's the vibrational frequency as a function of temperature or like of the spring constant and temperature I don't know anything about quantum mechanics but um or we wrote things like um set up an MD stimulation from these Smiles like generate a starting structure and write an input file for um for grow Max uh for lamps "
    },
    {
        "start": 2500.32,
        "text": "incredible but there's a caveat here is that when we did this project to begin with we had like 20 accuracy we actually had pretty poor accuracy um oh I sorry so what can you do with this or there's just a little bit so it isn't just write code it can actually like obviously write code that's number one number two is interesting you can give it code and then ask it what the code does like you put up code and then you put a comment below and says this line of code does and then you let it fill in the rest and it will fill it in or if you have a bug in your code you can say this code has a bug and then it will put describe the bug in your code right it's it's a quite remarkable or you can have it do things like adding typing or const correctness if you write C plus plus or even if you do typing in Python one of the kind of mundane things of code is keeping track of what type everything is this will fill it in for you it can compute runtime complexity if you get a line you know code and you ask it to write the runtime complexity on each line it'll do that and it'll give you the whole algorithm runtime complexity the current units tests you can put into "
    },
    {
        "start": 2561.579,
        "text": "code and say ready unit test for it so pretty cool um okay yeah so one of the interesting things about it is that it can also generate molecular structures so if any of you kind of doing if any of you do drug Discovery like one of the kind of things we do these days is make deep learning models that can generate structures here we took natural language models that were trained on like English language and we had them generate structures now I gave this talk at a Gordon research conference on chemistry a little while ago and somebody was complaining to me that the like functional groups weren't an exact match okay like this is um not an amine sorry I know this is I mean this is like not a fennel so like that was like one of the so I put red X's here so for those of you who are real organic chemistry purists these are not exact matches for the functional groups I said but they're pretty close so this is I wrote down in English this is a drug like molecule the tertiaryamine and a phenol and it generated this structure I said this is a drug-like molecule with a bicycle ring this is not a bicyclic ring but it's close "
    },
    {
        "start": 2621.88,
        "text": "said this is a weekly lipophilic drug like molecule of two rings okay it's not quite two rings but it's close and so what's amazing about this is this is all just an accident like this is a quirk that in the language models that can generate structures um I think the next group that does this you know that connects English language to chemical structures is going to be it's going to be much better this one's funny it says this is a very lipophilic drug like molecule with two rings it gave me c 538 which is very lipophilic I'm not going to disagree but all right now here's the weird part remember I told you the accuracy wasn't 84 percent we found out we had to add some aggressive statements this is a new area of science science called prompt engineering we found out that if we put a copyright notice in all of our statements and improve the accuracy by quite a bit why because well if somebody put copyright notices that means the code must be important or well maintained or something like that and it gets the language model ready to write good code "
    },
    {
        "start": 2684.7,
        "text": "you may be thinking like well how would that work well it's because the language model was trained on every piece of code available out there and there's a lot of bad code out there right but there's some correlation between good code and copyright notices but then we also found that you can actually be a little bit more direct if we just write down this is written by an expert python programmer that also improves our accuracy um so we have here like we looked at a few models here um you can see like with this one uh here's no prompt engineering like the accuracy is I guess about 30 with no prompt engineering then we did some prompt engineering specific for chemistry like just saying you know if you just sort of set the domain up right like if you ask it about a harmonic oscillator but you have no context it could interpret that from quantum mechanics or classical mechanics so we just add a little bit of contacts there then um we found you know basically you get saying I'm a very confident python programmer improves your accuracy by two points um adding the copyright notice in some cases improves it it depends on the model so like this one here improved it "
    },
    {
        "start": 2745.119,
        "text": "quite a bit this one here improved from here to here so these kind of weird prompt engineering things are actually important um so anyway I have to put like a motivational quote so computational chemists will not be replaced by machines computational chemists who know how to talk to machines workplace computational chemists uh so that's my um My Philosophy for the day but it's really true is that they've done these like little studies where um you have somebody write code and one person has access to a language model and one person doesn't and when I say have access I mean it literally is in your terminal and you start typing something and it proposes completions so you have to go to website look it up it's just they're ready at all times for you people write code twice as fast and it's like not even close I think it's faster especially with new graduate students who aren't familiar with code writing or aren't familiar with apis having this tool available is just remarkable and how fast somebody can become an expert um computational chemist okay so whoo that was a whole bunch of science I don't have time for questions um and here is the proposed protein "
    },
    {
        "start": 2805.839,
        "text": "Emoji so um I propose this protein Emoji is not mentioned and um we are officially under consideration so we've made it past the first round but it's tough we're competing against a bag of chips cabbage bamboo uh a drone um like a VR headset uh bow like the the Chinese food so it's gonna be tough but I really hope protein makes it through and then we'll have a protein Emoji uh ready for our phones all right so thank you very much I'm happy to take questions I have a question yeah have you used other methods besides Alpha fold for protein structures and confirmations like I taser um no we haven't we've looked at this um a little bit we just did that protein folding work "
    },
    {
        "start": 2867.88,
        "text": "maybe three months ago and our goal there was just trying to find um a black box method with our Bayesian optimization but in principle you could use any protein folding method for this problem um so no we haven't explored it very much but that's definitely something that can be done so the NLP model used the same as what github's copilot does yeah so good question so is the NLP the same thing as GitHub hope so the tool I was mentioning is called GitHub copilot um pilot is slightly worse than what I presented the results for I don't fully understand it and I don't know if GitHub will ever say what the difference is but it is based off the same one but that one was built with I think six variants each from you know one was the the big one that we use called codex DaVinci then there's smaller ones I think Cushman is another one we tried and there is a big gap in performance and I don't know which version GitHub copelt "
    },
    {
        "start": 2928.66,
        "text": "uses but I think it's either Cushman the smaller one or some other version because GitHub copilot is able to ingest all open tabs you have at once whereas when we use these models we're not able to put in that much context so I think there's some kind of trade-off going in there where they're using less powerful models or some variation to be able to ingest more information at once cool thank you yeah um hello yeah I I also have a question on uh the results you showed comparing for the second project uh where you know there wasn't a pre-trained model and where you didn't use Bayesian optimization um yeah so uh for that I noticed that uh the use of one hot one heart like At first wasn't as good as your best model system but as you approach like 40 iterations it "
    },
    {
        "start": 2989.26,
        "text": "actually became quite close so I was wondering what is the the size of the whole whole pool of candidates um so the whole pool of candidates uh I don't quite I I understand where you I understand the context but what is the question the whole pool of candidates uh so yeah can we can we is it possible to pull up that uh slide please oh we should be looking at it maybe my screen is not sure sorry let me yeah this one so what is the maximum like number of like experiments that we can make so what's the maximum value for the x-axis so I in this task they uh they both saturate around the same value we do have another task in the paper the peptide world where one hop will beat out the pre-trained model um and and there's two two reasons this is "
    },
    {
        "start": 3049.559,
        "text": "true so we're two two things to think about one of them is that a one-hot model is where you're basically learning the the embedded feature space right like you have a one-hot model that goes through dense layers so you're learning a new representation so as you gather more information especially on a local problem like the the peptide World we're trying to find a specific Target then yeah you'll end up with a better representation than the pre-train model which is trained to represent every possible sequence of every possible link so we think it's reasonable in our paper we talk about this that if you are going to be at the region of like 50 to 100 experiments a one-hot model will work better once you hit 50 and you can switch over from like a pre-training model to a one-hump model after 50. but the other thing to think about is a one hop model is fixed length you can't change the length of the sequence ever um if you change the sequence length by one then you can try to bootstrap your model but it's really not the same representation so whenever we compare against one hot we have to fix the length of the sequence whereas the pre-train model can work with any sequence length so that's just something "
    },
    {
        "start": 3110.64,
        "text": "to think about but yeah I it's uh definitely one hot models are basically learning uh training a completely new neural network can work when you have more than 50 examples okay thank you it says for the new similarity between later generations of suggested proteins are these exploring in the full space are they converging into region of space of hemolicity um okay so so I underst I believe that Eric is asking if um we're basically getting into one region of humanlicity hemolytic peptides or if it's exploring multiple variations um so I believe that uh you know I don't I don't I don't think I've really looked at this problem in depth but one of the things we do in the Bayesian optimization is that we start "
    },
    {
        "start": 3171.119,
        "text": "from 16 different random starting points and one of those random starting points is the current best and then we have 15 other random starting points and so it's definitely possible for us to propose a sequence which is outside of where we've been looking but on the other hand that relies on the the surrogate model to be able to want to look at things outside of the region we're at um so I'm not sure uh if how broad it looks I don't know it's a good question we haven't really looked at it but but I think it should be able to explore things outside of the region but I'm not sure how accurate the model is outside of that region um hey uh I really enjoy your talk maybe this is piggybacking a bit off of uh you guys question online um I'm curious about the difference between one hot and fingerprints and and physical descriptors and I mean you didn't really talk about it too much so maybe it's completely off topic for uh I'm just wondering if you could comment on how important or unimportant physical "
    },
    {
        "start": 3232.319,
        "text": "descriptors are because they're so hard to generate yeah um foreign so okay so so this question is is basically asking about um using fingerprints or descriptors or one Hots and I would say that they serve kind of different purposes so in deep learning we believe that the best features are the ones that our model will learn and then in that case we would want to use features which are like either exactly the data we have or something which is one to one with the data we have and so obviously with one hot is one to one with the actual sequence so it's like a direct mapping so that is the kind of features we want to use in deep learning because they are they allow the model to learn what is important if we make a decision to use descriptor then we're basically telling the model we've already discovered which features are important um and then that's fine it can be useful especially if you don't have a lot of data or if you have some physical reason to believe it but in general for in deep learning there's kind of a a belief that "
    },
    {
        "start": 3293.4,
        "text": "we don't want to use descriptors we want our model to learn it themselves fingerprints are an unusual case because fingerprints um are almost like one to one onto the structure so some people do you do machine learning or deep learning with fingerprints as like just a trick to turn like a graph into like a vector of numbers it's definitely something reasonable um and so I guess if your question is like what do I think about all these methods well we avoid descriptors we use fingerprints when necessary but we're not that excited about them we use one Hots as the standard approach but if we have something like a pre-trained model or we have um you know a more sophisticated representation that's still one-to-one will choose that it's a long answer all right yeah um could you speak the difference between Beijing optimization Active Learning and the multi-diarm band problem again mainly your acquisition "
    },
    {
        "start": 3354.599,
        "text": "function and kind of like are these different perspectives or are they generally just different use cases and and maybe an example for an abandoned problem sure okay so um yeah I guess like you could argue that Active Learning is like maybe it can be reformulated as Bayesian optimization but we usually think about them as separate areas so acquisition functions are more like the Bayesian optimization side of things that we think about um and active learning um usually what we're doing in active learning is we're like the classical acts of learning is we're trying to choose points where we're uncertain about so we have some model which we which we want to be the final outcome and we want to choose points where models uncertain where we think these points will add information to our model and so we're always thinking about our model as the outcome and active learning and so we think of which points to label what to add to it whereas in based conversation are surrogate models a lot of times they're not very good like our circuit model is just a means to an end to find a good model "
    },
    {
        "start": 3414.72,
        "text": "um so like for example gaussian process regression is commonly used in Bayesian optimization as a surrogate model but you don't see it really any not many other places because it's not really that great of a regression technique um and then abandoned problems abandoned problems are formulated quite a bit differently they're usually formulated I think with regret sounds to regret minimization um and so there what you'll do is you'll think about an algorithm of what to choose rather than having a kind of trained model um I don't know I'm not a small advantage of optimization but um yeah I I hopefully that answers your question a little bit it does uh I mean how about like expected Improvement probability of expression Improvement on your base and manifold for instance like is that Active Learning now or that's basic optimization right because probability Improvement is like Improvement of your point right it's how much you think the point will will be better than what you've seen before whereas like in active learning we're going to be looking at something like "
    },
    {
        "start": 3474.78,
        "text": "um how much our model will improve by having this extra points we're thinking about a model um you know like in all things there are acquisition functions I'm sure you can find the acquisition functions that are kind of like Active Learning um this is more just to get us a grounding like if you've read a paper and active learning then the kind of the process of active learning is to get the points which best help our model whereas if you read a paper on base anonymization then the model is not important what's important is finding the maximum so yeah but it's they're both very deep fields and I don't I'm not that familiar with um yeah thank you "
    }
]