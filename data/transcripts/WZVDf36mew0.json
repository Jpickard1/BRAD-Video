[
    {
        "text": "everyone thanks for joining us for",
        "start": 0.0,
        "duration": 5.64
    },
    {
        "text": "today's tools and Technology seminar",
        "start": 2.28,
        "duration": 5.819
    },
    {
        "text": "um I have a couple of announcements I",
        "start": 5.64,
        "duration": 5.52
    },
    {
        "text": "wanted just a reminder that today is the",
        "start": 8.099,
        "duration": 6.601
    },
    {
        "text": "last seminar for uh this semester we'll",
        "start": 11.16,
        "duration": 5.16
    },
    {
        "text": "be taking a break for the summer and",
        "start": 14.7,
        "duration": 3.54
    },
    {
        "text": "we'll be starting up again in the fall",
        "start": 16.32,
        "duration": 4.5
    },
    {
        "text": "and September I am scheduling speakers",
        "start": 18.24,
        "duration": 5.039
    },
    {
        "text": "for the fall so if anybody is interested",
        "start": 20.82,
        "duration": 4.26
    },
    {
        "text": "in presenting or has a recommendation",
        "start": 23.279,
        "duration": 4.08
    },
    {
        "text": "please let me know I'm Marcy Brandenburg",
        "start": 25.08,
        "duration": 5.82
    },
    {
        "text": "you can send me an email or a chat uh",
        "start": 27.359,
        "duration": 5.581
    },
    {
        "text": "right now during the zoom session and",
        "start": 30.9,
        "duration": 2.94
    },
    {
        "text": "let me know",
        "start": 32.94,
        "duration": 3.779
    },
    {
        "text": "uh the other announcement is that you",
        "start": 33.84,
        "duration": 5.1
    },
    {
        "text": "will uh if you're on the tools and Tech",
        "start": 36.719,
        "duration": 4.801
    },
    {
        "text": "email list you'll be receiving a survey",
        "start": 38.94,
        "duration": 5.4
    },
    {
        "text": "uh later today proudly uh just to get",
        "start": 41.52,
        "duration": 4.74
    },
    {
        "text": "your feedback on the semester's",
        "start": 44.34,
        "duration": 4.44
    },
    {
        "text": "presentations promotion Etc if you can",
        "start": 46.26,
        "duration": 4.139
    },
    {
        "text": "take just a minute to fill that out I'd",
        "start": 48.78,
        "duration": 3.66
    },
    {
        "text": "really appreciate it I do make changes",
        "start": 50.399,
        "duration": 4.561
    },
    {
        "text": "based on the feedback I receive so I I",
        "start": 52.44,
        "duration": 5.099
    },
    {
        "text": "do really appreciate that",
        "start": 54.96,
        "duration": 5.4
    },
    {
        "text": "um so for those who are online uh if you",
        "start": 57.539,
        "duration": 3.84
    },
    {
        "text": "have questions during today's",
        "start": 60.36,
        "duration": 2.94
    },
    {
        "text": "presentation you can put them in the",
        "start": 61.379,
        "duration": 3.78
    },
    {
        "text": "chat box I'll be monitoring that and can",
        "start": 63.3,
        "duration": 3.72
    },
    {
        "text": "let our speaker know if any questions",
        "start": 65.159,
        "duration": 3.661
    },
    {
        "text": "come in",
        "start": 67.02,
        "duration": 3.84
    },
    {
        "text": "um similarly uh if you prefer to ask",
        "start": 68.82,
        "duration": 3.9
    },
    {
        "text": "them verbally you can use the zoom",
        "start": 70.86,
        "duration": 4.079
    },
    {
        "text": "reaction to the bottom right to raise",
        "start": 72.72,
        "duration": 4.2
    },
    {
        "text": "your hand and then we can call on you to",
        "start": 74.939,
        "duration": 3.901
    },
    {
        "text": "unmute I think that's all the",
        "start": 76.92,
        "duration": 3.0
    },
    {
        "text": "announcements so with that I'll",
        "start": 78.84,
        "duration": 3.0
    },
    {
        "text": "introduce today's speaker we have Olivia",
        "start": 79.92,
        "duration": 6.739
    },
    {
        "text": "out who is a grad student in dcmmb",
        "start": 81.84,
        "duration": 4.819
    },
    {
        "text": "all right hi everyone I'm Olivia Piper",
        "start": 87.06,
        "duration": 6.12
    },
    {
        "text": "elge I'm part of the biomedical and",
        "start": 90.659,
        "duration": 6.541
    },
    {
        "text": "clinical informatics lab here in dcmv my",
        "start": 93.18,
        "duration": 7.02
    },
    {
        "text": "advisor is Dr kabanjarian and today",
        "start": 97.2,
        "duration": 5.4
    },
    {
        "text": "we're going to be talking about an",
        "start": 100.2,
        "duration": 6.26
    },
    {
        "text": "algorithm called taught strip",
        "start": 102.6,
        "duration": 3.86
    },
    {
        "text": "so first before we get into the",
        "start": 107.579,
        "duration": 4.561
    },
    {
        "text": "algorithm itself I'm going to give a",
        "start": 110.399,
        "duration": 3.36
    },
    {
        "text": "very broad overview",
        "start": 112.14,
        "duration": 3.659
    },
    {
        "text": "so pretty much if you remember anything",
        "start": 113.759,
        "duration": 4.68
    },
    {
        "text": "from this talk this overview would be",
        "start": 115.799,
        "duration": 5.721
    },
    {
        "text": "what I'd want you to take away",
        "start": 118.439,
        "duration": 3.081
    },
    {
        "text": "so first and foremost what is taught",
        "start": 122.1,
        "duration": 3.659
    },
    {
        "text": "straight",
        "start": 124.74,
        "duration": 3.42
    },
    {
        "text": "so taught string uh going forward I'm",
        "start": 125.759,
        "duration": 4.92
    },
    {
        "text": "going to be abbreviating it as TS",
        "start": 128.16,
        "duration": 5.82
    },
    {
        "text": "is a unique function that's performed on",
        "start": 130.679,
        "duration": 4.801
    },
    {
        "text": "an input signal",
        "start": 133.98,
        "duration": 4.259
    },
    {
        "text": "it has one parameter Epsilon that you",
        "start": 135.48,
        "duration": 5.16
    },
    {
        "text": "set at the beginning of the algorithm",
        "start": 138.239,
        "duration": 5.041
    },
    {
        "text": "and what it does is it creates the",
        "start": 140.64,
        "duration": 5.88
    },
    {
        "text": "effect of denoising the input signal",
        "start": 143.28,
        "duration": 4.8
    },
    {
        "text": "that you give it",
        "start": 146.52,
        "duration": 3.9
    },
    {
        "text": "and then once you've created this",
        "start": 148.08,
        "duration": 4.44
    },
    {
        "text": "denoise version of your input signal you",
        "start": 150.42,
        "duration": 4.74
    },
    {
        "text": "can compute features related to that the",
        "start": 152.52,
        "duration": 5.359
    },
    {
        "text": "noise version of this",
        "start": 155.16,
        "duration": 2.719
    },
    {
        "text": "episode so you might ask why is this",
        "start": 158.94,
        "duration": 4.86
    },
    {
        "text": "algorithm called thought straight",
        "start": 160.92,
        "duration": 4.02
    },
    {
        "text": "so",
        "start": 163.8,
        "duration": 3.9
    },
    {
        "text": "first we're going to be looking at a",
        "start": 164.94,
        "duration": 5.04
    },
    {
        "text": "baseline signal this is just like a",
        "start": 167.7,
        "duration": 5.64
    },
    {
        "text": "standard ECG signal this was taken from",
        "start": 169.98,
        "duration": 7.44
    },
    {
        "text": "the publicly available PTP database BC",
        "start": 173.34,
        "duration": 6.06
    },
    {
        "text": "is like it's a periodic signal of",
        "start": 177.42,
        "duration": 4.44
    },
    {
        "text": "electrical activity in the heart",
        "start": 179.4,
        "duration": 5.16
    },
    {
        "text": "if we wanted to perform a top string on",
        "start": 181.86,
        "duration": 5.459
    },
    {
        "text": "this signal we'd first take our Epsilon",
        "start": 184.56,
        "duration": 4.08
    },
    {
        "text": "value",
        "start": 187.319,
        "duration": 3.721
    },
    {
        "text": "and use it to shift that signal of",
        "start": 188.64,
        "duration": 4.56
    },
    {
        "text": "interest from its Baseline",
        "start": 191.04,
        "duration": 4.619
    },
    {
        "text": "so this top waveform here is our",
        "start": 193.2,
        "duration": 5.34
    },
    {
        "text": "original signal plus Epsilon and this",
        "start": 195.659,
        "duration": 4.681
    },
    {
        "text": "signal down here is our original signal",
        "start": 198.54,
        "duration": 5.119
    },
    {
        "text": "minus Epsilon",
        "start": 200.34,
        "duration": 3.319
    },
    {
        "text": "and so if we were to say perform top",
        "start": 206.04,
        "duration": 4.68
    },
    {
        "text": "string on this starting from our origin",
        "start": 208.26,
        "duration": 3.899
    },
    {
        "text": "point at zero",
        "start": 210.72,
        "duration": 4.939
    },
    {
        "text": "we'd have a straight line",
        "start": 212.159,
        "duration": 3.5
    },
    {
        "text": "however if we change this Epsilon value",
        "start": 216.42,
        "duration": 6.739
    },
    {
        "text": "we can see that",
        "start": 219.659,
        "duration": 3.5
    },
    {
        "text": "you can see that it starts to interact",
        "start": 224.64,
        "duration": 4.92
    },
    {
        "text": "with the troughs and the peaks of our",
        "start": 227.04,
        "duration": 4.919
    },
    {
        "text": "original input signal",
        "start": 229.56,
        "duration": 4.98
    },
    {
        "text": "and so we can see as we move Epsilon",
        "start": 231.959,
        "duration": 4.681
    },
    {
        "text": "smaller and smaller we get this",
        "start": 234.54,
        "duration": 4.68
    },
    {
        "text": "piecewise estimation of the signal at",
        "start": 236.64,
        "duration": 5.159
    },
    {
        "text": "different resolutions",
        "start": 239.22,
        "duration": 5.159
    },
    {
        "text": "and we can see that as Epsilon continues",
        "start": 241.799,
        "duration": 5.101
    },
    {
        "text": "to get smaller it gets more and more of",
        "start": 244.379,
        "duration": 6.301
    },
    {
        "text": "an approximation of the original signal",
        "start": 246.9,
        "duration": 6.44
    },
    {
        "text": "foreign",
        "start": 250.68,
        "duration": 2.66
    },
    {
        "text": "useful",
        "start": 255.0,
        "duration": 3.0
    },
    {
        "text": "so first and foremost it's useful",
        "start": 258.78,
        "duration": 4.199
    },
    {
        "text": "because you can use it to compute",
        "start": 261.419,
        "duration": 4.921
    },
    {
        "text": "features from an input signal so one",
        "start": 262.979,
        "duration": 4.681
    },
    {
        "text": "thing that's going to come up later is",
        "start": 266.34,
        "duration": 3.84
    },
    {
        "text": "for example how bendy your signal of",
        "start": 267.66,
        "duration": 4.5
    },
    {
        "text": "input is",
        "start": 270.18,
        "duration": 4.16
    },
    {
        "text": "in addition to being able to use",
        "start": 272.16,
        "duration": 5.52
    },
    {
        "text": "features you can also use it to remove",
        "start": 274.34,
        "duration": 7.06
    },
    {
        "text": "noise from a signal in O event time and",
        "start": 277.68,
        "duration": 5.4
    },
    {
        "text": "so if you're not familiar with Big O",
        "start": 281.4,
        "duration": 4.019
    },
    {
        "text": "notation o of n time just means that",
        "start": 283.08,
        "duration": 4.8
    },
    {
        "text": "it's very efficient and the amount of",
        "start": 285.419,
        "duration": 4.681
    },
    {
        "text": "time it takes the amount of time it",
        "start": 287.88,
        "duration": 6.36
    },
    {
        "text": "takes to produce this denoised signal is",
        "start": 290.1,
        "duration": 8.96
    },
    {
        "text": "linear compared to your signal of input",
        "start": 294.24,
        "duration": 4.82
    },
    {
        "text": "and then specifically why taught string",
        "start": 299.88,
        "duration": 5.16
    },
    {
        "text": "is useful which I'm going to go over in",
        "start": 302.58,
        "duration": 4.32
    },
    {
        "text": "the applications part of this talk is",
        "start": 305.04,
        "duration": 4.74
    },
    {
        "text": "that you can create tensors to create",
        "start": 306.9,
        "duration": 5.34
    },
    {
        "text": "these piecewise estimations",
        "start": 309.78,
        "duration": 4.68
    },
    {
        "text": "uh with these linear piecewise",
        "start": 312.24,
        "duration": 6.06
    },
    {
        "text": "estimations at different uh levels",
        "start": 314.46,
        "duration": 6.0
    },
    {
        "text": "and so you can generate features from",
        "start": 318.3,
        "duration": 3.899
    },
    {
        "text": "the same input signal at different",
        "start": 320.46,
        "duration": 3.9
    },
    {
        "text": "levels of denoising",
        "start": 322.199,
        "duration": 4.321
    },
    {
        "text": "and then this information can be stored",
        "start": 324.36,
        "duration": 5.3
    },
    {
        "text": "in a tensor format",
        "start": 326.52,
        "duration": 3.14
    },
    {
        "text": "so now that I've given you a brief",
        "start": 332.039,
        "duration": 3.061
    },
    {
        "text": "overview",
        "start": 334.08,
        "duration": 3.24
    },
    {
        "text": "the rest of the talk is going to focus",
        "start": 335.1,
        "duration": 5.7
    },
    {
        "text": "on first the top string algorithm itself",
        "start": 337.32,
        "duration": 6.84
    },
    {
        "text": "and second how this top string is",
        "start": 340.8,
        "duration": 7.16
    },
    {
        "text": "applicable in current research",
        "start": 344.16,
        "duration": 3.8
    },
    {
        "text": "okay so first how it works",
        "start": 348.24,
        "duration": 5.22
    },
    {
        "text": "but before we get into the nitty-gritty",
        "start": 350.58,
        "duration": 4.44
    },
    {
        "text": "of the algorithm itself I just want to",
        "start": 353.46,
        "duration": 3.54
    },
    {
        "text": "define a couple of functions",
        "start": 355.02,
        "duration": 4.94
    },
    {
        "text": "uh first is this function called diff",
        "start": 357.0,
        "duration": 5.58
    },
    {
        "text": "where it goes from the space of real",
        "start": 359.96,
        "duration": 5.799
    },
    {
        "text": "numbers in N to n minus one and it's",
        "start": 362.58,
        "duration": 5.82
    },
    {
        "text": "really simple pretty much if you take a",
        "start": 365.759,
        "duration": 5.821
    },
    {
        "text": "sequence of inputs diff is just taking",
        "start": 368.4,
        "duration": 5.46
    },
    {
        "text": "the difference between each sequential",
        "start": 371.58,
        "duration": 5.399
    },
    {
        "text": "set of input so if you have like say a",
        "start": 373.86,
        "duration": 6.36
    },
    {
        "text": "signal W that is like n if would just be",
        "start": 376.979,
        "duration": 8.28
    },
    {
        "text": "taking W2 minus W1 W3 minus W2 Etc until",
        "start": 380.22,
        "duration": 7.5
    },
    {
        "text": "you reach WN and take difference of w",
        "start": 385.259,
        "duration": 6.081
    },
    {
        "text": "and WN minus one",
        "start": 387.72,
        "duration": 3.62
    },
    {
        "text": "uh diff star is related it's dual to",
        "start": 391.5,
        "duration": 6.24
    },
    {
        "text": "diff and it moves from the space of our",
        "start": 394.68,
        "duration": 5.94
    },
    {
        "text": "n minus one to R of N and what this does",
        "start": 397.74,
        "duration": 6.959
    },
    {
        "text": "is it takes your input sequence and it",
        "start": 400.62,
        "duration": 5.94
    },
    {
        "text": "takes the differences",
        "start": 404.699,
        "duration": 3.84
    },
    {
        "text": "uh pretty much going the opposite",
        "start": 406.56,
        "duration": 4.979
    },
    {
        "text": "direction so you start with zero minus",
        "start": 408.539,
        "duration": 7.561
    },
    {
        "text": "W1 W1 minus W2 onward until you reach w",
        "start": 411.539,
        "duration": 6.901
    },
    {
        "text": "n minus one",
        "start": 416.1,
        "duration": 4.439
    },
    {
        "text": "and so in practice",
        "start": 418.44,
        "duration": 4.08
    },
    {
        "text": "what we're going to look at with taut",
        "start": 420.539,
        "duration": 5.94
    },
    {
        "text": "string is this function diff star diff w",
        "start": 422.52,
        "duration": 6.72
    },
    {
        "text": "which is pretty much approximating the",
        "start": 426.479,
        "duration": 5.22
    },
    {
        "text": "second derivative of w",
        "start": 429.24,
        "duration": 3.84
    },
    {
        "text": "and just because I like looking at",
        "start": 431.699,
        "duration": 3.901
    },
    {
        "text": "visual representations I made this tiny",
        "start": 433.08,
        "duration": 5.339
    },
    {
        "text": "graph here using a very small example",
        "start": 435.6,
        "duration": 6.599
    },
    {
        "text": "sequence where W is this original black",
        "start": 438.419,
        "duration": 7.321
    },
    {
        "text": "line diff of w is this big Gray Line and",
        "start": 442.199,
        "duration": 6.06
    },
    {
        "text": "then div star diff W is this blue line",
        "start": 445.74,
        "duration": 4.88
    },
    {
        "text": "here",
        "start": 448.259,
        "duration": 2.361
    },
    {
        "text": "okay",
        "start": 454.919,
        "duration": 3.661
    },
    {
        "text": "for Todd Street let's say that you're",
        "start": 456.419,
        "duration": 4.201
    },
    {
        "text": "interested in using it for Signal",
        "start": 458.58,
        "duration": 4.38
    },
    {
        "text": "processing and you have a discrete",
        "start": 460.62,
        "duration": 3.66
    },
    {
        "text": "Signal app",
        "start": 462.96,
        "duration": 3.359
    },
    {
        "text": "this could be something like heart rate",
        "start": 464.28,
        "duration": 4.859
    },
    {
        "text": "variability or arterial line readings or",
        "start": 466.319,
        "duration": 7.021
    },
    {
        "text": "ECG and we refer to this signal of F and",
        "start": 469.139,
        "duration": 6.78
    },
    {
        "text": "it's of length n so every value in the",
        "start": 473.34,
        "duration": 6.56
    },
    {
        "text": "signal is F1 up to F of n",
        "start": 475.919,
        "duration": 3.981
    },
    {
        "text": "so they their heart like heart rate",
        "start": 481.02,
        "duration": 4.5
    },
    {
        "text": "variability it would be an individual",
        "start": 483.12,
        "duration": 4.38
    },
    {
        "text": "data element that would contribute to",
        "start": 485.52,
        "duration": 3.299
    },
    {
        "text": "our understanding of heart rate",
        "start": 487.5,
        "duration": 3.18
    },
    {
        "text": "variability yeah so like if you had",
        "start": 488.819,
        "duration": 3.6
    },
    {
        "text": "heart rate variability it would be like",
        "start": 490.68,
        "duration": 4.62
    },
    {
        "text": "if you have your B2B intervals it would",
        "start": 492.419,
        "duration": 6.241
    },
    {
        "text": "just be each duration in sequence yeah",
        "start": 495.3,
        "duration": 5.16
    },
    {
        "text": "and it would be the same for the other",
        "start": 498.66,
        "duration": 4.74
    },
    {
        "text": "ones yeah just just a way to measure to",
        "start": 500.46,
        "duration": 4.38
    },
    {
        "text": "actually determine",
        "start": 503.4,
        "duration": 4.079
    },
    {
        "text": "whether you had it how much all that",
        "start": 504.84,
        "duration": 5.16
    },
    {
        "text": "kind of stuff right yep okay",
        "start": 507.479,
        "duration": 5.341
    },
    {
        "text": "okay and so then you set your parameter",
        "start": 510.0,
        "duration": 5.339
    },
    {
        "text": "Epsilon which is a number greater than",
        "start": 512.82,
        "duration": 4.019
    },
    {
        "text": "zero",
        "start": 515.339,
        "duration": 4.021
    },
    {
        "text": "and then we have this function X which",
        "start": 516.839,
        "duration": 3.961
    },
    {
        "text": "ends up being our top string",
        "start": 519.36,
        "duration": 2.7
    },
    {
        "text": "approximation",
        "start": 520.8,
        "duration": 4.56
    },
    {
        "text": "and X is the unique function such that",
        "start": 522.06,
        "duration": 6.42
    },
    {
        "text": "it's infinity or Chevy Chef Norm",
        "start": 525.36,
        "duration": 5.64
    },
    {
        "text": "of f minus X",
        "start": 528.48,
        "duration": 6.9
    },
    {
        "text": "is the max value of I for every F minus",
        "start": 531.0,
        "duration": 5.58
    },
    {
        "text": "X of I",
        "start": 535.38,
        "duration": 4.32
    },
    {
        "text": "and this max value is always going to be",
        "start": 536.58,
        "duration": 5.759
    },
    {
        "text": "less than or equal to Epsilon",
        "start": 539.7,
        "duration": 6.24
    },
    {
        "text": "what this does is that it forces this",
        "start": 542.339,
        "duration": 6.481
    },
    {
        "text": "top string approximation X to never pass",
        "start": 545.94,
        "duration": 7.28
    },
    {
        "text": "beyond that value Epsilon",
        "start": 548.82,
        "duration": 4.4
    },
    {
        "text": "and then the second part of taut string",
        "start": 554.76,
        "duration": 6.66
    },
    {
        "text": "is that the euclidean distance or L2",
        "start": 557.7,
        "duration": 6.9
    },
    {
        "text": "Norm of the diff of X is equal to this",
        "start": 561.42,
        "duration": 4.8
    },
    {
        "text": "computation",
        "start": 564.6,
        "duration": 4.799
    },
    {
        "text": "and it's going to be minimal",
        "start": 566.22,
        "duration": 6.9
    },
    {
        "text": "so what this means is that the",
        "start": 569.399,
        "duration": 6.241
    },
    {
        "text": "the amount of movement that happens in X",
        "start": 573.12,
        "duration": 4.38
    },
    {
        "text": "is going to be minimal so it's going to",
        "start": 575.64,
        "duration": 2.879
    },
    {
        "text": "have",
        "start": 577.5,
        "duration": 4.019
    },
    {
        "text": "as few local Maxima and local Minima as",
        "start": 578.519,
        "duration": 4.38
    },
    {
        "text": "possible",
        "start": 581.519,
        "duration": 3.901
    },
    {
        "text": "it's what makes the string in top string",
        "start": 582.899,
        "duration": 4.94
    },
    {
        "text": "taut",
        "start": 585.42,
        "duration": 2.419
    },
    {
        "text": "and so something else to keep in mind",
        "start": 592.2,
        "duration": 5.4
    },
    {
        "text": "about X is that this dip star dip of X",
        "start": 593.88,
        "duration": 5.459
    },
    {
        "text": "so B",
        "start": 597.6,
        "duration": 6.54
    },
    {
        "text": "uh the L1 Norm of its second derivative",
        "start": 599.339,
        "duration": 7.981
    },
    {
        "text": "is going to be minimized as well",
        "start": 604.14,
        "duration": 5.28
    },
    {
        "text": "so that goes again with",
        "start": 607.32,
        "duration": 6.3
    },
    {
        "text": "minimizing how much this function X will",
        "start": 609.42,
        "duration": 6.68
    },
    {
        "text": "bend",
        "start": 613.62,
        "duration": 2.48
    },
    {
        "text": "okay",
        "start": 621.24,
        "duration": 3.719
    },
    {
        "text": "so we know that X now is the taught",
        "start": 622.56,
        "duration": 5.94
    },
    {
        "text": "string approximation of that",
        "start": 624.959,
        "duration": 5.101
    },
    {
        "text": "but",
        "start": 628.5,
        "duration": 3.959
    },
    {
        "text": "we can also consider it as a",
        "start": 630.06,
        "duration": 5.339
    },
    {
        "text": "decomposition so if our input signals F",
        "start": 632.459,
        "duration": 5.581
    },
    {
        "text": "we can consider it being decomposed into",
        "start": 635.399,
        "duration": 5.701
    },
    {
        "text": "the top string approximation X plus some",
        "start": 638.04,
        "duration": 5.94
    },
    {
        "text": "noise value y",
        "start": 641.1,
        "duration": 5.58
    },
    {
        "text": "so X would be the phenoid signal and Y",
        "start": 643.98,
        "duration": 5.82
    },
    {
        "text": "is the noise with its Infinity Norm less",
        "start": 646.68,
        "duration": 5.219
    },
    {
        "text": "than or equal to Epsilon",
        "start": 649.8,
        "duration": 4.86
    },
    {
        "text": "so you can interpret that as meaning",
        "start": 651.899,
        "duration": 6.361
    },
    {
        "text": "that the smallest upper bound of Y is",
        "start": 654.66,
        "duration": 6.179
    },
    {
        "text": "your noise level Epsilon so the amount",
        "start": 658.26,
        "duration": 4.56
    },
    {
        "text": "of noise you have will never go beyond",
        "start": 660.839,
        "duration": 4.461
    },
    {
        "text": "Epsilon",
        "start": 662.82,
        "duration": 2.48
    },
    {
        "text": "and now diving deeper into the actual",
        "start": 668.399,
        "duration": 6.0
    },
    {
        "text": "algorithm for computing taught string",
        "start": 671.519,
        "duration": 6.801
    },
    {
        "text": "I'm going to go through this process",
        "start": 674.399,
        "duration": 7.321
    },
    {
        "text": "with words first and then I'm going to",
        "start": 678.32,
        "duration": 6.16
    },
    {
        "text": "show an illustrated example to show one",
        "start": 681.72,
        "duration": 5.76
    },
    {
        "text": "iteration of the algorithm in action",
        "start": 684.48,
        "duration": 5.76
    },
    {
        "text": "so I know that these text Heavy slides",
        "start": 687.48,
        "duration": 4.44
    },
    {
        "text": "can be a little overwhelming but it's",
        "start": 690.24,
        "duration": 3.539
    },
    {
        "text": "just background before we actually see",
        "start": 691.92,
        "duration": 4.5
    },
    {
        "text": "the algorithm in action",
        "start": 693.779,
        "duration": 3.841
    },
    {
        "text": "okay",
        "start": 696.42,
        "duration": 4.8
    },
    {
        "text": "so the most common uh version of top",
        "start": 697.62,
        "duration": 7.02
    },
    {
        "text": "string was developed by babies and Kovac",
        "start": 701.22,
        "duration": 5.76
    },
    {
        "text": "in 2001.",
        "start": 704.64,
        "duration": 5.22
    },
    {
        "text": "what it does is it accepts a set of data",
        "start": 706.98,
        "duration": 5.22
    },
    {
        "text": "points as input so that's formatted here",
        "start": 709.86,
        "duration": 7.08
    },
    {
        "text": "is I comma y i where I is a set of",
        "start": 712.2,
        "duration": 7.68
    },
    {
        "text": "values between 0 and N where n is the",
        "start": 716.94,
        "duration": 5.459
    },
    {
        "text": "total length",
        "start": 719.88,
        "duration": 4.74
    },
    {
        "text": "and then the integral integrated process",
        "start": 722.399,
        "duration": 6.961
    },
    {
        "text": "y 0 is computed as such where K this",
        "start": 724.62,
        "duration": 7.8
    },
    {
        "text": "value K is the number of knots or the",
        "start": 729.36,
        "duration": 5.52
    },
    {
        "text": "number of times the top string changes",
        "start": 732.42,
        "duration": 4.68
    },
    {
        "text": "Direction",
        "start": 734.88,
        "duration": 5.519
    },
    {
        "text": "the upper bound of our integrated",
        "start": 737.1,
        "duration": 6.96
    },
    {
        "text": "process of our input signal f is marked",
        "start": 740.399,
        "duration": 7.62
    },
    {
        "text": "by U and its lower bound is marked by L",
        "start": 744.06,
        "duration": 6.42
    },
    {
        "text": "and when we're initializing the function",
        "start": 748.019,
        "duration": 5.101
    },
    {
        "text": "the endpoints of the thought string are",
        "start": 750.48,
        "duration": 6.0
    },
    {
        "text": "fixed so that's saying that the lower",
        "start": 753.12,
        "duration": 5.64
    },
    {
        "text": "bound and upper bound are the same at",
        "start": 756.48,
        "duration": 3.599
    },
    {
        "text": "the very beginning and the lower and",
        "start": 758.76,
        "duration": 5.16
    },
    {
        "text": "upper bound are the same at the very end",
        "start": 760.079,
        "duration": 7.26
    },
    {
        "text": "so when we start at our origin point of",
        "start": 763.92,
        "duration": 7.68
    },
    {
        "text": "zero l0 or it could be uh U zeros pretty",
        "start": 767.339,
        "duration": 5.94
    },
    {
        "text": "much starting at that origin point where",
        "start": 771.6,
        "duration": 4.26
    },
    {
        "text": "the upper and lower bounds are the same",
        "start": 773.279,
        "duration": 5.821
    },
    {
        "text": "for a given value of I we compute s x i",
        "start": 775.86,
        "duration": 6.599
    },
    {
        "text": "which is the greatest con convex minor",
        "start": 779.1,
        "duration": 6.6
    },
    {
        "text": "of the upper bound and SBI which is the",
        "start": 782.459,
        "duration": 6.901
    },
    {
        "text": "least concave major of the lower bound",
        "start": 785.7,
        "duration": 7.139
    },
    {
        "text": "and so Computing each of those is in O",
        "start": 789.36,
        "duration": 4.68
    },
    {
        "text": "of I time",
        "start": 792.839,
        "duration": 4.56
    },
    {
        "text": "where I is the number of",
        "start": 794.04,
        "duration": 5.099
    },
    {
        "text": "values that you're Computing that's a",
        "start": 797.399,
        "duration": 4.341
    },
    {
        "text": "subset of n",
        "start": 799.139,
        "duration": 2.601
    },
    {
        "text": "and so",
        "start": 805.139,
        "duration": 4.801
    },
    {
        "text": "once you've computed those two uh",
        "start": 806.82,
        "duration": 6.959
    },
    {
        "text": "variables you check if the right hand",
        "start": 809.94,
        "duration": 6.54
    },
    {
        "text": "derivatives of each of you check the",
        "start": 813.779,
        "duration": 4.68
    },
    {
        "text": "right hand derivatives of each of them",
        "start": 816.48,
        "duration": 6.78
    },
    {
        "text": "if the right hand derivative of B",
        "start": 818.459,
        "duration": 7.201
    },
    {
        "text": "uh great greatest convex major end of",
        "start": 823.26,
        "duration": 5.22
    },
    {
        "text": "the upper bound is greater than the",
        "start": 825.66,
        "duration": 5.179
    },
    {
        "text": "least concave major of the lower bound",
        "start": 828.48,
        "duration": 5.52
    },
    {
        "text": "then that is considered",
        "start": 830.839,
        "duration": 5.261
    },
    {
        "text": "of the condition being met and you",
        "start": 834.0,
        "duration": 4.019
    },
    {
        "text": "continue Computing each of these values",
        "start": 836.1,
        "duration": 3.72
    },
    {
        "text": "iteratively",
        "start": 838.019,
        "duration": 3.601
    },
    {
        "text": "you keep you continue to do that until",
        "start": 839.82,
        "duration": 5.16
    },
    {
        "text": "this condition no longer holds",
        "start": 841.62,
        "duration": 5.76
    },
    {
        "text": "and once that condition is broken",
        "start": 844.98,
        "duration": 4.74
    },
    {
        "text": "you create the first knot in your",
        "start": 847.38,
        "duration": 3.6
    },
    {
        "text": "thought strip",
        "start": 849.72,
        "duration": 3.6
    },
    {
        "text": "so one of all the knots you concluded",
        "start": 850.98,
        "duration": 5.88
    },
    {
        "text": "you choose the leftmost in that sequence",
        "start": 853.32,
        "duration": 6.06
    },
    {
        "text": "and you once that's done you move your",
        "start": 856.86,
        "duration": 5.159
    },
    {
        "text": "origin point to that leftmost knot and",
        "start": 859.38,
        "duration": 4.44
    },
    {
        "text": "then start repeating those calculations",
        "start": 862.019,
        "duration": 4.26
    },
    {
        "text": "iteratively again",
        "start": 863.82,
        "duration": 4.38
    },
    {
        "text": "oh",
        "start": 866.279,
        "duration": 3.961
    },
    {
        "text": "and so I'm going to show that",
        "start": 868.2,
        "duration": 4.439
    },
    {
        "text": "Illustrated with this set of sample data",
        "start": 870.24,
        "duration": 4.86
    },
    {
        "text": "points that Davies and Kovac created",
        "start": 872.639,
        "duration": 5.221
    },
    {
        "text": "uh this set of triangles here is the",
        "start": 875.1,
        "duration": 5.22
    },
    {
        "text": "upper bound or U and the set of orange",
        "start": 877.86,
        "duration": 6.56
    },
    {
        "text": "triangles is the lower bound or l",
        "start": 880.32,
        "duration": 4.1
    },
    {
        "text": "and so if we look at this graphically",
        "start": 885.12,
        "duration": 5.399
    },
    {
        "text": "this Computing of the greatest convex",
        "start": 887.82,
        "duration": 5.519
    },
    {
        "text": "minor and the smallest concave matrant",
        "start": 890.519,
        "duration": 5.88
    },
    {
        "text": "is just looking for this path",
        "start": 893.339,
        "duration": 6.06
    },
    {
        "text": "from point to point up until the two",
        "start": 896.399,
        "duration": 5.521
    },
    {
        "text": "curves intersect",
        "start": 899.399,
        "duration": 4.44
    },
    {
        "text": "and so as we continue to do that we find",
        "start": 901.92,
        "duration": 3.84
    },
    {
        "text": "that at some point these two curves",
        "start": 903.839,
        "duration": 4.74
    },
    {
        "text": "intersect and so that condition about",
        "start": 905.76,
        "duration": 4.92
    },
    {
        "text": "the right hand derivatives no longer",
        "start": 908.579,
        "duration": 4.7
    },
    {
        "text": "holds true",
        "start": 910.68,
        "duration": 2.599
    },
    {
        "text": "and so from there we Mark our left most",
        "start": 914.82,
        "duration": 6.54
    },
    {
        "text": "uh not and then reset the origin point",
        "start": 917.699,
        "duration": 8.121
    },
    {
        "text": "to this point right here",
        "start": 921.36,
        "duration": 4.46
    },
    {
        "text": "okay and so as I mentioned before there",
        "start": 928.459,
        "duration": 4.541
    },
    {
        "text": "are a lot of features that you can",
        "start": 931.68,
        "duration": 4.019
    },
    {
        "text": "compute from taut string that end up you",
        "start": 933.0,
        "duration": 7.139
    },
    {
        "text": "can use for any number of purposes in my",
        "start": 935.699,
        "duration": 7.621
    },
    {
        "text": "lab specific uh approach what we do is",
        "start": 940.139,
        "duration": 6.721
    },
    {
        "text": "we use these features extracted to give",
        "start": 943.32,
        "duration": 6.06
    },
    {
        "text": "to machine learning models",
        "start": 946.86,
        "duration": 5.82
    },
    {
        "text": "soaps from the testing approximation you",
        "start": 949.38,
        "duration": 5.22
    },
    {
        "text": "can compute statistical measures like",
        "start": 952.68,
        "duration": 5.04
    },
    {
        "text": "mean median standard deviation Etc",
        "start": 954.6,
        "duration": 6.239
    },
    {
        "text": "but in addition to just looking at the",
        "start": 957.72,
        "duration": 5.58
    },
    {
        "text": "thought string approximation you can",
        "start": 960.839,
        "duration": 5.94
    },
    {
        "text": "also look at that noise estimation so",
        "start": 963.3,
        "duration": 8.219
    },
    {
        "text": "again if we look at dot string as that X",
        "start": 966.779,
        "duration": 6.0
    },
    {
        "text": "Plus y",
        "start": 971.519,
        "duration": 3.0
    },
    {
        "text": "what we do is",
        "start": 972.779,
        "duration": 4.86
    },
    {
        "text": "we compute these statistical measures",
        "start": 974.519,
        "duration": 5.82
    },
    {
        "text": "again on y",
        "start": 977.639,
        "duration": 4.681
    },
    {
        "text": "and in addition to that we can also look",
        "start": 980.339,
        "duration": 4.201
    },
    {
        "text": "at the number of inflection points in",
        "start": 982.32,
        "duration": 4.92
    },
    {
        "text": "within the top string estimate so again",
        "start": 984.54,
        "duration": 5.22
    },
    {
        "text": "looking at how much movement there is in",
        "start": 987.24,
        "duration": 4.76
    },
    {
        "text": "x",
        "start": 989.76,
        "duration": 2.24
    },
    {
        "text": "okay and you might be saying well I",
        "start": 992.88,
        "duration": 4.74
    },
    {
        "text": "don't know if path string is that useful",
        "start": 996.0,
        "duration": 5.16
    },
    {
        "text": "I like to use say Fourier analysis to",
        "start": 997.62,
        "duration": 7.56
    },
    {
        "text": "look at my signals for Featured stretch",
        "start": 1001.16,
        "duration": 5.58
    },
    {
        "text": "and I would say that these two",
        "start": 1005.18,
        "duration": 4.86
    },
    {
        "text": "approaches are not mutually exclusive so",
        "start": 1006.74,
        "duration": 5.099
    },
    {
        "text": "free analysis is a very good way of",
        "start": 1010.04,
        "duration": 3.78
    },
    {
        "text": "looking at uh",
        "start": 1011.839,
        "duration": 4.5
    },
    {
        "text": "frequency domain features its time",
        "start": 1013.82,
        "duration": 5.819
    },
    {
        "text": "complexity Is Not Great compared to top",
        "start": 1016.339,
        "duration": 5.581
    },
    {
        "text": "strength so even if you use fast for",
        "start": 1019.639,
        "duration": 6.3
    },
    {
        "text": "your transform its uh time o of n log n",
        "start": 1021.92,
        "duration": 6.72
    },
    {
        "text": "but it is shift invariant which means",
        "start": 1025.939,
        "duration": 5.281
    },
    {
        "text": "that when something occurs in the input",
        "start": 1028.64,
        "duration": 5.64
    },
    {
        "text": "doesn't affect the output that fast for",
        "start": 1031.22,
        "duration": 5.359
    },
    {
        "text": "your would",
        "start": 1034.28,
        "duration": 2.299
    },
    {
        "text": "uh something else that's very useful is",
        "start": 1036.74,
        "duration": 4.5
    },
    {
        "text": "discrete wavelet transform which is also",
        "start": 1039.14,
        "duration": 5.039
    },
    {
        "text": "computed in O of n time but it's not",
        "start": 1041.24,
        "duration": 4.8
    },
    {
        "text": "shift and buried",
        "start": 1044.179,
        "duration": 5.401
    },
    {
        "text": "but there is a method of computing",
        "start": 1046.04,
        "duration": 5.46
    },
    {
        "text": "discrete wavelet transform called dual",
        "start": 1049.58,
        "duration": 4.92
    },
    {
        "text": "tree complex wavelet transform which is",
        "start": 1051.5,
        "duration": 4.86
    },
    {
        "text": "nearly each shift invariant but it does",
        "start": 1054.5,
        "duration": 4.86
    },
    {
        "text": "require more computation time",
        "start": 1056.36,
        "duration": 5.22
    },
    {
        "text": "in the application section that I'm",
        "start": 1059.36,
        "duration": 5.1
    },
    {
        "text": "going to be discussing next uh we'll see",
        "start": 1061.58,
        "duration": 5.04
    },
    {
        "text": "that top string can be used in series",
        "start": 1064.46,
        "duration": 4.5
    },
    {
        "text": "with these other transforms so you're",
        "start": 1066.62,
        "duration": 4.62
    },
    {
        "text": "not locked into using only cost strength",
        "start": 1068.96,
        "duration": 4.88
    },
    {
        "text": "clear analysis",
        "start": 1071.24,
        "duration": 2.6
    },
    {
        "text": "okay so next Real World applications for",
        "start": 1076.1,
        "duration": 6.5
    },
    {
        "text": "using pot string",
        "start": 1079.52,
        "duration": 3.08
    },
    {
        "text": "foreign",
        "start": 1084.26,
        "duration": 3.0
    },
    {
        "text": "of course yeah all right so just to give",
        "start": 1090.88,
        "duration": 7.78
    },
    {
        "text": "a brief overview of the types of signals",
        "start": 1095.48,
        "duration": 4.699
    },
    {
        "text": "that we're looking at",
        "start": 1098.66,
        "duration": 5.58
    },
    {
        "text": "electrocardiogram in our lab so ECG is",
        "start": 1100.179,
        "duration": 6.041
    },
    {
        "text": "I'm sure you've all seen these QRS",
        "start": 1104.24,
        "duration": 4.16
    },
    {
        "text": "complexes before but it's looking at",
        "start": 1106.22,
        "duration": 4.68
    },
    {
        "text": "electrical activity that occurs in the",
        "start": 1108.4,
        "duration": 4.899
    },
    {
        "text": "heart and it's measured using electrodes",
        "start": 1110.9,
        "duration": 4.019
    },
    {
        "text": "that are placed on the body",
        "start": 1113.299,
        "duration": 3.901
    },
    {
        "text": "of the relationship between these",
        "start": 1114.919,
        "duration": 5.041
    },
    {
        "text": "electrodes leads uh generates leads",
        "start": 1117.2,
        "duration": 6.14
    },
    {
        "text": "which we use for analysis",
        "start": 1119.96,
        "duration": 3.38
    },
    {
        "text": "so",
        "start": 1126.679,
        "duration": 5.821
    },
    {
        "text": "heart rate variability is computed from",
        "start": 1129.26,
        "duration": 5.34
    },
    {
        "text": "these ECG feedings",
        "start": 1132.5,
        "duration": 4.26
    },
    {
        "text": "so the way we obtain heart rate",
        "start": 1134.6,
        "duration": 4.92
    },
    {
        "text": "variability is by first identifying",
        "start": 1136.76,
        "duration": 6.539
    },
    {
        "text": "these R Peaks within the ECG signal",
        "start": 1139.52,
        "duration": 5.82
    },
    {
        "text": "and then Computing the difference in",
        "start": 1143.299,
        "duration": 4.861
    },
    {
        "text": "time between consecutive Peaks",
        "start": 1145.34,
        "duration": 4.92
    },
    {
        "text": "and so like part of everybody is very",
        "start": 1148.16,
        "duration": 3.96
    },
    {
        "text": "useful for looking at features like",
        "start": 1150.26,
        "duration": 4.44
    },
    {
        "text": "heart rate uh beat-to-beat data and",
        "start": 1152.12,
        "duration": 6.08
    },
    {
        "text": "other frequency domain features",
        "start": 1154.7,
        "duration": 3.5
    },
    {
        "text": "and then the exciting part for me at",
        "start": 1159.679,
        "duration": 6.0
    },
    {
        "text": "least is tensors",
        "start": 1162.74,
        "duration": 5.7
    },
    {
        "text": "first what is a tensor",
        "start": 1165.679,
        "duration": 4.261
    },
    {
        "text": "so we can see from this illustration",
        "start": 1168.44,
        "duration": 3.359
    },
    {
        "text": "here it kind of looks like a cube but",
        "start": 1169.94,
        "duration": 3.66
    },
    {
        "text": "it's not always like that",
        "start": 1171.799,
        "duration": 3.841
    },
    {
        "text": "a good way of looking at sensors is",
        "start": 1173.6,
        "duration": 4.439
    },
    {
        "text": "saying that a tensor is a way of storing",
        "start": 1175.64,
        "duration": 4.22
    },
    {
        "text": "information",
        "start": 1178.039,
        "duration": 6.121
    },
    {
        "text": "so tensors have different modes or",
        "start": 1179.86,
        "duration": 5.62
    },
    {
        "text": "different dimensions along which",
        "start": 1184.16,
        "duration": 3.24
    },
    {
        "text": "information can be stored",
        "start": 1185.48,
        "duration": 3.78
    },
    {
        "text": "something like a vector of data or",
        "start": 1187.4,
        "duration": 3.42
    },
    {
        "text": "sequence of numbers would be considered",
        "start": 1189.26,
        "duration": 4.86
    },
    {
        "text": "a first order tensor Matrix which has",
        "start": 1190.82,
        "duration": 4.92
    },
    {
        "text": "data along two Dimensions would be",
        "start": 1194.12,
        "duration": 3.72
    },
    {
        "text": "considered a second order tensor",
        "start": 1195.74,
        "duration": 4.08
    },
    {
        "text": "and then a tensor that has information",
        "start": 1197.84,
        "duration": 4.56
    },
    {
        "text": "in three or more dimensions",
        "start": 1199.82,
        "duration": 4.979
    },
    {
        "text": "would be considered a third or sorry a",
        "start": 1202.4,
        "duration": 4.92
    },
    {
        "text": "higher order tensor",
        "start": 1204.799,
        "duration": 4.081
    },
    {
        "text": "it's like we have in this illustration",
        "start": 1207.32,
        "duration": 4.32
    },
    {
        "text": "here a third order concern which you can",
        "start": 1208.88,
        "duration": 5.22
    },
    {
        "text": "use it's indices i j and k to select",
        "start": 1211.64,
        "duration": 6.12
    },
    {
        "text": "individual data points within it",
        "start": 1214.1,
        "duration": 6.42
    },
    {
        "text": "and so you might ask what what use is",
        "start": 1217.76,
        "duration": 5.46
    },
    {
        "text": "the fencer is a tensor in my machine",
        "start": 1220.52,
        "duration": 4.68
    },
    {
        "text": "learning lab",
        "start": 1223.22,
        "duration": 4.199
    },
    {
        "text": "and so if we look at traditional",
        "start": 1225.2,
        "duration": 4.8
    },
    {
        "text": "learning methods like say random forest",
        "start": 1227.419,
        "duration": 5.701
    },
    {
        "text": "or support Vector machine these learning",
        "start": 1230.0,
        "duration": 5.52
    },
    {
        "text": "methods expect that the data that you",
        "start": 1233.12,
        "duration": 5.4
    },
    {
        "text": "give to them will be in a vector format",
        "start": 1235.52,
        "duration": 7.519
    },
    {
        "text": "so in that first mode",
        "start": 1238.52,
        "duration": 4.519
    },
    {
        "text": "the disadvantage to this is that you can",
        "start": 1243.14,
        "duration": 5.22
    },
    {
        "text": "lose a lot of structural information if",
        "start": 1245.78,
        "duration": 4.92
    },
    {
        "text": "you break your data down into these",
        "start": 1248.36,
        "duration": 4.8
    },
    {
        "text": "information vectors",
        "start": 1250.7,
        "duration": 4.14
    },
    {
        "text": "so you might not be able to see",
        "start": 1253.16,
        "duration": 4.379
    },
    {
        "text": "interactions between variables uh in",
        "start": 1254.84,
        "duration": 6.24
    },
    {
        "text": "relation to time or see how variables",
        "start": 1257.539,
        "duration": 6.541
    },
    {
        "text": "correlate with each other",
        "start": 1261.08,
        "duration": 6.24
    },
    {
        "text": "so by storing information as a tensor we",
        "start": 1264.08,
        "duration": 4.979
    },
    {
        "text": "hope to preserve a lot of that",
        "start": 1267.32,
        "duration": 4.58
    },
    {
        "text": "structural information",
        "start": 1269.059,
        "duration": 2.841
    },
    {
        "text": "so the specific example I'm going to",
        "start": 1274.039,
        "duration": 5.221
    },
    {
        "text": "talk about today is predicting",
        "start": 1276.14,
        "duration": 5.94
    },
    {
        "text": "hemodynamic decompensation after cardiac",
        "start": 1279.26,
        "duration": 4.56
    },
    {
        "text": "surgery",
        "start": 1282.08,
        "duration": 4.26
    },
    {
        "text": "so the original setup was that a set of",
        "start": 1283.82,
        "duration": 4.68
    },
    {
        "text": "patients had it was a retrospective",
        "start": 1286.34,
        "duration": 4.92
    },
    {
        "text": "study where a group of patients had",
        "start": 1288.5,
        "duration": 4.679
    },
    {
        "text": "undergone some kind of cardiac surgery",
        "start": 1291.26,
        "duration": 4.74
    },
    {
        "text": "and they were monitored to see if they",
        "start": 1293.179,
        "duration": 5.821
    },
    {
        "text": "would have a decompensation event",
        "start": 1296.0,
        "duration": 4.2
    },
    {
        "text": "so this could be something like heart",
        "start": 1299.0,
        "duration": 3.659
    },
    {
        "text": "failure it could be like need of",
        "start": 1300.2,
        "duration": 5.28
    },
    {
        "text": "vasopressors it could be any kind of",
        "start": 1302.659,
        "duration": 7.281
    },
    {
        "text": "adverse effect after cardiac surgery",
        "start": 1305.48,
        "duration": 4.46
    },
    {
        "text": "uh the data we had available to our",
        "start": 1309.98,
        "duration": 5.699
    },
    {
        "text": "group was ECG data electronic health",
        "start": 1312.02,
        "duration": 5.279
    },
    {
        "text": "record data which would be information",
        "start": 1315.679,
        "duration": 5.341
    },
    {
        "text": "like age demographics in addition to",
        "start": 1317.299,
        "duration": 6.841
    },
    {
        "text": "medications administered and Labs",
        "start": 1321.02,
        "duration": 4.98
    },
    {
        "text": "provided",
        "start": 1324.14,
        "duration": 3.96
    },
    {
        "text": "in addition there was also arterial",
        "start": 1326.0,
        "duration": 6.0
    },
    {
        "text": "blood pressure and spo2",
        "start": 1328.1,
        "duration": 6.059
    },
    {
        "text": "so the goal of this retrospective study",
        "start": 1332.0,
        "duration": 5.34
    },
    {
        "text": "was to see if",
        "start": 1334.159,
        "duration": 6.301
    },
    {
        "text": "hemodynamic decompensation could be",
        "start": 1337.34,
        "duration": 7.459
    },
    {
        "text": "predicted early and if so how are we",
        "start": 1340.46,
        "duration": 4.339
    },
    {
        "text": "so a big part of how the lab approached",
        "start": 1347.12,
        "duration": 5.76
    },
    {
        "text": "this problem was using signal processing",
        "start": 1349.88,
        "duration": 6.06
    },
    {
        "text": "so for example this is a breakdown of",
        "start": 1352.88,
        "duration": 7.76
    },
    {
        "text": "the processing of the input ECG signal",
        "start": 1355.94,
        "duration": 4.7
    },
    {
        "text": "first the EC P signal was pre-processed",
        "start": 1361.46,
        "duration": 4.62
    },
    {
        "text": "using a butterwork filter just to get",
        "start": 1363.919,
        "duration": 6.201
    },
    {
        "text": "rid of Baseline Monitor and some noise",
        "start": 1366.08,
        "duration": 4.04
    },
    {
        "text": "but then we use our taut string method",
        "start": 1370.34,
        "duration": 8.04
    },
    {
        "text": "so we use top string on the ECG to",
        "start": 1375.14,
        "duration": 6.3
    },
    {
        "text": "extract those statistical features like",
        "start": 1378.38,
        "duration": 6.419
    },
    {
        "text": "the mean median pertosis and skewness on",
        "start": 1381.44,
        "duration": 4.14
    },
    {
        "text": "that",
        "start": 1384.799,
        "duration": 2.481
    },
    {
        "text": "thought straight",
        "start": 1385.58,
        "duration": 5.16
    },
    {
        "text": "decomposed ECG signal",
        "start": 1387.28,
        "duration": 6.58
    },
    {
        "text": "but in addition to that we also use this",
        "start": 1390.74,
        "duration": 6.179
    },
    {
        "text": "dual tree complex uh wavelet packet",
        "start": 1393.86,
        "duration": 5.76
    },
    {
        "text": "transform to get more of those frequency",
        "start": 1396.919,
        "duration": 5.88
    },
    {
        "text": "domain features from the ECP signal",
        "start": 1399.62,
        "duration": 5.88
    },
    {
        "text": "we apply those to the tot string the",
        "start": 1402.799,
        "duration": 4.74
    },
    {
        "text": "noise dcg",
        "start": 1405.5,
        "duration": 4.98
    },
    {
        "text": "and then we extract features for that",
        "start": 1407.539,
        "duration": 5.401
    },
    {
        "text": "and then in addition to both of those",
        "start": 1410.48,
        "duration": 4.74
    },
    {
        "text": "processes that were done on the top",
        "start": 1412.94,
        "duration": 6.3
    },
    {
        "text": "string ECG we also detected Peaks the",
        "start": 1415.22,
        "duration": 6.66
    },
    {
        "text": "RPX in the signal to generate heart rate",
        "start": 1419.24,
        "duration": 5.96
    },
    {
        "text": "variability data foreign",
        "start": 1421.88,
        "duration": 7.44
    },
    {
        "text": "on that hrb data to get a the noise",
        "start": 1425.2,
        "duration": 6.219
    },
    {
        "text": "approximation and extract the features",
        "start": 1429.32,
        "duration": 4.46
    },
    {
        "text": "from that",
        "start": 1431.419,
        "duration": 2.361
    },
    {
        "text": "and so one of the ways that we can model",
        "start": 1436.82,
        "duration": 6.719
    },
    {
        "text": "this data as a tensor is varying the",
        "start": 1439.88,
        "duration": 6.36
    },
    {
        "text": "value of Epsilon",
        "start": 1443.539,
        "duration": 6.421
    },
    {
        "text": "so for each Epsilon we create this",
        "start": 1446.24,
        "duration": 5.28
    },
    {
        "text": "thought string approximation of our",
        "start": 1449.96,
        "duration": 2.94
    },
    {
        "text": "input signal",
        "start": 1451.52,
        "duration": 4.62
    },
    {
        "text": "and so we just note it as like x to the",
        "start": 1452.9,
        "duration": 5.46
    },
    {
        "text": "Epsilon",
        "start": 1456.14,
        "duration": 4.14
    },
    {
        "text": "and so I'm going to skip to the next",
        "start": 1458.36,
        "duration": 3.42
    },
    {
        "text": "slide because I think it's a bit more",
        "start": 1460.28,
        "duration": 5.399
    },
    {
        "text": "easy to see so if we have our input ECG",
        "start": 1461.78,
        "duration": 4.98
    },
    {
        "text": "signal",
        "start": 1465.679,
        "duration": 3.541
    },
    {
        "text": "what we do is we break it down into",
        "start": 1466.76,
        "duration": 6.68
    },
    {
        "text": "tumbling Windows of equal size",
        "start": 1469.22,
        "duration": 4.22
    },
    {
        "text": "we compute a feature Matrix where we",
        "start": 1473.72,
        "duration": 5.339
    },
    {
        "text": "compute a set of features from the",
        "start": 1477.26,
        "duration": 4.08
    },
    {
        "text": "thought string approximation",
        "start": 1479.059,
        "duration": 4.441
    },
    {
        "text": "for each time window so we end up",
        "start": 1481.34,
        "duration": 4.14
    },
    {
        "text": "generating this Matrix where it's a set",
        "start": 1483.5,
        "duration": 3.84
    },
    {
        "text": "of features computed for each time",
        "start": 1485.48,
        "duration": 3.54
    },
    {
        "text": "window",
        "start": 1487.34,
        "duration": 4.14
    },
    {
        "text": "and then we repeat that for each value",
        "start": 1489.02,
        "duration": 5.34
    },
    {
        "text": "of Epsilon so we end up with this",
        "start": 1491.48,
        "duration": 5.46
    },
    {
        "text": "feature tensor where we have a feature",
        "start": 1494.36,
        "duration": 6.559
    },
    {
        "text": "Matrix for each value of Epsilon",
        "start": 1496.94,
        "duration": 3.979
    },
    {
        "text": "oh wow yeah yeah this is one of the",
        "start": 1501.94,
        "duration": 5.02
    },
    {
        "text": "figures from the paper but it's breaking",
        "start": 1505.039,
        "duration": 5.281
    },
    {
        "text": "down for each patient uh how many",
        "start": 1506.96,
        "duration": 6.66
    },
    {
        "text": "tensors we created so for each patient",
        "start": 1510.32,
        "duration": 7.26
    },
    {
        "text": "we would create this taut string pot",
        "start": 1513.62,
        "duration": 5.82
    },
    {
        "text": "string",
        "start": 1517.58,
        "duration": 4.02
    },
    {
        "text": "for each patient we breed this taut",
        "start": 1519.44,
        "duration": 5.58
    },
    {
        "text": "string tensor for HRV data",
        "start": 1521.6,
        "duration": 7.079
    },
    {
        "text": "the ECG data and for this dual tree",
        "start": 1525.02,
        "duration": 7.8
    },
    {
        "text": "complex wavelet packet transport data",
        "start": 1528.679,
        "duration": 5.761
    },
    {
        "text": "and then in addition to that we also",
        "start": 1532.82,
        "duration": 4.02
    },
    {
        "text": "perform taut string on arterial blood",
        "start": 1534.44,
        "duration": 5.76
    },
    {
        "text": "pressure and on spo2",
        "start": 1536.84,
        "duration": 7.199
    },
    {
        "text": "and this was repeated for each patient",
        "start": 1540.2,
        "duration": 8.339
    },
    {
        "text": "and so once we have those n by five",
        "start": 1544.039,
        "duration": 7.681
    },
    {
        "text": "tensors for every patient okay",
        "start": 1548.539,
        "duration": 6.481
    },
    {
        "text": "yes what we do is we stack all of those",
        "start": 1551.72,
        "duration": 7.079
    },
    {
        "text": "tensors along a fourth mode so we take",
        "start": 1555.02,
        "duration": 6.0
    },
    {
        "text": "all of the patient tensors and stack",
        "start": 1558.799,
        "duration": 5.161
    },
    {
        "text": "them together to create one tensor of",
        "start": 1561.02,
        "duration": 7.7
    },
    {
        "text": "each signal type for all the patients",
        "start": 1563.96,
        "duration": 4.76
    },
    {
        "text": "and so from there because these tensors",
        "start": 1570.62,
        "duration": 5.159
    },
    {
        "text": "you know there are a lot of features",
        "start": 1573.559,
        "duration": 4.561
    },
    {
        "text": "being computed and there are multiple",
        "start": 1575.779,
        "duration": 5.041
    },
    {
        "text": "patients we use something called higher",
        "start": 1578.12,
        "duration": 5.82
    },
    {
        "text": "order singular value decomposition to",
        "start": 1580.82,
        "duration": 5.7
    },
    {
        "text": "reduce the feature space of these of",
        "start": 1583.94,
        "duration": 4.08
    },
    {
        "text": "some of these tensors",
        "start": 1586.52,
        "duration": 4.08
    },
    {
        "text": "ho SVD is a little bit out of scope for",
        "start": 1588.02,
        "duration": 4.5
    },
    {
        "text": "this talk but I can answer questions on",
        "start": 1590.6,
        "duration": 4.079
    },
    {
        "text": "it if you'd like later",
        "start": 1592.52,
        "duration": 4.139
    },
    {
        "text": "pretty much the important thing to take",
        "start": 1594.679,
        "duration": 3.661
    },
    {
        "text": "from it is just that it's a way of",
        "start": 1596.659,
        "duration": 3.601
    },
    {
        "text": "reducing the feature space to make these",
        "start": 1598.34,
        "duration": 4.5
    },
    {
        "text": "tensors smaller",
        "start": 1600.26,
        "duration": 4.62
    },
    {
        "text": "and so once we have these groups tensors",
        "start": 1602.84,
        "duration": 3.719
    },
    {
        "text": "for all the patients",
        "start": 1604.88,
        "duration": 4.56
    },
    {
        "text": "we combine them all together to create a",
        "start": 1606.559,
        "duration": 5.041
    },
    {
        "text": "large tensor that is just the entire",
        "start": 1609.44,
        "duration": 4.64
    },
    {
        "text": "data set",
        "start": 1611.6,
        "duration": 2.48
    },
    {
        "text": "and so once we have that large set of",
        "start": 1614.12,
        "duration": 4.159
    },
    {
        "text": "tensors we can compute other",
        "start": 1616.76,
        "duration": 4.32
    },
    {
        "text": "decompositions like canonical parallel",
        "start": 1618.279,
        "duration": 5.201
    },
    {
        "text": "factors to be able to apply this same",
        "start": 1621.08,
        "duration": 6.599
    },
    {
        "text": "decomposite decomposition to any unknown",
        "start": 1623.48,
        "duration": 7.02
    },
    {
        "text": "data so like for example if we were to",
        "start": 1627.679,
        "duration": 5.641
    },
    {
        "text": "compute uh canonical parallel factors",
        "start": 1630.5,
        "duration": 4.679
    },
    {
        "text": "decomposition on our trade set we can",
        "start": 1633.32,
        "duration": 6.66
    },
    {
        "text": "use a decomposition to create this",
        "start": 1635.179,
        "duration": 7.321
    },
    {
        "text": "reduced feature space on any set of",
        "start": 1639.98,
        "duration": 5.9
    },
    {
        "text": "sensors like our test set",
        "start": 1642.5,
        "duration": 3.38
    },
    {
        "text": "so these are all",
        "start": 1647.299,
        "duration": 4.62
    },
    {
        "text": "physiological various physiological",
        "start": 1649.419,
        "duration": 5.681
    },
    {
        "text": "traces and the hypothesis I guess is if",
        "start": 1651.919,
        "duration": 6.481
    },
    {
        "text": "you bring these kind of complementary",
        "start": 1655.1,
        "duration": 4.98
    },
    {
        "text": "physiological measurements together",
        "start": 1658.4,
        "duration": 3.48
    },
    {
        "text": "you're going to get a better indication",
        "start": 1660.08,
        "duration": 2.94
    },
    {
        "text": "of",
        "start": 1661.88,
        "duration": 4.14
    },
    {
        "text": "I mean what's what's the advantage of",
        "start": 1663.02,
        "duration": 4.32
    },
    {
        "text": "this",
        "start": 1666.02,
        "duration": 4.5
    },
    {
        "text": "this integrative approach I mean it's a",
        "start": 1667.34,
        "duration": 5.699
    },
    {
        "text": "better predictability I mean what well",
        "start": 1670.52,
        "duration": 5.159
    },
    {
        "text": "the Hope Was That by using a multimodal",
        "start": 1673.039,
        "duration": 4.861
    },
    {
        "text": "approach we could use multiple different",
        "start": 1675.679,
        "duration": 5.761
    },
    {
        "text": "physiological signals to look at the",
        "start": 1677.9,
        "duration": 6.42
    },
    {
        "text": "status of the patient I see because the",
        "start": 1681.44,
        "duration": 5.7
    },
    {
        "text": "the alert really what you want is alert",
        "start": 1684.32,
        "duration": 4.68
    },
    {
        "text": "at the end of the day this patient's in",
        "start": 1687.14,
        "duration": 6.08
    },
    {
        "text": "trouble yeah and by um",
        "start": 1689.0,
        "duration": 4.22
    },
    {
        "text": "there's",
        "start": 1693.26,
        "duration": 4.74
    },
    {
        "text": "reasonable that if you bring these these",
        "start": 1694.82,
        "duration": 5.4
    },
    {
        "text": "measurements together",
        "start": 1698.0,
        "duration": 4.5
    },
    {
        "text": "you know some of the interplay might",
        "start": 1700.22,
        "duration": 4.92
    },
    {
        "text": "actually alert you earlier I mean that's",
        "start": 1702.5,
        "duration": 4.919
    },
    {
        "text": "the Hope right exactly so that's why we",
        "start": 1705.14,
        "duration": 4.26
    },
    {
        "text": "wanted to format the data's offensors to",
        "start": 1707.419,
        "duration": 4.74
    },
    {
        "text": "see these uh changes in the variables",
        "start": 1709.4,
        "duration": 4.68
    },
    {
        "text": "relative to one another that might",
        "start": 1712.159,
        "duration": 4.02
    },
    {
        "text": "otherwise be lost if we just modeled",
        "start": 1714.08,
        "duration": 5.839
    },
    {
        "text": "every single thing as a vector",
        "start": 1716.179,
        "duration": 3.74
    },
    {
        "text": "okay so this I think is a good",
        "start": 1722.419,
        "duration": 4.441
    },
    {
        "text": "comprehensive set of the results",
        "start": 1725.299,
        "duration": 4.141
    },
    {
        "text": "apologies it is a bit small",
        "start": 1726.86,
        "duration": 5.4
    },
    {
        "text": "and hard to read but what each of these",
        "start": 1729.44,
        "duration": 5.88
    },
    {
        "text": "graphs is doing is it's looking at the",
        "start": 1732.26,
        "duration": 5.0
    },
    {
        "text": "area under the receiver operating",
        "start": 1735.32,
        "duration": 4.8
    },
    {
        "text": "characteristic curve for different",
        "start": 1737.26,
        "duration": 4.6
    },
    {
        "text": "prediction windows for different",
        "start": 1740.12,
        "duration": 4.14
    },
    {
        "text": "learning models",
        "start": 1741.86,
        "duration": 5.819
    },
    {
        "text": "and so like for example if we look at a",
        "start": 1744.26,
        "duration": 5.82
    },
    {
        "text": "naive Bayes here we can see that the",
        "start": 1747.679,
        "duration": 4.62
    },
    {
        "text": "prediction window which is saying how",
        "start": 1750.08,
        "duration": 4.62
    },
    {
        "text": "far in advance of the decompensation",
        "start": 1752.299,
        "duration": 6.781
    },
    {
        "text": "event can we predict that there might be",
        "start": 1754.7,
        "duration": 7.979
    },
    {
        "text": "an onset of this event happening",
        "start": 1759.08,
        "duration": 7.199
    },
    {
        "text": "it's like this is in a number of hours",
        "start": 1762.679,
        "duration": 4.921
    },
    {
        "text": "for example",
        "start": 1766.279,
        "duration": 5.341
    },
    {
        "text": "even this the Y the y-axis is the area",
        "start": 1767.6,
        "duration": 7.559
    },
    {
        "text": "under the RFC curve",
        "start": 1771.62,
        "duration": 5.52
    },
    {
        "text": "so it's split up into different learning",
        "start": 1775.159,
        "duration": 4.981
    },
    {
        "text": "methods so this is Knight fades",
        "start": 1777.14,
        "duration": 5.46
    },
    {
        "text": "this is support Vector machine random",
        "start": 1780.14,
        "duration": 5.7
    },
    {
        "text": "Forest uh Luke which is a learning",
        "start": 1782.6,
        "duration": 4.92
    },
    {
        "text": "method developed by one of our",
        "start": 1785.84,
        "duration": 4.079
    },
    {
        "text": "collaborators Dr Hartford",
        "start": 1787.52,
        "duration": 4.5
    },
    {
        "text": "and at least",
        "start": 1789.919,
        "duration": 4.681
    },
    {
        "text": "and what each of the colored lines on",
        "start": 1792.02,
        "duration": 5.399
    },
    {
        "text": "the graphs represent is a different uh",
        "start": 1794.6,
        "duration": 5.34
    },
    {
        "text": "data set that was to train the model",
        "start": 1797.419,
        "duration": 4.26
    },
    {
        "text": "it's like the red and pink are just",
        "start": 1799.94,
        "duration": 3.3
    },
    {
        "text": "looking at all of these tensors put",
        "start": 1801.679,
        "duration": 4.141
    },
    {
        "text": "together but with no tensor reduction",
        "start": 1803.24,
        "duration": 7.86
    },
    {
        "text": "uh the blue is tensors of the signal",
        "start": 1805.82,
        "duration": 9.479
    },
    {
        "text": "data which has been reduced via hospd",
        "start": 1811.1,
        "duration": 7.439
    },
    {
        "text": "uh the green our tensor reduction plus",
        "start": 1815.299,
        "duration": 5.701
    },
    {
        "text": "electronic health record data and then",
        "start": 1818.539,
        "duration": 5.76
    },
    {
        "text": "the purple and magenta are non-reduced",
        "start": 1821.0,
        "duration": 6.0
    },
    {
        "text": "tensors so all the data plus electronic",
        "start": 1824.299,
        "duration": 5.6
    },
    {
        "text": "health record data",
        "start": 1827.0,
        "duration": 2.899
    },
    {
        "text": "okay so what we can see is the general",
        "start": 1830.799,
        "duration": 5.62
    },
    {
        "text": "trend from these graphs is that at least",
        "start": 1833.84,
        "duration": 6.959
    },
    {
        "text": "in naive Bayes random forest and Luke we",
        "start": 1836.419,
        "duration": 6.721
    },
    {
        "text": "consistently see higher area of the",
        "start": 1840.799,
        "duration": 5.661
    },
    {
        "text": "curve when using both tensor reduction",
        "start": 1843.14,
        "duration": 6.899
    },
    {
        "text": "and electronic health record data",
        "start": 1846.46,
        "duration": 6.599
    },
    {
        "text": "uh this uh",
        "start": 1850.039,
        "duration": 6.181
    },
    {
        "text": "at a boost and support Vector machine",
        "start": 1853.059,
        "duration": 5.081
    },
    {
        "text": "tend to do better when they're using",
        "start": 1856.22,
        "duration": 4.74
    },
    {
        "text": "non-reduced tensor data plus electronic",
        "start": 1858.14,
        "duration": 5.159
    },
    {
        "text": "helpbreaker data but generally we see",
        "start": 1860.96,
        "duration": 4.26
    },
    {
        "text": "that using the signal data plus",
        "start": 1863.299,
        "duration": 3.661
    },
    {
        "text": "electronic health record data is better",
        "start": 1865.22,
        "duration": 5.66
    },
    {
        "text": "than using just signal data by itself",
        "start": 1866.96,
        "duration": 3.92
    },
    {
        "text": "um the other information that we can",
        "start": 1872.12,
        "duration": 4.08
    },
    {
        "text": "glean from this is that generally when",
        "start": 1873.5,
        "duration": 5.1
    },
    {
        "text": "the prediction window is smaller so",
        "start": 1876.2,
        "duration": 4.74
    },
    {
        "text": "we'll be looking at about like",
        "start": 1878.6,
        "duration": 5.04
    },
    {
        "text": "what the two hours in advance it's the",
        "start": 1880.94,
        "duration": 5.64
    },
    {
        "text": "models tend to be more predictive than",
        "start": 1883.64,
        "duration": 5.46
    },
    {
        "text": "at spaces like 12 hours",
        "start": 1886.58,
        "duration": 4.74
    },
    {
        "text": "and that's not entirely unexpected",
        "start": 1889.1,
        "duration": 3.959
    },
    {
        "text": "because",
        "start": 1891.32,
        "duration": 3.719
    },
    {
        "text": "we would assume that physiological",
        "start": 1893.059,
        "duration": 5.041
    },
    {
        "text": "signals like ECG and blood pressure",
        "start": 1895.039,
        "duration": 5.76
    },
    {
        "text": "would be more predictive of short-term",
        "start": 1898.1,
        "duration": 5.1
    },
    {
        "text": "events than these longer term events",
        "start": 1900.799,
        "duration": 5.961
    },
    {
        "text": "like over the course of 12 hours",
        "start": 1903.2,
        "duration": 3.56
    },
    {
        "text": "and so the setup for this experiment",
        "start": 1907.399,
        "duration": 6.301
    },
    {
        "text": "predicting onset of chemodynamic",
        "start": 1911.059,
        "duration": 6.0
    },
    {
        "text": "decompensation is very similar to the",
        "start": 1913.7,
        "duration": 5.459
    },
    {
        "text": "setup that I'm using for my thesis work",
        "start": 1917.059,
        "duration": 4.321
    },
    {
        "text": "which is looking at",
        "start": 1919.159,
        "duration": 6.601
    },
    {
        "text": "uh septic decompensation but also using",
        "start": 1921.38,
        "duration": 8.159
    },
    {
        "text": "auton approximations of ECG and",
        "start": 1925.76,
        "duration": 5.94
    },
    {
        "text": "criterial blood pressure in addition to",
        "start": 1929.539,
        "duration": 5.581
    },
    {
        "text": "using electronic health rigger data",
        "start": 1931.7,
        "duration": 5.88
    },
    {
        "text": "right I was really wondering about how",
        "start": 1935.12,
        "duration": 3.84
    },
    {
        "text": "you were going to integrate some of",
        "start": 1937.58,
        "duration": 4.319
    },
    {
        "text": "these non-continuous variables",
        "start": 1938.96,
        "duration": 5.099
    },
    {
        "text": "maybe there's co-variance that you might",
        "start": 1941.899,
        "duration": 4.02
    },
    {
        "text": "find or something I mean I was just kind",
        "start": 1944.059,
        "duration": 3.961
    },
    {
        "text": "of curious because when you were",
        "start": 1945.919,
        "duration": 3.781
    },
    {
        "text": "integrating these different kinds of",
        "start": 1948.02,
        "duration": 3.06
    },
    {
        "text": "data you know this is pretty",
        "start": 1949.7,
        "duration": 2.88
    },
    {
        "text": "straightforward because in the end of",
        "start": 1951.08,
        "duration": 3.12
    },
    {
        "text": "the day they're physiological they're",
        "start": 1952.58,
        "duration": 5.339
    },
    {
        "text": "related you know cardiovascular stop or",
        "start": 1954.2,
        "duration": 5.94
    },
    {
        "text": "you know coded in time",
        "start": 1957.919,
        "duration": 3.961
    },
    {
        "text": "but you have these other kinds of",
        "start": 1960.14,
        "duration": 4.5
    },
    {
        "text": "variables that are really kind of",
        "start": 1961.88,
        "duration": 3.84
    },
    {
        "text": "different",
        "start": 1964.64,
        "duration": 3.18
    },
    {
        "text": "so I was too excited to bring all that",
        "start": 1965.72,
        "duration": 4.5
    },
    {
        "text": "together yeah so for electronic health",
        "start": 1967.82,
        "duration": 5.76
    },
    {
        "text": "record data things that are constant",
        "start": 1970.22,
        "duration": 6.78
    },
    {
        "text": "like demographics like age or like sex",
        "start": 1973.58,
        "duration": 5.219
    },
    {
        "text": "that's definitely something we can just",
        "start": 1977.0,
        "duration": 5.1
    },
    {
        "text": "easily encode as something like a binary",
        "start": 1978.799,
        "duration": 5.821
    },
    {
        "text": "variable but for things that are more",
        "start": 1982.1,
        "duration": 5.16
    },
    {
        "text": "time dependent like lab data yeah",
        "start": 1984.62,
        "duration": 5.34
    },
    {
        "text": "medications yeah medications that's",
        "start": 1987.26,
        "duration": 5.34
    },
    {
        "text": "something that we would at least in this",
        "start": 1989.96,
        "duration": 6.24
    },
    {
        "text": "setup I know that what was done was the",
        "start": 1992.6,
        "duration": 6.24
    },
    {
        "text": "labs and medications that were",
        "start": 1996.2,
        "duration": 5.4
    },
    {
        "text": "administered during that prediction time",
        "start": 1998.84,
        "duration": 4.98
    },
    {
        "text": "were the only ones included in model",
        "start": 2001.6,
        "duration": 3.66
    },
    {
        "text": "training",
        "start": 2003.82,
        "duration": 3.599
    },
    {
        "text": "so it didn't it didn't look into the",
        "start": 2005.26,
        "duration": 6.98
    },
    {
        "text": "future to see what labs were performed",
        "start": 2007.419,
        "duration": 4.821
    },
    {
        "text": "right but it's not looking into the past",
        "start": 2012.519,
        "duration": 5.821
    },
    {
        "text": "either in terms of like a total Stat or",
        "start": 2014.5,
        "duration": 6.419
    },
    {
        "text": "some kind of bats that you know usually",
        "start": 2018.34,
        "duration": 4.86
    },
    {
        "text": "they come in with several Acts",
        "start": 2020.919,
        "duration": 3.48
    },
    {
        "text": "yeah",
        "start": 2023.2,
        "duration": 3.3
    },
    {
        "text": "um I'd have to look into that to see how",
        "start": 2024.399,
        "duration": 4.221
    },
    {
        "text": "far it goes back",
        "start": 2026.5,
        "duration": 5.64
    },
    {
        "text": "that'd be good for you no it's you know",
        "start": 2028.62,
        "duration": 6.46
    },
    {
        "text": "some of the meds might be you know being",
        "start": 2032.14,
        "duration": 4.919
    },
    {
        "text": "administered to take care of some of",
        "start": 2035.08,
        "duration": 4.319
    },
    {
        "text": "these problems yeah you know blood",
        "start": 2037.059,
        "duration": 4.261
    },
    {
        "text": "thinners you know something for heart",
        "start": 2039.399,
        "duration": 3.601
    },
    {
        "text": "rate variability I mean it would be",
        "start": 2041.32,
        "duration": 4.02
    },
    {
        "text": "really good to know yeah no I know that",
        "start": 2043.0,
        "duration": 5.04
    },
    {
        "text": "stuff like uh",
        "start": 2045.34,
        "duration": 7.259
    },
    {
        "text": "face oppressors for sure recorded",
        "start": 2048.04,
        "duration": 4.559
    },
    {
        "text": "really indicative",
        "start": 2054.22,
        "duration": 2.959
    },
    {
        "text": "right well that is my thought for today",
        "start": 2059.619,
        "duration": 4.8
    },
    {
        "text": "these are my sources please let me know",
        "start": 2062.44,
        "duration": 4.32
    },
    {
        "text": "if you have any questions",
        "start": 2064.419,
        "duration": 5.96
    },
    {
        "text": "yeah that's really good",
        "start": 2066.76,
        "duration": 3.619
    },
    {
        "text": "were those online if you have any",
        "start": 2072.94,
        "duration": 3.479
    },
    {
        "text": "questions uh you can put them in the",
        "start": 2074.74,
        "duration": 4.56
    },
    {
        "text": "chat box or uh use these reactions in",
        "start": 2076.419,
        "duration": 4.861
    },
    {
        "text": "the bottom right to raise your hand and",
        "start": 2079.3,
        "duration": 2.98
    },
    {
        "text": "we can uh",
        "start": 2081.28,
        "duration": 3.54
    },
    {
        "text": "[Music]",
        "start": 2082.28,
        "duration": 5.359
    },
    {
        "text": "throw away RC to stick the pictures on",
        "start": 2084.82,
        "duration": 7.519
    },
    {
        "text": "the screen now we can't yes yeah",
        "start": 2087.639,
        "duration": 4.7
    },
    {
        "text": "it's different world isn't it I mean you",
        "start": 2095.74,
        "duration": 3.9
    },
    {
        "text": "know you've got three or four people",
        "start": 2098.02,
        "duration": 4.079
    },
    {
        "text": "here and you know a dozen or more in the",
        "start": 2099.64,
        "duration": 5.36
    },
    {
        "text": "zoo I mean it's just",
        "start": 2102.099,
        "duration": 2.901
    },
    {
        "text": "I I was just getting ready to log on to",
        "start": 2109.8,
        "duration": 5.08
    },
    {
        "text": "zoom and I said well I gotta go down",
        "start": 2113.32,
        "duration": 2.94
    },
    {
        "text": "there but maybe I should serve a pizza",
        "start": 2114.88,
        "duration": 5.88
    },
    {
        "text": "well at least you were here yeah",
        "start": 2116.26,
        "duration": 4.5
    },
    {
        "text": "any questions",
        "start": 2126.3,
        "duration": 3.42
    },
    {
        "text": "[Music]",
        "start": 2130.42,
        "duration": 3.11
    },
    {
        "text": "yeah all right",
        "start": 2136.359,
        "duration": 3.081
    },
    {
        "text": "okay uh no questions but great talk",
        "start": 2146.099,
        "duration": 5.221
    },
    {
        "text": "great job",
        "start": 2152.94,
        "duration": 3.179
    },
    {
        "text": "give it another minute or so",
        "start": 2156.82,
        "duration": 3.38
    },
    {
        "text": "do you think so where are you in your",
        "start": 2161.8,
        "duration": 3.059
    },
    {
        "text": "thesis progression",
        "start": 2163.18,
        "duration": 4.439
    },
    {
        "text": "uh I've got a pretty nice sized data set",
        "start": 2164.859,
        "duration": 6.181
    },
    {
        "text": "it's just a matter of fine-tuning the",
        "start": 2167.619,
        "duration": 5.761
    },
    {
        "text": "work that I'm",
        "start": 2171.04,
        "duration": 4.799
    },
    {
        "text": "have some tensor decomposition code but",
        "start": 2173.38,
        "duration": 4.08
    },
    {
        "text": "we're trying to implement this new",
        "start": 2175.839,
        "duration": 3.421
    },
    {
        "text": "pipeline to make things a lot smoother",
        "start": 2177.46,
        "duration": 3.84
    },
    {
        "text": "and more reproducible",
        "start": 2179.26,
        "duration": 4.14
    },
    {
        "text": "now I've got this really nice data set",
        "start": 2181.3,
        "duration": 6.62
    },
    {
        "text": "that uh Dr Van Epps and",
        "start": 2183.4,
        "duration": 4.52
    },
    {
        "text": "it's pretty much just a matter of",
        "start": 2189.579,
        "duration": 4.381
    },
    {
        "text": "getting reliable and reproducible",
        "start": 2191.8,
        "duration": 4.08
    },
    {
        "text": "results from that data set",
        "start": 2193.96,
        "duration": 4.32
    },
    {
        "text": "thanks",
        "start": 2195.88,
        "duration": 5.1
    },
    {
        "text": "but just I'm underselling how hard that",
        "start": 2198.28,
        "duration": 6.54
    },
    {
        "text": "is but well well yeah but at least",
        "start": 2200.98,
        "duration": 8.119
    },
    {
        "text": "I think most of us can appreciate that",
        "start": 2204.82,
        "duration": 4.279
    },
    {
        "text": "and also getting a data set was a very",
        "start": 2209.32,
        "duration": 4.68
    },
    {
        "text": "big part of that too",
        "start": 2211.78,
        "duration": 5.339
    },
    {
        "text": "how many participants do you have uh so",
        "start": 2214.0,
        "duration": 6.92
    },
    {
        "text": "minus Ultra also retrospective",
        "start": 2217.119,
        "duration": 3.801
    },
    {
        "text": "more than like 2 000 actual patients in",
        "start": 2225.579,
        "duration": 5.52
    },
    {
        "text": "it but once we start narrowing down for",
        "start": 2228.579,
        "duration": 4.561
    },
    {
        "text": "criteria of okay they experience some",
        "start": 2231.099,
        "duration": 3.961
    },
    {
        "text": "kind of a decompensation event okay they",
        "start": 2233.14,
        "duration": 3.959
    },
    {
        "text": "were probably septic it starts getting",
        "start": 2235.06,
        "duration": 3.9
    },
    {
        "text": "whittled down further further and then",
        "start": 2237.099,
        "duration": 3.541
    },
    {
        "text": "once you start looking for okay do they",
        "start": 2238.96,
        "duration": 4.56
    },
    {
        "text": "have usable signal data and that gets",
        "start": 2240.64,
        "duration": 4.439
    },
    {
        "text": "even smaller",
        "start": 2243.52,
        "duration": 4.14
    },
    {
        "text": "right yeah just trying to find usable",
        "start": 2245.079,
        "duration": 5.54
    },
    {
        "text": "signal data in and up",
        "start": 2247.66,
        "duration": 2.959
    },
    {
        "text": "it's yeah like electronic health record",
        "start": 2251.74,
        "duration": 3.3
    },
    {
        "text": "data we can get pretty easily it's just",
        "start": 2253.119,
        "duration": 4.641
    },
    {
        "text": "signal data",
        "start": 2255.04,
        "duration": 2.72
    },
    {
        "text": "so they were what what was the study",
        "start": 2261.599,
        "duration": 5.081
    },
    {
        "text": "that they were uh was there you know you",
        "start": 2264.28,
        "duration": 5.22
    },
    {
        "text": "mentioned retrospective so it was a",
        "start": 2266.68,
        "duration": 7.32
    },
    {
        "text": "this a clinical study that was uh so the",
        "start": 2269.5,
        "duration": 6.119
    },
    {
        "text": "data that I'm pulling from I think was",
        "start": 2274.0,
        "duration": 6.0
    },
    {
        "text": "from a biobank uh set up",
        "start": 2275.619,
        "duration": 6.48
    },
    {
        "text": "I see so it's not even really from here",
        "start": 2280.0,
        "duration": 4.859
    },
    {
        "text": "uh no it is from U of M I think it was",
        "start": 2282.099,
        "duration": 6.26
    },
    {
        "text": "under Dr Ward that it was set out um",
        "start": 2284.859,
        "duration": 5.401
    },
    {
        "text": "yeah",
        "start": 2288.359,
        "duration": 4.181
    },
    {
        "text": "I think there are a couple of data sets",
        "start": 2290.26,
        "duration": 3.66
    },
    {
        "text": "I've looked at and I know that they kind",
        "start": 2292.54,
        "duration": 4.88
    },
    {
        "text": "of get blurred a bit in my head together",
        "start": 2293.92,
        "duration": 3.5
    },
    {
        "text": "questions online so well you just you",
        "start": 2303.18,
        "duration": 5.62
    },
    {
        "text": "know I thought it was a great talk I'm",
        "start": 2307.0,
        "duration": 3.9
    },
    {
        "text": "really glad I had a chance to see it and",
        "start": 2308.8,
        "duration": 4.5
    },
    {
        "text": "uh you know it's a good approach and uh",
        "start": 2310.9,
        "duration": 4.56
    },
    {
        "text": "you know I'm sure that you're gonna find",
        "start": 2313.3,
        "duration": 4.86
    },
    {
        "text": "uh results that you could find that uh",
        "start": 2315.46,
        "duration": 4.56
    },
    {
        "text": "you know other less sophisticated ways",
        "start": 2318.16,
        "duration": 4.02
    },
    {
        "text": "or really anxious to hear about how that",
        "start": 2320.02,
        "duration": 5.099
    },
    {
        "text": "works out for you to like inform us",
        "start": 2322.18,
        "duration": 4.02
    },
    {
        "text": "about that",
        "start": 2325.119,
        "duration": 4.761
    },
    {
        "text": "thanks thank you",
        "start": 2326.2,
        "duration": 3.68
    }
]