[
    {
        "start": 22.54,
        "text": "[Music] welcome everyone we're going to go ahead and get started [Applause] hi everyone today about our souls are related to continuous capture of metadata so I'm with the CSE department I work with Professor Jagadish and this is a three year project working with icpsr of our University I'm not sure whether you are familiar with icpsr it's "
    },
    {
        "start": 83.04,
        "text": "a thinner how forces of sine theta pretty popular and we also worked with MDMA collected and several other groups to make this possible so talking about the data we know it's a pretty hot topic recently but before actually using the data for analysis people collect data manipulate the data and do some like unity of adaptions to their own these cases so the data that helps us to identify the proper data for their use is that called the metadata melody hey is that they have a discarded data so as many communities are pushing towards like making they have published together with their master data so there these efforts try to make this more widely available nowadays but different community they have our own like the standards for my husband for example "
    },
    {
        "start": 144.12,
        "text": "through science through science we use the TDI format mostly so it's an XML XML based and format and the ecological data they use EML format also xml-based some other domains biological data they have their like formats called urban core and some other general-purpose formats such as doubling horn walls I recently Google has published their like the search engine for datasets it's called the Google DNS search and they create here their own standards called schema.org so this there is a basically pretty simple so the standards define the tags so if you say I include this tag I will provide some description a corresponding to attack so I will show you example of this Mahadeo standards in DDI format so this is a snapshot of "
    },
    {
        "start": 207.5,
        "text": "potatoes that are called world population from 1950 to 1975 and on this page for the data set we can see the rotate information so we don't see actually the data content right onion but we see some other like methylating information for example okay the burgeoning inflammations the principal investigator for the dataset and some other information such as this is approached a high-level description it gives a summary of a cool collected data which kind of like people are evolving the process and how to cite the data in a standard format other than that it gives like other meta information such as documentation level variable information and a related publication so you and this one it has "
    },
    {
        "start": 270.99,
        "text": "like they have really publication only one of them the one complication has referred to this data set to all this data this information they are call metadata and the metadata can be exported in some standard format so this is our the metadata of the previous D herself in DI format as I said it's in XML format so a started with this tag hard-coded this is DV is our name for our metadata documentation then it has text such as document descriptions are including the title statement the product statement and copywriting informations other than that it has the study level description as what we have seen in the previous page the name title after they have called word population and some other information related to that so it's pretty clear it provides "
    },
    {
        "start": 333.81,
        "text": "like a user a brief view of what this is about and whether it can be used for your research purpose later the whole or this research data collected oh we just like target the social science data from icpsr and this is the process of the are collected so they are generally collected through surveys and the surveys are conducted through computer assisted interview software [Music] but there's no paper involving the process so it's like a questionnaire so basically as a recipient also like the questionnaire you are asked questions such as what's our gender by age and like how many people are there in your family if it's for population information project and so those kind of questions are part of the metadata we "
    },
    {
        "start": 393.879,
        "text": "have say it's like they can be listed after variables in a cup in column format in a data table so as we have thing this year to see AI program itself is a questionnaire and a questionnaire information is in my Hadiya so if we have this question there then we'll get a better understanding of data content okay let's say we first have the computer assisted interviewing and the people have answered questionnaires and what's good about it that CI itself they have the automatic conversion functionality to convert the CIA questionnaire into EDI format so it automatically creates this magnetic information for the data collected and that's the original data we have this is "
    },
    {
        "start": 454.06,
        "text": "our original data this is the original metadata in DI phone oh it's not that as I have talked about that people may use other original data that's they want to do some modifications to the original data before they use it so they will modify the data so generally before you some cities or software do the modifications they will form the command script in some offices or languages for example SPSS is the like a pretty old language opera in 1970s and then reason one such as - instead of the general-purpose language our iPhone equal they are all part of this process so with this return command script um there are seven together with the original data there are sensuous little "
    },
    {
        "start": 515.83,
        "text": "packages to do the processing and then we will get a revised the data digital analysis all or downstream of applications but the problem with that is the revised data is no longer matching the visual metadata so the transformation process is not included in the metadata its describing the original data rather than the current data so if we want to publish the revised data then we need metadata for the revised data rather than the original data the original data will come at the archive to be published but it has no Austrian contact no question contacts no interview flow no variable problems and the transformations are not documented the holo are were able to include that documentation information "
    },
    {
        "start": 577.63,
        "text": "make the our reality has to have its own metadata the whoopi intend to do is that will extract the metadata from the transformation script and then into the DDI format and then publish the revised data together with their own metadata but the problem is that the transformation our document documented by hand so let's say we have original metadata and that we have the transformation on in order to gather new leichter metadata we need to like manually include the transformation in the metadata that's not so we want to make it automatic so you don't have to do it manually yeah so "
    },
    {
        "start": 643.29,
        "text": "citizen themselves that will have limited my editing for example variable names labels value labels but there's no prophets such as how the variable is derived from the original variables so what variables have influenced the derivation of this new variable between enables automatic capture of the metadata of the transformation my idea what we want to do it that you know automatic way we want to like okay great escape hazard such that will convert the command script in some office digital languages into a standard data transformation and then we'll have an update her through embed transformation information into the original metadata so that will arrive the updated "
    },
    {
        "start": 706.27,
        "text": "mahat data for the revised data do you have any questions up to this point okay so those are the missing links we want to view first a converter a script parser that converts this little language to a CTL and then updater that embed that information information into the original metadata okay what kind of script has re-implement we can't like name all of them those are the major ones so we collect the icpsr they have said download information from year 2015 to 2016 and that's the percentile of the download by format so what is in percent our luminol test so those data can be processed by cities or language whichever like user truth so other than "
    },
    {
        "start": 769.41,
        "text": "that SPSS data and are they all composed like a large amount of the downloads so we'll assume that people downloaded in this format they will process data with their that which is corresponding so those are the four languages we choose to ours in a convert as part of those great puzzle I'm trying to say the problem why we create the standard they have has formation language it that digital packages they are disparate they have different data models the representation for transformations and the Scopes of transformations covered so that's why we don't say I choose one of the languages as the standard transformation replication and convert all other languages to this language so it's not a good selection okay so I was "
    },
    {
        "start": 829.86,
        "text": "showing example of how they are disparate so this is example of like two scripts spss scraped and still has great they are doing the same thing so basically with the input dataset they try to create a variable called party k1 by summing the three rules and then add a variable label to this numerical do some filtering and then store that in yourself so this two scripts they are doing the same thing but their syntax are different um so that's what we call the representation of the transformation are different in these languages and also as I have talked about the Scopes of transformation supporters are different like some other languages they support like deviated transformations as some others they don't and like integral for example in some of the sequel versions like before 2014 they have the pivot "
    },
    {
        "start": 890.4,
        "text": "function they don't have the pivot but after that they will have a support material function though this kind of like gold supported difference will make it hard for using one of our statistical averages as a standard representation that's why we've proposed a new representation of our own it's called a CTL standard data transformation language so we'll create a common syntax for the four cities or packages so this package our scripting in these languages they can be converted to our language at CTL and then as it here will be embedded into the metadata information it has the format in a like represent in JSON format so it can be easily reused by other computer applications but I won't talk like the details of a CTL it's too specific so I would just jump to the use "
    },
    {
        "start": 951.689,
        "text": "case of the system so then as we have the standard defense information language this workflow has been completed with this part we have a command script and parser will convert it to the transformation language at CTL and then the updater will update seti representation - you mean like executable engine yes now we don't have a execute or engine but we have a functionality to convert as VTL to one of those difficult languages of your choice so if you really want to process the data in one of the select languages we can do that [Music] "
    },
    {
        "start": 1022.99,
        "text": "so our claim that will ensure that they have before and after trust will make the process with the previous language and the new language they are functionally so basically the Google content will be a same so even though like the art from our past DTL and then actual are the commands used may be different but the result you get will be missing so I I won't talk too much details about like how we implement this but this is a general workflow so the first given the command script as the input will have a distance like a squid pasta as I talked about this great passer will adopt like we're past the original script into abstract syntax tree that maybe took a pair of science-oriented yeah yeah so "
    },
    {
        "start": 1084.24,
        "text": "basically it will convert it into SD TL and then we'll have a pseudo code generator that provides a more natural language like description of what they'll comment us so even though we don't understand one of the physical languages you will get the understanding of what this command is about and then using the XML updater will update the original match method a with a CTL into the new DDI file so it will work like a further converted to the codebook format as we have seen in the in a previous example the transfer of the were currently implementing spss stata start an art in the past three years we have completed the first three ones and we are working on our Python and sequel and the the XML update her the addition to a "
    },
    {
        "start": 1145.74,
        "text": "DDI format I have talked about we are also doing in for XML format the both of them are axonal pays the use tags to like identify medical information um so instead of the icpsr codebook format we're also targeting other code book format that is and their development so here I will show you example of how this process works this is a variable called well it refers to the dollar amount of this data aging it has in the sense of control today ways you can see it has this values as possible values and the values will have any labels or frequency with the frequency from the "
    },
    {
        "start": 1207.599,
        "text": "questionnaires and the percentage the balance or some conversion like transformations we have talked about it is converted to a categorical variable so this categorical will like this we form them into different labels into different groups and then recompute it like meditating information the color to do this categorical conversion it's like that this is in SPSS so busy it generates a new variable called Wow something and recode this variable by assigning but these values as one you have to add two different categories and then we convert into s DTL this is the s detail representation for this command "
    },
    {
        "start": 1269.119,
        "text": "so it's a command we call it compute the computer will have a target variable which is the output of the computer and expression will have arrabal name at the input of this computer and there's no condition for this computer and the commander will called recode so so first we'll have a creative variable colors and then we have a record transformation the this information will take the stores variable and the target variable and the record dates on these rules so it writes it either is like an internal representation of the transformation you don't have to like know or understand it but will convert it into this would a color representation the color will say this is the target variable we have created it is pipe the household income groups and then the derivation process "
    },
    {
        "start": 1331.29,
        "text": "of this variable will be first will accept a variable the name of the variable equal to the variable if the input variable and then we record the variables of this variable such that this values are recorded following these rules so it's easier for you to understand in this initial language like our structure as part of the code book the don't worry about a TTL just a look at it slow code and that's what we contribute to the description so then "
    },
    {
        "start": 1395.22,
        "text": "after that we'll update the racial metadata with our transformation information this is the part that involves our presentation or derivation so how we derivate derive this variable are included in this book and after that it can be converted to HTML format which is a code book format are you see I as if yes are so in a web page we will show something special oh yeah this is a reinforcement of that pipeline so now I will show you an example of actual process for this updater so this is "
    },
    {
        "start": 1469.27,
        "text": "this is after tool we have used to the prints link I show to link in the on the PowerPoint but the first one what you do that you upload your original metadata in gif format you also upload your script for transformation and it will return you the updated metadata but that one is like there's no interaction you can see the process so that's why we use the second link at the MT and a version it will show you how it works step by step the first are upload Lyon transformation script "
    },
    {
        "start": 1533.07,
        "text": "found with as high as that means it's returning SPSS language so it automatically identifies it retain ESPN says if it can tell which language is returning and it will ask you to identify yourself then so it first through the patterning it can like tell it has a one computer one willows one we called renamed several a one server value labels those are the commands existing scripts tt "
    },
    {
        "start": 1600.62,
        "text": "very is converted to s meteo in the JSON format as we have seen this little program the disinformation the program level information has ID'd at the source file name a third source language like Amelia Rose are there in this frame for some like method any information about the scripts itself and then those payment plans include like the transformation actually like personally including in the script they first have a load information at the lowest grade command we have a question I'm wearing okay the the script parser can handle custom and or language specific functions custom custom Elwood language we're getting a custom and or language specific function I am tell her I'm north and or Endor no I think that's not "
    },
    {
        "start": 1666.56,
        "text": "supported for now so forth commander we can't do we were just to show imagine are supported so it will not like like interrupt the conversion process but just for that script it was for that command it will show command support in then we have the cute right we call so those commands are arguing order and right that's original a CTL now we'll upload the original metadata yeah gif format "
    },
    {
        "start": 1727.83,
        "text": "you can feel like a additional managing information as part of automatic so this is a new it's a malformed magazine of the i-49 we have we're in a xml format and the deceleration information are yes yes so previous one "
    },
    {
        "start": 1794.24,
        "text": "they are the regional magazine of the original data and this part below are the transformation information for continued script and I updated a codebook will look like this it's a channel HTML page so those are the metadata format and the verbal descriptions before transformation and we're currently met verbal descriptions to those after transformation those are the variables now in the update everybody has said it also have the links to like how the variable the Convair will argue arrived from other "
    },
    {
        "start": 1854.83,
        "text": "variables you click one of those I'm not sure it's working now they should have this crowbar and jump to the part of not working this is not the online this is loco so that's all right other than that we provide some proven provenance tracking functionalities so I'm not sure how many of you are familiar with provenance one question before the timers and it said when converting to J's times the process also preserve the original script as well we don't preserve the original script but if you want to you can usually do that "
    },
    {
        "start": 1917.44,
        "text": "but here's the thing so for each command converted will have adjacent like property called original command so it will show the original command in like original language like SD is something but not in a like in a section of the code but each code each command have their own corresponding imagism so yeah we will provide this tracking functionalities for provenance information understanding the provenance basically like to give you an idea of how the data are derived the derivation process of the variable as I have talked about because this one is other maintenance it's not online so I just took some snapshots offered this tool it's a byproduct of our like the main so "
    },
    {
        "start": 1982.22,
        "text": "basically we upload this quick to this packing tool and then it's able to tell how we provide like the nose our data set around those are business eyes it's very brackets are the function the transformation functions the basically we upload for data sets and they are merge together through the pen they have set operation and this is a intermediate like they have said you can derive and then we do Menard is for listen to me hitting aside as well as a computation operation so arriving another data so it goes through this process like a tree like structure to basically tracking both variables or dataset are deriving this whole like a command to make a transformation process and clicking on like there's a videotape "
    },
    {
        "start": 2047.66,
        "text": "table tag and also a view operator attack the clicking on the round nose it will be able to see the content of the data itself intermediate data order returns it up and clicking on the operator here I'll show the original script in SPSS both this is a command and now a translation from SPSS to a CTL this is a corresponding transformation the translation this is a little table as we have clicking on like it has a note it will show the current content of the data set and some other like a functionality we have provider like scored deter the user can't get a better understanding of with our actually in this current stabbed there is that in "
    },
    {
        "start": 2110.75,
        "text": "other than that we have the tracking function so an interesting question like you has been studying in computer science is why and why not our problem the why is like why they still appeared in the current posture and why not it that why this table doesn't appear in the car is that so it will help you to debug whether you have written a correct like transformation command or like help you like track how you derive the tube home in this old transformations process those we enable that functionality we have a tracking of like function provided here the first you select that if we want to track and also select the table it's a long wait you want to track that you home um then the tracking will give you like how the current like this terrible our processor along this like a transformation this is a mean table here "
    },
    {
        "start": 2174.32,
        "text": "and we track after this information it appears here and we then track this intermediate table after another yeah so those are basically what we have done so far and I have a talk about as I have talked about we are still under development so some of the functionalities are not fully available but if you want to try out the updater from a tool you can try it with SPS and says those functionalities are available now yep hey any questions yeah I don't talk much about like the technical difficulties we have encountered in the process of converting between languages because well if you really look into that you will see many of the functions "
    },
    {
        "start": 2234.74,
        "text": "even though they look the same like addition that's a simple function but different languages do it differently and also for example SPSS will treat missing values as negative infinity but estadual treated as R to infinity and some others are they will treat it as zero so you have to deal with all koala cases like that to make sure that before and after transfer conversion you get the same like data set so that's what we expected to make sure that the melody the documentation is accurate well we don't like where another processor as we talked about we don't have an engine or processing so we're just a translate whatever even in two hours then boundary over that well no we "
    },
    {
        "start": 2307.89,
        "text": "want to do that so basically the original script we just translate with the original pretty right so if the original script has error in it we will not we can we are not able to tell there's arrow it was being and the next beam so we mean sort of the cycling pool yeah the cousins were able to tell that so it's different to update her tool it doesn't involve the data but the truck in school it involves if they have actually the tracking to is more compact than the operators work but right as you have said in the hour in the in the tracking tool usually in content arrow where witches in reported [Music] "
    }
]