[
    {
        "start": 2.06,
        "text": "it's Python there is no actually "
    },
    {
        "start": 72.689,
        "text": "probably it's only simply a class you can always step out of the class a few minutes early while the rest of the constants in there versus if I start 2 minutes late everybody misses that 10 minutes so I think overall it old sin durability while still giving students a flexibility unless you have that professor that puts these you know quiz and the last five minutes of class and that would just suck but I guess you could do that now and have the quiz in the first go-around sign in as always it helps us with beacons up so I'm pleased to present today's speaker we have fan Jang who is a student and DC M&B and is inhuman conference hello everybody today I'm going to present one of our recent work very five MIT two point no ancestry agnostic detection and "
    },
    {
        "start": 133.42,
        "text": "estimation of sample contamination from ding a sequence data so comfortable okay so many of you must have heard of this tool many of you haven't so it is a soft well that was designed to detect and estimating a sample condemnation from sequence data so far we have already received a lot of usage report and according to our hope phone home status it has been ran more than 2.6 million times across 25,000 different IP addresses and our our wiki pages were so accessed more than 27,000 times and this tool has been adopted to be a part of many standard a sequence QC pipeline and "
    },
    {
        "start": 193.75,
        "text": "a sting using right now in like tom map project and CC DG and it were so was used by many major sequencing center including broad Sanger and so on so let's first explain what the problem exactly is so when sequencing doing a sample normally we will see we will categorize into this four different configuration base down if we are starting human DNA and if the if it is non human DNA and considering the symbol you are of interest and the sample and the ding accountant that is trying to contaminate in your sample we will have these different configurations and normally we will focus on human being a study related part and so for and then "
    },
    {
        "start": 258.01,
        "text": "later you can basically divided this into two categories one is across species and the other is within species and most of the time the cross wishes will be easier to detect because you can search your sequence try to match it with candidate database and you will find out those alien DNA sequences and one of the common scenario is this like in 2011 11 there were several reports that many non human being a genome database was found to be contaminated by human DNA and of course this should not be a single case and and they should it's not surprising that there is were so excessive contamination happened to be within human being--a genomes so today we we will only focus on this other problem "
    },
    {
        "start": 322.45,
        "text": "that to detect the contamination level between human tas and one of the possible explanation is that the introduced the human gene contamination could be lead by could be introduced by human activity because all those experiments was carried out by human and those condemnation could be like pre-existing in your spaceman or could be introduced while hiding a padding and also could happening shipment like two little dry ice and your sample is melted and or so it could be introduced when you do the fragment size selection and we're so sometimes you will come across unexpected new or weird weirder weird protocol that will give you unexpected "
    },
    {
        "start": 385.0,
        "text": "at that turn and even could be this could be happen that sometimes you will have incorrectly the barcode for your foot when you try to sequence multiple samples in your single run and and we're so it could happen after all these things you manage to survive but at the end of their you may experience some problem in silico demultiplexing alignment or using around those problems are our problems we met in real practice and why we need to so the those problems introduced in this complicated process explains a little bit of why we need to pay attention to this problem and sequencing is not it's a much more "
    },
    {
        "start": 447.37,
        "text": "complicated procedure comparing by comparing with array based genotyping and so this method is specifically designed for the sequence data and we're so talking from the usage report we have received people really care about this problem and the holy people don't brag about it because you know generated ninety rings out and if you haven't heard of it and we're so most of the time secrecy project will tend to be marked with cost of more money and time and if you can you can detect this problem before you're doing downstream analysis it will save you a lot of money and time so let's look at this problem in a ideal theoretical way so suppose now we are looking at Jones DNA and each "
    },
    {
        "start": 512.14,
        "text": "row here is a reads from his genome and the first row is a reference genome and this is a snapshot after alignment and you can see the highlighted column is a place where we will call the children to be a a and it is different it is different from the reference you know and everything is fine and sometimes you will encounter some hydros I go side and you will see most of times you will be like a half a half because the diploid he nature you know and these were so fine that if those two germs of sequence and you didn't mix them up and you can tell very easily and here for example we got GG and the CC for James Dean a and the TP and AC for Jose D again "
    },
    {
        "start": 572.49,
        "text": "however if contamination happens then the genotype cannot be confidently chord and here you will get two heterozygotes and the view at the ratio here is not even close to 50% so to model this problem we here is the assumptions we need to do first our goal is to estimate the contamination level we say our bahir and we were soon adding a simple that may be contaminated by at most one not one a dirtying a sample and we were assumed population allele frequency of common sniffs unknown and there we assumed independence across different reasoned different sites and the "
    },
    {
        "start": 633.61,
        "text": "approach we take is to use mixture model to calculate the probability the civil series and allowing further contamination specifically the notation here we provide this so do you know the beers the beta for the specific site specifically read and the you know e to be the error genotyping error of sequencing error alligator and G 1 is true genotype for the internet sandwich - is that true - depalo contaminating sample and are about to be a contamination proportion so the probability of observing a certain based certain base giving this settings can be modeled as a linear combination of the two of the two probability that is observing certain base underlying truth "
    },
    {
        "start": 698.08,
        "text": "okay so do like you who function for this problem will can be formulated this way so this is the probability of certain base pair we just saw previously and then we will marginalize out all the possible citizen error state and then we because we assume the independence across all the race at a certain site we multiply the promoter for all the basics in certain sign and then further because we are considering two genomes now we will submit something over all the I know impossible but we know two genotypes and the prior probability of each of this to genotype can be modeled as the binomial can be model as modeled "
    },
    {
        "start": 763.03,
        "text": "by the allele frequency because the Jill had follows a binomial distribution with low frequency as the per meter and if we assume it's sorry Wenberg equilibrium and p here is a fresh skill pair based quality which we can get from the sequencing data and then consider we have M different science across the genome then because we assume is independent so we can multiply them together then maximize this like to function ideally we will get a let's make estimation of the academician harfa question so you just said that you can for that location because you're assuming that they're dependent across multiple sites but if there's contamination when you expect perhaps individual haplotypes to not be independent because the variation would be would be shared almost haplotypes in "
    },
    {
        "start": 823.87,
        "text": "that contamination would thus be shared so they would be yeah they would some correlate with each other so in real practice we pre-selected those markers so basically we have so first of all I can we share away selected markers from the hem map and tend to restricted edges adjacent distance and the word so like we mask out the certain regions and different certain allele frequency and certain like meirin type like adjacent india or something so basically we are choosing a subset of those all possible you're basically already subsetting by haplotype and then you're treating the haplotypes a sentence yes make sense and the prep this is the illustration of the previous way in verify parity one so in "
    },
    {
        "start": 889.11,
        "text": "originally did it kind of give you the option to in cover it with the original health because we usually treated original hem as a much more competent in the information so treaty that's a huge effect and now here we get rid of this information and just to use sequence reads and earlier frequency so to evaluate the performance we constructed some silicon contamination and here we selected individuals from european population and mixing them up with proportion from 1% to 20% and the in total we get we got 98 occasionally contaminated samples with an average security gaps around 4 X and here is a figure that the x-axis Eastern DDR pop and that white X "
    },
    {
        "start": 953.269,
        "text": "exceeds estimated Afra you can see most of those ladies centered around the diagonal which means the accuracy is good well valleys or that is the work we have done there pipe mid-1 and but there as we just saw there are some strong assumptions one of them is that we assume that we know the correct allele Pryor frequency but this is not always true like for example when we clever example and itself identified ethnicity is not correct or when popular when the ethnicity the sample claim is now in your database for example in Canada we we don't have a representative population information so far I guess and or even the Assemblies and mixed "
    },
    {
        "start": 1018.16,
        "text": "it's not that as as clear as we saw in like such a genome so we will need to take care of this problem so here is a figure that are showing even we meet specified frequency what will happen again the x-axis is internally recombination level and the y-axis is the ratio between estimated and internal contamination level so the red dotted line will be the optimal solution and the anywhere away from it is a deviation so the red bar is for every kind of in frequency green is for you for Asia East Asia an early pregnancy ended the European early pregnancy and the purple is the pool of improv in C like we don't we calculate early previously we don't care about where those sample "
    },
    {
        "start": 1079.78,
        "text": "come from and here are two similar sets one is the finish which we treated as a European sample sample set and all of them are contamination in silico contaminated with another sample from the same population and same goes for a similar scene is which is a beacon sample so we can see that when we specify the correct audio frequency like European here and Africa here the estimation will be closer to the ideal solution comparing to specifying the wrong loop frequency so the problem is in now that the problem is in the audio frequency estimation now we will take a step back to look at how we exactly calculated our producing so first of all "
    },
    {
        "start": 1143.47,
        "text": "in a homogeneous population we usually like calculate the or allele frequency this way so G is the genotype normally we'll take value 0 1 2 and the divided by a total number of chromosome in its population and sometimes when your host symbol is is currently structured by which I mean like in thousand genome again you will have a clear classification with African people European people and Asian people in that case you can calculate allele frequency by age group and in general that you can represent your low frequency with weighted summation across all the individual's genotype so for the homogeneous population the weight is actually just 1 over 2n and for the discredited "
    },
    {
        "start": 1205.48,
        "text": "structure of population it is just indicator function and you develop devalue to by the total number of chromosome in that specific population how about that this represented this representation doesn't include like continuously structure the population so but this problem about this representation continuously representation of each sample is not new we have always seen it like in the PCA PC components based method already like here and in 2016 there are several papers introduce the idea of a individual specific allele frequency so basically the idea is that "
    },
    {
        "start": 1265.69,
        "text": "we use PC as a as a the mole of the allele frequency as a function of the pieces that you will assign to this specific sample so what we do now is that we take the reference handle for example the human genome diversity panel or southern genome project and the way pre calculate the PC or SVD of this reference panel then we can use the pre calculated the projection matrix to to model our specific sample that you are interested so here is the detailed formula so here the G's and genotype matrix of the reference panel and we Center it and then we can do SVD analysis on it and then we take like take the first okay principal components "
    },
    {
        "start": 1328.74,
        "text": "now the first K will be approximated to the original original genotype matrix and then we can use this approximation to to calculate our genotype other influences so yeah so here this G's estimated it June type and this is the one you observed your sample from me which means that it could be a this could be a continuous value but this one is always a zero one two and then now look who can our method we talked about before ideally if there is no condemnation our wending cover it weighs the PCs we will only estimate the "
    },
    {
        "start": 1390.46,
        "text": "PC as per meter in this likelihood function that is so ideally either will generate a TC coordinates that will coincide with other related assessment method method so we compared it with trace trace is a method that we used often to estimate PC coordinates of each individual so on the left is full of trace and on the right is for the verify pmid those points are those great points are HTTP points but this colored points are southern G points so you can see generally speaking those two figures are almost identical which means that our PC coordinates estimation is accurate and this is for the inside samples those are African American African samples so you "
    },
    {
        "start": 1451.33,
        "text": "can see previously here blue is mostly European sample and yellow is everything sample so American African spread across this line which I think him represent his genome content so now that we show that the PC the PC coordinates as a I would say a bridge bridge between the individual Lea frequency and our likelihood model is a successful case now will we will try to make it harder to introduce contamination level into this into this so then we continue to construct the 20 artificial samples but this time those samples are not mixed with individual from its own population rather it was "
    },
    {
        "start": 1513.01,
        "text": "mixed by individual from African samples so you can see web 10 seed shared symbols mix with 10 African samples 10 CU samples mixed up by 10 where I samples and the mixing level is still ranging from 1 percent to 20 percent and the way so every can be a minor portion so now here is the a PC estimation a PC coordinator estimation in this new setting like now our symbols is our symbols are contaminated and you can see those symbols are drifting from it's supposed to be ordinate towards the African origin or point and moreover if you look at those color with more heavily contaminated for example the purple color here you "
    },
    {
        "start": 1574.04,
        "text": "will be tracked much more towards the African points so it means the condemnation indeed affect our PC coordinated estimation and now this bring up the question which comes first to be PC convolution with PC will will be essential for your contamination estimation and also your condemnation will bad to a PC according to destination so the only ways to incorporate this to parameters in the same iku the framework so this is the the formula above is the one without condemnation so here the probability of observing certain places there's not depending on the Arve here it depends on our fam and "
    },
    {
        "start": 1634.42,
        "text": "we were so think that now that we are considering the PC origin a nation we would say we will consider if there are two genomes that Eve if they are coming from the same background and possibly it will do some damage to our estimation so here in the waving ancestry model we assume those two samples are coming from the same background alright for the between ancestry model we assume those two are coming from the different background and now in total we will have like 1 2 3 parameters need to be estimated together so this figure shows that even we consider the contamination level into our like who the model it will correct the PC coordinated estimation so these three panels are the "
    },
    {
        "start": 1696.62,
        "text": "first one is withing ancestor mode one is withing ancestry model but I know the first one is one will you specified earlier frequency the second one is out will you we'll assume there is no condemnation the second one is that you use a within ancestry model but you assume that there is condemnation and the third one is you use between ancestor model and assume the earliest condemnation and do those as a symbol we used previously that are contaminated by the African sample we can see that the Driftless now are gradually corrected even if our models are becoming more complex complicated and it is what we expected and now that "
    },
    {
        "start": 1762.45,
        "text": "PC estimation are corrected we would think that we would ask if our contamination level estimation is also corrected so we constructed those samples further for with the ancestry condemnation we randomly choose 10 pairs of individuals in each of this four population and the way contaminated materials each other with England within each population respectively and we're so when considering between ancestry contamination the intended sample was contaminated from samples from another population for example the configurations here we are using this CHS we see where RI or GPR was Mexico and so well the mixing Propulsion steel is running from 1 percent to 20 percent "
    },
    {
        "start": 1822.479,
        "text": "in total we collected 400 artificially contaminated samples sue is secreting depths of around 4 X so now these three panels demonstrate the result for the way withing and says waiting accessory condemnation and still apart from the allele frequency pre-specified allele frequency here we also introduced two additional bars which are the wheezing and sensory model and between ancestry model so notice that all the data sets here are actually withing ancestry mix contamination so as we expected within ancestor model and between and session model basically have the similar performance here however if those data sets are between "
    },
    {
        "start": 1882.93,
        "text": "ancestry contamination for example here CHS is contaminated by where I Mexico is animated by CHS where I is coming back PBR then the between ancestry model will always be better than the weighting and such model since we introduced them all degree of freedom minute and basically these two result proves our or method and shows that you can simultaneously correct the PC coordinates estimation and the worst of the contamination level estimation and here are some accommodation of notes we maximize those mac-like function using their the neither made method and the between SS "
    },
    {
        "start": 1945.33,
        "text": "model because of its complexity like the in D for the way we calculated up to for PC components for each individual in that case we will have like nine parameters in the model so it sometimes it's hard to converge so here we use the we first use the wheezing ancestor model to optimize it which will give a decent start point and then way use those estimation and start point for between SS model and the next way we have a very nice convergence rate generally and currently implication consumes around 500 mega base regardless we are using 10k or 100k variants and it took 16 minutes and 120 minutes hmm for "
    },
    {
        "start": 2005.81,
        "text": "even individual and it supports both spam for my format and their creme format and so in summary our sequence contamination method is a simple mixer model base method to allow us to detect and estimate a contamination and the accurate the condemnation in as we enable more accurate you don't having for tip genome data in a joint estimation with genetic ancestry increases robustness against population and agility and the method the were so works well for detecting RNC core epigenetics and post animation I've likes to say thank my advisor Hamming Kong and Mathieu and the goo and Gasol and Mike thanks so it's great this is "
    },
    {
        "start": 2066.77,
        "text": "actually really cool one of the I mean I don't have to say that obviously all the users using it agree with me one one potential I don't know maybe you can tell me because I don't really know it seems intuitive that based off the types of contamination you describe that collecting family samples might be you know we're collecting trios or pedigree that might be a case where you're accidentally mixing samples or contaminating from other family members I imagine those would be a lot harder to piece apart due to the relatedness have you looked at that at all and how well does with this would you imagine this would work we we have to look at with the family combination here that's too close it seems like that we think this history is managed a stream seems like yeah but they within ancestry seems like that would be difficult enough but yeah but however we have another modification or extension to this model we will call it verify onyx is seeing developing and actually it will now so far I've got the "
    },
    {
        "start": 2128.059,
        "text": "data and you can most of time could successfully distinguish between the trails or parents or cousins and basically those written close they're related in relationship yeah so based on that we can further distinguish oh thank you so it's a strong something here is that your termination from one person yes I do you have so I think to extend that to Margo Sango is straightforward but there are two problems one is that you were introduced we much more parameters it's hard to convert second is that when there are multiple samples to effectively information for the like the third or it may be the second sample "
    },
    {
        "start": 2189.289,
        "text": "will not be sufficient to detect so basically is hard to distinguish if they're the contaminating parties from one sample to sample okay I guess the thing is that that kind of cases we don't really care oh oh well sorry for certain studies we don't really care about like a who are the contaminations right yeah the most important thing I want to know is like how much is thin content yes like that racial and I I don't even care about the ratio for individual contaminator so I only care about the ratio of the main sample line or the major components of I guess do you see so in my opinion as long as you don't care the rest of the terminating part of the contributing part that carefully it might not "
    },
    {
        "start": 2249.69,
        "text": "necessarily improve the accuracy of your condemnation level estimation like for example if you have 4x and only one base pair you observed and you assume there are two samples how can you distinguish them apart right so I'm guessing or some dag underlying it through a sigma grab station model so it's the only parameter inferring Kappa fraction yes the only unknown parameter the PC coordinates and the PC learns okay oh are you inferring the parameters are using expectation magnitude here in your model we tried but it's hard to derive the clothes for it so how I just use them they don't meet ok and how are you assessing accuracy besides simulation uh parents "
    },
    {
        "start": 2312.54,
        "text": "really we don't know that except for delivery here so that's always when the trickier parts with like yeah you just say accuracy but you're really only assessing it with data simulated when you already know the answer yes so then you have a known knowns unknown unknowns yeah of course that's a general question but being you know our simulation procedure we didn't have like really that much relax for a strict cut straight down it's a natural mixing thing can you think of a natural way to design an experiment not a simulation but it will a biological experiment or you can already know the answer and airtight and assess your models so this thing of a hypothetical experiment so actually our data is already very high "
    },
    {
        "start": 2375.569,
        "text": "level it's not from like a like a marker wise or base favorites it's just basically mixture makes up to two dataset so that is so far that is the way most obvious way to do it I think yeah I'm just asking you through way you can do that instead of simulating a computer do that and I will do the same thing in a what lab experience there's that there's always a difference between theory and practice theory theory is the same as practice but well we want to introduce kinda machine well that if you want to badger it before you mix that then your sample is consumed right and will you mixed up how can you like you don't have a wet lab way to to valuing this beforehand I assume I think you can tighter we actually think Jeff kids group we did this looking has a way to measure "
    },
    {
        "start": 2436.97,
        "text": "somatic mutations right using known genotypes we took some different cell lines and just tighter them together at certain percentages we might be probably sure but I don't know if we did a WGS or just the main tears on that but if we have it I'd be good good way to look at it in a physically mixed sample as opposed to a synthetically an example that's where we need export the UN that I am NOT that but you know the other thing is that so following the Daniels question we may apply withing ancestry modern real data do you see no matter the one that you show is from simulated that I'm talking about like real-life sequencing data there's that would be the case there is for ease and that's going to ask is like for most comedy like comedy producer "
    },
    {
        "start": 2498.81,
        "text": "yeah I we use this method on that do that and the behavior is similar to this finger so basically waiting ancestry normally will try to inflate a little bit we only tried Forex where the bottom line where we can go okay then thank you guys ready [Music] "
    }
]