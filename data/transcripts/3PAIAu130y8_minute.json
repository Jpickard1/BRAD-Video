[
    {
        "start": 47.5,
        "text": "hey welcome could trout good to see everybody this nice kind of late summer day I'm of the camp that believes we have a few more weeks of summer joining the calendar oh this is a really exciting part of our seminar series that was started last year which is the emerging topic seminars and I put a lot of fun and it allows us to engage new faculty from around the campus and we've got some exciting ones that Brian will mention here in a minute you know the most exciting thing for me as chair of the department is the students run it fantastic and you know they fix things are interested in they find people that they're interested in and it brings me blood into the organization and new ideas this particular topic is dub you know it's an emerging topic but maybe like infectious diseases as it merges andrea merges i mean it's been around for a while you know rorke uuk here who you know I remember when I was a grad student we used to work together you know you know I heard that he said these are common pitfalls but we don't know that yet "
    },
    {
        "start": 108.4,
        "text": "because we don't know what pitfalls were talking about but there's no no doubt that data analytics is on everybody's mind today you probably noticed that the university announced a hundred million dollar data initiative you know on the 6th of October we've got a launch of that initiative down at the Rackham auditorium if you haven't signed up do so we have 1,100 people did did sign up it's going to be a big event a big day be part of that and be part of this you focus on data analysis you know pitfalls and opportunities and I think I've said enough and we're glad that you're here and let's have some good socks and some good discussion Brian right it's a it's great to see a really big crowd here you know I thought that it was an interesting topic and I guess that a lot of people agreed I warned the speakers you know to expect about 30 to 40 people and I guess that I kind of underestimated so as dr. AC had mentioned you know we chose this because this is kind of a reimbursing topic sure "
    },
    {
        "start": 170.14,
        "text": "you know data analysis has always been an issue but with the rise of bioinformatics and big data and with the lack of quantitative training in a lot of areas of biology we thought this is really kind of a press issue that's going on right now so without further ado I'm going to go ahead and introduce our first speaker it's going to be dr. Daniel Almirall so dr. Almirall earned his PhD in statistics here at the University of Michigan and he now works at the Institute for Social Research he works in adaptive intervention for for medical treatments and he specifically focuses on a child and adolescence I'm sorry mental health and substance abuse and so easier to talk to you about a method that's developed being developed at the ISR thank you all right can everybody hear me okay I'm really happy to be here I'm kind of an oddball in the sense that I'm I'm not part of your community necessarily not yet I'm over at the ISR so and actually today I won't be talking about pitfalls per say about pitfalls of data analysis because much of my talk is about design "
    },
    {
        "start": 230.92,
        "text": "design of experiment so let me just get right to it I have 15 minutes and I have 12 slides and I hope to get through all of them in 15 minutes so the very first thing I want to talk about is why do we need sequences of treatments and interventions okay intervention science what is a dynamic treatment regimen I'm going to define that for you there are some unanswered questions when we're thinking about dynamic treatment regimens I'm going to talk about I have one single slide of potentially this is MIT is maybe the only slide that fits under the pitfalls idea the single only type as a statistician as you guys know systems we try not to be dogmatic and say you ought to do things this way so I didn't want to make the talk about pitfalls and then I then I talked about a potential solution this is really the exciting part something called smarts and I'll summarize this and take home points ok sequences of treatment are often necessary what am I talking about so let's think about any disorder okay diabetes autism HIV the areas I work in substance use and Child and Adolescent Mental Health and all of these areas most of most of the diseases "
    },
    {
        "start": 291.31,
        "text": "we're talking about are chronic and because they're chronic the disorder is waxing and waning because the disorders waxing and waning and probably stays with you for a very long time there's no sense in which we think of one treatments fixes it rather we think of we manage the disorder over time and when you're thinking about managing a disorder over time you think about sequencing the treatment I might give you you might one medication for two years and that might have to change and so on and so forth so here I try to flesh that out a little bit because of the waxing and waning course there's multiplied for example in substance use sobriety's early on is often marred by relapse and multiple wounds up so what are we doing those situations of relapse okay life events comorbidities arise non-adherent you may be adherent to a certain medication for a short period of time but if you become non-adherent treatment strategy probably ought to change if you're not taking that medication you get the idea so there's disorders for which there's no widely effective treatment in those settings you might sift through various "
    },
    {
        "start": 352.36,
        "text": "treatments until you find one that works and then there's disorders for which there are widely effective treatments but they are costly and burdensome so in those settings you might hold out the very effective flagship treatment until someone really needs it okay all of these are settings in which require sequences of treatments is everybody with me and these are all settings in which we have high heterogeneity in response between both within person but which terminal doesn't work for me later and between persons what works for me doesn't necessarily work for Brian everybody with me okay that's my basically my lead-in slide now what is a dynamic treatment regimen raise your hand if you have already heard of dynamic treatment regiments I just want to get it at a steer really high be plowed okay good so I'm glad I'm giving this talk all right all right so here's a definition it's a bit of a mouthful a dynamic treatment regimen is a sequence of individually tailored decisions decision rules that specify whether how or when and based on which measures that that part is key right there to alter the dosage dosage means many things duration intensity frequency of "
    },
    {
        "start": 413.68,
        "text": "treatment during the course of care if you read this again and again and again you'll realize hey that sounds just like clinical practice in fact the dynamic treatment regime the whole goal of it is to help guide the type of sequential treatment decision-making that doctors clinicians social workers make in their clinical practice that is in fact the whole goal we want to create a guide let me give you a concrete example this is a very simple example by the way because this is an introductory talk okay so concrete example this is in child ADHD children in schools with ADHD who ages 6 to 12 okay this is very simple adaptive dynamic treatment regimen by the way we call them different things we call them treatment algorithms we call them adaptive interventions they go by many different names so here's one a simple one I might start I have a childhood at HD he's in the school setting I might start with Ritalin we all know about ritalin for ADHD right if at any point in the school year once a month the teacher might evaluate this child for a response on response say based on two measures say okay "
    },
    {
        "start": 474.34,
        "text": "and in fact I can tell you what the two measures are every single month the teacher evaluates this child for how they're responding to treatment everybody with me the moment a child is identified as a non responder so this could happen at any month during the school year okay the moment a child is identified as a non responder they transition to an Augmented strategy where I add B mod behavioral modification so here I'm working with the families I'm working with the child they're coming in for sessions and we're doing basically a cycle therapeutic approach in addition to the medication everybody with me if if it isn't broke don't fix it right that's what the same size so as long as the child's responding they continue on that this is an example a simple example I'll be as simple of an a dynamic treatment regimen everybody with me and I can think of many more complicated examples maybe many of you in this room are probably your wheels are already spinning for example the decision to do medication versus something else I'm not showing that here might be based on biological markers right everybody with me it might be mace on your neural neural markers "
    },
    {
        "start": 534.43,
        "text": "genetics right maybe the decision between medication a versus medication B in addition even among non responders the decision to do augmentation might differ and the treatments might differ like maybe instead of augmenting I can just intensify the dose of the medication right okay so your wheels are probably spinning about all the other ways we can embellish this dynamic treatment regimen everybody with me and that's good that's what I want you to do so in general we could also individualize treatment using baseline factors I talked about that I don't do that in this example there are their baseline time changing biological factors I don't do that in this example but you can okay I thought that would interest you DTRS dynamics emissions are are an important part of this new move toward personalized medicine precision medicine in a sense okay because they help individualized treatments overtime for the patient everybody with me okay okay so you drank the kool-aid right at this point I'm hoping you drank the kool-aid so dynamic stream regimes are way great way cool "
    },
    {
        "start": 594.829,
        "text": "but there are so many unanswered questions what do I mean by that when the expert the docs the clinicians of social workers psychologists sit around the room and they think well how exactly odd I build a good dynamic Sir Eugene they have lots of questions that's what I'm going to talk about next unanswered questions when building a dynamic room let me give you a concrete example in the context of this child ADHD example so medication and behavioral modifications they've been shown to be implications they've been around for very long time so we already have the randomized trials that say these things work everybody with me however there's much debate in this literature on whether first-line interventions should be pharmacological or behavioral you can imagine this debate and the debate grows stronger the younger the child is as you can imagine right there's certain guilds and certain people that say we shouldn't be medicating the younger children and there's certain people that say well it's safe we have our safety data it's perfectly fine further even even if you've settled this there's a need for a "
    },
    {
        "start": 656.209,
        "text": "rescue treatment if the first treatment doesn't go so well and we know from the data even though these are two evidence-based treatments anywhere between 20 and 50 percent of children don't improve so it's kind of like even if you decide you've decided what treatment to give first we got to decide what to do when you're non-responder ready with me therefore some important questions in this setting for clinic practice include what treatment do I begin with and one Hmong non-responders of which there are many what second treatments best is everybody with me okay yes yeah that's a great question that's a good question so and maybe the use of the word rescue rescue here is not a good idea the idea is the child's not doing good on medication well I need to do something about it what is it that I do do I and actually you've nailed it on the head do I meant the dose do I switch to something else and abandon it do i augment the medication with behavioral "
    },
    {
        "start": 716.749,
        "text": "modification you nailed it on the head what is it that we ought to do that's the second part right here okay how would I have tackled this problem if I was not thinking very hard okay this is my only pitfalls my only pitfalls by only through the singer because the title of the of the of the colloquia okay I only decided so I would have and indeed I did actually okay so let me tell you what I would have done I would have pieced together analyses from multiple RCTs what do I mean by that I'll take data from one randomized trial that start to answer do i do medication or do i do Bheema everybody with me and I'll get an answer from that randomized trial suppose I get medication is better everybody with me all right then I'm going to do another study another data analysis where I'm going to choose the best second line treatment on the basis of a second RCT so in this second study I gave all the little kids medication because that's what I found with Gus in the first study and then I asked the question among those that were non-responders what do I do that next and then I answer that questions like I pieced together the analyses from two "
    },
    {
        "start": 778.009,
        "text": "separate studies from two separate analyses that's what I would have done and maybe some of you would do the same thing what are the concerns here and I'm not going to have a lot of time to get into it but just think about this for a second there's this notion of delayed therapeutic effect thinks Bellman's principle of optimality for those of you that that are more inclined to think that way just because the treatment works myopically now does not mean that long-term that's the thing I should have done so what I did when I piece together these two studies is I chose the best first-line treatment on the basis that first study and then I moved on but that decision was a myopic one I didn't see what happened to the child under subsequent treatments or what would happen if the child didn't respond everybody see that that's the big idea and I don't want to call this a pitfall because sometimes the piecing together might work just fine but in this if there are delays therapeutic effects to the extent that those exist and we believe they do in intervention science you might run into trouble everybody with me ok so what's another approach and I did a lot of this this was my PhD was on observational "
    },
    {
        "start": 839.48,
        "text": "comparisons you know and causal inference so for example I might take data from a big longitudinal randomized clinical trial and I happen to observe that some children get a higher dose and I happen to observe that some children get augmented and switch medication and then I try to make something of the observational data there's nothing wrong with that I did that for a long time but there are the typical problems associated with observational studies right the confounding issues and so on and we can have lectures and lectures on that idea so quickly we come to realize that we don't have the right data let alone I have five minutes great let alone let alone analyses everybody with me enter smart okay so that's what I'm gonna talk about next sequential multiple Simon randomized trial this represents a potentially better approach so what is a smart it's nothing more than a multi-stage trial okay for the same participant run through the trial all through the whole trial and the trial is multiple stages of treatment here's an example back to the back to the ADHD study I'll take say I take one hundred and fifty three children I randomize them to med versus D mod why "
    },
    {
        "start": 900.35,
        "text": "did I do that because that answers the question what do I start with once the child is a non responder at any month irie randomized this is the cool idea right here i reran demised the child to augment with the the treatment they didn't get or intensify the initial treatment so if i intensified emod i add a session during the week i might add a saturday session if i intensify the dose I might actually up the dose or give an afternoon dose so everybody see me this is an example of a smart okay so a smart address is in one study those two questions that I talked about earlier and it lets me get around that problem of the optimality problem in fact data that arises from the smart or let me understand which of these sequences is that so if you think about it embedded inside of the smart are going to be four different sequences four different dynamic treatment regimes start with med intensify or augment start with v mod intensify augment everybody see that that's the data that arises from a site and then I'll learn which of these four "
    },
    {
        "start": 960.949,
        "text": "strategies does let me just show you the only day analysis slide and I'm going to show you the only results side so here's an actual analysis of this study okay so here are the four regimes it turns out both regimes that start off on medication look pretty much the same but take a look at what happens when I start off on vemma things don't look so good early on however however over time starting off on B mod has greater gains you see that and in fact we didn't see great differences here on this outcome yeah go ahead you have a question this is real data this is in fact real data okay so this there's a model on top of this okay so there's a piecewise linear model that's sitting on top of it and there's a piecewise quadratic mode I have exactly one minute left all right but what's that what's the message here the message here is that the kind of data that arises from the study lets you understand the trajectory under these fours and in low and behold we have we learn that medication is great myopic aliy but it might not be great in the long term everybody see that okay take-home points and then I'm going "
    },
    {
        "start": 1021.52,
        "text": "to wrap up alright so dynamic treat Marines you learned what they are they helped us individualize treatment both up front and throughout that's the big idea that's the thing we want that's the thing doctors want clinicians want smarts can be used to build better dynamic treatment regime I gave you an introduction to that idea we'll skip this but smarts are not adaptive trial designs are confusing there's a confusing terminology in the literature okay smart aim to develop adaptive treatments but they're not a depature and a major theme in this intervention science is replication I'm going to end with this idea Brian wanted me to talk about this if you think about it the whole goal here is to is to try to be able to replicate what the sequences of treatments are and to understand how to best do them so that we can build our science about replicating about sequencing treatments over time okay I'm going to end there thank you very much [Applause] "
    },
    {
        "start": 1082.68,
        "text": "so I don't want to delay before the next speaker I just want to mention that in order to keep things on time if we could hold questions until the end we'll have a question and answer panel yeah so our next speaker is dr. Jim Lee dr. Lee is an associate professor in the chair for research and the Center for computational medicine and bioinformatics as well as an associate professor in human genetics in the Center for statistical genetics and the Comprehensive Cancer Center has research focuses on the basis of complex human disease using genomic approaches I make the slide intended for this to be the first talk of the three so I had more introductory material and also aimed to be covering a wider range of topics to try to fit the ambitious title so I feel Brian really hit a very timely topic there you know the reasons I can think of there is currently ongoing urgent "
    },
    {
        "start": 1144.69,
        "text": "type conversation about research participatory it has been odd topic but it just has always been quite fresh and quite urgent and now we also have a great deal of challenge in dealing with big data and to example well I know genomics but I heard of think they have rushing in from a human interaction from social media and of course in medical school environment we constantly worry about how to train train the next generation of automatic students and in the medics coop we also hope by now we don't do much we hope to have continued education of practicing doctors and nurses and those who really practice medicine but hope they have some about better practice for data analysis and it was a difficult topic because it's a very easy to fall into an hour-long tirade about a bad behavior and and also "
    },
    {
        "start": 1208.17,
        "text": "the perfect very often you run into people you think who is less stringent less careful less stringent than you but conversely others may see you as being overly stringent and paralyzing a good data discovery process another thing that makes this difficult is I started checking on what has been said regarding pitfalls turns out much have been said so I challenge for all three of us to say something fresh so I want to start out with my own perspective these are for our published studies that involve some kind of a peaceful we we had two studies where at inner age of candidate you know Association we found the widely reported biologically reasonable genes actually do not or fail to replicate in three important association with article then we found you know in retrospect it's quite trivial or when you're cases of "
    },
    {
        "start": 1270.0,
        "text": "controls have different inequality RNA quality you end up having a major discovery that turned out to be a brain pH defect so that's all then the TCGA I had the first paper on GBM and we continue to feel the very discovery of subtypes are overstated so we including the my lab and including a student in the bio informatics program have tried to reshape the narrative about GBM classification later on involving another bio informatics student we look at the methodology and how it's being applied in terms of classifying samples so that's my own direct encounter indirectly okay just very generally almost every day you read papers you wonder about whether they have been completely boring or halfway in a pitfall of course the review in "
    },
    {
        "start": 1332.7,
        "text": "supervising interaction with colleagues or just every year give me more experience and perspective on this topic that's my Murphy's Law and because this has always been true to me a extraordinary discovery have very very high chance better than other discoveries to be wrong and often for very trivial reasons so it doesn't need a very sophisticated detective mind you just need a little bit of skepticism and a little bit of practice to uncover a trivial reason however this is not always easy I want to make some comments about a culture in Research Institute and how it's challenging most of us don't really have time to hand check every piece of students work so it turned out to be that turned out to be very important so I whenever possible try not to take a shortcut but I cannot "
    },
    {
        "start": 1395.09,
        "text": "claim to have done this to to its logical extreme that is a leader of the lab I the best I can do is to a sniffed outside of the result if I smell something wrong I say there must be something wrong but there is always the chance something smells right that is kill run those that were passed and eventually publicly publication if it blow up later that just cannot be helped and then there is another real thing which is so much collaborative work class or write minimalist fashion and then you have to go dig and this involved the sociological conflict because everybody has ego that they want you they tell you this journal values conciseness therefore they didn't say everything even in supplemental material and furthermore they are a lot of projects collaborators engage you when they de "
    },
    {
        "start": 1457.73,
        "text": "for the grand supination or one week before the paper submissions and that's when it's a little too late to choose fear away the course of the research so and then there is also so these are my examples about this measure of evidence or it could be about the perspective very that is most of us hoping the study can go like this you have nothing that you have one result so that would be the best paper I this is not too bad you have a generalized now and then you have a signal that lifts up with a clear kink but most of us actually are motivated by finding such dream situations we when that happened we are extremely motivated we drop everything to write a paper but one LAN is an end state you get engaged you want to tell them no it's just this or it looks like this we need to dig more we need to probably tear apart the whole thing I really do data cleaning "
    },
    {
        "start": 1518.08,
        "text": "that's what it's very very difficult to to overcome the culture clash where each in science so then now not talking about methodological pitfalls but more of a social and the culture peaceful another example would be if the real classification is this somebody just cut the pie into four pieces but then if they're not careful they find the best markers and then so that would be a misdemeanor to say the four classes divide this well you know a greater misdemeanor or felony to choose the best best markers to say well classified so this happens all the time the marker collection are the bias that even the best practitioners sometimes put in so I will slide with four or five a common errors built from my reading and from the papers I've read and one is really what's also called data dredging "
    },
    {
        "start": 1579.46,
        "text": "that is you have the you have the result you report the best ones and then you kind of amplify the exploratory analysis of cellular the inference and then essentially confusing what you have funds to be consistent with the best scenario with what you have proven to be true to be the one to be the case and then the other pitfalls would be this can be addressed is really listened you know introductory statistical course that is if you have rejected and now you have not proven alternative especially when you haven't considered other alternatives I would again in time you develop more a fear about the uninviting alternatives and you're more passionate in chasing and done but not everyone are as passionate in finding the least "
    },
    {
        "start": 1642.08,
        "text": "interesting explanation of your data and then these are other ones that we see very often so even though I should have prefaced this talk by saying I have no conflict interest sometimes I wish there is a way someone that have a hedge fund so I can bet against bad papers that would be would be the conflict for me to talk about this if you monetize your journal Club that's an idea last week I America venture capitalist who read literature and recommend buy and sell of different biotech product oh good I'm doing fine yeah no problem so I said uh your life is easy you just need to make a decision than is a binary decision but for me these are my students and colleagues cannot better give them and you work with them and refashion the product so the actually finish early then this is "
    },
    {
        "start": 1706.19,
        "text": "also quite often confusing significant see the effect size that they are not all that leads some but they would say all my replication failed to reach technicians therefore I not only verify found comes with conflicting evidence in literature but actually it's not right you just failed to replicate because they don't have power or things like that so I encourage you to read this article very short one two-page about different types of analysis sometimes been mischaracterized and published as a different type of an hour I have only one example on without 99s just tell you there is a generic aging study where you have epigenome data then you just regress against age but after decided to instead of regret against age you know bendable age transformed it so that it's a exponential before a young age then in your other doubt so after "
    },
    {
        "start": 1766.539,
        "text": "regressing against this transformed age from 300 best markers use that to predict the epigenomic age then found there is a logarithmic dependency until adulthood then it's linear so curiously the author forgot this then to a curve then say this is a predictive versus actual age may the interpretation that epigenomic marks take a fast rate early then constants later on I think it it's very trivial almost oversights by experienced analysts another paper hook you can catch is by showing Eddy a made a strategic comment about what are the big projects intended for so if you have three terms where is a map when the tool what is the where is it on big signs right big sign is that all of these are either for map or for tool but often "
    },
    {
        "start": 1827.289,
        "text": "published of a big science paper as if these maps making effort of trying to do an experiment with positive control directed controls with a rejection of now and sometimes these are purely operational studies are very select Excel lines that end up so does hypothesis testing for very specific function or you know evolutionary fitness so it is this over a over ambitious claim beyond what the project actually are that gives you some other pitfalls that is are not really about individual studies but about strategic decisions on where to invest in in you know consortium level research that the comment I think show edit night about bring activity Matthews also these are the tools if they are successful it can be used individual Abba Tories for "
    },
    {
        "start": 1888.27,
        "text": "very specific questions and then now those can really be turned into big signs of brain function unless you cumulative small signs so my second last slide is again the comments about what our advocate should be if you are a student you should try to build both built your tools the class of your trade but also sharpen your attitude and your instinct and some of those is hard to learn but you really have to learn by practice so the things think I would encourage you to be hyper sensitive just to develop the best nose to smell out unreliable influences and then be very experienced you think about simple trivial alternative interpretations another phrase I want you push out is personalized data analysis because you can treat every data set of a patient it "
    },
    {
        "start": 1949.68,
        "text": "may have the history it has been misdiagnosed and have been put in the wrong treatment you need to see through its entire lifetime a precision data analysis the sociological part is actually quite real you have expertise but if you cannot sell say it if you can update it diplomatically you don't change the culture you don't have the patience to change the culture then you still get your friend for you into those pitfalls so my very last one is about the attitude described by copper it's saying don't fall in love with hypothesis but be passionate about your effort to refute it so that's the best way to avoid falling into individual [Applause] [Music] final speaker is dr. James Joyce who is the Cooper Harold Langford collegiate "
    },
    {
        "start": 2009.88,
        "text": "professor of philosophy and statistics and the college of lit colon thank you there we are he teaches several courses and philosophy including I believe philosophy of science and his research or interest focused on causal inference and Beijing and approaches to statistics now so it in my discipline we come come with paper to these things so I got a bunch of handouts okay so so when I was asked to do this and thanks I was kind of curious how and to what extent Bayesian approaches had been making inroads in bioinformatics I gather from my that in some small ways but just out of curiosity how many people are sort of familiar in some vague way what sort of Bayesian approaches to statistical inference okay good good excellent okay so I wasn't sure so life for a couple slides are going to tell you what Bayesian inference is but but once we have that "
    },
    {
        "start": 2070.2,
        "text": "we'll go along a sort of the pitfall that I'm identifying here and there is one clear pitfall is that in you know adds Bayesian methods make inroads into your disciplines which is which they surely will do you want to sort of guard against the following pitfall pretending you have evidence that you don't actually have the Bayesian makes it local what what Bayesian is and does is it makes it easy to do that all right so here is sort of the simplest way to describe sort of a Bayesian approach and very simple case you've got some observed random variable as an observable whose value depends on a parameter you've got you've got a likelihood function which a beige instinctive as giving you the probability of the data given values of this parameter and then the thing that makes Bayesian is and what they do is is a is a prior probability function that's where the action will be today and these prior probability function are our folks encode all of your "
    },
    {
        "start": 2130.52,
        "text": "prior information about the problem that you have kind of coming in and then the idea is that what the Bayesian idea is is that what the difference is involved ultimately is a revising probability estimates for hypotheses using your prior and the likelihood function and the rule is phase rule which tells you I've got things nicely colored here which says if you get certain data X and you want to know what the probability of a certain hypothesis about this parameter is you calculate calculate as follows you use your prior probability for the hypothesis and the likelihood function and and you calculate the probability of the evidence or even better is this form now it's not so crucial that you understand every bit as I'm going to give you an example right now and everything I do is going to apply to this example so I started trying to make a somewhat biological example but it's pretty lame I agree "
    },
    {
        "start": 2191.63,
        "text": "it's a very simple case I wanted to choose sort of the simplest case that I can imagine so imagine that a gene has two alleles a and B and imagine we have like an ancestral kind of a population which we happen to know for some reason just contains a a homozygous and a B so there's no sort of VB in the population and what's going to happen in this little little experiment is we're going to learn the genotypes of ten offspring of two parents the question we're interested in is exactly is exactly one parent a homozygote but turns out we're interested in that question for some reason and we're going to get we're going to get information in the following form we're going to get paired em n where n tells us the number of AAA offspring and n is the number of a B offspring all right and you can have you can't have a 4 DB offspring well and so just the numbers aren't so important here just put them down in case anybody "
    },
    {
        "start": 2251.96,
        "text": "want to look at them but anyway here so the likelihood function and then the idea is weird imagine we get some data and I want to imagine we get the following data we learn that sort of five of the offspring or AAA types and five RAB types and none are bb types alright and the hypotheses we're going to be interested in here are is exactly one parent in a a or our both parents babies so those are hypotheses we we we we are going to compare and what you sort of might do and thinking about these hypotheses is you might ask gee what sort of the likelihood races that would be really sort of a natural thing to do and if you do nee look at the likelihood ratio the likelihood ratio of the one a a to the both a b hypothesis is 32 which is gigantic so in other words this is really excellent evidence in a certain sense for that hypothesis over that hypothesis and on that basis you might "
    },
    {
        "start": 2314.329,
        "text": "decide to reject to this hypothesis now in general for bayesian tend to do is they don't tend to accept and reject they are interested in that so much what they want to know is what's the posterior probability of the hypothesis after the evidence so this this this is going to be the question here and the answer that question hole examples isn't at the illustration point the answer to this question depends on the prior right so i've chosen a very sort of skewed prior here where it turns out a huge proportion of the population is of the heterozygous type right and if you use this prior and the likelihood function i had before and the evidence i had before you get this posterior here and you'll notice there's still a very large large sort of probabilities that you've got to "
    },
    {
        "start": 2375.44,
        "text": "a be parent all right now so dependent example is just to say these priors really really sort of matter in the whole thing right and here's a good question when should you be a Bayesian I think there's a good answer to this question the good answer is you should be a Bayesian if you know you have a good prior because if you know you have a good prior there's nothing better than the Bayesian apparatus for telling you how to extract information from it right and so I won't go I won't both won't go down this list there are sort of all kinds of sort of nice reasons to be a Bayesian when you have have a good prior you have you have a nice sort of a principled way of combining the data with your with your prior information that's what's a Bayes theorem base your it gives you you handle sort of small samples in the same way as large samples you've got a nice simple sort of simple sort of a theory of estimation to make an estimation and "
    },
    {
        "start": 2436.01,
        "text": "calculate an expected value using your prior this helps with things like so a nuisance parameters and things like that Phase II is obeys the likelihood principle some people like that some people don't Beijing's like it and I think the most important thing is it sort of gives us the kind of answer that we're really looking for the ten answer we're looking for is how probable is the hypothesis in light of the evidence and that's what the Bayesian apparatus is intended to do okay so you should be a Bayesian if you know you have a reliable prior and I'm not going to read this but you know I think there are some cases where we have pretty good priors in those cases you should be a Bayesian but there are other cases where we don't know too much we don't know enough to have a really good prior and the question is what to do in those cases and this is the so-called problem of the priors and this is what I want to speak a little bit about how much times I got 7m okay so I'm going to skip through some of this stuff the "
    },
    {
        "start": 2497.6,
        "text": "problem of the priors is basically but given a certain amount of initial information which is insufficient to convince you that you've got a good prior what should you do all right now there are a number of different and responses with Bayesian given I'm going to go through only one of them one response which I'm not going to talk about is the most common one subjective evasions which is sort of historically I think the most comic I'd think it would think of these priors as sort of a kind of a codification of your prior hunches and views and judgments your personal evaluation of the situation and I've got a couple of votes here but I won't read them I'm going to skip over this this approach for time there's one very common and popular form of the Bayesian approach which I'm going to call objective Bayesian and what the "
    },
    {
        "start": 2557.719,
        "text": "objective agents think is they think that there's a right way to choose a prior given a certain initial body of information so this is a kind of et change if physicist Gustav Gustav nice quote sorry Jane says look consistency demands the two persons with the same relevant prior information should assign the same priors objectivity requires that a that a Cisco analysis should make use of not of anybody's personal opinions but rather of the specific factual data which their base that comes great ah but there's this question still how do we choose priors in a way that meets these conditions well what James does we're going to skip it over some stuff here there's James and others have proposed certain general rules for choosing priors I'm going to just consider one the sort of max entropy rule the maximum "
    },
    {
        "start": 2619.15,
        "text": "entropy rule says this what you want to do when you choose a prior is you want to first of all get all the information that you do have to wind up back if you do it right James says it's going to determine a class of probability functions gives me a pretty wide class the class consistent with the evidence and then what you do is you go into that class and you choose the one that maximizes entropy because that's the one that adds the least amount of additional information the problem so what James says he says so so so this is how to be amazing what you're going to do is you're going to start gather whatever evidence you have once you've done that maximize entropy and then use the Bayesian apparatus okay lovely little "
    },
    {
        "start": 2679.39,
        "text": "picture sounds great here's an example of this lovely little picture you know let's say suppose we somehow come into our problem or a little problem about the elbow and we happen to know that you know a a moms tend to like a a dad and so we happen to know just making this upset you know this conditional probability here is between two-thirds and one and it turns out that a B mom and two dislike a a dads and so this conditional problem and a quarter and third and then we happen to know the population of a mom it looks like this now these three inequalities of picked a very sort of easy case just because it's easy to work with but these kind of constraints might be you might know the value of some random variable or you might know certain events are independent or dependent a lot of things you might know but these three constraints here they determine a class of probability functions and "
    },
    {
        "start": 2739.72,
        "text": "in if you take that class I've done the work here and you apply some of them accent and sir it turns out you get that as the right prior right so put somebody like jane says that's that's the uniquely right answer to this problem all right and so and then if i if i sort of update on my evidence there are five AAS and five eighties i end up getting this posterior probability here and you'll notice I get to draw quite striking the conclusions like so the probability that an a/b mom given this evidence mates with an AAA dad is 0.94 1:1 now when I look at that number particularly when I start to look at with the fourth decimal place gave you that to me looks a little bit precise it looks to me like I've done something wrong if I've gotten an answer that good out of information that bad and this is "
    },
    {
        "start": 2801.43,
        "text": "I think the core objection to to Bayesian approaches which goes back to Fisher and Fisher I'm just going to read it he basically what we see basically he basically says this look I understand that you've Asians have these ways of choosing priors that are supposed to be they're supposed to somehow give you the priors that add the least amount of information just the problem but in fact even if that's true when you settle on a single sharp prior you introduce lot of information into the problem that you don't already have all right so so what Fisher says is is is subject to Bayesian ISM on the grounds that as he says it's abstract it extracts a vital piece of knowledge the exact form of the distribution out of ignorance complete ignorance all right and if you're impressed by this there's a question about what to do well one thing you could do is you could be Fisher and say "
    },
    {
        "start": 2863.76,
        "text": "eliminate the Bayesian approach all together and I there's a book that I included in the references it's a great discussion of fisher on bayesian ism in the 30s it's really interesting to really sort of a well-done book there's another alternative however with some extent i got two minutes i got two minutes to give you it up and i'll printer that's two slides in it there's another approach to sort of begins it begins actually in the 60s but it sort of doesn't get pursued very much but has become relatively some sort of relatively more more more sort of popular into recent years and this is the kind of thing that may be on a new directions kind of a talk you might want to think about this is imprecise bayesian ism and the idea I said love this quote from Peter Wallace he says the problem is not the Bayesian because yet to discover truly uninformative priors all right that's not the problem "
    },
    {
        "start": 2924.11,
        "text": "Bayesian spent all this time trying to figure out the right sort of recipe to find a prior that would capture their information that's not the problem the problem is that no precise probability distribution can accurately represent ignorance and so what the imprecise probability approach does is it says well look don't try to be more precise than your initial data allows this your initial data as it's often will only places incomplete constraints on what your prior probability should look like you got to learn to live with those constraints and treat the prior not as a single sort of a probability function but as a but as as a set of them just giving you the whole the whole theory right here basically in three lines what the idea is is if we go back here the imprecise probability approach says you "
    },
    {
        "start": 2984.86,
        "text": "know once you get here you've defined a class of probability functions stop gather your evidence and then update piece by piece the idea is for each one of the probabilities you update on that evidence then you're going to be able to have sort of theory err which which itself is a set of probability functions and then the idea is you get to sort of draw inferences of the following form if everything in that set agrees about some fact then you can you can make that inference for example in the problem we had above it turns out you can say oh gee the probability that an ad mom will mate with Navy dad is at least 0.9 1/4 but that's the best you can do and then anything I handed out you'll see one of the one of the nice things about the imprecise approach is it gets it it allows you to express a wide range of relationships that you can't express in other way okay thank you so I think that "
    },
    {
        "start": 3052.119,
        "text": "we've had three really great if it's very fast talks we could talk about this for a long time but I hope that these talks have convinced you that this is an important and a difficult challenge for us to face in the future we're going to have a quick question and answer session right now and so if the panelists could could come we have some microphones set up and water but while we're getting down here you know you saw that the problem is very complicated and not always just a problem of mathematics there's cultural problems and there's there's inference problems you found that sometimes the analysis that you need to do to figure out what your data says doesn't even exist which can be of course a major problem and then it can be clear that sometimes we come with with preconceptions about how statistics work that are just not really quite correct and it's a lot more complicated so there's a whole wide range of questions that could be arising here and we had three experts here to talk to you about whatever you want to ask two of us will be going around with microphones so if you have a question please just raise your hand I know that when talking about "
    },
    {
        "start": 3116.53,
        "text": "pitfalls and analysis a lot of the attention is given to talking about what researchers should do while formulating experiments and sometimes it seems like a lot of that advice falls on deaf ears I recently read a paper where the first author had a master's degree in biostatistics and they were doing the comparison physiologically between animals under two different conditions compared 60 parameters two of them came up as barely statistically significant with tiny effect sizes and they confident these results with no multiple powers of control so it seems like sometimes people fall prey to pitfalls even when they shouldn't and my question is do you think it would be legitimate or appropriate to try to put an explicit data analysis pitfall control of some kind into the review process would that be feasible would it be appropriate or is that not would that be cumbersome or even counterproductive what do you think "
    },
    {
        "start": 3177.64,
        "text": "this is a great piece I'm going to take the lead if it's okay with the other panelists if I understand you correctly you asked whether it might be appropriate to put together your data analysis plan ahead of time and maybe put the write that up and put it somewhere I think that well I think that that might have been your question and yeah yeah so let me let me answer the question maybe it's not a direct answer okay but we talked about it in preparing for today's panel so it's kind of an indirect answer your question so there is a history and I don't know how what is in bioinformatics but in intervention science there's a culture right now of publishing your design just to design the data collection process you publish the actual design and how you plan to analyze the data and you and that paper "
    },
    {
        "start": 3238.58,
        "text": "gets published way before you ever actually even see the data and so there's a culture of that developing an intervention science the journal editors are on board with this idea and and the reason they're excited about that is because it helps ensure the replicability because you can look back at that published paper three years ago and see if what you did is actually what you said you were going to do and if it is close to it or identical you have greater faith in the results of that so it's akin to clinical trial registry with one caveat this publication actually lays out what the actual data analysis plan would be when the data comes in when you when you register a clinical trial you don't know we actually layout the entire analysis done so this helps a little bit it's an indirect answer to your question what what I would like the panel and I think all three kind of you can maybe address this issue is how to put the human factor into the problems that we have "
    },
    {
        "start": 3299.029,
        "text": "with pitfalls so if you could put it in as a Bayesian you could put say that a prior could be that people want to have Korea's ppl want to have papers and people have biases in their thinking they have biases in their hypotheses being correct but they also have biases and I want this to be published and that is certainly part of why Joon's pitfalls are happening because students tend to want or need a thesis in order to to get on and journals don't like papers that say I tried hard to show XYZ but I couldn't and so I think this reminds me a little bit of what happened in economics earlier where people were making all kinds of wonderful models of how people should behave but they don't behave the way economists predict because people don't act like a mathematically statistically trained robot that knows exactly how about how to behave and I think it's also true for "
    },
    {
        "start": 3359.809,
        "text": "what you were talking about some people believe in behavioral therapies and some people believe in drugs and how that works in these people may affect the outcome so how can this bias of people thinking we incorporated into science my marker I don't quite actually get your question but I and to come and talk together with a previous question about the publication process it's of course it's a mechanism for peers to tell each other what the think each other's work represent you would be nice what you said made me think it'd be nice that we write papers with a very amazing frame of mind that is your introduction they out clearly what prior is in your discussion lay out exactly what is updated information is perhaps that will "
    },
    {
        "start": 3422.39,
        "text": "help the reader see more clearly what events you have made often we don't do that we write a paper on just reporting the posterior it is actually hard to know whether that's already known try what a study has been conducted and in I guess we we kind of have to ask a little bit about sort of the institutions I think this is true in every academic discipline where there's you know very great pressure on young people to publish in particular you know you need need to get some papers out of your dissertation if you don't do that early there also I suspect to situate every discipline there also but what I've heard sometimes called sort of write-only journals which you know so the journals that people submit things to but the papers tend not really to get "
    },
    {
        "start": 3484.85,
        "text": "read so so you know read only write only this uh but I mean I think that those incentives in place is going to be you know it's going to be extremely hard to get people to avoid certain pitfalls because if it if it's so strongly in your interest to you know get a significant result to get a paper that's going to be accepted and if sometimes the cost of having a paper that's wrong are accepted aren't really existence the sense of paper won't be rightly read then it seems like you're in a position where you're sort of asking for exactly the kind of problems that we're talking about it I think this is a hard topic I think because I totally agree right we have students they're doing all this hard work they want a product out of it I think at the end of the day I'm going to follow June's advice if I interpret it correctly we we just I think we have to coach ourselves and our students to have that certain moral "
    },
    {
        "start": 3545.88,
        "text": "moral ground than that ethic so if what you did ultimately was a massive data analysis massive comparisons with massive amounts of searching for an effect try your to try your best to be above water about that idea and the fact that this was this is a hypothesis generating analysis there's absolutely nothing wrong with being above water with that at least in my culture in the intervention science culture if I write a manuscript and I say this was a hypothesis generating analysis that's how I'm describing it and you're trying to being above water about it and right I and I know that's hard but I think I think that's the best we can do is just coach ourselves and coach our students to have that higher moral ground I don't know I don't know it any other way around it to be honest we go so we are living in a big data kind of environment these days we have the ability to acquire all kinds of "
    },
    {
        "start": 3608.14,
        "text": "dimensions of data about people objects anything that we we put our mind on I was wondering if you have some suggestions about certain types of data that we can acquire in order to overcome some of the pitfalls that you have outlined in your presentations I think the question of the spherical question is can we be smarter of gathering data I think it's absolutely important because our national leaders sometimes think all they need to do is to aggregate existing data so I'm sticking off with several examples in mind after 9/11 the people learned lesson thinking we just need to pour the CIA they'd have an FBI data maybe we could have learned so much so "
    },
    {
        "start": 3669.04,
        "text": "and then in the world of psychiatric genetics there is also the idea that we should just pour different comparative data in the same warehouse perhaps insight or knowledge or true risk and we'll just automatically rise above it so I think that quite misleading the just by adding one pile of noise to another planet tiles noise you don't get signal they're there for the investment whether it's in the national security or in genomic data mining or precision medicine should be more algorithm and on collecting the right amount of data but unfortunately we rarely invest in that you don't find a faculty who is 100% mission is to do bio banking and the record kitchen generating prospective data so much investment "
    },
    {
        "start": 3731.21,
        "text": "these are building your database or analyzing existing data and there is this illusion of having already enough existing data to but what we have are not we have samples of convenience in everybody freezers collected for all those idiosyncratic reasons they're actually terribly inadequate for tween research for flour bipolar disorder schizophrenia disorderly we know very little about people's clinical history or drug history we just have their DNA in the freezer so I think that used to be resolved I think my talk was on this topic my talk was precisely on this idea that I'm not going to talk about data analysis or finding the right data house as it was on oh you wanted to you want to discover how to best build an a dynamic Syrian regime well let me talk to you about a study design that will help you collect that data in the world I live in actually all of my projects every single project I have has a data "
    },
    {
        "start": 3792.23,
        "text": "collection component and we're out gathering data to answer a particular question I just want to add on your on your question is that if you're in that situation where you have a burning scientific question you want to answer and the data doesn't exist you can develop the tools the methods that would allows you or somebody else to collect that data that is a methodological advance and so you can write that up you can think about that that could be that could be part of your scholarship just one more question before I think that that's pretty much going to be it I was given this before the talk and this is actually from dr. Kurt Magni from data speaks and I think that it's related of course to biology but also to general questions and specifics about outliers and distributions and that is group averages wash out the effects of individual differences in the genomics accentuates so it is unnecessary use of group averages to assess causality as in for instance conventional clinical trial designs is this a pitfall and data "
    },
    {
        "start": 3853.25,
        "text": "analysis or is this something more necessary that we can't really avoid yeah I read that increase earlier in an email on so if I have some time to think about it the question is actually about the granularity in decision-making of course we all have the intuition that in empirical studies we don't know the level of classification that's just right for decision-making if you divide people into large groups you'll suffer the heterogeneity within the group but if you over divide you or you doing the truly individualized medicine then you learn no lesson for anybody else so it's still not good so the trick is still the empirical research on the right granularity that takes a lot of time to to build "
    },
    {
        "start": 3915.6,
        "text": "I have a question for Daniel about the dynamic decision-making while you are talking I think it sounded very much like even though it's a very kind of far-field example very much like writing a program for playing chess because you want to play your next move conditional on all the moves that have been played before so what happened in that field that we know computer chess is not better than human it's actually not due to a tremendous conceptual advance the understanding humans decision-making and pattern recognition it disappointingly it's a triumph of beautiful computing and learning from a database I'll say a hundred thousand previously played games so it's a it's almost a recap on previously played next moves how the outcome is I guess that's not quite possible for your design because you rarely have a hundred thousand previous "
    },
    {
        "start": 3976.23,
        "text": "trials to learn from but is there a way to come by however limited prior data with your best judgment on how to design that's bifurcating decision trees yeah that's that's a hard question it's really interesting um okay I think I think the ultimate idea that you're dynamically programming you know that you're trying to decide what to do now that's going to get me to the best move later that is the kind of an underlying idea and that idea is the same idea in that situation as was here I don't really have an answer to your question about how I can build in prior data about and it's dovetails a little bit with with James um James talk I don't have an exact answer except that except that when clinicians come to this problem they their hypotheses are based on their clinical expertise in their prior data their hypotheses both about what the next "
    },
    {
        "start": 4038.12,
        "text": "question ought to be and what the what the patient characteristics are for someone who might benefit from one treatment or the other and those can be tested with the data that arises the designs that I thought that's the closest I can get to answering the question something like the best-case scenario is for all the practitioners to have this national centralized data so that grant information can be build up right right that's true we could have a way to have all the hypotheses in place and see which ones are strong DSM imaginet be great just a quick question about the statistics pause so like we cover a lot about it and so for by informatics students the statistics is required and what happens like we take that cause and the teach a lot about like how we constructing hypothesis testing and after that we feel like yeah great we now have tools to construct our test the writer even is like common test that doesn't work we can find some way to fit our test against the data and "
    },
    {
        "start": 4099.259,
        "text": "there are a lot of the comics that machs on that sale you know like if p-value does not show significance what you do next step right and eventually what happens like late earlier this year not biomedics as a journal social science says like we now reject all paper that using p-value as a their their the evidence to show significance and the causes a lot of debates at that time so I would like to ask like a your comments on this kind of a reaction yeah I don't know about so is this a particular Journal that said that yeah is it I figured what type of journal is like a psychology Journal that said they just don't use p-values anymore because we don't think they're valid evidence or something I think you know I don't I don't know I don't have an answer for these these are hard questions I don't have an answer but I think this comes back to the very first question and my my kind of gut response to it so the whole notion of the p-value and all that "
    },
    {
        "start": 4159.56,
        "text": "stuff right that's based on one procedure you know using it one time it's not based on a search not based on repeated you know June talked about this a little bit okay and so to the extent that we can kind of like my only response is what I said earlier to the extent that we can write up ahead of time what the design was what my planned analysis was then that PV value you report might actually be meaningful that's my only response to that endo in those settings where I tell the world what my analysis plan was going to be and then I share it with you that actually is a meaningful p-value in that setting I could close to it yeah yeah and I I can't remember which Journal was either but when I heard that I was trying to figure out what had motivated them and my hunch is something like this p-values are so widely misused that they just decided we're going to get rid of them like you know but you know if if you found out that people were using "
    },
    {
        "start": 4221.3,
        "text": "sort of sledgehammers to fix TVs and that was a bad thing you wouldn't dance sledgehammers I think rather you'd say well you got to use them properly so so you know p-values of course can be really useful things but I mean I think a part of it is to you know to have procedures in place where it's not sufficient as it was I think it's some girl for some time to you know show it sophistical II a significant event that what that was enough to get it published and so I think I think its own over a reaction for sure I don't doubt that p-values get misused a lot but I mean and you know you don't want to get rid of a useful tool just because people sometimes misuse it my reaction is my field of genetics and genomics Shibata is still very useful although I have my own grudges against for its misuse one "
    },
    {
        "start": 4285.05,
        "text": "kind is to have a tiny effect by the way the tremendous Peabody's and clinical sample sizes there of course amount until you kind of have one percent effect have a key of minus 10 to minus incredible so that happened another is to leave specifying the null hypothesis to set up to be so easy to reject and that we end up learning nothing so so those are the miss uses but still what else would you adopt to speak of uncertainty in India's one so feel that after 10 to minus 8 it's kind of a community gets cleaned up we are reproducibility just skyrocketed way better than the kind of age in age where reproducibility is are horrible so we look at sentiment of eight out of our garden guarding of our reputation of and "
    },
    {
        "start": 4346.25,
        "text": "we can transmit the knowledge from one study to next that's quite useful I just want to thank everybody for their attention and I think this is a great talk if we could just give one more thanks to our speakers [Applause] "
    }
]