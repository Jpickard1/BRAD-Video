[
    {
        "start": 0.03,
        "text": "um forgot here sorry okay thank you haha yes very glad to be here I totally come to this summer every other year talk about what I am doing really like the setting is finding for most I like today for the informa as well so people didn't rob me with questions or you know things I shouldn't really get it right you know so today I'm going to talk about you know few works we did you know me my farmer students my colleagues and my collaborator stealing the past few years about you know around this topic so the papers are you know I will give the reference of the paper in the end most of them are okay all of them are they online so you can check all the technical details so I sort of uh skip most of the technical detail talk about ideas week we put together so it's basically how we develop you know efficient algorithm to accelerate resampling based upon testing in genomic data analysis so yeah well quick you know introduction of you know the "
    },
    {
        "start": 60.69,
        "text": "background of the work right so basically I will be focusing on two Heidelberg resampling based about testing permutation tests and bootstrapping and the permutation has was really you know develop very early in the age of the statistics I Aria Fisher and Pitman in the thirties you know they develop this permutation test for many different kind of tests so you have a two sample t-test then you have a two sample permutation test so there are corresponding versions and the permutation has by that time was called the randomization test or actually exact test you know the seizure exact pass was actually a sort of a permutation test and then he works for many different a class of testing like to a multiple group comparison matched pair study of you know two variable you calculate correlation or any relationship between the two variables that's about what I know so it's quite challenging to apply to other most sophisticated modeling like you know regression kind of stuff it's not impossible but it's not very easy so so the limitation I would say for permanent has is that you know it's "
    },
    {
        "start": 122.85,
        "text": "a fight will still a kit you know a number of different uh you know scenarios and then a now have housing here is actually basically for example I will talk about a two comparison which is the most widely used passing genomics I would say to know half of the genomic data to group comparison right cases per second so so in that case the now have is basically the group route labor of the data are exchangeable so another now the true guru have no difference therefore the one you know individual one data point come from one group we just by chance versus the other so there are polar exchangeable so that's the only now Hippolytus they made no assumption on a distribution of the data digitally doesn't have to be normal or anything right so that make it very very obvious sort of a nonparametric there's no distributor sumption however data exchange ability or something is actually priced wrong for example there are paper pointed out that it's not always really a rigorous or you know appropriate to apply for example permutation test when you're comparing a "
    },
    {
        "start": 182.94,
        "text": "normal work versus cancer because even though you're attaching the mean are different their distribution are not really the same so for example will they do two samples it has if the berries are different we should use a different statistic right but if you have to group them you're testing the mean whether they are the same but the berries are different actually the permit is not do not apply so there is some caveat you know underlined so that now about is really they're the same distribution both the mean and berries and everything is the same so that could be a very weak assumption because there are no distribution assumption but it's actually very strong sometimes well saying all the data point have the exact the same distribution so one way or another oh that's a you know kid value mind when you're biased but other than that you know you work with almost any statistics you know you can catch whatever statistic you know for for your application that's that's what strong you know advantage of permutation says for example this disease I will talk about later and it gives exact p-value we all know for example heat has "
    },
    {
        "start": 243.12,
        "text": "actually given exact p-value but you know that's a bit on a normal assumption but most of the tests like chi-square test they are asymptotically right the p-value is only you know accurate when the sample size is very long but this this permutation are give exact p-value even the sample size is small and I give a you know quote there because when a sample that is reasonably large like twenty or thirty data point you cannot computationally in numerous other permutations if you can then you've got exact p-value but commentator you cannot therefore you still approximate a p-value but not in the same way because here you are computationally of proxy some exactly body underneath now like in the inner symbolic attached like chi-square test your automated p-value you know your approximation is limited by your sample size you can never really get the exact one there this one you can get if you you know kind of for more and more computation so basically you can get higher higher accuracy as long as you afford more intensive computation so it's sort of a different so even though "
    },
    {
        "start": 304.9,
        "text": "the permutation test was invented in the sort of a thirties it wasn't really that popular and hew in recent years for the reason of computing right because permutation has basically you premiere the data many many many times and then the more you're immune you know the more accurate the p-value you get and that take a lot of computation which is not available thank you the past one you know 20 or 30 years so to my knowledge it was really popularizing genomics and even in many other you know fused by this paper of the significant alice is a micro rain in the know I think it's what's wrong could be 2001 get correctly wait Waverly I love it should be put out of what was that the microarrays so these paper was highly slightly more than like 10 thousand citations they use from it agent has to detect differential expression right and to calculus passed "
    },
    {
        "start": 365.529,
        "text": "it's every way and they use this moderated he statistic do it to sample comparison where there's a sort of a ionize sort of irritated state and on you know ionize a state and compared to condition and then the sample mean difference is a statistic normalized by the standard deviation if there is no this part it's pretty much the key statistic but then the ad is to stabilize it to make a you know test statistic knowledge division to be almost you know sort of a consistent across all the genes therefore they can gather FDR across so I think this is no longer a key statistic and therefore you cannot really use devalue from the t-distribution and they use fermentation so basically that demonstrate me that you know a lot of times we use cheetahs not because we you know the t-statistic is really that good of you know ethical you know biological selling just because it is a sort of a convenient to to give us a p-value very quickly and accurately "
    },
    {
        "start": 426.75,
        "text": "so we make this normal assumption we use the T statistic because you know that's a whole happy usually had to use it right but in many real applications you may want to use some other statistic but when you use other statistic then you don't really have to you don't really have to an illegal form or the p-value anymore right so it's sort of a trade-off whether you want to use a better statistic in your cleaning or you want to have a better sort of a tool do your analysis but one example right I think in biology we always talk about volchok yeah it stood out on what sorry is a type of yeah pretty sure of that so thanks yeah so in in the primary trend analysis or how about full check right so if you have to groove with replicate major gene right into group if I ask you the pole change between sake you know treated group versus control group the most reasonable way if you ask any you know incoming freshmen in biology department would be you take the average in the treated group you take the "
    },
    {
        "start": 487.47,
        "text": "average measurement of the control group take the ratio right that's the PO change by in reality actually nowadays if you you know look at all this analysis or this software most of the time that's not what it's done most of the time was done is that they take the lot of all the values and then take the average of the log into group and take the difference they called a lot fold change and then if you want to get a for changes you expose exponentiate right and the reason they they do everything in the log scale to my knowledge is because once they take the law they are detecting the difference in me right and that's he passed there you can get p value and the p statistic is pretty much you know it's the difference in me but the average of the log is not log of every right if you take you compare the to average of the lobby you are pretty much doing the sort of geometric mean rather than a Richmond unique so they are now the same with already enough sample size they sort of converge to the "
    },
    {
        "start": 548.01,
        "text": "same value but the final sample they're not the same thing right the average of log is pretty much you'd multiply everything and take square roots something like that so it's not like we don't want to take the mean and take the ratio it's just once we do that we cannot assess key value right and I will show later with this you can actually take whatever stage in your life like the ratio of the two mean and then get a p-value boy you don't have to be limited by the it has cream or other kind of thing okay so that's the benefit so you can work with whatever statistic is like a lot of clinical so this will be most of the focus and I'll talk about also talk about bootstrap hats which a different kind every sampling has divided by a Brad a from from Stanford and a 79 about 90 80 you will the book how many papers you know so remember the permutation has has many advantage but the disadvantage I applied to limited number of settings "
    },
    {
        "start": 609.24,
        "text": "and this is more widely applicable you can be by the regression you know other kind of most specifically modeling I'll talk about later on with the example and the summoning are different there are many different version nonparametric bootstrap parametric bootstrap or nonparametric bootstrap basically you you take the data and you sample it with words replacement so you take a data point and then the next time you steal sample from me you have the chance of getting repetitive samples in fermentation you sample without replacement you take one out it's no longer there next time you get something else so that's the only difference and but it can be applied to assemble the data points or sample residuals and for parametric boost ready to actually sample from a parametric distribution so as many different application there there's a again is nonparametric doesn't really assume distribution of something this one assumes and there are also you know other restrictions but pretty much work with any statistic as long as it's not so uh so you know badly define some "
    },
    {
        "start": 671.7,
        "text": "discontinuity so that I could break but other than that it works so now I move on talk about you know in general we can put this to kind of test in one framework where we want to evaluate a p-value and the p-value by definition is the probability you observe something you observing the data right characterized by the statistic you define and the probability observe that big of statistic if the data really follow for now right so that if you follow from that definition the way to get a p-value is that if you can similar data from the mouth and then for each similar data we call the reason all the data you calculate a same statistic key so these are the simulated data from the now the tribution your kind of statistical key whatever statistical you can define and then compare that to the statistic you kept it on the observe the data and then suppose this one the larger that more extreme then the chance you see these prefigure than that "
    },
    {
        "start": 732.12,
        "text": "averaged out over many different simulations is your p-value right so simply put if you similar the thousand times out of the thousand times ten times you have data a more extreme than one deserve the Nokia at least ten over a thousand is 0.01 right that's a simple definition and these two times are exactly doing this except that it's the way they similarly from the knowledge are different okay and I have part about their different way of you know doing this and then they work for you know they have many advantage work for my final sample size you see that this work for any model any statistic so it's very flexible but the key is that is combating the very intensive if you want to give a little point of one then you need at least 100 simulation you want to give a little you know 10 to the minus six like in genomic or you know genetic studies you need to simulate a meeting times at this right and that's not too bad our computer is "
    },
    {
        "start": 792.3,
        "text": "fast but that could mean nowadays you know make study will do many tests right or different expression we at 20,000 genes but jiwa fleet has like a million markers then each one I need a similar this many times and that given make nowaday computer you know very slow so that's why we work on this problem we try to make it faster by by basically doing this in a more smart way in a smarter way so the first paper is a from a former student and also my colleagues so basically this is ideally the ideas give the formulas and let me know if you know you have any question we focus on two sample comparison this is the you know pace group is the control group where these are measurements you want to somehow match the difference between the two right and by definition you can do permutation test and a way to do that is randomly familiar data many many many "
    },
    {
        "start": 853.68,
        "text": "times for each permutation you compute a statistic again again again see how many times you get how you have a statistic more extreme than one here right so suppose your p-value is pretty small which means the difference are pretty striking which means the number here are somewhat different from the number here in some way right so there are many different permutation but you think about for any permutation if you compare the permitted data with the original data you can see the belong you know the group membership right so this data point II may look already per million is in the same group but this one come from a different group right so now we can compute the distance between the two permutation by the number of elements you have to swap group membership so here forget from here to here you need to move three elements this way three element that way so the distance we call it a 3 B is three right so now you can you can imagine that the smaller the distance which means you swap fewer "
    },
    {
        "start": 915.93,
        "text": "elements between the two groups then the the closer the two statistic will be right you know in a more general sense not necessarily for every single one so the idea is this we cannot really afford to permute a lot of times right if you think about your funny data point here the total permutation is 20 factorial funny that's no way you can compute and if you partition all the permutation by their distance to the original one and that gives this one so they we have a hundred data points and 15 each group then the distance for any permutation for the regular ones range from zero to 50 you at most you need to switch 50 data points right so and then we draw this way and then you can compute a p-value restricted to a particular we cut a partition so if you learn all the permutation with a given distance together you can get a key value within that partition an idea is "
    },
    {
        "start": 976.78,
        "text": "that a pivotal in the party and the p-value will be larger which means is now that's significant the p-value will be larger in a partition with a smaller distance and the p-value will be smaller in the Cartesian with a larger distance so put it for the folded partition with distance 0 you've got a p-value of pretty much what because that's that's the only thing available but once you you know increase the distance the p-value drops down say with a distance of 50 well this is a promise theoretical sort of a calculation it's no way we have verified that you know a numerically but a few model goes down to 10 to the minus 30 and so key values be a little 10 to the minus 3 or 10 to minus 6 which means that say you want to calculate a p-value in that partition you need to simulate change to the 1 this permutation to hit 1 bigger than that which is hopeless so to sample beta what I'm saying is that will sample partitions in those larger distance you "
    },
    {
        "start": 1036.959,
        "text": "know permutations is a waste of time you're not going to see anything bigger than that very very small probability so just don't waste your time there please are computing there what we proposed is that you you sample from a permutation with a smaller distance has to be better there and use things here to extrapolate you know the curve and then to predict the people found there you're not going to really observe that computational once you have this curve you add up everything together this is kind of risky because you are extrapolate but I don't really observe things there but with some theory guided we think there is a curve like this and we have a lot of you know simulating the show so this is an idea even though those fermentation with the larger distance has a very tiny key right but there are a lot of them because the common a forest once the you know the distance increased to some point the number of permutation increasing financially so most of the most of the "
    },
    {
        "start": 1098.59,
        "text": "permutation in the whole permutation space in the 2400 factorio space have a larger distance so by not sampling there you you're avoiding a lot of computing right but you cannot but basically what I'm saying is that if you say the pure is so small why don't we ignore them what I'm saying is you cannot ignore them because there are tons of them even though every single one a small probability of you know exceeding exceeding your statistics but there are tons of them like they grow exponentially so you cannot ignore them either so you can compute is accurate combinatorial number so in the end what we do okay I try to skip this basic idea is that once you increase the partition this is the persistence one five and a fifteen where you got more and more but the statistic has a smaller and smaller probability of you know being significant so this is how algorithm like I said before for the partitions with smaller distance which simulates a few like a thousand iterations to get "
    },
    {
        "start": 1159.37,
        "text": "the estimated key value in that partition and then we got a few data points here you have got a few data points here as well but let's assume they are symmetric and then you extrapolate the curve use Poisson regression so basically under some theory we show that if the tailor follows on you know distribution and if the statistics are sort of a pump--ow mean function of sample meaning then there is a symbolically there is this this curve okay so we use lot linear model to fit the curve and finally you know to a weighted sum to get a final key value and because the theory right only works for functions of sample means and to sample problem it's not like the algorithm work with any statistic but for others to do we cannot guarantee you know you got reasonable Peabody's but we try to actually work very well so this is simulation so we do a to some comparison and here our statistic is the "
    },
    {
        "start": 1221.24,
        "text": "ratio of sample mean like the p-value from kid has but this is this and then we similar to group also for some random variable with different mean and variance each one has a samples as a 100 and then the red are our algorithm and the the green and the blue are the other you know earlier algorithm using MCMC trying to do the same job and we run the other algorithm for one meaning sort of MCMC iteration so that we got a pretty accurate because we don't know the truth we run the other algorithm for a long time together sort of a you know accurate p-value then we compare to it and with the other algorithm run humor time so basically is quite you know our the red points all types agreed with the other algorithm when they run a long time but the other will be if they run shorter time become you know not so accurate you compare it here so these are the algorithm the other we can run a long "
    },
    {
        "start": 1282.2,
        "text": "time so a thousand times longer than our algorithms we got about the same result but if you you know reduce number iterations for the other algorithm it startled got inaccurate with us so basically we achieve achieve about the same accuracy but a thousand times faster from this simulation and that's all right here you know MCMC algorithm designed to accelerate this so we apply this this method to a you know real data set from TCGA we compare two different kind of lung cancer final attack genes are different it's a large sample size for the p-value a tiny just to demonstrate so we detect many genes of course it's at the top ten and nine of them the both one or two are also detecting in another paper by doing the same analysis but we decide a different statistic but I just want to show is that we use the you know the full change the racial statistics and the p-value are 10 to the minus 200 from our our method we write a many times actually it's not that accurate "
    },
    {
        "start": 1342.83,
        "text": "because if we write a hundred times they read from - 200 - you know 220 so even the log of the p-value has a range but at least you can get an order of the p-value right we have many many other simulation comparison in the paper but you know this is just what I show just to tell you that we really get keep out of this tiny which is hopeless if you run a naive fermentation however any questions so far the last column are basically the statistic right the ratio of the means expression level in the two condition alright so now I talked about a second idea of addressing the same problem the previous you know method works but for a limited number of statistic this has to be function of needs and to group right so this one worked with more "
    },
    {
        "start": 1404.75,
        "text": "general statistic and and has you know better theoretical guarantee that you are actually getting the p-value accurately not a thing ally so so the idea is that in the end we are basically trying to ask me the probability right and from using a Monte Carlo kind of thing so you wrote draw sample use Monte Carlo from a distribution and measure a small probability in the tail that's all I do all we do right and that problem has been widely studied you know how do all those you know basically how to detect a small probability using on the color and one approach is to do important sampling so instead of sample from the original mouth attribution you assemble from something else concentrate it on the tail and then you get more efficiency you've got a higher chance of hitting you know the the significant you know data and then by adjusting for the different density in your proposal words of the true you can actually sort of a get a true "
    },
    {
        "start": 1464.96,
        "text": "probability so you know important something his textbook stops so this is the probability you want to compute right so probably almost statistic people that I was observed only difficulties at least it's a very tiny probability so if you really draw on an out the transcript it is is very very tiny and then if you use important sampling you try another density and then you can increase this probability and then we actually know what's the best position to sample from is actually called optimal density which is defined this way unfortunately actually you can sample here I will show later you cannot really use this to to help you activate the probability because this density involved with quality which is the very quantity you want to ask him so it's sort of a chicken that you cannot really use this to help you but if you can get a density you know similar to this where you can compute the density and your sample from that then you actually can "
    },
    {
        "start": 1526.03,
        "text": "accelerate the process and so this is the so-called cross-entropy method which was an optimization method of you know developed in the 1990s you know overview to actually to ask me a small probability for this problem well idea is that you cannot do this but you can final density in a some family of interest you can you can easily sample and compute the density which mimics this by moving I mean you find a distribution in a family where the distance between that distribution and this one the optimal density is small measure in all back labeler distance so if you minimize this and then your list it will look similar to this therefore you increase your efficiency right and doing some algebra in the end you want to find anything which you know minimize this is maximize this because this is constant and you know I'll move on to the next but this is actually pretty hard to find directly how to find a density which "
    },
    {
        "start": 1587.41,
        "text": "maximize this it's actually hard and then there is this this idea called adaptive cross M trees interview method which is that they replace the gamma by something smaller so that you actually can sample from some density which you know where you hit this more often than you you are with the true gamma and then you gather you get a density which approximate that you know optimal density in that or that gamma then you're gradually increase gamma finally you can get there so which is saying that if you go directly there you have no hope you're not hitting but if you you know do one step at a time you can get there I will show and then for each step there's optimization going on together you know the density in your family and this is a weighted sort of a weighty likelihood in most of case if you pick this your family sort of accordingly well-liked from the Gaussian then you can solve this analytically so this matter has been widely used in the "
    },
    {
        "start": 1648.58,
        "text": "overview and also used to to accelerate like dumb parametric booster here we used to accelerate from an agent that I will show example I think is disparities so this is a Gaussian density right and suppose this is our null distribution and the observe the statistic is six so in the end of the p-value is the probability that are talking in normal zero one random variable is bigger than six because it's costing we know the true value is ten to the minus H or 1000 minus nine it's very tiny so if you directly sample from here why not other billion times your hit on the right hand side it's you know not very efficient so how do we use you know adapted cross-entropy method to find a better proposal density this is how it does so in the beginning we sample from this now see of course we're not going to hit here but it's ok we sample from this "
    },
    {
        "start": 1709.36,
        "text": "style scene we draw understand random sample and then we calculate say of given point he'll like 90s long health which is one point three so 10% our data points about this this is below six but that's fine let's start from here let's assume this is what we want and then we find a Gaussian another Gaussian which best because you know the actually the right hand tail here this is actually the optimal you know density for this particular gap now we find another girl scene to mimic this to bear support approximately stay tributed by solving that weighted likelihood this is a Gaussian we got we need a sample from a mu is 1.8 so does him from you is 1.8 as the minimum distance among all the Gaussian to this tail here you can solve that and the next time you sample from this Gaussian same story and this is the 90% won't help you move forward you've got my three and then you keep keep going after after five iterations "
    },
    {
        "start": 1774.34,
        "text": "now the tenth percent Hong hell is six point two nine speaker than the six you want and you go back there six find another Gaussian finally example from here so this Gaussian is your proposal density because you can easily sample from it and you can compute the density easily and you have a lot of data points bigger than six now you have a very efficient important sampling scheme finally you calculated the density now apparently to give at it which is very accurate nine times ten to the minus ten so this whole thing see you have a 1,000 sample for each step finally use sample 5000 of them with about 10,000 sample you value out of this small right how do you know we checked which is such it because we know the truth this is the Gaussian so we know the truth is right this is my time eight seven and that's 907 yeah you can increase this and this "
    },
    {
        "start": 1838.27,
        "text": "the the the arrow dose top right you can increase from 5,000 to 10,000 this goes you know error keep going down but it's really up to how much accuracy you want you can increase the sample size it's just computed okay yeah I have a man example showing the accuracy but you know to apply this method to permutation test is actually not that easy because this is Gaussian where we can sample we can compute but in the permutation test we are sampling from the permutation station to apply this adaptive cross-entropy method we need to prove your 300 we need to know how to parameterize the permutation space and then by a family of distribution and then the distribution has to be easy to sample from also easy to able to you know compute the density which answer not easy but luckily we find some paper in the you know twenty years ago in a specific literature they actually have a particular distribution area called a conditional Bernoulli not for this "
    },
    {
        "start": 1900.309,
        "text": "purpose of course but happened to fit here and the conditional burn Bernoulli is basically like this I mean talk about the way I use them here so you have data points on the to group then you assign each data point of probability P in the property they appear in the first group versus the other so if the probability initialized at 0.5 which means every data point has equal poverty in one group or the other that's a naive permutation but now you parameterize a permutation space you can assign some data point has higher probability one group than the other right by tuning those parameters you have Sam you are basically doing important sampling in a permutation space and there they have a very sophisticated sort of algorithm to sample from conditional Bernoulli and also compute a density so we use all of them we also modify some of them because there's a way they like people you need to stop we modify some of their algorithm mostly dynamic programming based we modify their algorithm for our purpose here and finally this approach "
    },
    {
        "start": 1960.419,
        "text": "doesn't really work that well if you have tons of parameters or you know for curse of dimensionality because there is a likely racial something going on if you have high dimensions that thing blow up so if you lost stability so we adapt a newer method called screening from the same author who developed the seee method you know to basically reduce the number of friends in the beginning we only tuned some of the purpose now the other to make them fast so I'll skip this and talk about real data so here is a real data center for children AOL where we want to compare to grew 67 versus 124 they are the children who have some you know residual disease remaining after the treatment we do a to group comparison here we use the moderated key statistic so that you know Jeep has going up I because it's not a piece of disease of all the ways and then these are the top needs we found okay to get accuracy we "
    },
    {
        "start": 2022.98,
        "text": "actually run the crude permutation so we just write many many times like a billion times to get a p-value from the crude permutation also around our algorithm and these are the top things and that's the p-value from the crude permutation and that's their standard error because we run each one one hundred times we can ask MIT this is a relative stand error so basically this gene has a p-value of three point eight times ten to the minus nine and the relative error is about two times ten to the minus 1 which means twenty percent relative error okay so the relative standard given is is twenty percent of this number from the crew crude permutation and we got four point three ten to the minus nine is close but now the same our relative error is one percent so this is 20 percent off we are one percent off so this is much more upgrading from such accuracy and the time we are about 30 30 times faster "
    },
    {
        "start": 2084.6,
        "text": "thirty to forty times past and it grew from it so we used three percent of time but yeah you know much higher accuracy in the TX so these are these are the it's the main thing if you want to run the game by their p-value this number are very accurate so later on the part 15 twice where I didn't halfway down offline VAR p 15 there's just two entries there also did you want that I guess it's a they have different numbers i guys say so so it could be a typo when I copy it here last night or I would tag it would be yeah it could be different different yeah MTU possibility woman yeah I call the probe side ID there's a profile from Adam and Matt you know double have good "
    },
    {
        "start": 2147.19,
        "text": "eye yeah so yeah so basically we can get you know Q value of this range pre accuracy okay any other question if I move on so the next I'll move quickly about matters right now so the previous matter for permit agent has what's okay but still we can now get extremely small key value also we can already do with a large very large sample due to the high demand led to and now we move on to parametric bootstrap okay and actually the adaptive see matter sometimes you know because each step they are optimizing a sort of a non convex problem they can actually not converging sometimes there are usually there are some you know improvement in it in a few years ago in a fuel they're actually non adaptive see you matter we don't have to adapt if you go there you "
    },
    {
        "start": 2207.58,
        "text": "can directly go there to the optimal one but the the requirements that you need to be able to sample from the teo direct and which does not really apply to any case every case such apply to some case due to the you know the recent development in MCMC you all heard of like the hamiltonian poly follow you know gibbs sampling or some other you know 2000 approach so in some particular family of distribution you can actually sample from the tail directly once you assemble from the tail you can use those sample to fit to solve our mo each problem to get the optimal density in your family then you use that sample again because even though you can assemble on the tail that cannot be used as the important sampling density because you don't know the density you can sample but you still don't know the density because the normalization constant is unknown but once you assemble from there you can use that to fit another than me in the family that you can compute "
    },
    {
        "start": 2268.38,
        "text": "they'll example again from there there you had to uh you know important sampling very efficient so three steps so I will show it this doesn't apply to any problem because the first step is very demanding you need to be able to sample from a payoff of particular distribution but we apply y example here is a gene set pathway in Richmond Alice's so say you have many genes from patients and you have many gene sets or paths we define you want to assess whether a particular gene set is associated with with a particular outcome right and this was a model of proposing the past basically the outcome Y is some sort of a you know linear function of some transformation of Amenia predictor an acts are the covariance to address the other gene expression in a particular pathway or gene set and beta the coefficient you want to assess so the null hypothesis are all the beta is 0 which means the "
    },
    {
        "start": 2328.38,
        "text": "outcome is not associated with any of the genes in the pathway you know Colbert adjusted or versus beta is non zero which means the gene is associate with the pathway and then they use the scores they use are sort of a random effect model uses for statistic given here would have so you can you can fit the model another now which means beta 0 and then once you've got an alpha you can predict the Y using X you know alpha then you're subtract value multiply Z these transpose you got a sports statistic and this statistic can be used to assess whether you are you know significant out this score statistic will be asymptotically chi-square actually a mixture of PI Square people know that but data approximation is only reliable to ten to the minus six or something and to get smaller one they use better approximation but from all the literature I mean we compared to those from all the literature in the in the field no method is accurate up to ten to "
    },
    {
        "start": 2390.839,
        "text": "the minus sixteen not one beyond that is all of King Kong is no longer active so here we can understand I mean mathematic processor vocals your land that's amore united so it it's good at below off of it so it's amazing that it even works at end on the - sex yeah it's pretty accurate but you know just not for the full of purpose of our purpose here so so we do a parametric bootstrap because this is a parametric model so in this particular case we this is identity because we assume of by Skousen and the wives accurate I'm a biological you know measurement from melanoma patients and we adjust for gender and age we have you know many pathways to test this same model was actually proposed and used for gene based path in diwas so here it has the association with some trait versus many snip in the gene right it's actually the same model or actually you can convert it to patch "
    },
    {
        "start": 2451.06,
        "text": "the racial statistic and differentiation so exactly why they use in many different settings so now we need to sample from the tail when you look at this we are super super wise cows in and this is the MU is a function of the data it's also you know a linear function of y and z are known on it so this whole thing is a quadratic function of tau C random variable and if you want to sample and get a pail of T bigger than something you are basically sampling from a multivariate Gaussian and look at the tail is characterized by some quadratic constrain your basically sample form of you know tall tale of multivariate Gaussian with quadratic constraint and people have already designed you know sampler to do that like the Hamiltonian Monique holiday our assembler available so with their app you use that it's very efficient and our matter because we do not directly use this as our proposal "
    },
    {
        "start": 2512.68,
        "text": "density we're just used as a drop to get our purple identity so this doesn't have to be doesn't have to be very accurate you can somewhat roughly from the tail that's fine so we draw some sample from this using this method published in 2014 and then we plug in two hours you know steps with three step algorithms leave the final result so we got this you know the top gene sets' associate with the outcome and that's the p-value and these are standard deviation of the p-value is smaller than divided these are the time which need we're uninteresting 100 times forget a standard deviation and every time on every standard deviation of time so basically for each gene took less than a second and the PMO is on the order of 10 to the minus 50 and the standard deviation you know it's two other men if you've magnet your one or two other magnet is smaller than the p-value so it's rather accurate I will show some simulation to show how accurate they are but this is way smaller than 10 to the 6 "
    },
    {
        "start": 2574.93,
        "text": "minus 16 or something so so how I upgrade are they right we do not know the true so we and there's no other matter give us the truthful comparative but because we are something you know we are basically calculated the tail probability of multivariate Gaussian in special case for example chi-square if you add assess the tail of the chi-square it is actually a multivariate Gaussian hey it's a special case of that and there you know our package doing that so we run on a method from for Chi square with W is Chi square v this is high score 100 look at the tail - very very poor tail we go down to the order of 10 to the minus 100 and we compared to the true pay probability calculate our function versus the operating capital using our you know approach they are these are the agreement right on top so if you can "
    },
    {
        "start": 2635.53,
        "text": "remember that the relative error is less than 3% even down to 10 to the minus 100 but you can really cross these numbers and the other question you know I got a lot from all these also from reviewer is that do you really need to make this so active at that small number yeah that's a but I mean it's a fair question so what we did is that we run another study of differential expression using a racial statistics and here you know you know ridiculous study are given here also a cancer we do a lot of cancer genomics to group comparison so basically here we compare two methods why is that the permutation the brute force from not permitting a bruise boss you know Monte Carlo for our bootstrap statistic using a racial statistic here is using our method so that we got better accuracy I think here we use ten to the five fifth number of rotation for each gene so so they're the "
    },
    {
        "start": 2698.66,
        "text": "smallest few matter is ten to the minus five right so basically use two different Monte Carlo approach to run the same tests of course we got slightly different p-value out of them and ours are more accurate key values and these are after FDR adjustment and you actually end up getting different results so the key is that if you use after your color of 15 percent you've got the same genes same number of genes I assume they're the same genes but once you go down the FDR cut out you start to get different genes different number of games so because the the p-value here are not so accurate so they look back you ask your adjustment in the area of effects that the genes number of gives you so happy but I so things actually sort of a affect each other once you don't have very accurate key value the downstream of that program FDR adjustment of the ranking will be affected so I wouldn't say it's really necessary to get a value of 10 to the "
    },
    {
        "start": 2759.53,
        "text": "minus you know it's before but people actually examine the G was field they run by their findings like you I despite a lot you know because in gos us that this the effects has a tiny anyway so they ranked by their key body did you look did you have somebody looked at like the 50 to change versus 217 to really see whether you know could be the 52 genes what's the magic number and actually going to 17 I wouldn't suggest just the runningback you especially in the very expression we all have run gene by by for change or we rank them by their biological sort of the impact right but if you want to use a value as sort of a criteria to look at your gene you need an accurate p-value right otherwise you know for all these gene they have similar people in the same order if you don't calculate them you know accurately the order might actually give you I'm saying if you use more rough approach to get p-value the "
    },
    {
        "start": 2819.68,
        "text": "ranking of the genes and these are actually paths with the ranking of them can shop up by the bit Oh anything so far yeah quickly wrap up so all of these are you know papers either under revision but we have a manuscript online already so finally I will talk about something we did a while back and there are some you know follow-up work in the field but idea is single so everything I've talked about in the previous three work is that for one task we get very very small but very accurate Peabody but remember there are 20,000 genes to test right most of them will not be significant and for the previous master even though they are fast they still take you know a hundred thousand iterations something like that right so you cannot really afford to do that for every single gene so another demand in the field is that you want to quickly find out the gene which has a tiny "
    },
    {
        "start": 2881.24,
        "text": "p-value then focusing on converging on those how to quickly screen out a gene which has a degree may pivot and throw away those are insignificant and that's a that's a sort of a demand in the future so for example people want to see find all the gene with the p-value on the order of 10 to the minus 3 we need a 1000 resample which is not too bad but the statistical are working for taking a long time to compute then this would take a long time and there are some you know intuitive ad hoc matters develop in the past is that we can say first run 100 sample if after that already 50 is bigger than Y observed then a few will not be smaller than 0.05 then you can move on you don't have to do a call then you 400 already give you in significance right so we further push this further you know to a more sequential adaptive way we sample one at a time I use a sequential rule to stop so basically you pick a few color based on "
    },
    {
        "start": 2941.24,
        "text": "your need which will determine your computation or saving and then you stop the sequential sampling using a simple rule like this if the p-value observed so far after the pace-it preaching speaker that's I'm cut off then you stop and there are many follow-up paper this was a pretty dirty paper from us there's many follow-up paper in the field basically try different starting rule they have different theoretical guarantee right so basically there are many routes such rule available you don't have to run a hundred or a thousand for every single gene you can start way early and save the computing for those you know select a subset of G here we have the guarantee for false negative rate and FDR but they have other kind of guarantees their features so as a example we run a differential isoform expression analysis here we use a you know bootstrap sampling and then in this case we saved about 20 times of you know running computation and get "
    },
    {
        "start": 3001.27,
        "text": "exactly the same result so there's a paucity sorry guarantee which kicky so you can actually save a lot of time doing this so that's about it and this my former students my colleague and my collaborators I want to thank them and these are the papers I mentioned and this four paper are the four you know topic you have other about the day these are publish design the review but it's the archive paper online this is already online and this our last post so before I stop you know I want to do a small advertisement so I'm running a special issue in a journal of genes which is a you know this publisher called mdpi and the central issue is due on September the topic is about the to the matters of genomic data right the journal is open access is very quick you know review time so if you're interested let me know I can send you an invitation because this is open access they have a sort of you know publication publication fee I think it's a $1,800 pretty "
    },
    {
        "start": 3063.79,
        "text": "expensive but standard for and I colonel but you're interesting submitting let me know I can send you an invitation so I got a discount okay thank you very clear students yeah I mean all of this you know for all my papers nowadays I put a link and give the Ebro webpage where you can download the code which reproduce everything everything in the paper but I have to say the final goal is to make some packages are packages user-friendly by Hardy got their resource but let me know you but somebody you know they could be a good project you know and this one has a our package and the some of the them "
    },
    {
        "start": 3125.02,
        "text": "has you know this one has our package father has our Kodama thank you any other questions and give our minds good to see you [Applause] "
    }
]