so welcome everyone i think we are at the hour so we can go ahead and get started um welcome to the tools and technology seminar series uh it's occurred to me that previous weeks i've never introduced myself so if anyone doesn't know me i'm mercy brandenburg i am the facilitator of the seminar series uh if anyone is interested in presenting next semester i am currently scheduling so i'm always happy to take volunteers and also recommendations for others so um not a big crowd right now in person uh we may have some come in after the seminar starts we'll see uh i imagine we'll also get some more people logging online in the next few minutes um if you are online you have questions please feel free to put them in the chat box or you can always meet yourself too we'll be paying attention to the chat box so uh with that i'd like to turn over the presentation to our speaker today's uh speaker is dee wong and from biostatistics thanks thanks marci uh hello everyone um i'm d wong i'm a first year phd student in the department of biostatistics and today my presentation is about a newly developed data integration framework in soil analysis which is called cal back level based discrete ratio risk models for integration of published prediction models with new time to event data site and this work is co-authored with doubters when he randall sang huijin jeremy taylor and kevin he i will start my presentation with a very brief introduction and overview of the field of survival prediction and data integration prognosis prediction is one of the important topics in slow analysis in clinical studies prediction models are very useful tools for risk or evaluation and large-scale databases have been widely used in the development of prediction models due to their massive sample sizes however those databases only include traditional clinical factors with limited predictive abilities so that uh but uh recently we have uh we have the new techniques help us to collect those high dimensional risk factors uh such as the genetics of the ehrd datas which can be used as potential predictors in the prediction model however those high dimensional risk factors are often collected within a small set of subjects so if you if you just build a prediction model based on such a small sample size uh our production model may suffer from low signal-to-noise ratios or high-dimensionality so to improve the prediction performance on search of limited sensor data some new techniques to incorporating information from external large scale studies is needed to be developed recently most of the data integration methods they were developed for the generalized linear model citing and indeed there is really network has been done as well analysis as a motivating example i would like to mention the study of end-stage renal disease understand renal disease which is also called esrd it is the last stage of the chronic kidney disease and it represents 7.2 percent of the entire medicare budget in the united states so uh the kidney transplant is the preferred treatment for esrd however the demand for existence supply and only 2.9 percent of instant srd patients receiving a kidney transplant in 2017 and even if within those patients who received the kidney their transplant can filled after a while after the transplantation and there are some studies show that 20 of the kidneys recovered for transplant are appropriately discarded which means those patients they are experienced or post-transplant failure that's to optimize the treatment strategies for esrd patients one important aspect is to increase the outer organ allocation frequency efficiency and also reduce the post-transplant group failure rate there has been many prediction models developed for post-transplant outcomes for example there is a model called estimated post-transparency our model which in short is ebts model eps model was derived using the srtr keynes transplant data set it has a very large sample size that's a limit that uh this model is developed based on a subset of the predictors and previous external validation results showed that actually the ebts score has limited discrimination with the syntax of 0.69 so to improve the prediction performance new methods for integration or to incorporate those external uh uh external prediction models or from large studies and with the newly collected dataset together to improve the prediction results as needed to be developed uh that is our motivation to work on the data integration problems and here we we can have take a look at a very simple disengaging example so this figure shows that uh the problem to integrate into internal data sources with an internal data set so basically the data integration method helps us to develop an integrated model based on the information from both of the external data sources and the internal data however this problem can be communicated sometimes because in reality the underlying distribution of the internal data can be very different different from the what we have in the external data sources and sometimes we cannot get individual level of external data so it's hard to summarize those information from those individual level data and besides um those data resources can be very different you know in the sample sizes or in the field sizes or even they could include different set of predictors so if we just trivially just combine those data together uh most of times it won't lead to a very a good prediction performance so that we need to develop a bunch of determination methods as i just mentioned most of the data english methods were developed in the jm settings conventional integration method they required external individual level data to be available but due to the restrictions on the uh privacy we directly sharing the external sensitive individual level data is restricted so many problems uh they only develop their data integration method based on the they only have the internal summary information however for those distinguishing methods they uh they often have a very strong assumption that the underlying distribution are exactly the same on the external population and the internal population however in reality that a data assumption is validated for most of times if it's just ignoring these differences or cross populations uh that can yield a substantially best parameter estimates and it can it can also lead to a loss of efficiency in the prediction performance recently several studies have already relaxed this transportability assumption uh for example uh this uh this author uh they developed an implementation information based on my intelligent method under the gm setting but even for their method or they have some still have some very minor assumptions on the distribution because they use the mutation method and their method sometimes it's hard to extend to a situation that we have a very enriched predictors to be used and later on most of the data integration methods stay conditioned or they can just consider the situation that we have multiple external data sources so basically uh under that situation the most challenging part to for the method is to adjust for the heterogeneity heterogeneity among different data sources uh if we just ignore their differences uh can lead to misleading prediction results so again there are still several status having developed under jam settings to accommodate this heterogeneity for example they could adjust for this heterogeneity by assigning luxury to more compatible ethanol studies however all of the methods i have just mentioned they are developed for gm settings really little work has been dinosaur analysis that is because uh install analysis we have sensor outcomes which is very different from other data times we have for for the prediction models for example for sensor outcomes we we do not know that within the observation window we do now know that uh whether that subject will have an event after it after the observation so we cannot observe the full pro full probability distribution of that outcome uh so it is always to more complicated to uh build prediction models for soil analysis and there is one method which employ the idea of static regression in the cox models which basically treat the prediction of the external model as a predictor variable in the internal model however because this method it highly depends on the quality of the external model so sometimes it is prone to model misspecification and sometimes it is it will lead to the overfitting issue uh before we move to the master part i would like to uh highlight some aspects of our proposed data english method uh in the industrial analysis so first of all our proposed english method uh use a discrete time to run model as the as the underlying working model and our goal of the of our method is that we want to incorporate the summary level prior knowledge gathered from external resources within the internal study and and we and we need to propose a very novel time dependent care discrimination information which could help us to measure the disparities between different discrete soil models and used as kl measure we could accommodate the heterogeneity across different external resources and our method it is robust to the model miss specification and our model is also robust under the violation of the transportability so to introduce our method i would like first to mention some basic notations in the slow analysis uh we use i to denote the index of the subject and because we are we consider the discrete time demand models which means that the runtime they are observed on discrete scales so for our time here we consider the use tk to denote the distinctive runtime and um we use ti to denote the time of events and use ci to denote the sensor in time and again we use x i which is the minimum of ti and ci to denote observed time and we use this ci to denote the covariate vector this lambda function it is or denotes the hazard function at time tk for the patient i before i uh introduce the general formulation of these discrete time dry models i would like to mention some reasons that why we choose these discrete temperature models as our underlying working model the first reason is that in clinical studies event times are often observed on a discrete scale so it is a very natural choice for us to use the discrete time time to run models to model those discrete events and the second reason is that this cruiser model it has an advantage that it won't cause problems with tests a test means that more than one subject have exactly the same observed time uh if if we if we model those uh tightest subjects in another continuous models probably we need to employ some estimators to adjust for those test subjects however because in the disgraceful models our time is naturally discrete we we do not have any problems with those tag subjects and the most important reason is that we could observe a full probability of distribution under the discretionary models and the hazards of the discriminant models can be formulated as conditional probabilities and i will revisit this point in the further development of the time dependent kl measurement so i will introduce the general formulation of the discrete times branch models here again this lambda function is the hazard function and if we consider a general formulation of this hazard function with a g as a link function uh we could very easily write out the log likelihood of this discrete time divided models and again we will use g to denote a link function i will introduce more more information about this link function in the next slides and in the discrete time rebound models actually the coefficient they consist of two parts one part is we called eta it is the beta hazard and another part is called beta it is the uh it is the conventional um coefficients associated with our queries and here we use y i y t k to denote the at risk indicator which means that if y k equals to one which means that this subject i it is at risk uh at this time point t k and we use delta i t k to denote the dash indicator if delta i t k equals to one means that or this subject i uh or experience an event at time t k uh if it uh event equals to zero means that there the subject i did not uh experience anyone at time tk um so i would like to still give some remarks of the discrete time t1 uh first of all as the discretionary models they can pro provide the full uh probability distribution of all of the subjects and it can also provide a very natural estimate of the patients of our distributions so that we could uh use it as as an uh as as an underlying working model to integrate others or probabilities from external models into our current study study and the second point is i would like to give a more introduction about the uh this link function g uh we have some common choices for these link functions in the discrete timeline models probably the most common choice is the logic link which will lead to a discrete logic logistic model and for our future uh simulation uh settings we will also use this logic link as a example link function and another choice is log link log link will uh these two disquality risk models um the log link is uh is appropriate to model the recurrent event in the slow analysis and the last link function is called complementary log negative log link this link function will lead to the grouped relative risk models and here is an interesting fact that if we consider the complimentary log native log link under the discrete time 21 models the coefficients associated with the covaris which is the beta and it is it is actually equivalent to a cause model if we if we group those uh continuous time into several intervals so so so because we have this underlying connection between the uh group reality risk model and the cox models so if we want to integrate uh install information from a cox model it would be it would be a very uh uniquely a pro or perfect choice for us to use the complementary loving acting longing the most important part of our methodology development is the cal back leveler discrimination information um i will first introduce the classical cal distance so the classical distance it is a measure of disparities between two probability distributions if we consider two probability distributions p and q the qr measurement can be formulated use this formula which is basically you know start summation over all of the subjects and this one is the uh distance between between the two uh probability distributions and this kind of classical chao distance has already been used in the development of data integration free data data integration methods under the glm settings for example uh this is a method called kl boosting which i'll use the kl distance to incorporate external information to improve the classification of banner outcomes and another method is called parallel so it also it is a cal based lasso approach to integrate install information to improve the variable selection for generalized linear models however although we have already uh have some method developed based on how care measurements in the field of determination we cannot apply the classical occult measurement in the survival data because again in through analysis we have sensory outcomes and for those sensory outcomes we cannot observe the full probability distribution however the kl measurement the classical cow measurement it is developed based on based on the full probability distribution so so we can now just directly apply the classical health measurement here and if we want to use this uh the scale measurement in our methodology development we have to define a time-dependent kl measurement uh in a you know uniquely in the direct you know discuss models however there there is a very useful fact that if we consider those subjects the observations in the discrete temporal models actually all those observations can be viewed as a sequence of ballooning trials and using this this very interesting fact we could propose a time-dependent kl measurement for discrete time one models um so again uh as i mentioned that the hazards in the discrete time models can be formulated as a conditional uh probability so our so our development is based on that the conditional on on this uh this subject is at risk at this time this visit this specific 10 point tk um so this uh to this formula is the time dependent kl measurement at time t k for subject i um this or this formula can be can be viewed as two as a combination of two parts the first part is that we conditional on this subject i is uh at risk at time mtk what is the kltl measurement uh if this subject or indeed has an event another part is can conditionality subject as risk what is the kill measurement in this subject do not have a um event at this 10-point tk so our so our the proposed time dependent count measurement is a combination of these two situations uh and and indeed this can this time demand and count measurement it is a natural measure of the disparities between two header functions p1 and p2 at this specific time point decay and i will also give a more intuitive interpretation of our proposed time dependent care measurements for example if we consider this p1 p2 uh we consider that we use this delta ik to denote a time wiring binary outcome for subject i i time t k which is basically conditional conditional that the subject i is at risk at time decay uh that the indicator of the subject i indeed had an event at time ticket uh actually uh our proposal time dependent count measurement uh it is a measure of divergence between the two probability distributions of a time varying bandwidth outcomes the p1 and p2 as would we as we just introduced uh as we just defined by this delta ik so um so this gives us an interpretation of the newly proposed time-dependent care measurement as our classical care measurement uh before i introduce the uh cal based integration framework i would like to briefly mention proposition that we proved so here still consider the p1 corresponding to an external model and p1p2 can correspond into our internal discrete timetable model and this data it is the parameters associated with this internal discrete time event model um if we ignoring the terms now involving the theta actually the cal measurement uh we just defined at time tk for subject i between the historic historical model and the current model uh it is proportional to this logarithmic code of subject i times uk which is the local likelihood based on the predicted outcome for something i had at mdk um using the data in the internal model and using the parameters from the historical model so based on this uh this proposition uh we could uh very uh naturally to define a cumulative kl measurement which is basically we take the summation over all of the subjects and over all of the distinct time points um take this and this one is the is the likelihood uh from our proposition one and then with this cumulative care measurement we could define our objective function which is a hydro formal for penalize the log likelihood so the first part of this uh penalized log likelihood it is the log likelihood based on the internal model and also the internal data side and the second part it is it is uh it is based on the cumulative count measure and as as i said this is a penalized likelihood um we use the care measurement as a penal pin uh penalty part so we need to uh manage this kr measurements our goal is to maximize this level likelihood and together we want to minimize the cal measurements which means uh we want to uh if if the internal model and the install model they are very similar we want to borrow some information from this term model to improve the prediction of the internal model if they are very different we do not want those misleading information from this the external model to uh um to lead us to a misleading prediction results and islam that is a tuning parameter help us to uh control the relative importance of the external model and the internal model and this lambda will be very large if if the two models are very similar and of course it can be very small it can cell to be zero if the two models are very different so this lambda can be selected by cross validation so again uh this is this our lambda theta is our penal likelihood under some duration this uh our lambda theta is proportional to this following objective function uh which if i take a look at this function it actually has a similar formulation as the standard disgraceful models so all of the method that can solve the discrete standards crystal models can be used to solve this integrated model uh yes with this objective function and what i just introduced it is to just invert one external model to our internal dataset however our method uh can be very easily to extend it to integrate multiple external models into our internal data sets so if we consider a situation that we have several external models uh suppose we have our external models and the pinot noir electrical can be defined as steel may we have the local angle from the internal model and again we have we could uh manual the summation of several cal divergence uh from those from each of the cell model and our internal model and again we have this penalized log likelihood and we want to maximize this level of electrical waste the condition that we want to minimize this kl measurement and again the lambda here is still the tuning parameters this this lambda m here could represent the relative importance of the of each install model and the internal model and it can also control the relative importance of different or external models um and and this pinot noir likelihood can be sealed uh to a proportional to this uh standard standard uh discretionary models and all of the methods to solve this uh discretionary models can be used to solve this um objective function so uh before i go to the numerical results part i would like to again to review the our proposed framework for the scale-based integration so uh the input we need is an internal data set we have from the current study and several external models that derive it can be like previously published models it can also be the models prediction models developed based on external resources and based on this input we could have log likelihood uh based on the internal data and also of the internal model and we can also have several kl measurements between each of the external model and the internal data sets and here we define this penalized level likelihood to combine the two information uh from the internal model and the care measurement and we use okay we use the cross-validation to define this uh tuning parameters and we recompute the model using uh using a standard uh standard uh estimation wave for the standard discrete time two models and finally we get our integrated model and we use we use this integrated model to do the further predictions um for for this internal data for for future data size uh i will briefly mention some simulation results uh we have uh in in this method so here in the simulation we consider a setting that we have very limited size internal internal data which is only consists of 350 subjects and we have a total of 10 predictors and the internal model it is estimated from a very large external cohort simulated under the same distribution of the internal data with a very large sample size of ten thousand and uh yes ten thousand and and then uh we assume that observed external samples uh make only canadian subject subset of the covaries in the internal data set which means our observed external samples and and the models derived from the observed itself samples can be misspecified and the prediction performance it is evaluated on an independently simulated testing data site and this this testing data is simulated from the same distribution from the internal data and it also has a same symbol size with a with this very limited sample size and we use predictive events as our evaluation matrix predictive givens can be defined as the negative long-lasting code under the discrete soil models for binary outcomes on the first side of similar settings it is for the situations that we only have a single external model um scenario a mimics the situation that uh our already slow model it is indeed a very ideal model that we can see all of the information we have in the internal however the scenario b and c mix meaning the situation that on the install model only contains a subset of information uh of what we have in the internal data set so those two install models only provide partial informations uh here um again we use the predictive device as our performance evaluation metric and here from the left to right uh the first one is the proposal kl method and the second one it is we just directly apply the prima the installer model to our current internal data set and the second the third one it is the local model which is basically just derived directly or a discrete time prime model derived directly from the internal data side and the third the last one is called stacking second model and as it applies the static regression method i we introduced in the simulation or in the introduction setting so under all of the three scenarios um we could we could see that actually our proposal method uh it uh it achieves a a compatible uh performance of better performance than all of the competing methods and and if we take a look at this tuning parameter actually under the scenario where we have very ideal iso model our model tends to select a very large uh training parameter and under other settings that we have or we have a uh it's a cell model that only contains partial information our method tends to select a smaller tuning parameter and to adding a smaller weight to the external information and the second size of the simulation or the second set of the uh simulated settings is we can see there are scenarios when they have multiple user models so again the scenario uh it mimics the situation that the both of the external models they are very ideal models that contain all of the information we have in the internal data set and enf they are the install models they only they only contain some information and and for the for the scenario i've actually this is the situation that even if we have two recent models our internal data set still provides additional potential predators for the for the prediction so for for these settings uh with multiple small models again our kl method still performs uh performs a harder comparable performance or have the better performance than all of the computing methods and for the tuning parameter or for the selection weight lambda um it we still uh our method still has a preference for the ideal or for the better or with the idealistic models and we will assign a smaller rate for those for those install models only contain sub-information of the internal data side and then i will introduce a briefly introduce uh application setting so again the conditional um we can consider on the study of kenyan transplant um uh the some model we use is the ebts model we introduced in the introductory section and the internal dataset it is a kidney transplant dataset uh we obtained from a local hospital and we only have have like four 498 subjects in in this data set uh for the patients who receive the kidney transplant between january 2011 to december 2015 and affiliate time is defined as as the time from transplantation to where failure or death and here uh we can only consider the first year survival which means uh all of the subjects will be censored as the first year of their transplantation and the overall sensing rates are worth uh 91 which is a relatively high uh substitute rate if we directly build a threat a prediction model based on this limited size prediction of lincoln data it will lead to a very pro production performance and because that the epks model it it was derived from a cox proportional headless model and as we mentioned that the complementaries log likely without link uh it is uniquely appropriate to integrate extra information from derived from cost model to the to always uh or based on discrete types of models here in this application we apply this c log log link this table shows the results of the application uh application example so again we use predictive demands as the model for as the predict prediction performance matrix and this ebts is our external model this this discrete r model is our internal model build based on our internal data science and this kl model is our proposal method and and and it's very intuitively that our proposal care method achieves the best of performance among all of the competing methods and at the end of the presentation i would like to again give us some remarks of our proposed methods so first of all um our proposed klk integration framework could incorporate multiple external data sources and we could allow allow for heterogeneities across those external populations and our method could also accommodate the situation that external status may only provide partial information on subsets of the potential predictors in the internal disk in in the internal study and uh we propose a k or time dependent care measurements to help us to control the relative weight of the internal information and our our model is very robust to model mean specification and we could incorporate various types of internal prediction models if we recall our integration framework what we need is just a prediction uh it's just a prediction based on the install models so as long as the install models could provide a prediction based on the intel dataset we could use that as other installed information to help us to improve the prediction performance and again because we only use the sony prediction models we do not require any sensitive sensitive individual level data to be obtained from the external data side and for and what what we just introduced is just a fundamental chao integration framework for the low demand for the load manual setting we will assume that we will use all of the predictors we have in the internal data set however in reality we may have more communicative situations for example for heavy metal settings we have already extended our uh integration framework uh two you are based on a boosting algorithm to a as a chaotic variable selection for disgraceful models and for other situations where we we consider the company complex interactions and timeline events we have developed an extension called care based on functional gradient boosting trees for disgrace or models and for our future work uh we would like to extend our proposed cal integration framework to continue type scales probably we will uh develop that innovation framework based on house models as our online working model and here are some references i mentioned in the slides and thank you very much for your attention and i'm very happy to take any questions that you may have if anyone has any questions uh you can put them in the chat box or unmute yourself and ask we'll just give it a few minutes in case anyone thinking of their questions or spending a little time typing or if you have any questions you can send me an email after the presentation that is also okay the message is thanking you for interesting talk oh thank you maybe let's give it one more minute in case anybody you know mute and ask me questions or put any questions in chat another just uh thank you for the presentation uh i'm joining the chat box thank you but this time i'm not seeing any uh anything else in the chat box or questions and nobody i'm not hearing anything about themselves so i think we can end the presentation so i want to thank you yandi for taking the time to give this extra presentation today thank you everyone for your attention and thanks marci for calling this seminar