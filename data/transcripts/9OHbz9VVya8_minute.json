[
    {
        "start": 0.03,
        "text": "you're way back and Nicholas grew up here in town and you know he graduated from Erie with the Kalamazoo College but then majored in all things philosophy got degrees in philosophy but made his way to epidemiology in public health and Mike banky who's out of town today what's his PhD thesis advisor and then you know he entered the academic track and started off at Case Western Reserve and in Cleveland and then started migrating for the West Coast and he's got lots of different positions here at the West in the West Coast now you know with craig Venter at the J craig Venter Institute where he's a professor and director and oversees the next-gen sequencing kind of activities there which was really saying something giving what they're doing with Illumina he's at T January has an appointment there the professor you know he's been involved with the human longevity Institute interesting story there "
    },
    {
        "start": 61.41,
        "text": "he's been at Scripps you know he was with I think many of you know he was with Eric Topol but at the Scripps translational research institute before he went to work with Greg he's adjunct in several departments including psychiatry at the University of California San Diego and he's a great friend and he's a visionary in terms of how we might develop in what he will be talking about today are a new class of clinical trials they're adaptive they're statistically reasonable and they're a personalized you know I think I think I pretty much summarizes the tide of your talk and it's just wonderful that bad great without even looking okay good wonderful to have you back well it's a real pleasure to be back in as Brian kind of alluded to and as my father and Pat know all too well sometimes I have like lots of material in my talks so if I go on for a long time Pat without the "
    },
    {
        "start": 123.24,
        "text": "hook and just get me off the stage as you've done many times in the past so so here are the topics that I want to touch on today and I have maybe five six slides for each one first talk about how one goes about identifying variants that influence idiopathic disease and how one might be able to design studies around treating people based on the insights obtained from identifying genetic variants then I'll talk more specifically about the design of single subject clinical trials this has been a growing area of interest of mine and in particular talk about not just the motivation but the design of those trials and have examples the trials we've actually pursued then talk about cancer genomic trials and this concept of vetting algorithms versus vetting compound this is an emerging theme in oncology but it comes with it all sorts of repercussions for the design and implementation of trial also "
    },
    {
        "start": 183.52,
        "text": "touch a little bit on my exchanges with the FDA about the design and implementation of the trial then talk about actually monitoring people for early signs of disease and the need to pay attention through personal thresholds versus population threshold this concept is very intuitive but it's not often exploited and kind of preventive medicine studies and then I'll just close with a comment on how epidemiology shapes personalized medicine and personalized medicine in shape an epidemiological study so for that some financial disclosures as Brian knows I also have my hands and a few companies the other thing I'd like to point out is I'm actually on the external Advisory Board Commission are here so I come back every now and then and review the programs in translational science here really flattered to have been asked to be on that board okay so what is it about precision "
    },
    {
        "start": 243.79,
        "text": "individualized to personalized medicine that makes it so interesting in this day and age well not so long ago there was an editorial in Nature Biotechnology asking whether personalized medicine was finally arriving and there was a lot of buzz that surrounded the introduction of personalized medicine into the into the community so much so that textbooks were written my hero former chairman of mine Willard wrote a textbook and talked about the need for med students to be aware of emerging themes and personalized medicine ironically four years later Nature Biotechnology published another editorial what happened to personalized medicine so one has to what was the deal what I believed happened is there simply wasn't enough attention paid to how one actually vets personalize that show that it actually leads to better outcome and traditional approaches to practicing medicine so it's in this context I agree "
    },
    {
        "start": 309.84,
        "text": "with that 100% it's getting buy-in from the funding agencies so at any rate it's in this context that I want to know go about getting the rest of my presentation well first talk about identifying variants that cause idiopathic diseases and filtering strategies that are used to identify the variants that one might build some insight around to craft an intervention or someone with disease but either pictures of children mostly that have had idiopathic diseases and for those of you that don't know what an idiopathic disease an idiopathic disease is simply a disease that defies easy diagnosis every hospital system on the planet has some number of patients that come into them that present some kind of mystery to the physician and they typically get passed around from clinic to clinic position to position to see if some insight can be obtained what is actually bothering these people and here I'm not "
    },
    {
        "start": 370.95,
        "text": "talking about diseases like fibromyalgia and other ones that have kind of vague neurological symptoms I'm talking about severe debilitating condition of the type that said Nicholas Volker had had a hundred or so surgeries before he was five years old with serious gastrointestinal problems and no one knew actually what was going on what was ultimately done to these people in particular is their genomes with sequence the idea being that if they could identify a variant that showed some kind of defect that was sistent with the molecular physiologic problem they could explain their condition then maybe they could craft an intervention around that insight well all these people have had their genome sequence with the exception of one there have been treatments fabricated around the identification of the pathogenic variant I've had the pleasure of working with Lily Grossman here and I'll explain a little bit about my experience sequencing her genome with my colleagues "
    },
    {
        "start": 431.43,
        "text": "and coming up therapy that showed some benefit to her so the real question is how does one identify the variant there it's possible to these idiopathic conditions you're true in of one studies in fact the assumption is that the variants responsible for these people's diseases must be unique to the genomes that these people have so think of de novo variations that these people possess that cause disease now why is this the case well if in fact other people on the planet have a similar condition then this person's disease would not be idiopathic could be shared with other people so if one makes the assumption that the variant responsible for the disease that these people have is unique to the genome of the individuals then you should not see those variants in any other gene so as part of a filtering strategy or a strategy for identify the variants responsibility one could sequence the patient's genome look at variations in "
    },
    {
        "start": 493.77,
        "text": "those genomes compare them with other genomes say referencing and only those sites that are present their variants that are present in the patient's genome are candidate for being the pathogenic variant now there's a little bit more to it than that because each of our genome says anywhere between five to ten to fifteen thousand unique probably de novo of variants so the question is how could you filter them even further who may be stumble on the offending or pathogenic variants well the idea is to leverage annotations of one sort or another kind of annotations about what the nucleotide substitution might actually do so what one could do is throw out all the variants that aren't novel or unique to the patient's genome and then ask questions about the remaining variants and what they might do in impacting whatever functional elements they might reside in so you could ask whether or not say a coding variant that is unique to the patient's genome actually damages "
    },
    {
        "start": 555.839,
        "text": "the encoded protein of the gene it sits in to a degree that might explain the disease or if the variant sits in a micro RNA binding site is it likely to disrupt micro RNA binding to the point that it brings down the functioning of a particular gene or system but what one can do is is simply filter down all the variants in the individual genome to hopefully arrive at a pathogenic variant now this is not trivial but it has been done and done with some success as I had alluded to earlier so I'll just talk about the strategy for making claims about the functional significance of these nucleotide substitutions to populate the genome that people with idiopathic disease and then talk a little bit about how one decides whether in fact they're novel and what kind of reference databases are needed to make the claim that variants are novel so here's just a cartoon of a gene that they stole from a textbook while ago and "
    },
    {
        "start": 618.63,
        "text": "it just has all the elements in and around genes that if disrupted might cause the gene to dysfunction and with current computational tools it's possible to predict whether or not nucleotide substitution in these various functional elements likely damage those elements now this is not an exact science but it's better than a flip of the coin and you might ask the question well why does one have to rely on computational pool well it goes back to what I said before there's probably too many variants to subject the kind of functional assays and they claims about whether or not those variants actually do upset molecular physiology to the point of leading some kind of pathology so suffice it to say the community as a whole has been developing tools to annotate variations that could be used in this bill current process my lab over the years is kind of contributed to the literature but many many other groups "
    },
    {
        "start": 679.91,
        "text": "have engaged well and had some success being able to differentiate variants that are likely to damage the function now once one has a variants that look as though they might be a functional significant now you also have to couple that with which sets of variants are novel to a particular individual genome so on take a step back and ask what kind of diversity that humans show at a functional level in their genome on a global scale so some years ago in collaboration with complete genome ik which was the only whole genome sequencing player in the community we sequence the genomes from individual to different populations around the world we took all the variants and subjected them for those computational tools that I described to assess the functional diversity of the variant on a global "
    },
    {
        "start": 742.14,
        "text": "scale hook and like these didn't carry over too much but these lines shouldn't be there but I think we can work our way through this so we had African populations European populations Asian population and admixed population on the y-axis here is just the number of derived alleles so these are the number of variants that we identified in these individuals different genomes and these block bok-bok just represent the average number of variants that populated those genome and as you'd expect we saw a greater frequency of variants in individuals with African descent and this is consistent with what we know about the founding of the human population and where Africa is the most diverse population on the planet now since there is a greater number of variants in the African individual genome there's a greater number of likely damaging variant variants that "
    },
    {
        "start": 804.44,
        "text": "are likely to be a functional significance greater number of variants greater number of likely functional variants however if you look at the frequency of homozygous variants in the population then despite the fact there's a greater number of variants in the African population there's actually a greater number of homozygous variants in the non African population you might ask yourself why is that the case just test a little bit to do with migratory patterns and the effective population sizes of the non African population so after the migration Out of Africa a smaller subset of people that knows living in Africa at the time made their way into the Middle East and ultimately up into Europe that led to a smaller effective popular finding population size in those populations which led to a higher kind of random in greeting rate which increase the levels of homozygosity so again this is nothing that I discovered many people know about "
    },
    {
        "start": 864.5,
        "text": "this but we've been able to catalog this on a genome-wide scale so again greater number of variants in African populations but a greater number of homozygous variants in the non African population as a result of there being a greater number of homozygous variants in the non African population there's a greater number of likely damaging homozygous genotypes in the non African population in fact if you just look at all the variant category functional variant categories it's kind of consistent with that cartoon I had showed earlier you can see that there is kind of a depletion in the non African populations either negative coefficient means there's less of a frequency per genome of these variants lesser frequency of likely functional variants in all categories in the non African population however if you look at homozygous genotypes there's a greater frequency in the non African population so this pattern is not confined to say coding variants or promoter variants but "
    },
    {
        "start": 925.07,
        "text": "variants of all types that might actually impact gene function and these results are all highly statistically significant and you encourage you to look at that paper if you're interested since the publication of that paper have been many other papers have shown similar patterns including those from groups working with the thousand genomes data and other data sets now what does this have to say about that filtering strategy well it's pretty obvious if you're a patient say is a European it sent but your reference population is made up of individuals of African descent then you're likely going to claim that a variant in that Europeans genome is novel when in fact it's not it's just you're comparing it to the wrong reference population right so this is intuitive given everything that I said so these little plots here just show the number of false positive leads you might have in trying to determine what variants are novel in the genome of "
    },
    {
        "start": 986.51,
        "text": "a patient with an idiopathic disease if you use the wrong reference population and of course we could turn this around and ask well what if our patient was of African descent and we only had a European panel to compare his or her genome to so the moral of the story is you'd need a appropriately diverse reference panel and again this seems obvious but in the context of this filtering strategy nobody had really ever shown them so you know not that having one or two additional variants that you might have to consider as a pathogenic would be a bad thing but if in fact you've got hundreds of the dish variants that may or may not be candidate to the pathogen that created problem okay so it's an one other thing that he is very important for these filtering strategies is the attention that needs to be paid to phase information so I don't know how have you familiar with phase but when we sequenced genomes with today's sequencing technologies we don't get phased information by default now what is phase information well it's the "
    },
    {
        "start": 1047.29,
        "text": "nucleotide content of the two chromosomes the one we inherit for Mom and the one we inherit from dad typically with sequencing technologies that are available today you kind of get insights into whether or not a position in the genome is homozygous non-reference or heterozygous but you're not actually given information about what variants were inherited together on at chromosome that came from mom or dad in order to establish that those kind of haplotype you'd need phase information you don't get that by default now why is phase information important especially in the context of searches for variants that might explain idiopathic disease well here's a little cartoon that I think could bring this out so let's say there are two neighboring sites in an individual genome where the individual is heterozygous and say the bold nucleotides there are known to be damaging they disrupt the functioning of the gene they fit within or around since there this person is doubly heterozygous "
    },
    {
        "start": 1109.9,
        "text": "we wouldn't know whether the following situation occurred whether or not both of those damaging variants were inherited on the same gene homolog such that we had one doubly mutant form of the gene and one wild-type or whether or not one of the damaging variants they occurred on the paternally derived version of the gene and the other one was on the maternally derived version now why is this important well it could be that the wild-type version of the gene can make up for the two mutations on the other one and lead to kind of normal function and be a compensation or if they're on opposite homologs both copies of the genes are leading to a situation known as compound heterozygosity and many rare diseases are known to be caused by compound heterozygous combinations of variant so you need phase information that might require you to say sequence the parents "
    },
    {
        "start": 1170.78,
        "text": "or leverage phasing approaches of other sorts in your strategy so this is very important and some time ago my students and I had a review on the importance of phase information up and under look by contemporary genome sequencing so how did we apply this well lily Grossman the young woman who had the idiopathic condition that I mentioned whose story has written up in National Geographic and then recently the Atlantic was had a idiopathic condition had been confined to a wheelchair for the last 16 years no one quite knows what's going on suffers from very severe nighttime tremors so bad that it wakes up the parent in the middle of the night they have to go running into the room to a very brutal condition that she suffered from so we seek Lichter genome the two of my former postdoc colleague laura Cavani and cinnamon Blas now both faculty members grip the SP and we subjected the "
    },
    {
        "start": 1232.1,
        "text": "variants to this filtering strategy finding those that were novel from those that weren't or likely novel and then using the annotation tools that I had described to kind of narrow down the number of likely candidates for explaining the pathology and we did identify a number of situations where Lily was compound heterozygote where the two non-reference variants were likely to damage the encoded protein but there was one that made more biological sense than other than that had to do with variants in this teen 80 ty5 with some colleagues we subjected these two functional assays and it looked as though the ad cy5 mutation was a good candidate for it planing Lily's pathology so on that basis we then ask the question well what could what drugs were known to target a TTY v and which might "
    },
    {
        "start": 1293.32,
        "text": "compensate for the problems induced by the particular mutation that we had so we've scoured the literature and identified a couple of studies that had looked at a knockout of the ADC y5 gene and mice that hadn't been hit with different drugs etc etc and on that basis settled on a particular drug that we thought might be of benefit to Lily with respect to kind of decreasing the number of severe nighttime tremor that she was showing so we outfitted her with a device that could measure the number and intensity of the tremors that she was having at night and then we gave her the drug to see if we could make a difference and lo and behold if this is the number of Kersh she was suffering from we initiated the drug and they kind of went away now sadly this was initiated about a year and a half ago and since that time she's shown some resistance to the drug through the "
    },
    {
        "start": 1354.82,
        "text": "tremors are coming back but we had a short live success in identifying a drug that seemed to benefit her on the basis of identifying the pathogenic mutation and then going through the literature to see if we could identify a drug that might benefit her now since the publication of her genome and those insights and ADC wive commons a research group has been started and some 30 individuals worldwide have been identified that actually had a TTY v mutation so not only do we think we discovered the pathogenic variant but we sort of kicked off a little campaign to find other people that suffer from this very debilitating disease who are likely to have a TTY v mutations and who could benefit from an effort in repurposing drug therapies and testing them out now in conducting this study I mean we didn't need any statistics to really make the claim that the drug worked here but oftentimes that's not the case and we were lucky that the drug had the "
    },
    {
        "start": 1416.289,
        "text": "dramatic effect that it did and Lily so the next question is how could one design trial it might bring out effects in a more sophisticated way so recently I had an editorial in nature on the need for designing these trials but this grew out of conversations that I've had for some time with the FDA about designing trials that are more efficient either by leveraging genetic information or some way of selecting patients that are more likely to respond to the particular drug and ultimately designing trials that really focus on the patient's well-being not necessarily on the drug or intervention what I mean by that is many trials are done can make claims about the utility of the drug and they're done on a population basis right you give you know 5,000 people you know your cholesterol-lowering agent and 5,000 people you know placebo and then you see if the average reduction in cholesterol is greater in the active group in the placebo group but what you're not doing is really asking the question whether or not any one "
    },
    {
        "start": 1477.97,
        "text": "particular patient benefited you're making sweeping claims about the utility of the drug in the population at large and that kind of flies in the face with this contemporary emphasis on personalized medicine so hence the the request from the nature people for me a very flattered have been asked to do this to put in a little piece on the need for individualized trial well then how does one design these studies well it's it's pretty simple in orientation not so simple in implementation and here's how it goes let's say we have some measure that we're interested in affecting by a particular drug such as blood pressure well we can measure a person's blood pressures then we'll give them a particular drug and see if their blood pressure dropped maybe take them off that drug give them another one do this enough so that the end of today we have enough data to make an objective claim about whether the drug is what's possible for the reduction in that patient's blood pressure and again this is not what is often done in large the LP free trial where you might get by "
    },
    {
        "start": 1540.12,
        "text": "with one or two measures on many thousands of people so think of this is many thousands of measure and a small number of people rather than one or two measures on thousands of people right and of course you could take another patient to pursue the same kind of trial and them and maybe they responded to a different drug at a little review or five years ago on now the design of these trials can leverage all the kind of statistical technology that goes into the design of large-scale Phase three trials a traditional trial so you can randomize the patient to which drug they get at which time you can blind the patient to what drugs they're on you can have washout period fetal controls they can be designed in a sequential or adaptive manner you can look at multivariate phenotypes there are some data analysis issues probably the most pronounced that I'll talk a little bit about the de not meant to be a P's and Q's talk but a little bit of that serial "
    },
    {
        "start": 1600.24,
        "text": "correlation between the observation needs to be taken into account there might also be carryover effects and a whole slew of other issues that one would need to be sensitive to in carrying out the trial and there of course lots of extensions with I'll punch on briefly these studies have been pursued and if you go through the literature you'll see that many n of one studies have been initiated and had IRB approval for and everything else that are documented in the literature and we'll go through these just suffice it say that people have leverage subject you're in of one trial with some varying degrees that looking at a whole slew of condition now here's the most bio statistically oriented part of my talk talk a little bit about the design you can ask questions well how many measurements you need to be able to make a claim that a drug is actually working better than another drug do you need run in periods washout period etc etc so "
    },
    {
        "start": 1662.539,
        "text": "what I did with one of my former postdocs in and a guy Andrew Viterbi many of you may know Andrew Viterbi he is the discoverer of the Viterbi algorithm every cell phone that people have got some signal noise filter it's probably based on the deter B algorithm have Andrew as the founder of Qualcomm so we lived in San Diego and I went to him and said well I think the venom one studies with lots of data kind of like signal practical thing so we parked at the conversation and decided we've worked together some of what I'm about to say was the result of those conversations with we had simple linear model with Auto regressive kind of serial correlation then we can look at kind of the power to detect signal to the function of that serial correlation strength the effect size the number of time points etc etc so here's a reasonable question to ask if one had a finite number of measures that they were going to make how should you break up those measures throughout the trial so "
    },
    {
        "start": 1724.25,
        "text": "let's say we had two drugs should you measure the person while on drug a and then stop them giving drug B or would it be better to give them drug a then drug B back to drug a and then drug B how would you break up the number of measures that you had with respect to what drug they're given which would be more optimal if there was strong serial correlation would say would this design be better than that design these are the sorts of questions that one could ask now what about the use of washout period would you strategically place them at certain points in the trial how long would they be but that might be a function of theta half-life of the drug or other sorts of thing or could you just let the number of observations collected while on each drug because I ran what what is the optimal that was the question I'm not going to go through this in tremendous detail just say we kind of work this out mathematically making the little assumptions about the AR process there's a serial correlation and what we found is you can make up for "
    },
    {
        "start": 1789.04,
        "text": "the strong serial correlation it might be intuitive by breaking up the big treatment periods into smaller periods or collecting more measures right so if this is the strength of the serial correlation and here is the number of periods we could either have two very long period or forty very small periods where they're on the drug just alternating back and forth between the drug all with 400 observations made with a certain effect size picked up you can see that the stronger the serial correlation would be offset by simply breaking up the number of time for this may or may not be practical depending on the length of time you'd expect to see an effect on the drug but it does give us some insight into how we can go about optimizing the design of these sorts of trials should we want to implement now in order to carry these out you need some way of measuring whatever it is that you want to measure and unfortunately in today's kind of biomedical science community there's a lot of emphasis on developing that our "
    },
    {
        "start": 1850.69,
        "text": "phenotyping protocol talked with David earlier about this he has a very big interest in this so all sorts of wireless devices could be used in the context of blood pressure there's little band-aid like devices you can slap over someone's heart the measure blood pressure 24/7 heart rate kind of worry about the reliability these instruments but suffice it to say that there is emerging technology that could be used to carry out the sorts of study right here is an example where he actually looked at blood pressure in a patient we compared two drugs for participants with recently diagnosed kind of essential hypertension so it had never been treated before walked into the clinic and showed hypertension and the question was what drug would you give him and this is often a question that plagues physicians treating people with a site essential hypertension for the first and there's so many drugs a position could choose from datablock the dates inhibitors diuretics you name it so we did a little study on a patient and "
    },
    {
        "start": 1912.94,
        "text": "rotated the drug had washout period had kind of a funky design but that had to do with logistics actually doing the study because we did want to blind both the patient and the physician to the drug so we have the pharmacy cook up pills that had that were identical in weight in color and everything else including pills that were more or less with kilos for the washout and I can imagine this was complicated but we were able to do this you did it and it looked like the patient's blood pressure went down to a greater degree while I'm drugged be here Nathan inhibitor unfortunately when we measured the way to defeat her fortunately for the patient the patient lost about ten pounds while on the drug in this period so when we treated way to the covariant we couldn't explain the greater drop in blood pressure to the drugs effects but to her weight loss again wonderful for the patient because he became more health-conscious lost weight actually normalized her blood pressure not so "
    },
    {
        "start": 1974.33,
        "text": "good for the trial because we couldn't attribute the blood pressure drop to the drug another example of an end of one study that we did had to do with addressing this issue of polypharmacy something Brian and I had talked about great detail so we had a 60 year old female who had been treated lifelong for anxiety depression and a genetic sleep phase disorder so she actually had clock gene mutations that caused her to be up in the middle of the night and not sleep well and as a result she was treated with many different drugs one for depression one for anxiety one to facilitate sleep and all those interfere with each other as is well known and documented in the literature right so the antidepressants impacts sleep but sleep aids impact mood and the hints that could exacerbate the depression etc etc so it's kind of an ugly cycle for a patient to be in so we designed a study where we test different combination antidepressant drug used to maybe enhance sleep etc etc and then we used a "
    },
    {
        "start": 2035.21,
        "text": "number of what we're available at the time devices to measure sleep restless leg activity vital sign and whatnot and we'll go through all the results you can read about it in the publication which we just put online at the Journal f1000 but we did establish times in which we're going to try different dosages and different combinations with a drug including some where she was on no drug to kind of get a good baseline and what would amount to kind of a wash the upshot oh I think so I think you know right there are costs associated with conducting these trials there's no question but maybe with the availability of wireless devices and the amenability of patients to undergo this it wouldn't be huge costs and they probably offset all the downstream costs you paid for by having side effects and all the problems associated with not "
    },
    {
        "start": 2095.51,
        "text": "giving optimal treatment so there's a real balance in terms of the economic wage but the bottom line of this study was if you looked at say deep sleep the amount of time the patient was actually sleeping restfully the more drugs she was on the worse and the fewer drugs she's on better so the take home was this is that she actually did better when she was on healer drug right which isn't gonna make the drug manufacturers happy but certainly made the patient happier now one thing that did happen is when we remove the drug we were measuring vital time we uncovered sort of a sleep apnea condition so her breathing change when she was not on the drugs if he wasn't induced into kind of some sleep and that unmasked this sleep apnea condition so kind of created a situation where indeed how are we going to go about optimally treating this patient if you've got all these exacerbating condition right this is a "
    },
    {
        "start": 2157.059,
        "text": "problem and may be the perfect setting for actually conducting these trials because her condition was very nuanced there's not everyone walking around who has exactly the same condition as the other patient but she is one who for you know 40 years of her life was trying to deal with all the clinical conditions that she in fact okay now with these end of one trials of course you can conduct them on a single person try it on someone else find out if they responded or didn't respond do this for a bunch of people then you've just brought out a phenotype in a very objective and sophisticated way once you've identified people that do and don't respond to drugs unequivocally using this data then of course you could ask what they have in common and it could be that they have certain genotypes in common or a certain omec profile and in that way in the future on the basis of the genomic profiles decide who should get one drug and not have to resort to the end of one so the end of one trials can also be used to actually bring out phenotype "
    },
    {
        "start": 2218.89,
        "text": "that otherwise wouldn't contain if you didn't go through this more sophisticated in the labyrinth now there are some people in fact Susan Murphy at the University of Michigan who is designing kind of optimal ways of conducting such trial I'll just give props to her work knowing that I'm at the University of Michigan and say these are very sophisticated and wonderful approaches conducting personalized trials okay so you guys are all familiar I don't see why you wouldn't be but you know she is really pushing for more efficient in optimal trial to benefit patients wonderful it so now bouncing to kind of the third topic I want to talk about in cancer genomic trial this concept is vetting algorithms versus vetting compounds and if they're oncologists in the room just bear with me if I have my first couple slides are really basic so this table which is more of an eye chart than I apologize read it just reflects "
    },
    {
        "start": 2280.33,
        "text": "some state of the thinking in patient genomic profiles tumor profiles harvested from patient and what drug might be the best to throw at that cancer given what was found in the patient's tumor so if in fact a woman has breast cancer and there's her two overexpression then you should get Herceptin if someone has EGFR overexpression in their colorectal tumor then an EGFR inhibitor should be given the community as a whole is developing insights into these rules that kind of match drugs to tumor perturbation right and they're growing in number in fact this little review paper by Richard Simon it's somewhat dated but the list of drugs and targets that have been identified is growing by leaps and bounds the problem with this is if one wanted to actually vet the utility of giving that drug in that condition you might "
    },
    {
        "start": 2341.06,
        "text": "have to conduct a large number of trial testing each one convince someone that that match is appropriate right and that's complicated by a number of factors now my group over the years has tried to identify a mutation that are likely to drive cancer and hence be targets and we're not alone and there's many many other groups pursue that sort of thing that we published papers over the years describing how one might be able to predict which mutations identified in the tumor might be driving tumorigenesis and hence e the target for specific therapies and we've work with different groups to you know show that these predictions actually do work out either in vitro or in the clinic now how does one actually then test whether these rules lead to better outcome than say the standard approaches to treating cancer well this emerging trial design known basket trials and umbrella trials have "
    },
    {
        "start": 2401.119,
        "text": "been proposed and there's a simple intuition behind those you just enroll patients maybe with different tumor types profile their tumors find out what the mutations are that are present in each one of these and on the basis of what mutations are present in their cancer steer them to a basket that just reflects what drug they should be given so for example patients with mutation to here are steered towards basket one patients with this particular mutation are geared towards basket 2 now what's important is you have this algorithm for deciding what basket the patients should go in there's some strategy some scheme that Maps the drugs to the patient profile and questions arrive with respect to actually conducting the trial example what is the algorithm based on intuitions the mouse model clinical experience what is it what about the "
    },
    {
        "start": 2461.209,
        "text": "fact that there might be multiple genetic perturbation in which basket do you put someone who has two likely driver mutation right what about combination treatment right these questions all arise in the context of basket trials that people conducting these trials are certainly thinking about them claiming no originality for pointing out these issues right and there are many trials like this going on many of you may know about this I'll have a little bit to say about the Shiva trial because it basically showed that the people who were given the genomically guided therapies didn't have any greater benefits people who didn't and there are reasons for that their algorithm likely flawed because it turned out that only one of the baskets that they lump people into is really filled with enough patient to be able to be powered enough to make claims about the utility of that match so their algorithm with squat that's there's many other trials including Jim "
    },
    {
        "start": 2522.48,
        "text": "which is one that I'll talk about in a minute that I'm involved now this matching on the basis of the belief that a drug target the specific driver mutation is also flawed for the reasons that they alluded to that patients typically have more than one driver mutation present their killer so it's not the case that genes work in these nice linear pathways where if there's one perturbation we just need to correct it instead of genes that work together in networks that are replete with feedback and redundancy mechanisms such that if you target one gene another one compensates bla-bla-bla with a leaper Kimani my former student postdoc we published papers that many other people have published papers showing this to be the case there's a unfortunate but well-known story about a man with metastatic melanoma who was found to have a beer at the 600 mutation in his tumor which suggested that he should get a drug called the maratha nib that patient would given BEMER after him and miraculously the tumors melted away but "
    },
    {
        "start": 2584.31,
        "text": "because there was another mutation present in the cells that will be combated eight weeks later the tumors came back and the man died d so it's well known that if there are multiple drivers in a mutation targeting one might not supply that's the problem and in fact Bert vogelstein and colleagues over the years and others and published papers just kind of cataloguing the number of likely driver mutations present in a single patient tumor and although it varies from cancer type to cancer type you can see that there might be a large number of likely drivers of tumorigenesis present in the cancer suggesting that simply targeting one of those the drug might not so this leads to then questions around how can one match drugs or combinations of drug to the tumor profile of the patient and there's multiple rules or algorithms one could exploit that the community is thinking about "
    },
    {
        "start": 2644.58,
        "text": "one way is to just leverage multiple simple rules so if someone tumor has EGFR overexpression and her2 over compression you give the EGFR inhibitor and acceptance something like that I'm making that up you just use multiple you could match drugs on the basis of how the collection of perturbations might be influencing network network you could just build a nova prediction model get a get a bunch of cell lines look at expression patterns across the cell line hit those cell lines with different drugs and then build predictive model build regression model or whatever you want that might be used predict what drug someone should get given their genomic coke talk a little bit about that the connectivity map is a tool out there to kind of match drug to the multiple perturbations that might be present in a tumor or a cell line of one sort or another I won't go through this in great detail but it was designed initially by some people at the "
    },
    {
        "start": 2705.81,
        "text": "Broad it's available in the public domain if you have the profile gene expression profile of a cancer you can run that profile against the connectivity map kind of query what drugs appear to reverse the perturbations in the tumor so if in fact gene was over expressed in the tumor under expressed in the tumor over expressed under expressed you'd want to find a drug that appears to counteract those and kind of reverse that process and you could do that with the kind of key it's the different approach for matching drug or a drug to it or you could combine all these now this is just a smattering of the different algorithms that people are thinking about and comes with matching drug or combinations of drug to tumor profile but there's other strategies your algorithms that are implicit in ways people treat cancers I'm not going to go through all these I'll just give you one example you know emerging a research into immunotherapy where one could find kind "
    },
    {
        "start": 2766.5,
        "text": "of neo antigen in the patient tumor and on that basis cook up a cocktail of P cells that have been sensitized to against those neo antigens and then put them back in the patient's body so they could go and kind of kill off the cancer cell now think of each patient as having a different neo antigen profile so the cocktail of t-cells it's cooked up would be different compassion to patient so now what you're doing is not testing the same construct against each patient you're basically testing a strategy finding the neo antigens cooking up the t-cells putting them back in the person's body so that's unlike testing a standard drug where you have the chemical composition of a particular drug and you're testing that on everyone this is a strategy right and many other strategies including whatever strategy the position resorts to when they decide what drug to get some of whatever the collective thoughts are whether they go to Google and look online for you know papers that might sway them one way or "
    },
    {
        "start": 2827.46,
        "text": "another that's a strategy of thort and it's not codified but it is some kind of strategy something to think about now people have proposed different algorithms and made claims about their ability to match drug against tumor profile so little less than a year ago Patrick soon-jong just two hours north of me in LA made the bold claim that he had created the Google of genome mapping that it's gonna lead us to the Nirvana of coordinated delivery here what he's basically saying his Providence Providence Health System has algorithms for matching drugs the tumor profile that will work better than whatever else the community is doing and he's gone on record for saying it now nobody's quite sure what is in the little black box that he has that matches the drug to the tumor profile right this is considered a trade secret or proprietary not so long ago a group of researchers argued that "
    },
    {
        "start": 2889.35,
        "text": "they could use Watson the IBM computing system to kind of match drugs to kill the profile again leveraging some kind of scheme or algorithm so a question is if these different approaches to matching drug the tumor profiles exist does it behoove the community to vet them and show that they actually have utility much in the way you would test a particular compound to show that it has utility who's to say that the soon-jong algorithm is not better than the Watson algorithm or or vice-versa should the FDA take it upon itself to pit one of those algorithms against another to show that one has utility in the same way that you would pit lipitor against simvastatin and a large phase three trial to compare the two here's your emerging question well I'm involved in a in a large study funded by the stand of the cancer organization in the 8th ER to see if algorithms for matching drugs to tumor profiles and pain from patients with unique melanoma "
    },
    {
        "start": 2951.54,
        "text": "condition be RAF wild-type melanoma is any utility over the standard of care whatever the standard of care a lot of research has have been involved this project has been running now for about three years I'm not going to go through the results because they're not complete yet Jeff Trentham ijen and Pat my booth though now at Yale but was it come on so he stood here in Detroit is the clinical with my former post collie and Rory at the Sanford Burnham previs medical Lisa we're involved in actually coming up at those algorithms leveraging whenever we can from the public domain like cell line hence my sensitivity to the issue took about two years to get this protocol through the FDA for reasons that will become clear here's just a few factoids about the trial and we actually have 37 drugs we can choose from and 17 of those are investigational agents those drugs that passed phase one but "
    },
    {
        "start": 3012.65,
        "text": "aren't widely distributed that the pharma companies gave us ACTA so once we profile a patient's tumor we look at the perturbations then we can choose among these 37 drugs to give to the patient right but we have to come up with some kind of algorithm we'll talk a little bit about that in a minute it suffice it to say when we started the trial we were told we had to lock down the rule for matching the drug to people Coco and this is highly problematic for reasons that will become clear that yeah I did it was an absolute nightmare now people have proposed that there's other pieces of information one could leverage to decide what drugs to give the patient so you could take someone tumor make a cell line out of it hit that cell line with all sorts of drugs and on the basis that how the cell line respond give the drug to the patient right this one's been done the recent paper by angleman but there's a problem with "
    },
    {
        "start": 3072.66,
        "text": "looking at sella and drug responsiveness the kind of guiding patient care and that has to do with the fact that many people believe they're unreliable and in fact a couple of years ago John Quackenbush and colleagues published in nature of paper questioning the reliability of drug screens against tumors Ella those got a lot of attention it was in you know currently the best journal on the planet nature and it caused a lot of concern last year late last year the group that was responsible for conducting these screens in the to the CTL II and the PGP responded again published in nature saying that no the results of their screens were reliable and reproducible well it's a little bit funny and I encourage you read the papers so you know take what I say with a grain of salt is both produced kind of correlations reproducibility somewhere between point four and point six so really it came down to interpretation whether that was considered evidence for a strong reproducibility or whether it "
    },
    {
        "start": 3133.95,
        "text": "was not strong enough to be considered reproducible to go back and look at the data it has more to do with interpretation and a few differences in an Alan very interesting so we did in the context of the stand up to cancer knowing that there was a controversy over guiding patient therapies in the basis of cell lines is we did an experiment we sent out cell lines to two different labs you used nine different doses of you know hundreds of drugs and we is treated is a big analysis of variance so we had as factors the laboratory and what the experiment was done because they were reproduced we had the drugs we had the dose of the the drug we had the plates on which the different cell lines were set up so we could actually look at the plate effect so just partition the variation in the entire experiment in two different sources and ask the degree to which those sources contributed this was not done by the two other groups by the way they just used kind of ic50 valued "
    },
    {
        "start": 3195.36,
        "text": "abstract with auth curve but we went back to the experiment of the whole what we found maybe not surprisingly we didn't see a whole lot of the lab collab variation although they did we did see pronounced drug and dose effects as you would expect and actually some importantly dose by drug effects suggesting that some drugs really had a more pronounced effect in bail line we also saw that some cell lines responded very differently to others keep in mind this is in the context of the entire experiment but wasn't any one drug we were looking at getting I see 50s and then looking at thing it was across all the experiment how long a good question so I think you want to talk to Christina and my colleague at Tegan just gave me the data to what the ultimate "
    },
    {
        "start": 3256.65,
        "text": "experimental conditions work you'd want to talk to them but cell viability was the the outcome that we we had looked at so you know these questions are important sure sure no no you're absolutely right there's a lot of nuance to this and you could do Co culture experiments you could do all sorts of other stuff to guard against safeguards whether the drug actually had an effect on the normal tissue but there's tons of stuff that you could build in and I'm not an expert on the optimal way of designing "
    },
    {
        "start": 3316.8,
        "text": "the wet lab side of this I just wanted the data to see if we could make some broader sensitive and we did actually see plate effects so maybe what wasn't reproducible in the previous studies was a couple of plates showed outlying values and if you got rid of those and then went back and reanalyzed the data you'd see greater consistency and we did see some of it now in analyzing the the cell lines again more anecdotal than anything else you know we had this cell line that was hit with a bunch of different drugs we thought some that didn't do much of anything some that appeared to have a pronounced effect in modeling this is more for the biostatistics people in the room and modeling this again our approach was not to kind of extract ic50 values and then use those with summary measures and then compare them we went back to the data using more of the analysis of variants like approach so what one could do is if you had a bunch of cell lines say 30 of them you could extract I see 50 values and then and then look at them or you "
    },
    {
        "start": 3378.66,
        "text": "could go back to the raw data and consider you know all the information that you had at your disposal to decide what drugs were and weren't having an effect in the context of the whole experiment so that's something you're working on context of matching the cell line the patients we did go through that exercise these are 27 of the 30 patients gone through the fan of the cancer trial numbered here we then have kind of color-coded different cell lines that look like they match the patient profile and then we could read off group are not that read off what drugs we should showed an effect in the cell lines now these weren't actually tested in the patient but we at least had this information to see if there was any likelihood that these drugs would have worked patients had to be given now what one would like to have it to her disposal is a database that has all this information at their fingertips so if you had patient profile you could match them against a database that had cell "
    },
    {
        "start": 3439.299,
        "text": "lines that were also genomically profiled that were subjected to the different drugs tumor graft data could be used could implant can look on the patient into a mouth hit that mouth with different drugs look at the continent this is also being done by by many group dogs are one of the few species that develop spontaneous killers like humans there's a lot of research in treating canine tumors with the drug bill updated there but of course what you'd really want is actual patients profiles and their outcomes and there are databases that are growing now one problem with vetting the algorithm that we had in the stand of the cancer that we discussed with the FDA had to do with the nature of our algorithm and what went into the algorithm so we had to biopsy the patient do the genomic profiling right come up with the matching algorithm give the result to a tumor board that then "
    },
    {
        "start": 3500.589,
        "text": "decided what drugs to give to the patient that's it so you might think well here's where all the action is it's matching all of them well they called into question the reliability with which we could do the sequencing kind of garbage in garbage out problem here if we can't reliably assess the expression levels with the jeans or the mutation profile then how could we make any claims about the existence of driver mutations we might be targeted with drug so they asked the question well it's part of our algorithm matching algorithm not just the scheme for a matching drug to the tumor profile but actually identifying the mutation profile itself the other thing there's deliberation by the board of experts the the tumor board looks at our results and on that base it's decides what drug the patient should those deliberations be considered part of our algorithm of strategy and of course that would definitely vary from the hospital to the hospital right now the other thing that was a little bit complicated in this is the people that we were studying the people with b-raf wild-type melanoma "
    },
    {
        "start": 3561.99,
        "text": "typically have when they're diagnosed and been on all sorts of other drugs you know three to four months to live so we didn't have the luxury of time in making these decisions so we had to do all the profiling and the matching of the drugs in less than a week and that is a problem as you know but without that it wouldn't do anyone any good if we're really trying to test this patient right now there's another fundamental problem with these designs that come up that you may or may not anticipate in the context of this trial what was originally thought is we'd come in get a patient and randomize to getting the genomically guided therapy versus the standard of care and then that basis provide them the drug well this was thought not to be effective because patients coming in for which there wasn't a good match like it wasn't obvious what drugs to give them on the basis of their genomic profile if they were randomized to the genomically "
    },
    {
        "start": 3622.03,
        "text": "guided group they'd water down the signal because there'd be no confidence that the drugs would actually work against them so to safeguard against that what we were - what we were recommended to do was bring in a patient right we then do the genomic profiling if there are good matches for that patient based on the genomic profile and our algorithm for matching the drug then they're enrolled in the trial if there's not they're gonna get standard of care anyway so they're not enrolled in the trial but if in fact they're enrolled in the trial then we randomize them to getting the genomically guided therapy or the standard of care now why is this problematic because many of the oncologists in the community when they heard about this is dying were confused and hurt by it because they said well if I have some intuition about a drug that's likely to benefit my patient I don't want them ramp up the possibility of them randomized the standard of care we pitch that to the FDA and they said well you're begging the question about whether your algorithms have any utility "
    },
    {
        "start": 3682.93,
        "text": "at all you have to prove it Kurt I have to say I'm on the fence about this one but you can imagine how this created controversy the trial was approved and we're moving through with this design with the patient they consent to all the content now in discussing this trial recently with the FDA so a little less than a year ago a number of issues came up when we talked about them I'm not going to go through them all just two that I will focus on what if incites external to the trial day to come up that might impact the physicians choice or the researchers choice for drug what I mean by that is we had an algorithm for matching the drug for the patient profile we locked it in and started the trial what if the day after we started the trial there was a nature paper published cover article that said if this mutation is present use this drug because we've shown in mouse models and in patients that if we're locked in and can't include that information then we "
    },
    {
        "start": 3744.519,
        "text": "wouldn't not only benefit the patients but we might be shooting ourselves in the foot with respect to testing a useful algorithm so the compromise was that we could have periodic changes in our algorithm over time to accommodate this sort of thing and this is actually impacted oncology trial so there was the battle trial lung cancer trial that Scott Littman the director of the UCSD morin Cancer Center was involved in where all this information about EGFR inhibitors came out after the trial was initiated but they were locked in to the little buckets that they guided people to and couldn't incorporate that information right well that's a problem so one has to be flexible in the way they conduct these trial now this raises important biostatistical question too interpreting the trial result because if you're allowed to adapt the rules that you use then the patient's enrolled at the tail end of the trial are going to have access to a better algorithm than the people early in the trial so then you have to wait the patients differently this gets complicated as well but this is a reality okay now in "
    },
    {
        "start": 3807.64,
        "text": "conducting these trials you need the appropriate infrastructure and here I've had the pleasure of working with a group involved in the med C consortium I don't know how many people know med thief but as we talked about before if you wanted to test all the different matching schemes that one could come up with how could you do that what infrastructure could you leverage it would cost the pharmaceutical industry and NIH zillions of dollars to pursue all these little trial so what was proposed by a number of people and minority from Roche Genentech is one of the leaders in it was to put together a pre-competitive infrastructure for carrying out trials in oncology that would benefit all pharmaceutical companies okay so the idea here is that if each pharma company PO need up some tens of millions of dollars to build infrastructure where they could recruit patient then each one could benefit from having a central group recruit patients and then they conduct trials with those patients steering them to the right baskets based on their genomic profile "
    },
    {
        "start": 3869.469,
        "text": "right if they didn't do this then each pharma company would have to go out and reproduce all this infrastructure and it cost them billions of dollars okay Oh am i all right so I will just win through what is a personal versus population threshold well if you're measuring the level of a biomarker and you want to decide whether someone has any signs of disease or not you'd do this on the basis of some population thresholds but this is cholesterol level this would be greater than 200 right but if you would establish kind of a historical precedent for a patient you might want to rely on that historical precedent to determine what is or isn't a reasonable value for that patient to have in the future so for example is this cholesterol level and the patient had low cholesterol levels well below the population threshold and then you saw on subsequent values they had elevation then despite the fact they haven't reached the population threshold on the basis of their deviating from previously established threshold you then claim that something is going on right so this "
    },
    {
        "start": 3931.209,
        "text": "is a personal threshold versity population threshold and always through this in spite say that there's a recent paper showing the hildie of personal versus population threshold been looking at ca-125 levels in ovarian cancer what they found is if they had used the personal threshold to determine who had signs of ovarian cancer or who didn't rather than the population threshold they could have captured the ovarian cancer almost a year earlier so I have a study studying people that are highly susceptible to cancer where early detection is absolutely crucial so these are people with germline p53 mutations other sorts of inherited cancer syndrome where early detection is important if you go up and look at this there's many other conditions that have this but what we've done is actually established a protocol to measure frequently many different biomarkers establish personal thresholds and then that basis see if there's any signs of cancer so again these are people that almost have a hundred percent chance of "
    },
    {
        "start": 3992.39,
        "text": "developing multiple cancers throughout their lifetime so early detection is absolutely crucial and we've been pushing this there's different signals that we've tried to detect from the data multivariate signals not going to go through that in great detail we had to design studies to deal with si drift over time most longitudinal studies you'll get all your samples and you'll analyze them together in one fell swoop at the end of the study if you're doing things in real-time you don't have that luxury so you have to do the assay then get another sample do deeossie again and that si - si variability especially with metabolomics where there might be changes in ionization pattern that can come back to haunt you so you have to design ways of minimizing these technical artifacts okay the last point I wanted to make had to do with a big study that I'm involved in looking at factors influencing lifespan we've got two different communities where we're looking at this one in bi ways : one in kolento Italy very contrasting communities we've "
    },
    {
        "start": 4054.13,
        "text": "decided to sample the individuals for this study through household we're going to get by default of people that are related right we'll have individuals well know their pedigree links will then have a household so we can use kind of variance component model do that doesn't look good to the Wichita block we can use variance component models which I'm not going to go through an any great detail this didn't show up in the transcript tonight my Mac but we can partition the different sources of variation right and ask how different factors contribute to health related parameters the reason I wanted to point this out is 20-some years ago I worked on this as part of my PhD thesis it turns out that two of my thesis advisors also worked on this it was an inspiration to me that's what led me to think about these model and of course someone else in the room also participated in this and my father published papers almost 40 years ago suggesting that one could actually "
    },
    {
        "start": 4114.489,
        "text": "leverage family based designs to look at household effects through genetic effects and break down that that variation so the moral of the story here is you can take the researcher out of Michigan but you can't take Michigan out of the researcher it so go blue here and I'll thank my lab members and if there's guy know anyone over I apologize thank [Applause] Pat where's the hook so more information is better and that applies to knowing what the paternal and maternal chromosomes look like doing a longitudinal study in all this how important is transcriptomics for example to looking at the RNA transcripts "
    },
    {
        "start": 4176.359,
        "text": "how important is epigenetic marks right and how compete of an information set do you need well I think you know taking this more integrated approach is the way to go and in the context of the cancer just to give you an example of the importance you know identifying the the likely driver mutations in cancer just based on genomic sequence is likely to fail because oftentimes you want to see that the mutations are in fact influencing expression levels of the gene or their levels of protein of the gene and you can't do that with just genomic sequence so many algorithms to decide what variants are likely driving cancer and what not make the assumption that the mutations are activated they might not be but the way to know it is to actually interrogate the expression levels of the game so there's a case where the more integrated approach I think is essential we're really nailing down what is it isn't driving the tumor but in other context it's equally one of the things we "
    },
    {
        "start": 4247.71,
        "text": "grapple with is two more evolution than what is evolving the host and a lot of what you are talking about puts sort of in a global sense perhaps samples the host factors but this ca-125 is a nice example that probably you so the questions are viewers is the coronal heterogeneity in that the tumor is evolving and so even knowing we we are capable at times of serial sampling but not often so what are your thoughts about longitude analogy in cancer monitor so yeah obviously it didn't time to go over a lot of this but yeah people are thinking about liquid biopsies now to kind of trace the evolution of the tumor either you know a natural history prior to treatment or after treatment so "
    },
    {
        "start": 4308.55,
        "text": "those could provide insight one thing that I would emphasize that it's consistent with part of my talk we're talking about algorithms is if you shape therapy on the basis of what you see over time then again you've got like a steam or an algorithm so it can say okay I'm gonna start out giving this drug and then if over time I see say from the liquid biopsy or and repeated biopsy the tumor a different set of mutations arise then I'm gonna switch to this treatment then you might ask yourself should you have to you know what is that on what basis are you making that decision so there could be a temporal component to the scheme or algorithm that's being implemented it just adds another dimension to think but then you'd ask yourself well does that have any merit and you have to test that so it's just like a series of conditional things if this is seen at time 1 but this is seen at time 2 you kick in you know different therapies it's just a different strategy and much of this is simply you know the practice of medicine physicians making "
    },
    {
        "start": 4370.48,
        "text": "decisions on the basis of what they see and maybe they would or wouldn't benefit from kind of objective claims about what might and might not be called or clinically but just because you have this temporal component doesn't mean it's not part of some broader schemer or a formal way of deciding what drug to give a particular patient um would you say a bit more about statistical analysis in the JEM trial sounds as though you don't have time to do a crossover trial on each patient right no you absolutely cannot not only do the patients not have enough time but ethically just putting him on a drug you know that they might not benefit purposefully to show that the one drug they better raises ethical concerns that's probably not the case for you know treating hypertension so there are some conditions that you know you wouldn't want to throw at the end of one design that I talked about with blood pressure in in in this context you "
    },
    {
        "start": 4432.46,
        "text": "wouldn't want to do that but statistically what the FDA decided on like my father was involved in some of the discussion was it's simple design it's the the PFS the progression-free survival and it's the experience of the people with the genomically guided versus the standard of care that's it it's like a two-by-two table on that basis we're gonna make the claim whether the genomically guided therapy works better enough that's what would settle up however there's all sorts of post-hoc analyses which we could do if the algorithm shifted throughout the course of the trial than this waiting the patients could be done etc etcetera but the primary analysis is so simple in orientation you know to really defy the complexity and the sophistication of everything that goes into it it was just what is the experience of the patients and the genomically guided versus standard of care group so exactly on this mark one of the issues why people prefer having these "
    },
    {
        "start": 4493.539,
        "text": "double-blind placebo control control trials is and not a trial of one is the placebo effect and I could imagine that a woman and in fact I'm kind of in doing a trial right now and I'm afraid of the placebo effect because the people know that you know Oh some big researcher somewhere in America has done something that suggests they should try this drug I mean wouldn't how do you know that when your drug has an effect in this one person and it deviates from the standard from the number of you know seizures at night or whatever isn't because they know that they are in such a wonderful new algorithm etc environment how how do you do placebo control in a nf1 trial well you could we could have given that say the patient on the Indy antihypertensive trial a placebo and blinded them to that so they wouldn't know right so it is possible to do "
    },
    {
        "start": 4554.59,
        "text": "though there are ethical reasons why that approach wouldn't work in other contexts like the cancer case you wouldn't knowingly take someone off there off of potential effective med in the blood pressure example you might do that because it's known that the placebo effect can be so pronounced you know in that context I'll just share with you I have contracts from Janssen to kind of look at the data from some of their large phase three trial and of course it's in the two worst areas I think for placebo effects looking at their experience in antidepressants and and and in pain medicines these are the right cause and you gave me the worst of all situations to work with with respected with SIBO it so they are it's crucial [Applause] "
    },
    {
        "start": 4617.449,
        "text": "yeah to polypharmacy "
    }
]