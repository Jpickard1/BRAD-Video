hi everyone uh i'm just going to share my screen here okay so thank you all for coming and thank you for the invitation to speak here um my name is abe huku i am a current phd student in the department of biostatistics and today i'm going to be talking about bagsy which is a bayesian hierarchical model [Music] enrichment analysis so at any time today if you have any questions about anything i talk about please just unmute and feel free to interrupt me whenever um and yeah so just as a general outline for this talk uh first i'll be talking about the motivation and background behind conducting jinsa enrichment analysis in general i'll then talk about some of the existing methods for gene sound enrichment analysis and their limitations i'll then introduce bagzi and some of its features and how we try to address those limitations and i'll go on and show bagzi's performance and simulations and real data applications before wrapping up with some conclusions so oftentimes the results of a g wasser differential expression analysis can give a very long list of seemingly biologically unrelated significant snips or genes and this is especially true over time as sample sizes have increased we end up with many signals of significant snips or genes from these studies and although having this many significant results is great it can become increasingly difficult to find any sort of unifying biological theme to link these results and so it becomes hard to subsequently use these results to gain more insights into biological mechanisms behind diseases and so there has become a need to you to develop techniques to extract more biological meaning from the results from these analyses to help to uncover these sort of biological causal mechanisms behind diseases and one such technique is known as gene said enrichment analysis so gene site enrichment analysis is performed downstream of a differential expression analysis or a g loss and the goal is to identify groups of genes that have a higher or a lower number of association signals than expected so oftentimes gene sets are defined as genes in the same biological pathway and in this way it can give more biologically meaningful results because now instead of saying we have these genes that are associated with this disease we can say genes involved with this biological function are associated with this phenotype or disease and so this can gain more insights to subsequently do further research into seeing maybe how that specific biological function could be connected with the disease and in this way genesis enrichment analysis was first developed in 2003 by mutha at all and they found that genes involved in oxidative phosphorylation had lower expression in human type 2 diabetic muscle and so this is just one way gene said enrichment analysis has been used to gain more biological insights into certain diseases so one way that we believe is a natural way to think about gene set enrichment analysis is through this latent two by two contingency table interpretation so each gene in a study is either unannotated or not in the gene set or annotated in the gene set and the placement of the classification of genes as in the gene set or not is going to be determined prior to the study by the investigator researcher uh who classifies each gene as in the gene set of interest or not additionally each gene is either associated with the phenotype of interest or not associated with the phenotype of interest and in a differential expression setting this would also translate to each gene as either differentially expressed or not differentially expressed so thinking about it this way each gene can be mutually exclusively classified into one of these four cells and so we can create this two by two contingency table and when this table is developed we can then easily calculate an odds ratio and this odds ratio falls very well in line with our definition of enrichment because the literal interpretation of it is a gene that is annotated has or times higher odds of being associated with the phenotype of interest than a gene that is not annotated because of this and because odds ratios are used so prevalently throughout statistics in biology we believe that this value is the ideal quantification of enrichment and it will be the value that mating with our method so because we believe this two by two contingency table is a very natural way to think about enrichment i will be referencing this throughout the talk especially when explaining our method so so please keep it in mind and i will reference this again so for the existing methods for genesis enrichment analysis they typically fall into two different categories the first category is gsea-based methods for gene sun richmond analysis so in this school of method first association scores are assigned for each gene and this association score can be a variety of different things it can be a z-score it could be a p-value or it could be just an effect size estimate like estimated differential expression and then within the gsea-based methods there is unweighted there's the unweighted gse based approach where the distribution of these association scores are compared between genes in the gene set in genes not in the gene set using a statistical test such as a kolmogar smirnoff test or a matte whitney u test and then there is the weighted gse based test where you first generate a list a ranked list of genes by that association score and then you calculate a running sum statistic down the list so starting at the top you go down the list and when you run into a gene in the gene set of interest you go up by a certain weight and then when you run to a gene not in the gene set of interest you go down a certain way and in this procedure you then use permutations to obtain an empirical p-value oh sorry and the enrichment score is the maximum deviation from zero while calculating this running some statistic and some of the approaches that use this type of method are gene ontology gsea and magma so the second category of gene set enrichment uh type methods is the two-stage procedure so the first stage in this two-stage procedure is known as a classification stage and here each gene is first classified as annotated or unannotated as defined by the investigator and then classified as either associate or unassociated through different types of methods such as p-values using a bonferroni correction or q values or a p-value threshold and of course once this classification stage is over each gene can be assigned to one of these four cells in this contingency table and once this assignment is done some sort of statistical tests can be used to to calculate to calculate if there is significant enrichment such as a chi-squared test a fischer's exact test or another permutations based test and some of the approaches that use this type of method are david enrich and magenta so one thing to note is that using a two-stage procedure you can get this ideal odds ratio of enrichment because you have generated this two by two contingency table so some of the limitations within these existing methods so first for the gse8 based methods the permutations based procedure for the weighted version to obtain an empirical p-value is not exact and it can also be computationally expensive especially as the number of genes being investigated increases doing a permutations-based procedure to calculate a p-value can end up taking a lot of computational power and time and also there's no easily interpretable value quantifying enrichment that can be used to compare gene sets so there's now like odds ratio that we had talked about earlier and then for the two stage methods the uncertainty that's introduced in the classification stage specifically the uncertainty introduced when classifying the genes as associated or unassociated is then ignored in stage two the testing stage so although type one error for the classification stage can be accounted for through type 1 error correction methods such as bonferroni correction or using benjamini hatchberg approach the type 2 error introduced is not necessarily accounted for and so to address these concerns we developed bayesian analysis of gene set enrichment and so we developed a bayesian hierarchical model for gene site enrichment analysis that can be that can be run using only summary statistics so only z scores or beta hat and standard errors from a differential expression analysis or g loss and we quantify enrichment as the log odds ratio of interest that we talked about uh which we call alpha one in our model which i'll explain further in the next slide and a couple other interesting features about bagsy the enrichment that we calculate can then be used as prior information to improve power of gene level testing taking advantage of an empirical bayesian framework and also bagsy is adaptable to testing multiple overlapping gene sets so here is an overview of the bayesian hierarchical model we developed so on the left here is a graphical model representation of this bayesian hierarchical model so the first level is gamma i which is the binary association status of gene i and the distribution of gamma i is impacted by the enrichment parameters alpha 0 and alpha 1 which we have to estimate and also impacted by di which is the binary annotation of gene i and i should say that this model that we're looking at here is for when we are looking at one gene set at a time so d i is either zero or one and so the prior distribution of gamma i here is a bernoulli distribution that with the parameter impacted by these enrichment parameters and di and then for the next level of the hierarchical model we have the true effect size of gene i so this true effect size is conditional on gamma i in that when gamma i is zero that means gene i is not associated with the phenotype and the true effect size of gene i will be zero and not have a distribution it will just be a point mass of zero however when gamma i is one then beta i follows this mixture normal distribution with k components and one thing to note is that this number k so the number of components and phi which is the variance of each component is estimated in a data adaptive way proposed by matthew stevens in a 2016 paper and this pi which is the probability of being drawn from each of those k components is a parameter that we have to estimate and one thing to note is that the pi parameters actually is different depending on whether the gene is in the gene set and the gene set of interest or not so that is one of the unique statistical capabilities of our model in that the prior distribution for the true effect size is not going to be the same across all genes it'll be different depending on if the gene is in the gene set of interest or not and then for the third level we have uh the observed estimated effect size beta hat i which is conditional on beta i and this distribution is just normal with mean beta and a standard error that we can estimate so one interesting thing to note here looking at the dis prior distribution of gamma i is that this distribution is exactly like how the distribution for a for an outcome of a logistic regression of the outcome variable for a logistic regression would look like and with d i being the predictor variable so when you think of it like that it becomes more clear how alpha 1 is that log odds ratio of enrichment and the only difference here is that in a real logistic regression setting the outcome variable will be observed whereas here of course gamma i the true association status of gene i is not actually observed so because of this to estimate maximum likelihood estimators for alpha 0 and alpha 1 we used an expectation maximization algorithm treating gamma as missing data but before i go on i want to pause for a bit because i know this slide is very involved with sort of the meats of our whole methods in case there are any questions or anything is unclear okay if there's no further questions i will go on to explain some of an intuitive explanation behind this em algorithm that we use to estimate the enrichment parameters so here i have we use this em algorithm to estimate alpha zero and alpha one and also pi but i won't really be going into the estimation of the pi parameters because they aren't as relevant as the enrichment parameters for this talk and they're not as intuitive and a little more computationally complex so here i'm going to be just explaining how we calculate alpha 0 and alpha 1. so first we initialize all the parameters that we need to estimate and then in the e step we have to find an expectation for the missing data so here we end up having to find the expectation of the indicator that gamma i equals one conditional on all the observed data like the beta hats and their standard errors and the annotation of the gene and also our current parameter estimates of alpha and pi so this is so this term is actually statistically equivalent to the probability that gamma i is equal to one uh conditional on all the observed data and the parameter estimates so if you can think about this again in a latent 2x2 contingency table setting uh this actually can be used to basically fill out iteratively this 2x2 contingency table so let's just say gene i is in the gene set of interest so it is annotated and we calculate this probability to be 0.8 so then we would fill in point we would add 0.8 to n11 and point 2 to n10 and we do this for every single gene for each iteration to calculate a teeth iteration of this whole 2x2 contingency table and you can see how contrary to the two-stage approach which each gene is put in as a one or zero in one of these four cells we incorporate the uncertainty and and insert this probability into one of these force into a couple of these four cells so then once this two by two contingency table has been developed for an iteration the m step can be used just as calculating the log odds and of course alpha 1 the log odds ratio and this will end up being the maximum likelihood estimate for alpha 1 which can then be used back into the e step so this is a natural way to see how our approach in bagsy incorporates the uncertainty from classification whereas the two-stage approach does not and so we can do this em algorithm until convergence and obtain the maximum likelihood estimates for the enrichment parameters and we can easily get standard errors for these parameters also through a profile likelihood approach so one other unique feature of our method is that the enrichment information that we calculate can then be used as priors using a parametric empirical bayesian framework outlined by matthew stevens and it can be used to boost the power of testing the hypothesis that gamma i is equal to zero which means gene i is not associated with the phenotype so we do this by calculating this local false discovery rate for each gene and this local false discovery rate is equal to the probability that the null hypothesis is true given all the parameter estimates and the observed data which is here equal to the probability that gamma i is equal to zero and one interesting thing to note is that this probability is actually just one minus this probability from the east up and so we actually already have this quantity from that em algorithm so no extra computational work is required to get these local false discovery rates for every single gene because we just use the from the last iteration and the em algorithm for every single gene and then this local false discovery rate can subsequently be used for gene level testing using a standard bayesian false discovery rate procedure so now i want to give a brief overview of how bagsy can extend from one gene set to multiple gene sets so let's assume we have l gene sets that we are simultaneously investigating so now d i instead of just being a zero or a one it is an l length vector where each component is either zero or one so this allows for overlap of gene sets because uh you can have multiple ones within this vector for each gene and each different configuration of the vector each possible different configuration of the vector has its own associated enrichment parameter so one of the drawbacks to this procedure is that we end up with two to the l enrichment parameters which you can see can become very computationally expensive as l increases however one of the benefits to this approach is that it maximizes the gene level discovery power for the incorporation of enrichment information to gene level testing which makes intuitive sense because the more information that you're gaining about the gene the more power you'll have to discover if the gene is associated or not okay so now i will get into the simulations where we talked about the performance of baxi so here is a overview of the simulation scheme that we use uh so for each simulation we simulate ten thousand genes and the proportion of genes annotated varies between one percent and twenty percent and the proportion of genes annotated does have a very profound effect on the power and stability which is like the variance of the enrichment estimate but the effect it has on all methods is around the same and so for each gene we simulate a z-score for null genes that are not associately associated with the phenotype we generate their z-score from a standard normal distribution and for associated genes we generate their z-scores from a t distribution with 10 degrees of freedom and the number of associated genes depends on the enrichment parameters so basically for each gene we already have whether it is in the gene set of interest or not and then we use the enrichment parameters and the annotation information to generate from this bernoulli distribution to classify it as either truly associated with the phenotype or not associated with the phenotype and so about these enrichment parameters alpha 0 we set to negative 1 throughout simulations and this value is just from what we observed in real data the log odds of a gene being associated generally seems to be around negative one and then we vary alpha one between zero and one for these simulations and for each parameter set we conduct 5000 simulations so this is the general simulation scheme we use um we make a couple minor changes for some of the individual simulations that i'll mention when we get there but in general this is the scheme we used in our evaluation of bagsy so the first thing we looked at was uh a closer look into the limitations of the two-stage approach so here we generate data only using an alpha one of one and a proportion of genes annotated of ten percent and we just look at how the gene level power which is basically the power to classify a gene as associated in stage one of the two-stage approach and how that impacts the subsequent estimate of the log-odds ratio of enrichment and we see that as expected as gene level power is very low or close to zero there's severe underestimation of the log odds ratio of enrichment whereas when the power gets close to one to detect an associated gene the underestimation is much smaller and is very close to the true estimate works to help confirm our hypothesis that the two-stage approach um the ignoring of type two error in the first stage uh does impact the subsequent uh estimate of enrichment for the gene set so now we looked at simulations to uh compare this to two-stage approach with bagsy in terms of enrichment quantification so this graph is looking at the results for when 10 of genes were annotated and we generate data from these five different true enrichment parameter values and we see the pattern we would expect in that at low levels of enrichment both are very close to the true value both bagsy and the two-stage approach but then as the true level of enrichment and the data grows higher this two-stage approach starts to severely underestimate the true enrichment parameter while bags he seems to maintain a mostly unbiased estimate of the enrichment we see here a slight overestimation but this doesn't seem to be a pattern as we also looked at higher levels of enrichment and this amount of overestimation was around what we saw and so yes this confirms what we saw in the last simulation that the two-stage approach at higher levels of enrichment tends to underestimate the log odds ratio of enrichment so then we looked at the power to detect enrichment and again here we looked at just 10 percent of genes annotated and so for this we also included weighted and unweighted gsea in our method comparisons so one thing to note is that when the true enrichment parameter is zero this value is actually the type 1 error rate and we see that all methods do control the type 1 error rate reasonably well at around the 5 level and we see that for the medium enrichment parameters of around 0.1.25 bagsy and weighted gsea do seem to over perform the other two methods uh and then once the enrichment parameter is around 0.5 all of these methods have very close to 100 power in detecting enrichment and so we believe that in terms of power once the enrichment level is at a reasonable level in real data all methods do have very good power in detecting the enrichment so now we decide to look at uh the incorporation of the enrichment information in subsequent gene level testing to see how that impacts the power of gene level testing and so here we use the same simulation scheme but we extend the alpha one uh parameter set in order to see how the pattern will evolve so we saw that so for this graph the y-axis is the percentage of power increment that we gain using bagsy's method to incorporate enrichment information from these two approaches which is a q-valley procedure and compared to a local false discovery rate procedure that does not include enrichment information and we see that low levels of enrichment which is like we were looking at with 0.5 and 1. there's a very modest power boost that we gain in gene level testing maybe around just two percent but then as the true enrichment parameter grows higher and especially as it grows very high like in the four or five range the incorporation of enrichment information can give a pretty substantial boost to uh the ability to detect gene level associations of near 20 percent from a q-valley procedure and from a local false discovery procedure so now i would like to talk about our real data applications of bagsy so firstly we applied bagsy to this differential expression analysis done in moyer brilliant at all in 2016 where they conducted differential expression analysis of 20 896 genes under cook's analysis we got p-values and beta-hat estimates from dc2 so from this information we're also able to generate standard errors of the beta hat which we could then use in our analysis because we because bagsy only requires summary statistics to be used and through our conversations with collaborators we expected genes and pathways corresponding with immune response to be enriched looking at this data set and we saw using the keg database that 21 pathways were involved in immune response and so because we weren't exactly sure which of those pathways should be associated um should be implicated in this analysis and each of these pathways had a very small number of genes uh present in our data set we ended up aggregating all the genes in these pathways for our analysis so what that means is that the annotation for each gene was one if it was in any of those 21 pathways and it was zero if it was not in any of those 21 pathways so using this procedure we have 2.7 percent of genes that ended up being annotated and then we ran all four methods on the data and saw each of them detected significant enrichment while bagsy estimated the alpha 1 enrichment level to be 1.31 and the two-stage approach had a 0.87 estimate of enrichment so this difference in enrichment estimates does fall in line with our simulation results showing that the two-stage approach may have underestimated the enrichment level and at this level of enrichment it makes sense that all four methods should have a hundred 100 power to detect the enrichment so we also incorporated this enrichment information to see if we could discover more genes in this gene level testing and we see that compared to a local false discovery rate we using enrichment information we can discover 90 more genes in the analysis so we also applied our method to a transcriptome-wide association study so a tiwas is a type of study that integrates eqtl and gwas to study relationships between the expression of candidate genes to the complex trait of interest and the way it does this is by first training a predictive model of gene expression using the eqtl data sets and then applying that predictive model to the g-loss so you're in using that model to impute gene expressions in the g-wasp data and then a t-wasp looks at the gene expression and complex trait associations to generate z-scores on the gene level so we get g-was summary statistics of hdl levels from the global lipids genetics consortium and we also use multi-tissue eqtl from the g-tech project and to conduct the t-loss we use s-predict scan to obtain tiwa's z-scores and p-values for 32 362 genes and so this was our way of uh getting gene level statistics from the g was so rather than using a method such as a proximity based approach to get the gene level statistics we used the tiwas and we compared our results of running ginsen enrichment analysis on this data to those from glgc which used magenta which i had mentioned is similar to a two-stage type of approach and they use magenta for the gene set enrichment analysis so we looked at six gene sets that were implicated in the glgc paper and we analyzed them using dagsy and weighted gsea so we see that looking at the uh actual point estimates of enrichment we get from backseat there does seem to be enrichment in each of these different pathways although looking at the p-values the p-values may not be as super significant as we might expect seeing such high levels of enrichment and we think this is because tl study the sample size isn't as large so that makes the tiwas lose a little bit of power which may have affected our subsequent analyses but we still think that the fact that we were able to see some all point estimates being above one is still a promising finding and we mostly wanted to demonstrate uh the ability to apply bagzi on this type of data set as we think that a t loss is a growing type of data used to uh to get gene level statistics from a g was study and again we used the um additional gene discovery feature of bagsy here to see how many more genes we can discover so using a local false discovery rate with no enrichment information there were 578 genes discovered using the cholesterol metabolic process level of enrichment which was the which had the highest point estimate of enrichment and using that as prior information we discovered 30 more genes and then if we use all three go terms in richmond as prior information so doing a uh using multiple gene sets to conduct enrichment analysis and then using that as prior information we discover another 18 genes so that just gives a bit of insight as to how this approach can be used to aid in gene discovery so now in conclusion uh our proposed method bagzi gives an easily interpretable and accurate measure of enrichment for gene sets that maintains the power to detect enrichment from the state-of-the-art methods also bagsy presents a framework to naturally improve power in gene level testing using enrichment as prior information and also bagsy can be naturally applied to t-loss which is a growing data type that integrates eqtl and gwas so for more details we have this paper published in bioinformatics and we have our software um at this github of william when um and the software is implemented in c plus plus so it'd be used in the command line and yeah thank you to william uh corbin quick roger p greggy uh francesca luca and gstp for funding this work and thank you to all of you for listening and i'd be happy to take any questions now thanks for the interesting method thanks for the talk um one of the applications of the gene said arrangement is as a retrospective analysis for a new data source to say if it's interesting for determining new gene functions but in this case part of the assumption is that that there's new gene functions to annotate to find um and so built into this application is is the idea that the annotations are somehow incomplete that uh that you have and there that there's um like say in the go term annotations that there are um some genes that are annotated for it and some genes that are not annotated and some of those are missing and i was curious if you could how you would incorporate that assumption or that prior or how that would fit into your model for the idea that there's uh that the annotations are there that are good but then there could be missing annotations so okay so you're saying that in a certain study where my genes are annotated but there's also a lot of genes not annotated that should be annotated that's right okay so rather than just the binary annotated not annotated you have kind of a missing uh or you know uh nicely one positive and unannotated versus positive and negative i see um yeah so that's not something we actually considered in this model and that would definitely be something interesting to look at because yeah in this model we assume that annotation is known as being classified in the gene set so that would definitely be an interesting future direction that we should definitely look into uh to maybe not dichotomize the annotation for each gene i think that would be interesting yeah yeah that would be interesting to think about thank you any other questions you