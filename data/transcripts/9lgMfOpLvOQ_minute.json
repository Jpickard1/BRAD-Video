[
    {
        "start": 64.159,
        "text": "everybody welcome uh today we're gonna have a seminar from xiang from the department of biostats uh he did his post doc at the university of chicago matthew stevens group he got his master's and phd at duke university and today's going to tell us about differential expression analysis for rna-seq data thank you thank you adam for the invitation thank you all for coming so today i'm going to talk about some differential expression analysis methods our group is developing and this main work is led by my talent to the students to transition and uh and and so as we all know that ange is very important right so ange is mediating the signal from this uh dna up to the protein so and it's essentially because it's a central role in this biological process it plays a very important role in say like disease etiology and plays an important role in development and so therefore it's important to understand the rna "
    },
    {
        "start": 124.799,
        "text": "it's a law of angular different gene expressions and to do that a lot of people nowadays can uh you really do a lot of rna sequencing studies or microarray studies and once you get those results forget this data the first important step is actually try to identify those differentially expressed the genes that are influenced by predictor of interest and those predictive interests can be many different things so there could be disease status there could be risk factors there could be environmental covariance or you know that in the case of this g maping studies this could also be genotypes so you're identifying those differentially expressed genes with respect to the environmental or genetic covariance is a very important first step to understand the genetic basis of disease and susceptibility and also understand the genetic and environmental basis of phenotypic evaluation now to to identify these differentially expressed genes the first important step will try to is to do ion sequencing nowadays and the rna sequencing can be "
    },
    {
        "start": 185.84,
        "text": "very easily summarized in this multiple steps so to do the ansi sequencing you first basically extract your rna from example right and then you do a reverse transcription to get your cdna so that it's more stabilized and then you can cut the bc dna into different fragments and the construct library so that you can sequence all those fragments and once you get a sequence of prevalence you can try to measure back to the whole genome so that the small short rates will be mapped into the axonic regions so if a gene is heavily expressed then you will see a lot of reads method to that particular gene and if g is slowly expressed then you tend to have a much less read map over there so as you can see that at the end of the day the angle sequencing data will give you a lot of read map read accounts data for every single gene so therefore the first important feature of rna sequencing data is that the data the law data is actually in the form of counts "
    },
    {
        "start": 246.0,
        "text": "so because this angular sequencing data are conch based and and you can see that this gene expression variation across the technical replicates can be usually accurately described by a poisson distribution and in addition in some once you have biological replicates and they usually have other compounding factors or biological noise then in those cases gene expression variation of causes biological levels it often displays over dispersion so in order to model those angle sequencing data especially once you have biological replicates sometimes you will have to use more over disperse the poisson distribution to model that us which are also commonly known as negative binomial models so for example the most widely used methods as we know it are hr and the thick or the second version of d6 and those methods are widely used to analyze perform differential expression analysis and sequencing data and the backbone of those methods essentially a negative binomial model they try to model these "
    },
    {
        "start": 307.68,
        "text": "low counts and directly so although there are also uh many non-counts based on the methods for example lima can also be used to analyze any sequencing data because the angle sequencing data itself is a low count and because of its over dispersion often observed in your slow-comb data it turns out that it's very useful to model this count directly using this type of negative binomial models in order to get more power under control for force discoveries in anger sequencing data so therefore over here you can see that the first important feature of angular sequencing data is it's in a counter-based form and then because it's in counter-basic form it's extremely important to account for this mean and variance dependence through my modeling either personal dispersive partial models so that's the first feature and the first features will recognize and they are used by many different studies but the second feature i will mention over here it actually has been largely ignored by all those differentially expressed angular "
    },
    {
        "start": 368.4,
        "text": "sequencing methods and this second feature is that a gene expression are actually heritable so as we all know that we we all know that the gene expression levels across the human population across multiple individuals they always have a hierarchicality component they always have a genetic component and in fact if you perform a relatively large population study and try to see how much proportional variance in gene expression level can be estimated by a genetic basis or to estimate this so-called gene expression highlightability you can see that those gene habitability are relatively large so the average gene expression say in the peripheral blood is estimated to be 15 to 34 percent and depending on which data source you collect which sample size you collect and also in other tissues say adipose tissues the average heritability is also estimated to be relatively high so it's 23 percent and in fact and across all those genes you can see "
    },
    {
        "start": 429.199,
        "text": "that the highest heritability estimates in all those tissues can be high as a 90 so therefore there's a huge genetic components and there are many reasons the gene genetic mapping studies try to identify those genetic components and they have identified the thousands of assisted eqtls so those are the sniff the nearby the genes that influence in the gene expression level so the highlightability can be easily explained by the cis components and besides the cis components there's also a large remaining trans component that still remains unidentifiable so for typical gene heritability in fact ninety percent of them can be still uh only ten percent of them are expanded by cis snips well ninety percent of them are remain to be discovered and those are expanded by trans components so therefore this gene expression levels have very large genetic basis and because gene expression levels are heritable so you can see that a gene expression will typically covary with the kinship or population structure so if you collect "
    },
    {
        "start": 490.96,
        "text": "the data set that contains related individuals and because these individuals are genetically related so that and because the gene expression levels are heritable so those genetically related individuals will also have a similar expression expression levels so the genetic relatedness will cause this uh similarity in g expression levels so therefore it's important to account for those individual relatedness of population structure we typically are counting this data and we all know that in genome-wide association studies it's important to account for those type of structure whether it be it's a family structure or whether it's a population structure and if you fail to do that then you are going to have a huge type 1 error and you are going to going to have a lot of spurious associations or even reduce the power and certainly gene heritability is not the only reason that can cause gene covariance uh sample non-independence so in fact there are other forms of confounding effects for example batch effects then can also cause sample "
    },
    {
        "start": 553.6,
        "text": "non-independence so for example if you process the data set those are completely uh consists of completely unrelated individuals so individuals are indeed unrelated but if you happen to process this data say there's two individuals in the same batch then because of the battery effects the expression level on many different genes between these two individuals are also going to have assured similarity between these two individuals so therefore you can see that a individual relatedness population structure and those hidden confounding and batch effects can of course simple and non-relatedness and it is actually extremely difficult to account for and some pronoun relatedness and and and so when you have a data and a sequencing data that has the first feature that is counter based and also has a second feature that gene expression are heritable and you have a sample non-independence then you definitely want to account for both two different features but those previous approaches have only been focused on one "
    },
    {
        "start": 614.0,
        "text": "of those two features so for example one approach they try to normalize the count to continuous values and then once they can normalize account to continuous values then they can use the linear mixed effects model to account for population structure individual relatedness or population stratification and in this approach while they deal with this population structure issue they actually fail to account for the counterbase nature of aniasic so naturally you would expect that this type of approach may lose your power on the other hand there's a second alternative approach and the second alternative approach is essentially to use the negative binomial based models like a dc kind of hr to analyze your rna sequencing data but this approach fails to account for those individual relatedness and sample non-independence data so intuitively once you do have those related individuals or once you do have those hidden confounding effects which are actually quite common in the sequencing studies then you would expect "
    },
    {
        "start": 675.76,
        "text": "that you are going to have a more spurious associations or loser power so therefore you can see that neither of those previous approaches are satisfactory in dealing with these two important features of rna so motivated by this observation then we decided to develop a new statistical methods called the positive mix effects model try to account for both features of this both this type of features observed in the data so basically well that looks ugly actually i don't know what's going on i probably made this slide in the mac and this is the windows tj so so basically the mixed model does essentially look at each gene at a time so we are going to do a differential expression analysis and to do that we basically look at each gene at a time and fit in this model and this model looks actually quite straightforward so we're going to model this comma data which is the y subscript i so this is the kind of data for ice individual and "
    },
    {
        "start": 737.12,
        "text": "because it's count data we naturally model it as a poisson distributor and so with the person distributed with the later parameter that depends on these two factors one factor is the total rate depth so intuitively if you sequence and get a more high coverage for that individual then you are going to have a more real account with that particular g right so therefore it scaled with n sub i and then in addition to this n sub i we're also going to have this lambda sub i that's the unknown greater parameter and so therefore y i follows the poisson distribution with right parameter that's a product of n sub i this read depth and the another unknown parameter lambda sub i and then we are going to model this lambda sub i as a function of the predictor of interest so because this lambda i is the value between zero and infinity so we take the log transformation and we model it as a linear function of many covariate effects and a predictor of interest so over here we have our log lambda sub i and then we have a w sub i times our "
    },
    {
        "start": 800.16,
        "text": "alpha so the w sub i is essentially all those covariance you want to include which also include the intercept you want to include what alpha is effective size and besides this standard covariates you want to fix that you want to control for example the coverage can include a g effect or sexy effect that you want to control for and besides this you also are going to have your predictor of interest so this x is our predictor of interest so this predictor of interest can be anything can be any continuous or discrete values so for example if you are interested in identifying and say this qtl then this x is essentially a genotype vector as coded as yellow zlo12 as copies of real reference allele alternatively if you are interested in say detecting those h associated or gender associated at g expression levels then x could be a h or x could be a gender level 0 or 1. so the x is our predictor of interest and the beta is the coefficient of "
    },
    {
        "start": 860.399,
        "text": "interest so our goal is really to estimate the beta effect size and test it to see whether it's equal to zl or not in order to identify differential express so these two are very natural components and standard components in possibility effects model but in addition to these two we're going to have two extra terms so we have this g term g sub i term and i also have this e sub item so this g sub i is the genetic effect for ice individuals while the e sub i is an environmental effect for ic individuals and our normal part of this model essentially introducing both the g sub i and e sub i to account for those genetic and the environmental effects so intuitively and those are the fixed effects the blue times are for x times data those are fixed effects while we will model both g and e as running effects and because they are random they are going to introduce more variation than you would expect under the postal model so they both of those terms are going to deal with this over dispersion "
    },
    {
        "start": 921.04,
        "text": "issue that's commonly observed in a sequence of studies so and the although both of gme their model over dispersion the other variants are not accounted for by the standard poisson distribution they actually model two different aspects of overdispersion so in particular the e models the variation that due to independent environmental noise so if you combine all those values of e together right so now the e the capitalized e is the one by one vector of environmental effects across any individuals now this will follow the multiverse normal distribution with the variance equals to sigma square times one minus h squared so basically the variance those h does these are assumed to be independently and identically distributed with the normal with the variance equal to sigma squared times one minus h squared so therefore the e that models a standard independent variation so intuitively if you only "
    },
    {
        "start": 982.8,
        "text": "have this e term then this looks very much like a negative binomial model except the functional form is slightly different so you can think of our model with only this e term is equivalent to this negative binomial models as people are always used but in addition to this e term we also have our g term now the g term is the key factor here try to model the sample number independence due to either individual relatedness population structure or hidden compound effect so this g term here is again the g is the m by one vector and we model this m by one vector as following multivariate normal distribution but this time our covariance structure is more complicated so instead of assuming all those eis are identically distributed now we have those gi's they're actually correlated depending on your structure of this case so the k matrix essentially try to capture the sample null independence so intuitively if i and a js individual are genetically related all i am just "
    },
    {
        "start": 1043.6,
        "text": "individual that comes from then batch then this case of ij is going to be large and positive and when k sub ig is large and positive then it will introduce this similarity in this g sub i and g sub j so that g sub i and g sub j will be similar to each other and therefore by modeling that by controlling for the g essentially controlling for their similarity between i am just individual so therefore you essentially can choose and controls for the sample non-independent c commonly c in rna sequencing studies and so therefore this g is a unique feature our model kind of models the sample non-independence due to uh individual relatedness of population structure and over here we'll assume that the k is a known f priori so if you have genotype data it's quite easy to compute a kinship matrix right or if you have category data you can also compute the pinch of matrix and alternatively if you only have g expression data and if you want to control for hidden confounding effects and because the gene "
    },
    {
        "start": 1104.4,
        "text": "expression data actually contains a lot of information on where the compounding effects are you can directly use the gene expansion matches to compute the gene expression covariance matrix as the k matrix of calculus here so therefore the k is essentially a pre-computed matrix that's models in individual non-dependents and then finally over here we can see that we have two factors the h one minus h square and also h squared so if you have do have genetic data then this h squared is essentially your habitability so in fact we can also use this model to estimate the heritability for every single gene so this is our model so you can see that compared with other models at the previous negative binomial model like h and d sig our key feature is we have this extra g term to account for individual relatedness so intuitively once you do have individual relatedness sample non-independent in your data then you would expect our model and to work relatively well so this model is straight forward and intuitively makes sense but to fit the "
    },
    {
        "start": 1165.76,
        "text": "model fitting this model is actually not fitting this model is actually quite difficult and that's because it turns out the model we're looking at is belongs to this um porcelain mixer it's called the porcelain mix model that's belongs to the generalized linear mixed model family which is a commonly referred to sglm an inference in this grmm family is very difficult because we have a n-dimensional integration that cannot be solved analytically and over here the integral is actually is un-dimensional so if the sample depends on the sample size so even if once you have an even larger sample size it actually gets more difficult to do the influence in contrast to other methods once you have a larger sample size you should make things easier so and and there are some standard frequencies approaches we can use right so let's include the numerical integration that's based on gaussian by the quadrature and all are placed approximations but neither of those approaches actually scales well with the dimensionality of this integral because "
    },
    {
        "start": 1227.28,
        "text": "once you have a huge dimension then there's neither numerical integration laplace approximation is going to give you accurate answers so therefore it turns out that those commonly used methods such as the pql often fails to account for model uncertainty and can give you unbiased estimates so it gives you by st not only by the beta estimates but also buys a standard error of betas so therefore we decide to took an alternative approach and this alternative approach is to do a markov chain monocolo or mcmc based methods as a nice feature of this mcnc best methods is that it has naturally accounts for this uncertainty in all those hyper parameters because it's actually drawing chain and in fact if you run this chair over and over long enough then you are guaranteed to get into the stationary distribution so therefore that's extremely beneficial of the obvious cmc type approach however the mcmc type approach also has a common known issue and that's its "
    },
    {
        "start": 1288.159,
        "text": "computational product so it's extremely computational heavy so in fact every iteration of this mcmc approach scales cubically with the sample size so that makes inference extremely difficult so therefore we decide to use mcmc but we make several novel algebra innovations to reduce the computational burden we have so in fact we decide to use uh to take advantage of two algebra tricks one trick is based on this recent auxiliary variable-based mcmc algorithm so essentially develop an auxiliary variable representation of this partial model so that you no longer need to sample the cost data but rather sample the continuous data and this makes things relatively easy and i'll get to this point in more details better shortly and the second trick we use is the recent mixed model innovations so essentially this type of innovations applies once you have a kinship matrix and essentially this allows you to do egg and decomposition at the beginning "
    },
    {
        "start": 1348.64,
        "text": "and then at the later steps later mcmc steps you no longer need to do those cubic operations and so therefore combining by both our methods can actually reduce a per mcmc iteration complexity from cubic down to quadratic so essentially gives you an n times faster algorithm and any of the sample size so if you have a larger sample size you can see that the speed then you are going to have it's much more dramatic than if you have a much smaller sample size and uh and and we and and once we have this efficient mcnc approach then we actually goes back to our frequencies approach so instead of doing a base in a inference we actually based on asymptotics and use the posterior samples to obtain approximate maximum likelihood estimates and also suspended errors and with that we can convert them into a p-value and we'll show you later that this project will give you where we have the p values so that's our main idea of doing influence on this possible mix effect "
    },
    {
        "start": 1409.76,
        "text": "model and we call our algorithm a core algorithm which stands for the mixed model association for counter data by data augmentation because one key feature is using this data augmentation approach so this data augmentation approach is actually relatively strata forward so basically right we have account data and then we have a counter data we have observed the y terms now observing white counts in an eye sequencing data that's also equivalent to imagine a portion process on this linear line right so if you imagine a portion process in this line and this event will happen randomly on this a particular line based on this proton process and observing y counts in a time interval between zero and one right observing r why a count of angle sequencing rates that's equivalent to observing y current in this time interval of zero and one of this poisson process and that's equivalent to say that your y is count happens before t "
    },
    {
        "start": 1471.52,
        "text": "equals to one right and take plus your y plus once count has to happen after g equal to one so therefore all you need to do is to just record the time of wise counts and record time of y plus once counts and make sure that equals one happens between these two and this will give you equivalent uh measurements on the wise counts in this interval so therefore in fact instead of observing wise counts we essentially can say that we observe our wise counts uh in an interval at t sub one that happens before t equals one and also the inter in arrival time between ys count and y plus once comes equals to t uh sub two and this t sub two includes the t equals to one template right so therefore instead of directly modeling this wise count we can just model this arrival time of course of 1 and the inter arrival time of total 2. and both tau sub 1 tells up 2i actually "
    },
    {
        "start": 1532.24,
        "text": "continuous data so therefore instead of dealing with very difficult poisson likelihood dealing with this very difficult account data now we can turn this com data into a continuous data and we can perform our sampling on this continuous data which makes things much much easier so that's the main trick of this data augmentation approach and because of this data augmentation approach we use and also because of some other novel algebra tricks we use for mixed effects models we can see that our mtmp is extremely fast so compared with the standard mcmc method which is called the mcmc glm method which is this orange line if we record the computation time and also this uh and also at the log scale you can see that our method of called purple line works much much faster and this difference is much obvious once you have an even larger data set so if you have a thousand individuals then our message is roughly about a thousand times faster "
    },
    {
        "start": 1594.4,
        "text": "than this computing method so therefore you can see that now this new computational method allows us to perform mcnc on this extremely difficult position mixed model and because that this allows us to actually apply this positive mix effects model to relatively large scale rna sequencing data is quite easily now after this mcmc computation what we do next is to compute the p values so again we're not interested in doing basic influence but rather we want to make use the mcnc as a machinery to get the p-value selected and to do that we essentially relies on this asymptotic normality of both the likelihood and the posterior so we all know that our posterior like record times the price right so if we want to make influence on the like clicker then we all we need to do is to compute likelihood as a ratio between your posterior and your class so because you already know your price and you also get your mcmp example and get your post "
    },
    {
        "start": 1656.32,
        "text": "area so therefore simply take the ratio between these two gives you a like function and so and so more more specifically we essentially will assume that the prime distribution beta follows the normal distribution zero and the sigma square zero one in fact it actually doesn't matter because the price influence will be cancelled out from this call here to give you likelihood and then we compute the post area and then we use the method of moments try to match the first and second moments and eventually we get back to our beta head aspects so this beta hat estimates is based on our posterior estimate of beta and apply estimates of beta or this bit head of estimate is actually a maximum likelihood and in addition to that we can also easily compute the standard error for those mlu estimates for data so therefore from this mcnc algorithm we eventually get our beta hat under standard error by the head and that gives you a similar direction so you can "
    },
    {
        "start": 1717.6,
        "text": "take the ratio between the two that's just t statistics and you can really convert it to a value now now you probably wonder that this is an cmc better approach you probably never heard of now never saw this type of approach before someone naturally wonder whether this works at all or whether this is stable at all because we all know that mcmc depends on largely on the initial value so it turns out that results are actually quite stable across multiple runs so if we we basically compute the uh we basically do three simulation studies with the heritability equals to either zlo upon 3 0.6 which i'll get to that in more details and shortly so in those three set of simulations we run two different runs of uh call so we starting with a different set of initial value for round one and the different set of initial value for round two and you can compute the they are minus log temperature values and correspond to each other so you can see that the p values from this one set of mcmp chains "
    },
    {
        "start": 1778.559,
        "text": "looks very similar to the second set of empty chains suggesting that our intensity better p value actually is extremely stable and cause multiple rods so now you can see that we now have this very nice positive mix effects model and if it's more normal part of this model you try to account for is dump on non-independence and we also have this very efficient and cnc-based p-value computational algorithm that actually makes it eventually scalable to large-scale online sequencing sets so now we want to see that our whether our approaches indeed behave as we would expect so to do that we first did a now simulation and so we did we simulated 10 000 genes and we we also extract the kinship matrix data which i described later on and so this is essentially a 63 uh so this is the angle secreting data done up by a water bubble population and that consists of 63 and and so this kinship is 63 by 63 matches "
    },
    {
        "start": 1840.24,
        "text": "and then we similar to the uh this bunch of different settings and so that all those settings they have relatively even small actually either zero or moderate or large genetic effects so we fix the heritability equals 2.3 or we also check whether it changes to when h squared equal to zero or 0.6 and besides that we also make sure that the predicted variable x is correlated with the pinch of matrix and so that it also has the heritability equals either zero upon 35 or 0.75 and then we'll compare a bunch of different methods so this method is included so the first one is a linear regression so what we do is essentially we just normalize the angular sequencing data and fit a linear model to that and the nsc and to compute the p value a second method we compare is the linear mix model and that's implemented in the geometric software package so linear mix model also normalizes sequencing data first but it contains this kinship "
    },
    {
        "start": 1901.76,
        "text": "matrix it contains a running effect so it can control for individual relatedness and population structure however it cannot account for this combination of sequencing data and certain method we'll look at the poisson distribution so as i described earlier if you only have technical replicates person is a very good model to describe the distribution but if you have biological replicates then you want to use this negative binomial model which have over-dispersed the components and similar to the negative binomial model we also compare with the h-i and do you think which essentially put more constraints on this negative normal model to make makes them more specialized so intuitively if you have a larger data set then you wouldn't then you would expect this hr to perform quite poorly and if you have a smaller set maybe you would expect easy to an agent from slightly better than negative models and then besides all this we also have our position mix effects model that's implemented in the macro package so those are the seven methods we compare "
    },
    {
        "start": 1961.76,
        "text": "with and we'll first compare them in this now simulation so basically there's no differentially expressed genes at all and so in this now distribute because this is a now simulation the p values from the methods i expected to follow a null distribution which is a uniform distribution so therefore if you plot the p values the minus log 10 p values on the y axis and plot it against this expected uniform distribution then if a method that's controlled for in some non-independence say individual related and that's a population structure and then you would expect this line uh on this diagonal all those thoughts lie on this diagonal line so therefore you can see that among these five different methods we look at we first look at the five because we are simulating x with the continuous and theta two and the agr cannot handle continuous data and you can see that among these five different methods there's uh two of them give you inflated type one error so in particular if you look at the porcelain "
    },
    {
        "start": 2022.0,
        "text": "model right so it's you can see a huge an inversion of p value suggesting you're going to see a lot of spurious association and the similar thing happens for the negative binomial model although to a much a smaller extent and in contrast to this models you can see our method and mccall and also the lydia version the linear mix model jammer all can choose for type one arrow pretty well and you can look if you look at this genomic control factor the genomic control factors all looks close to one so these are the results for five is five different methods and again dck and hr cannot apply to continuous activators so in order to see their behavior we basically have to finalize those x values and we and discretize the values so that we compare 18 in this hr results and these sequence and as you would expect you can see that this h and you think they all have inflated type 1 error and similar to this negative binomial model that you see earlier and this is consistent with the fact that h and dc are indeed based on "
    },
    {
        "start": 2083.839,
        "text": "negative binomial models so and this uh inversion of p values it turns out that it actually depends on the sample size so if you have a small sample size it's usually harder to see the impression of p values but if you have a relatively large sample size then you're going to see a much obvious impression of p values so to see that we basically did a uh now simulations again but now varying the sample size on the y-axis on the x-axis so now we simulate it from a small studies say 60 some individuals up to a relatively large study with a thousand individuals and they were basically plotting their genomic control factor on the y-axis so you can see that when heritability when hx squared equal to relatively yellow then all of the uh only this uh negative binomial model have a slight infection of type one error while other ones work very very well but once you have a relatively large h square x then "
    },
    {
        "start": 2144.24,
        "text": "you can see that both negative binomial and a linear model are going to show a severe infection of type 1 error while our method and also germa still remains the same remains control for the genomic control factors so you can see that now in the simulation study we basically can see that both our model and this linear version the linear mix model implemented germa can control for example a non-independence so how about power comparison stuff so one advantage of our method compared with the gema while both they can show for type 1 error well once you have related individual our method can actually model this count data directly and intuitively by modeling its comp directly you would expect it to get more power compared with the other approaches so to show this we basically did this power comparison and we simulated the alternative set of data we simulate ten thousand genes among which there's one song of genes that are differentially expressed and their 9000 genes are not "
    },
    {
        "start": 2205.68,
        "text": "differentially expressed and we're varying actually a lot of different parameters to we we compared our message on this but on this set of parameters but then we vary each parameter one at a time to see which parameters more influential underrated power comparatively among different methods and so we again apply for this 77 message over here and to see which one works better so here it turns out that once we vary in different separate parameters that two parameters are extremely important in determining the radical power of different methods so the first parameter is h squared so h squared is a heritable g heritability so if the g is not heritable so if the heritability is exactly yellow then we would expect our model actually to perform slightly worse than this an active binomial model simply because we have one more parameter to model and simply because the data is actually favorable in this negative binomial model so indeed you can see the negative "
    },
    {
        "start": 2265.76,
        "text": "monomial works slightly better although difference is actually quite small but now if you have a moderate heritable gene say the heritability equals to 30 percent or highly heritable g says heritability is equal to 60 now you can see that by accounting for those gene heritability and sampling dependence now our method can get more power compatible all the other approaches and then in fact similarly because germa also kind of for example similarities german method also performs well than others but if you compare our message with gemma because our method accounts for the combination of data so we have a slightly better power than the geometry so the first so therefore the first factor is the heritability of gene expression on the other hand the second important factor that affects the relative power is this coefficient of variation which essentially is a radio depth variation across different samples now we know that if you do angular sequencing data say if you give eight samples in a run "
    },
    {
        "start": 2327.52,
        "text": "those eight samples are going to have a different reader depth so although you try to control it on average they ten million they are going to have a variation on this 10 millimeter and so the cv is essentially try to quantify this variation so intuitively if every read every example have the same identical read depth then modeling accounts don't really have much advantage but on the other hand if your cv is relatively high so if you do have some variation across different samples which we do see in real data then we would expect the modeling contract going to get more power compared with not modern accounts so this is essentially what we said so if we increase the cv although all the methods start perform poorly because it makes them more difficult you can see that the power difference between our method versus other non-counter-based methods especially gemma gets becomes larger so if you don't have much variation then our messenger jammer looks relatively similar with only a small slight advantage but now with the cv equal to "
    },
    {
        "start": 2388.96,
        "text": "0.9 the difference becomes larger and in fact the performance of jammer drops quickly once you have a large variation with depth across different samples so therefore you can see that the power really affected by these two important factors highlightability of genes and the coefficient of variation of this with depth variation now here again because we simulated x as a continuous trade so we cannot apply this easier hr so we have to discrete uh discrete like this into a yellow one trade and that you apply bc kind of hr so as you would expect that h and dc doesn't really perform well compare all the other methods and qt rates simply because we have a relatively large sample so we have six some 60 some individuals and for those relatively large sample size these signal and will perform well because they are restricting this next binomial model so now those are the simulations we didn't now under the alternative so now let's look at data effects so we applied our method and compared it with other "
    },
    {
        "start": 2449.2,
        "text": "methods in three different data sets and there's three different datasets actually represent that different situations so the first data represents a related individual the second data represents the population structure while set of data contains a hidden compounding effect another batch so we can see that and our method can be applied to a wider variety of data types so the first data is this baboon study so it's a it's a data set collected by uh professor jenny talbon duke university so basically she went so this data is collected on this baboon population in africa and she went there every summer and studied this population at purified for quite a while and this uh babun population had been established since the early 1970s and the people have been tracing this bubble population for many different generations and so they collected their behavioral data they collected their blood types and they collected all their under they draw their detailed categories so there's a lot of "
    },
    {
        "start": 2509.52,
        "text": "information available in this data but the missing information this particular data is a genotype so therefore for this particular study they decide to do a genotype to perform a genetic study and by analyzing 63 individuals and performing anime sequencing among those individuals so this data are collected on whole blood from these bubbles and we have 63 individuals and we have about 12 000 different genes and about those problems although we have a lot of different coverage we decided to focus on the variable and sets simply because it's easier to see whether the detected sex associated genes are true or not because intuitively sex-associated genes that should be enriched on x and y chromosomes right and so in this particular data this feature of this data is that they contain this related individuals so in fact the 63 individuals are collected from relatively large categories so they are more or less related to each other and to estimate this relating to "
    },
    {
        "start": 2571.04,
        "text": "estimated kinship metrics we use the microsatellite data and collected on this baboon population to estimate the k so we can plug in the pay matrix and run our core analysis so you can see so here shows the results so on the left side that basically shows the power versus work discovery rate across a bunch of different methods on the right hand side i also show the enrichment of x chromosome genes and so that's at the indicator on whether method works better than others so in either case if a method works better than others then the products then the line should be higher than others so you can see clearly our method a macro works better than all the other methods and in fact if we just focus on the fourth discovery rate for example 10 yeah our method actually gives you 50 more power compared with the next best approach that's uh uh that's a negative binomial model and compared with the standard approach say h r and d6 those standard approaches are sitting over here consistently with our simulations "
    },
    {
        "start": 2632.96,
        "text": "right so it doesn't really give you much power if you use the hr and dc to analyze your data so this is a baboon data the second data we'll look at is the fusion data so this fusion data is collected so it's a study of finland and the united states uh investigation of this knight ddm uh genetics so they basically collect a relatively large on a sequencing study with skeletal muscles and the data is collected on 267 individuals and they measure 21 000 genes and among this data will primarily focus on two variables are particular of interest so the first one is the lgtt test or oral glucose tolerance test so this is essentially indicator on whether individual is a type 2 diabetes or not so this is a binary indicator and besides this type 2 diabetes status we also look at this glucose level which is a continuous trait and in this particular data we have this population structure because we are "
    },
    {
        "start": 2692.96,
        "text": "targeting data from finland and this data is actually collected samples from three different towns in finland so if you simply do a principal component analysis you will see that pc1 pc2 capture the three different tongue structure in the stimulant and in order to control for population structure as usual we use the genotype data to estimate relatedness matrix k and we plug in so we can apply the macro to and so we did a study compared with a bunch of other methods under the top two panels basically should the results will have two diabetes status while the bottom two panels choose the glucose levels so the first column shows again the power versus the first discovery rate and while the second column is different so in the first second column essentially to look at whether the top t2d identifier t2d associated genes whether they have a high overlap or not with this top gl associated genes so intuitively there's glucose levels and "
    },
    {
        "start": 2753.04,
        "text": "the type 2 diabetes they are really biologically relevant traits so by by looking at whether the method gives you a similar answer on those two associated genes then you would basically say that which method works better so if the method works better then you would expect to uh gives you a higher overlap so again among those different methods you can see our message in my car works better than all the other methods and in fact if you look at this gl associated genes right so at 10 fpi if you use the other methods you simply won't detect anything while you use our commas that actually gives you 100 figures and if you look at the overlap you can again see a very similar trend that our method works better than others now over here let's cheat well t2d this is a binary trait this glucose levels is actually not a binary trait so it's and so in order to apply dc and hr to this type of data we have to discretize this glucose levels into their one based "
    },
    {
        "start": 2813.2,
        "text": "on the media levels but discrete like this glucose levels to the l01 actually lose you a lot of power so in fact we no longer detect any gel associated genes at all and even at a relatively flexible uh yeah cut off let's say 20 percent however we can still do overlap between this hub gl the l01 associated genes with the top t2g associated genes always the top t2d associated genes with the gl01 associated genes so essentially doing this to a reciprocal enrichment analysis so you can see again our method works better than others and if you look at agent do you think again they are the the worst methods among all the seven methods and this also intuitively makes sense and finally oh there's two previous examples i focus on relatively related individuals and the relatively large data so the final data final analysis we decide to focus on are unrelated individual and relatively small data so to do that we look at this uluba and ange sequencing data that's performed on "
    },
    {
        "start": 2874.72,
        "text": "69 europa individuals who are a part of this happening population and we perform and we get this data analytic data and the and data measured on gene expression levels on thirteen thousand genes and and this is still relatively large data so in order to see our performances relatively small data we decided to do a subsample so we basically created six samples or 10 samples of 14 samples by random sampling from this 69 individuals and we repeated this many times so we can average the power causes different sub-sampling data sets so you can get a more robust estimate of power difference between various methods and in this data we again look at this sex as the predictor variable of interest and simply because it's easier to see the uh to compare the method we can use the enrichment of x chromosomes as an indicator to do that and in this particular data as mentioned there are all unrelated individuals there's not "
    },
    {
        "start": 2934.88,
        "text": "really much population structure going on but we do have some hidden confounding effects and we all know that a hidden company has a huge problem in all sequencing studies whether that's the angular sequencing or other parts of the frequency for example studies they're always going to see the skill in compounding effect then it's always important to control for that so therefore we are using this data to show how our method can also use to control volcanic compounding effects so should still get a better performance compared with other methods i think in order to control for the skin and confounding effects we basically use the gene expression data to construct the covalent matrix so intuitively if the two samples are placed in the two the same batch then many of the genes will have similar expression levels so therefore by computing these covariance matches then the corresponding covalent matrix would be able to capture those hidden confounders and so here are the results so i'm listing the results for unequal to six case i equal to ten case and also equal to fourteen case three different very "
    },
    {
        "start": 2995.2,
        "text": "small relatively small data sets and they will use the enrichment of x chromosome genes as an indicator to evaluate your message performance and this is simply because in this data we're focusing on very small samples and you actually don't have much power to identify it if you look at the traditional power versus fdr part so in this enrichment again you can see that our method works pretty well when sample size equal to 10 also equal to 14 and works similarly either as a portion model or negative binomial model depends on which region you look at when the sample size is very small equal to six and the amount of those comparisons if you look at standard measures say dc and agi and you can see that consider with all the previous data this method's also done to perform really very well using those relatively small data effects so you can see that in the set of data by controlling for those confining factors our method also performs better than others so finally i'm just going to list the "
    },
    {
        "start": 3055.44,
        "text": "computation term so one key downside of our method is that it is computationally slow so if you just look at using the other methods they're using germa to fit a linear mix model that only takes uh actually less than a minute to analyze the baboon data and about a minute to analyze human data but if you decide to use a markov method to do that then you are going to use the hour for the baboon data and almost a day for this human data so you can see that a huge a computational cost however even though our computational cost high hour is much faster than the uh comparative algorithm that mgm egrm raises about 10 times faster here and a hundred times faster there and uh and in fact even an hour one day is a relatively small cost comparing with the time we have to spend on collecting the data on generating our examples so so uh finally as a quick summary so we basically present you a method called a "
    },
    {
        "start": 3117.119,
        "text": "portion mixed model and also presented a very efficient macro algorithm to fit these type of models under this macro this methods are extremely important and to dealing with rna sequencing data when you have some pronouns so the sample 9 independence can come from many different ways so it can come from individual relatedness can come from population stratification or structure or it can also come from when you have hidden compounding effects in your angular sequencing data and by controlling for non independent sample non-independence you can get more power and you can also get a proper control for type 1 error so and under the manuscript now it's on bio archive and then you can also download the software macro from my website and if you are interested in using the app package just let me know we also have our package to do that so finally i would like to i thank especially uh suchan and he uh he is really talented student and he pioneered all the work on this study and i also want to thank michelle and "
    },
    {
        "start": 3179.44,
        "text": "who are collaborators on the study over here and also chinku and jenny they are also a collaborators on study and i especially want to thank mike benke for providing this fusion and finally thank you all and happy to answer any question [Applause] questions um so like you mentioned earlier in your uh the simulation study with the boring data so you choose a heritability of like each little h square is point three and then the kinship uh matrix you use on zero uh like point three five and point seven five right right i'm wondering like in real life data how do you determine these values so that's a very good point so over here in the simulation we basically try to make a simulation as "
    },
    {
        "start": 3241.119,
        "text": "real as possible and that's why we use the baboon data to do that and so we essentially use the same kinship measures as in the bubble data so the k matrix is identical data and so that's realistic and the second we also use the h square so as you mentioned we value three different values 0.3 and 0.6 and that's because the median heritability of g expression is around 30 so that's why we use 0.3 and then 0.6 is roughly the 90s percentile and the dialogue is the lowest value you can get and in fact for all the other data say the sigma squared over dispersion variance we also try to estimate it from the real data and do the simulation so we actually try to estimate all the parameters from the real data to make the simulations as real as possible uh yeah exactly so for the humans actually it well for the heritability for the you can still apply for the humans because for for most genes it depends on tissue the media heritability could have ranging from say fifteen percent to thirty some "
    },
    {
        "start": 3302.559,
        "text": "percent so the immediate heritability roughly matrix actually depends on what data structure you're collecting so if you're collecting a family study and then and if especially if you are doing a twin study then the teacher measure is going to be much much stronger than the baboon we're using because the baboon are still it's only involving relatively remotely related individuals but if you say collect the human data that's really a population scale study and all individuals are supposed to be unrelated then just have a much weaker right relatedness compared with the key matrix standard problem so the bubbling kind of intermediate between all those various scenarios hi i was wondering if you tried this with any mouse experiments um because you know investigators are often very reluctant understandably to use multiple mice from the same litter because they want to avoid any kind of "
    },
    {
        "start": 3363.76,
        "text": "you know confounding with that um would you advise people now with this method that it's okay to include yeah that's a very good pun kind of wasting a lot of mice you know by not using exactly that from the same letter and other other things if you did use it for that other other things that you need to take into account because they also have a much more similar environment being raised in the same cage yeah so that's a very good point so in fact my earlier works was motivated by some mouse studies so the early works on linear mix effects model in particular was motivated by the small study because now you have those uh related mouse inbreeding mouse inbreeding mouse and mouse relatedness and you always want to control for those weirdness so that's so that's why early on in juwa especially in juan studies in mouse population people have to use the linear mix effects model to control for those individual relatedness and if we don't control for that then you are going to see a huge signal even in the early linkage studies you see a huge signal if you don't control for those muscles "
    },
    {
        "start": 3425.2,
        "text": "so therefore this message i'm developing here the mccall is particularly useful for those mouse studies because there for this anger sequence that is the most population where you can both account for companature and you can also account for the most individual relatedness so also therefore i believe this model would be extremely useful and in fact we were trying to uh looking for some mouse data to demonstrate the effect of our methods but we're going to find some relatively large mouse data so all we can find is like three mouse in one place with two mouse in the other condition so those are extremely small data that then you know the power is extremely low so it's harder to compare it's different so i think it's very easy so if you know any good relatively large mouse data that you could let me know i'd be happy to apply to there and for a second question so that's a very good point so basically the mouse they are not only uh have a relatively large genetic relatedness but many mouths are coming from the same cage and there's a huge kg effect right "
    },
    {
        "start": 3486.0,
        "text": "because in the cage his mouth they share the same food they fight together they eat it together answers share the same environment so it's important that i think to account for the cage effect so in standard g1 studies we do see a huge kg effect and if you don't account for that you still can have invited type 1 errors so i believe i i i suspected the same thing for those are frequency studies and you want to control the cache too and there are two different ways you can control voltage for example you can level cage as a fixed effect right so kg2 kg3 laboratory can could have fixed effects and let's say easily done in the mccall you can directly do that and alternatively you can model that as a running effect so you can model using the cage labels as the covalent measures then provide another covariance matrix there and i think it can be done also the computationally you probably may not be able to use the user macro tricks uh over here so the computation uh might might be uh slower than the standard of algorithm but that's a production we're also looking "
    },
    {
        "start": 3546.16,
        "text": "at try to see if we're getting a full computation okay any other questions then we'll thank you again thanks for coming [Applause] yeah works with silicon "
    }
]