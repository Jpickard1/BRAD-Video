[
    {
        "start": 0.08,
        "text": "welcome everyone thanks for joining us for today's tools and technology seminar series uh um i also apologize for any inconvenience that anyone may have experienced by the fact that we sort of last minute had to pivot to remote only with no in-person viewing option hopefully that didn't cause too much inconvenience for anybody um so we have a couple more uh weeks we're not meeting next week of course because of the thanksgiving holiday and then we'll have two more uh weeks i think once we return i think that's the only announcement i have so with that i'll just introduce our speaker today we have jonathan griack who is a research assistant scientist in gcnb and midas and that i'll turn it over to you jonathan thank you marcy thank you for the invitation i'm excited too jonathan yes yes of course nuisance i um i just wanted to log in here right at the beginning and i'm i'm just i logged out of the executive committee otherwise "
    },
    {
        "start": 61.44,
        "text": "i would be here but i uh i asked for permission to log in so i could announce to everybody that um marcy won a big award yesterday uh from the endowment for the basic sciences uh as our uh our research staff member of the year uh which has a cash award a lump sum payment and a nice uh commemorative plaque and uh you know aaron i think was going to come and give it to her but i think everybody's like in zoom bill and one of my memos earlier in the day fix that one so we're safe and sound most of us at home but uh i just wanted everybody to know that we're very very proud of marcy's contributions to the department into the common health sciences library and uh for many many reasons that go way back she's like our first bioinformationist "
    },
    {
        "start": 121.52,
        "text": "and uh we're really proud to be working with marcy and we're very grateful for everything that she does so i just wanted everybody to hear that because we mean it and we marcy this is really special and and uh forgive us for not having you be present at the thing but that's another story that i won't get into but uh nevertheless know that you're valued by all of us thank you brian it it means a lot so i i really appreciate it right so um uh you know i i really um i'm looking at this um talk and i i'm now i'm sad of course because i'd much rather be learning about this than going and reviewing uh promotion packages but they were they were nice enough to let me go so i'm going to go back to that now but um oh and hey ray good to see you uh you know i i i'm going to turn it over to you jonathan and "
    },
    {
        "start": 183.68,
        "text": "and i just think this is uh very yeah it's wonderful to see you uh right uh it's uh just uh i'm sure it's gonna be interesting maybe jonathan if you'd be willing to send me your powerpoints i'd love to take a look at it of course sure thank you thanks so much guys and congratulations and let's give marcy a hand okay because uh you know thank her for everything she's doing she makes all this stuff go well thank you marty thank you okay i'll see you later congratulations marcy um thank you well hopefully um you know uh there'll be even more exciting things than that it'll be hard to top it though so um so today i'll be talking about tensorbase methods for biomedical informatics um you know marxis uh has agreed to graciously uh monitor the chat so as i will go along if there's any questions that you have please answer you know ask them via the chat and she can interrupt me anytime okay "
    },
    {
        "start": 243.76,
        "text": "um let's see here we go so um to start off i just want to give an overview of the talk we'll start with what are our tensors um and you know i think many people have heard about them or at least they're used a lot or the name of them is used a lot and of course in tensorflow and other things but we'll talk a little bit about mathematically what they are um you know again tensors are in some sense a generalization of matrices so that in other ways they're not and so i want to talk about the differences between tensors and matrices and hopefully use any knowledge you have for matrices and familiar algebra to sort of help you understand you know how tensors are and are not like matrices um there's a few different types of models that are used for tensors again just like with matrices what we want to do is we have some kind of data that gets put into a matrix or tensor form and then we want to sort of break that data down and and help use that structure to help understand the relationships in the data so we'll go over a couple different models for "
    },
    {
        "start": 304.639,
        "text": "uh for tensors for doing that i'll i'm going to go over some applications for biomedical information just like the talk says i'll focus on uh one which is work that i've done in using tensor analysis for as a precursor for machine learning but i'll mention a few others and they'll end with a few different references and software packages that you can use to further your education and to get started with using tensors in your own research so what are tensors um so there's a number of mathematical objects that are all referred to as tensors uh in some sense that makes it a bit challenging uh if you're just searching for yourself and trying to understand um you know which tensors were speaking about uh but for this talk and you know for most of you in terms of your use of tensors for data analysis we're mostly looking at some sort of tensor which has an order d uh and this is a some object would exist in this you know space and you can think of it as a d dimensional array as we'll talk about "
    },
    {
        "start": 366.08,
        "text": "shortly it's not really such an array that but you know for the purpose of this talk we can just think about it as sort of a you know higher dimensional matrix or a hyper matrix okay and you've already seen some kinds of tensors right you know them by other names so um in this definition you know a zero order tensor is just a scalar right just a number and a field we have a one order tensor which is a vector and then we have you know second order tensors which are matrices okay so these are lower order tensors but when we talk about a tensor we really mean the higher level ones meaning that you know d is three or higher right so what are they good for why should we bother looking at sensors so one of the main uses of them and this is what's useful for our purposes as uh informaticians is to look at multi-factor relationships so you know many of you who look in work in genomics or other or bioinformatics you know you're used to looking at correlation matrices other kinds of pairwise "
    },
    {
        "start": 426.16,
        "text": "distances and um you know in your analyses so those are matrices but when we have more than two factors um you know three or more we can utilize tensors to uh capture uh potentially those uh interactions between those factors something that's uh different about sensors at least in some circumstances is that unlike in the matrix case where if i take a matrix and i break it apart there's essentially an infinite number of matrices that could potentially represent that same matrix that i broke apart but for tensors for some of those to do because decompositions are actually unique and that's something that makes it potentially more useful if we're trying to understand you know which factors lead to say the result of a machine learning model or which factors contribute you know uh to some kind of analysis that we're doing the fact that these can be potentially unique uh gives them some more meaning maybe than the matrix case and yeah lastly another piece that's "
    },
    {
        "start": 486.639,
        "text": "also important about sensors is that if the tensors have additional structure beyond the matrix and so with that and some of those structures enable you to have certain kinds of um you know properties of the sensor and subsequently data that's contained in it remains sort of the same under certain kinds of transformations right so uh certain kinds of things and matrices that um are remain invariant or equivariant for instance like the determinant of a matrix if you uh manipulate it you know with some kind of orthogonal transformation then the determinant stays the same so tensors are have this property and this is in fact the first use of tensors right so tensors if you think of tensors as in tensor calculus and tensors that are used in sort of general relativity or physics the whole point of tensor in that case was to have a set of properties that were coordinate free and that didn't depend on any particular basis and so um this is where they were initially "
    },
    {
        "start": 547.279,
        "text": "used but again that's a different kind of tensor than the kind of data tensors that we're talking about so just to emphasize again this point which is that um you know while we will use a hyper matrix you know this higher order array to represent a sensor these are just representations so um if we manipulate these hyper matrices in um you know any which way we may destroy some of the structures up there or if we take the hyper matrix and break it apart we're not guaranteed that what what we do with it um when we do that decomposition will preserve you know some of the properties of the tensor um so that's just something to keep in mind um and um but again we'll talk about some of the ways in which people have tried to um you know still um carefully you know process the tensor so that you retain some of these properties okay so i'm talking abstractly and so i want to you know give you some examples "
    },
    {
        "start": 607.519,
        "text": "of different types of tensors and what type of data potential comes in in that format so there are you know types of data which naturally give rise to tensors all right so we can think about any kind of color image so a a red green blue uh color image using that color space it's a three-dimensional tensor as our hyperspectral images images which are taken with different wavelengths of light we have in the medical space we have you know ct and mri volume so we're taking slices of you know and visualizing a patient's internal anatomy and through that we're getting a volume which can represent you know the abdominal pelvic region or someone's head and so those are naturally third order sensors as well and video is you know color video um which now we have three frames you know red green and blue that's third order tensor and it's over time we have a fourth dimension so these are for the tensors okay um another thing which naturally arises "
    },
    {
        "start": 668.0,
        "text": "any kind of joint probabilities distribution for random variables you know more than two can also be represented as a as a tensor so the other but the piece that makes these interesting for us for the purposes of analysis is that we can arrange uh you know data in the forward sensor then hopefully use that for analysis right so what can that be so um as i mentioned you know the use of sensors is hopefully you know when their main uses multi-factor analysis so if we have say five variables uh we and they're all interacting with each other we can potentially construct a fifth order tensor that somehow represents all the interactions between these five variables um another thing which is useful especially for people who do certain systems you work with systems differential equations things like that is that if we want to take a function and discretize it all right because usually we have to solve such systems numerically we can take a function say it's a third order a function of three variables and "
    },
    {
        "start": 728.8,
        "text": "evaluate it at points say on a grid and then that tensor that we get out of that will be a discretized version of that function so these are two ways in which data tensors can come about but i'll give you some concrete examples now so here's an image of the outside of palmer commons and so this is a third or a tensor you can see that i've broken it up into uh three channels right so we have these first two which is uh colons here represent these first two indices so it's the you know width and height of the picture and then the last uh one is the third the third dimension of the tensor and that corresponds to the color channel so we see this is the the color one on top is broken up into the three channels red green and blue that we see here so this is a natural sensor that occurs we have another example here this is a hyperspectral image so as you'll see in this animation this is a satellite um taking an image of the kennedy space center in florida "
    },
    {
        "start": 790.72,
        "text": "at different wavelengths of light both infrared and visible light and so you get different images with each feature of frequency and consequently you get a through river tensor out of this um as i mentioned we have ct or mri volumes in this case we see here a this is from a ct volume of an abdominal pelvic region and we see that we've segmented uh the bones that are visualized in here uh including the ribs and the pelvis and then this is a 3d reconstruction of that segmentation so this is again a a tensor there's a third order one so here's an example of a data tensor this is something that's known as eigenfaces which is a way of use doing facial recognition using a linear or multilinear algebra in this case this is a sort of visualization of a fifth order tensor where we have different people's faces so it's a two-dimensional grayscale picture of a face but then "
    },
    {
        "start": 852.88,
        "text": "looking at the face from different angles looking at it from different illuminations and then looking at different people so what we see here again this is it's you know once you move about three dimensions it's hard to for us to visualize of course but at least in this case we're seeing what um you could potentially do by taking your data and putting it into a tensor and capturing relationships at higher dimensions with it um and then another example of this uh is for um which is a recent example and related to bioinformatics which is that this paper from siegel in 2019 they were looking at phosphocorrelation responses for certain molecules of breast cancer cell lines and they created this tensor here which again this is a fifth order sensor where they're looking at different cell lines which are they have different dynamic uh different diversities they have different ligands they're different doses of growth factors and then within these "
    },
    {
        "start": 914.079,
        "text": "three factors they have um you know time points for two different proteins there so this winds up being a fifth order tensor and again this is you know we're trying the best to visualize it here but again what we can see here is that i have a way of using a tensor to capture um these multi-factor relationships and then potentially use um you know techniques then analyze it okay so we have natural tensors we have sensors we can uh take our data and put them into so again you may have you know most of us have had experience with using matrices and looking at relationships uh and analyzing matrices in all different ways so even though a matrix is a second order tensor uh you know tensors are third order higher they tend to exhibit greater complexity and this winds up showing itself in various kinds of things we try to do an analysis so the "
    },
    {
        "start": 975.279,
        "text": "types of operations that we do how much computational power is required to perform these analyses and then also we have challenges related to how to determine the ranks of the tensor as well as how to approximate them and we'll touch on that a little bit later as well so in terms of operations um so now that we have more than you know rows and columns we now have other dimensions that they could potentially look at this you know this this tensor at all the other kind of common operations that we do with matrices now become more complicated in part because we have um you know more sort of flexibility there right so in a matrix what are the different parts we can have we've got rows the columns we can take a matrix and have a sub matrix out of it right we can segment it so in a tensor with we can think about um you know all there are many more different kinds of substructures that are there fibers might be a single column but again those columns you know can be in any dimension "
    },
    {
        "start": 1036.0,
        "text": "of the tensor we have slices which will be two dimensional pieces and then we have subtenders which can be all different kinds of sizes so if you have a fifth order tensor you could have a sub tensor of any you know dimension less than that four three two one when we have a transpose of a matrix there's only one transpose um and we take the transpose of itself and then we get the matrix back but with the tensor is actually a exponential number of transposes that is contingent upon the dimension of the tensor so like in a third order tensor that means we would have six possible transposes six ways of you know uh moving the tensor around um and changing you know which uh entries are in which dimension for a matrix we have you know we want to multiply two matrices we have a standard matrix multiplication um when we want to multiply a tensor and a matrix we actually have many different ways of doing that and the sort of generalization for that "
    },
    {
        "start": 1096.16,
        "text": "is something that's known as contractions and you can think of it as taking some part of the tensor and another part of another tensor and um you know somehow multiplying some entries together summing them together to get some other tensor they can be bigger or smaller than the original tensors again these are you know much more complicated than in the matrix case uh and there are other kind of products which exist uh for sensors that don't exist from matrices so i mentioned the further contraction which is actually a tensor tensor product but you can do tensor matrix products as well and that's what this end mode product is so many more options when you're looking at a tensor versus a matrix so in terms of complexity we could think about what are the things that we do for a matrix to figure out you know a structure or understanding so we can look at eigenvalues and symmetric matrices or singular values for you know rectangular matrices we can look at the spectral norm which is the measure of the largest singular value the nutrient norm which is the sum "
    },
    {
        "start": 1157.2,
        "text": "of the you know singular values in the matrix we could look at the rank all right how many columns of our matrix do we need to represent it or we can look at some kind of you know rank one approximation so again in the case of a matrix if i take just you know two vectors uh one from each vector space and i multiply them together i can get a learning common approximation of the matrix however for all of these questions in a tensor they're all mp hard right so they are as hard as any other problem uh in the mp that's 70 complete so what does that mean doesn't mean that we can't um it just means that we wind up having to use approximation for all these types of things okay so um this is something that to keep in mind that um both in terms of you know the the algorithms themselves are challenging and also the tensors can get very very large and this can start getting you know just in terms of raw processing power you may have you know "
    },
    {
        "start": 1219.28,
        "text": "even a third order fourth third tensor can have you know tens of thousands hundreds of thousands of entries and so we start uh getting running into memory and other kinds of computational you know issues when we work with them so um we've looked at how matrices and tensors differ and you know different types of tensors so i wanted to talk about you know some of the ways in which we can analyze them um unfortunately uh just like as you can imagine with all the different operations with tensors we start getting um you know the uh indexing them telling no knowing which you know which part of the tents are referring to starts getting complex and so there'll be some notation here that we have to use and there's no good way of doing this in a way that doesn't complicate it so i apologize for that but if you have any questions please or don't understand some notation i'm using um and please let me know so i will try to make it a little easier by using sort of scripts to denote tensors bowl to be matrices and then vectors it's just lowercase letters "
    },
    {
        "start": 1279.919,
        "text": "and so when i describe some of the ways in which you analyze a tensor there's a few kinds of products that we'll need to use um and so those will be the outer product which i'm sure you've seen before uh akatri rao product which um you may not be used to and of course is nml product which i mentioned which is a way of multiplying a sensor by a matrix there are other ones as well uh there's a chronicle product and hammer product that are sometimes used um the hadamard product if you're used to doing things with neural networks that's just an element-wise multiplication and so that's often used but we're going to skip those two for now and just focus on these other three as we'll need them to describe some of the uh analytical methods we use so um one thing that we often want to do in getting a piece of the tensor is to sort of unfold it in one kind of dimension the idea is to take this tensor and sort of turn it into a matrix and do so in a consistent way and with respect to one of its moments "
    },
    {
        "start": 1340.32,
        "text": "one of the dimensions of the tensors right so here's an example of this so i have a tensor here that this is a two by three by two tensor it's an order three tensor and i'm looking at these sort of slices of the tensor so this you can think of this a one and a two as the a1 sort of being if i stack them on top of each other uh sort of into the screen that this one would be the first one in front and this one would be the one behind so if i wanted to take this you know this little you know rectangular solid of a tensor and turn it into a matrix i could do so in the first mode and what what happens here is that i just wind up getting since the first mode has a um you know dimension two and then the other modes i'm multiplying together it's six i get a two by uh six matrix and you can see what this winds up being here is that it's just really a concatenation of these two matrices right a11 is here and "
    },
    {
        "start": 1400.64,
        "text": "a2 is here and so this is the flattening of it again you can flatten this in either respect to mode 2 or mode 3 but this is one of the ways in which you can represent the tensor as just back to a matrix now again if you make manipulations on the sensor in this uh unfolding you need to be careful because you may wind up you know changing the structure of the tensor so you have to be careful again about how how you use this unfolding um no there's a few products that i mentioned so there's the outer product and you may be used to this just from uh you know you can do this with vectors um and the idea without a product is that you take you know two temp vectors or matrices or tensors and you wind up multiplying every element of one of these objects by every other one right so the indices looks complicated but that's really all you're doing you're multiplying every possible combination of an entry in the a by area one and b so here's a you know concrete "
    },
    {
        "start": 1461.039,
        "text": "example um if i have two vectors right so just one two and three five i can take their outer product and i get a matrix you notice that now rather than like the inner product where i get a single number i'm getting a larger you know tensor in this case a matrix out of this outer product and so this you can use this for vectors or matrices or or tensors to form larger larger and higher order tensors so that's the outer product this uh cautry route product is sort of um kind of like taking a column of a matrix b and multiplying the column by every entry in the matrix a so um we have an example here so if i take a matrix a and a matrix b right and i get this is the katrina product of it you see that i have now a dimension these are both two by two right and i get a four by two matrix "
    },
    {
        "start": 1523.84,
        "text": "and what's in the first column here is simply the column b but multiply first by three and then multiply by six that's what i have here and then this right hand column is the second column b but you know the first one is scaled by five and the next one's scaled by 10. so this is another product that is used in matrices and we'll see how it's used shortly so please bear with me and then the last one i wanted to mention that we'll need is the endmill product and again the idea idea here is that i want to find a way of uh multiplying a matrix by a tensor but now the tensor has you know all these potential sort of slices two dimensional slices of it so what does it mean to multiply it by a matrix well i can take a matrix as long as it has the right you know dimensions i could take this matrix and multiply it in any which any of the modes and then here's an example from the tensor i had a few slides ago "
    },
    {
        "start": 1585.12,
        "text": "where if i took that tensor which is this tensor here you recall and i have this flattening i can multiply it by this matrix here b and i wind up getting this this is the mode one product so it's a i have a flattening of the matrix uh assuming of the tensor and i'm multiplying by this matrix in that with respect to that flattening and that's how this mode and product checks out so we have some products here and now we can talk about the ways in which we want to sort of break down the tensor and begin to analyze it okay so as i mentioned just like in the matrix case we want to find a way of taking this large objects breaking it into subject objects and somehow that tells us something about the data that's there right so there are multiple types of decomposition just like there are in uh you know matrix decomposition we have non-negative factorization and we have qr decompositions you know and lu and so "
    },
    {
        "start": 1646.88,
        "text": "forth uh the two that are most common and are amongst the oldest ones for tensors are i'm going to refer to them as the canonical poliatic or cp and then the tucker decomposition although they go by many names they were just rediscovered multiple times and and you know as often happens things are developed and over time people realize that these are actually you know different facets of the same type of object um so um those are the two i want to talk about today although i'll mention three other ones which are useful for other applications and they may be useful for you and you can look these up if you're interested so there is the orthogonal decomposition model sometimes known as odeco and this is mostly useful in higher order statistics if you have some kind of cumulant or other kind of uh you know tensor representing a joint distribution or some other statistical object then orthogonal decompositions have a lot of use there uh because a certain kind of property is about yeah that come from these from "
    },
    {
        "start": 1707.52,
        "text": "statistics that enable this decomposition to be useful there whereas those assumptions aren't true necessarily in general data you may see things that are known as tensor trains which also go by the name matrix product states so these are used in quantum information theory but they are all also being used uh tensor strains are also being used in certain types of neural networks and other kinds of decompositions again it's just another way of breaking down the tensor into parts and sometimes has more makes it more computationally feasible than others another one which um i think is more recent uh but uh also has some promise which is a quiver representation and the use for this for in terms of analyzing data is that you know if i'm thinking of a tensor and i have um you know think of it as you know the interactions of all these variables i'm looking at you know as a metric for cell lines ligands and and you know whatever so whatever else i was measuring about three things here "
    },
    {
        "start": 1767.52,
        "text": "but what if i don't have all that information for each of those modes or what if so they're i can't quite align them they don't have the same dimensions they don't line up so quiver representations can be used to sort of still match these things together in a way using tensors where the different kinds of data modalities you have may not have the same dimensions and so this is something as well that is relatively recent in its application to do data analysis with tensor analysis and it's something else that you know you can also look into further so i'm going to focus on cp and tucker so what is a cp decomposition so we take a a d ordered tensor and we break it up into a sum of rank one components okay so this is the outer product here and so what does this look like i have this is a you know depiction of a third or a tensor and that means i have i can break it up into vectors each vector corresponds to one of the uh vector spaces or the modes of the tensor "
    },
    {
        "start": 1830.24,
        "text": "and um i can form the outer product of them when i form the outer product of this i take two of them i get a matrix i take the third one i get a tensor so these wind up being uh you know essentially another third order tensors but i'm at i'm summing them together and i'm going to use this as an approximation of my larger tensor um you know in some sense if you look at the singular value decomposition in a matrix this is essentially the same kind of thing you can formulate a singular value decomposition as you know a outer product of the left singular vector and right singular vectors along with the scaling by the singular values and that can give you a approximation to your matrix and this is in some sense the the tensor form of that um by the way i'm leaving out certain kinds of scalar you know factors and stuff just to make it simpler um and um one of the things that we can do in analysis and we'll use this as we see our case study a little later is that um i can take these individual vectors here "
    },
    {
        "start": 1890.24,
        "text": "and sort of put them as columns into matrices that i'll call factor matrices and what that enables me to do is essentially uh calculate the tensor in each mode and it's flattening as a product of these factor matrices as well as uh you know it's other flattening so this is enables me to represent the tensor using these factor matrices and looking at it at any which mode that i want and sometimes you can just summarize this as here i'm just putting the factory matrices in this little double brackets so um the cpd composition is where we get one of the notions of the rank of a tensor right so the minimum number of factors the minimum number of uh of these tensor summons that we have is the is the cp rank or rank of the tensor this is kind of like svd where we're looking at um you know um the number of uh you know "
    },
    {
        "start": 1952.32,
        "text": "linear independent columns and that are giving me the rank of the matrix um so what's different though as i mentioned is that what's nice about the cv decomposition is that the if i do this decomposition it is unique up to you know a permutation of the factor so i can you know take this excuse me i can take this one here and i can move it over here just a sum doesn't matter um or i can permute the dimensions of it i can move c to b b to a and so forth rotate it that's not going to change things and i can scale it i can multiply this all these by scaling factors and it's not going to change your decomposition so again because of that uniqueness which is different from a matrix case this makes this suitable for doing either factor analysis understanding which of these factors are relevant to my problem and also it's useful as we'll see a little later for feature extraction where i want i have data in a tensor form and i want to extract features from it so that um i can potentially use them and say in a machine learning pipeline "
    },
    {
        "start": 2013.84,
        "text": "something like that right and the algorithm which is most widely used for this is is known as cp alternating least squares or cpils yeah this algorithm has challenges with it uh but overall um in terms of its numerical stability and and and other challenges but overall this is still widely used as a method for performing ct cp decompositions all right the other main decomposition that people use is known as a tucker decomposition and so in this decomposition what we do is we take the tensor and we break it up into some kind of smaller core sensor and then we have other matrices one per mode which are uh used as the as there as m mode products so we try to reconstruct the sensor by making a smaller core sensor along with you know matrices that are uh in each mode right so you can think of this as a higher order principle component analysis it's essentially the same thing and that's "
    },
    {
        "start": 2075.28,
        "text": "how it was originally created uh but you know unlike the c b d composition it's not unique right i i can get i can find matrices that i can multiply uh the core tensor by as well as multiply the factor of matrices by and i'll get the same uh you know approximation out so it's not necessarily good for factor analysis but what that does enable you to do is it allows you to sort of be selective about which sp tensor you want or which core tensor you want for instance you may want a sparse sensor you know one that has a lot of zeros in it and so what that enables you to do is hopefully get an approximation into your tensor which is much smaller and so that makes this kind of decomposition suitable for for compression uh also for denoising a tensor as well and there are multiple algorithms that you can use for a tucker d competition uh three common ones are something called hos vd so it's like a higher analog of signal biology competition there's higher order orthogonal iteration of hui which is a "
    },
    {
        "start": 2136.0,
        "text": "great name uh and there's also a tucker version of als that rather than producing a cpd composition it will produce a tech or decomposition so those are the two main decompositions that you'll you'll you'll see lots and lots of literature on again they're used extensively in their various kinds of data applications so now that i think you've seen some of the compositions uh i'm hoping that we can do a you know case study and how uh these could potentially be used as part of a broader you know machine learning pipeline so this is a work that myself and my colleagues in the majority in life dumb and this was looking at trying to make predictions about hemodynamic deterioration which you know those are potentially life-threatening events that happen following cardiac surgery and so you know what we proposed in this work was to take a multi-motor approach meaning that we have all the data that's available of different types including "
    },
    {
        "start": 2197.2,
        "text": "electronic health record data and physiological signals like electrocardiogram ppg arterial blood pressure and use all this information to make predictions about who was going to over a long period potentially you know hours in advance uh predict who might have they have an adverse event particularly something that leads to hemodynamic decompensation or deterioration which is life-threatening so the challenge though is that um you know we want to look at potentially since we're trying to make predictions we want to look at trends in the data particularly the waveforms that maybe are not just the immediate immediate trends like looking at just the heart rate at a given time or irregularity but potentially longer term trends in uh in the waveform and so the challenge is how do we capture uh the waveforms at different scales and then get features out of that but then still find a way of reducing the total number of features relative to the data samples that we have so that we can make a high performing model and so "
    },
    {
        "start": 2260.32,
        "text": "i'm surprising that the answer in this case was was utilizing tensor decomposition so here's a sort of overall pipeline of how this works and i'll go over these steps in a minute but essentially we have different kinds of waveform data so as i mentioned ecg blood pressure and ppg which is measuring peripheral oxygen saturation and we have a way of capturing the sort of trends in the signal at different scales and we extract some features from them and then we've turned them into sensors and then we perform some kind of dimensionality reduction okay so ultimately this whole process to predict uses tensor formation and decomposition to create features that then go into machine learning models on the ehr data side we did something you know it's a little less complicated in that we've take the ehr feature data we did feature selection and then use pca to sort of reduce those again down but we have two sort of uh arms of this pipeline one again is starting to use tensors to analyze the "
    },
    {
        "start": 2321.2,
        "text": "signals the other one is is analyzing ehr but both go into the machine learning model to make the prediction okay so uh the way that we can capture one way at least of capturing trends at multiple scales is the use of a taut string and so this is a method for sort of making a piecewise linear approximation of a function and we can control how good of a or how tight that approximation is by this parameter epsilon so here's a you know two beats of an ecg and from the left to the right we have you know uh different values of epsilon and we see that it's maybe a little hard to see in the uh um in the left and right but we have um you know some smoothing that occurs but on the right hand side we see that what we get is just almost pretty much the qrs complex right we get very you know we're really just getting the core pieces of this uh are the main the main features of the signal so again you can modify the epsilon to "
    },
    {
        "start": 2381.68,
        "text": "get different scales of the signal so this is how we can extract you know looking at the waveform at different scales so what how do we form tensors from that so here's a sample of ecg in addition to looking at the [Music] waveform at different scales i want to look at the waveform at different parts of the waveform so i'm breaking it up into different windows of signal and that's what these dashed lines here represent so now i have different epsilons different scales i have different windows and then i can take from those when each window and each scale i can extract uh certain features in this case there are six features we're extracting from this which relate to things like changes in curvature and and other kinds of morphological features of the signal and so from this i get a six a five by six by five tensor or a tensor okay so i could theoretically i could just take this sensor and you know flatten it and get a you know 150 features but "
    },
    {
        "start": 2442.88,
        "text": "instead what i want to do is i want to try to utilize the tension structure to find something which is a bit more you know meaningful than that so what we wind up doing is to take the sensors from you know again this is a machine learning problem so we have patients that are in a training set and patients and test set i can take the sensors from each of the patients in my training set and i can wind up stacking them together so now i have a fourth order tensor if i have m patients i have a five by six by five by m fourth order sensor i can perform cpa ls to get out certain factor matrices so a b c and d and so a might correspond to the windows and b corresponds to the actual features themselves c will be the the epsilon values and d are the patients that's the sort of patient mode so what i want to do is ultimately i want to find a way of taking or extracting the features which "
    },
    {
        "start": 2503.76,
        "text": "is this b factor from you know other other patients in um you know when i'm going to apply this on a test set so in the trainings part i take all of my patient tensors stack them and get these factor matrices out so what do i do in the test case so if i have a patient who's in the test set now what i want to do is somehow use the information from the training this is sort of trying to capture the overall trends uh that exists of these patients uh in these tensors and somehow use that information to create features uh for testing so what i can do is recall that i can take a tensor this is a third order tensor and i can i have this relationship between its factor matrices if i have a cpd composition and it's flattening this is the flattening in mode two so what i can do is i can take these matrices a and c from the training step and these correspond to the t1 and t3 matrices and what i need to do is find out what this t2 is and this t2 "
    },
    {
        "start": 2566.079,
        "text": "corresponds to the uh the features that i'm interested in and so what i can do is simply compute the zero inverse of this the two factor matrices and now allow me to solve this problem essentially it's just a least squares problem and it solves uh for the fact the this t2 which winds up being the features uh from this tensor for this patient so that's what's done um and then to test this uh we had um you know over uh almost a thousand cases of people uh recovering from cardiac surgery and about you know ten percent of them or sorry eight percent of them had events um and we trained and validated this on you know 75 percent of data 25 for testing there was a patient-wise splits no patients occur and any fold any you know only individual folds and we did shuffling of the data to sort of get a estimation of you know how this looks like uh over different patients uh different groups of patients so one of the things that came out of this is that "
    },
    {
        "start": 2626.72,
        "text": "the use of our multimodal approach produced the best results so if we just use ehr data we had lower results if we use waveform it was slightly more informative using again this waveform data here is the features extracted from tensor decomposition but of course when we combine them together we wind up getting better results so this helps show this validate this part of our hypothesis and then we can see from this graph this is looking at sort of the mean auc performance of a random forest model as well and the shaded area here is sort of showing this you know confidence interval over all those shuffles of the data and we see that even predicting up to 12 hours in advance we have relatively consistent performance in using this method so this is one example of taking tensor decomposition and putting it into a machine learning pipeline for other analysis so just to talk about a few other um applications that may be of "
    },
    {
        "start": 2687.68,
        "text": "interest so we have uh a former postdoc our lab and now poster and dr bulch martin bergerian she worked on utilizing um a coupled matrix tensor completion from doing predicting drug target interactions so in this case this is kind of like a you know we have a matrix let's say drug target interactions with unknown values in that we want to complete that using some side information which are in tensors we have a postdoc in uh mine in k-bond's lab christiane who has applied this to doing classification so looking at rather than taking a tensor and breaking it down into components we want to just keep things as tensers and try to find a way of determining if there's a similarity between them and using that directly for classification so you can do this in standard matrix case using linear discriminant analysis but when you move to the tensor form it's called multi-linear discriminant analysis and he and our collaborators have came up with a way of doing this uh um you know "
    },
    {
        "start": 2749.359,
        "text": "a better way than uh some of the other methods for doing this classification and another one that might be of interest to people who are working more in the genomic side is that there's a paper from uh in 2021 which is looking at the organization 3d organization genome and diploid organisms and essentially looking at rather than pairwise distances between loci on on you know chromosomes looking at uh you know is third order you know three different distances and then using that information in and putting in the tensor and using information to get better predictions on you know on the relation structural or spatial relationships of these tensors in these cells so you know if you are working on that side of you know of the apartment you you want to check this paper out so uh in summary there's a again a whole bunch of techniques that i've just briefly talked about and talked about so for cbd compositions those can be useful for feature extraction and factor analysis uh tekker decompositions again some of "
    },
    {
        "start": 2811.2,
        "text": "them algorithms that are husbd or hui and those can be useful for compression or for for denoising that's often for instance used in denoising of images um multilinear discriminant analysis can be used for classification of tensors directly rather than breaking them apart and putting them into models completion uh you can be used for modeling drug target interactions or any kind of pairwise interactions and other also other kinds of recommender systems any other kind you know things like like uh are done on netflix or when you look at you know you're shopping on amazon you say you bought this product you may like this these are recommender systems so tensor completion can be used to solve those kinds of problems but in the higher order format okay uh there's a few different packages that um have a lot of these uh algorithms and methods for manipulating tensors built in so in python there's a um package called tensorly and uh so you can use that to you know "
    },
    {
        "start": 2872.16,
        "text": "manipulate this in python as well as it has some plugins for some of you know for tensorflow and also mxnet other kinds of uh machine learning uh machine learning packages there's and then there's two other ones in matlab uh may not be the most popular choice but although again in our lab we tend to use matlab you know for many of our things but a tensor toolbox which is from sandia national labs and that's something we tend to use and there's another package from some other prominent sensor mathematicians uh tensor lab so those two are available for just wanted to draw your attention to maybe a few other references if you're interested in learning more so there's a seminal paper by cola and vader again from sandia national labs which talks about tensorflow compositions including cp as well as tucker and many many other types of people have developed over the years so you can take a look at that this 2015 paper from chicago talks about uh attention decompositions for signal processing but "
    },
    {
        "start": 2932.16,
        "text": "also for use in machine learning and again these is a tucker model which again i don't think is the best approach i think that cpd works better but there's still a lot of good information in that paper and there's another more recent paper on specifically on uh tensor-based approaches for cardiac applications this potty paper so um with that um i'll end if you have any questions you're welcome to contact me and if you have any now i'm happy to answer them thank you jonathan i'll give you a round of applause thank you if anyone has questions you can either put them in the chat box or you can unmute yourself "
    },
    {
        "start": 2995.359,
        "text": "not seeing anything yet but maybe we'll just give it a couple minutes in case anyone's typing dana did write in no question but great talking thank you thank you i appreciate it "
    }
]