so yeah I think it's probably wise look you know there isn't anybody on the internet yet but they're most likely will be you know what we've got here is a team you know in Melvin is got some of his team members here and Emily Provost from the engineering college as well known to us and is the fantastic collaborator for Melvin and the team especially bringing out capabilities and a lot of things you know engineering and machine learning and engineering inspired behavioral science you know you know tailored to your application we brought Melvin here from Johns Hopkins University after a career that spanned up in Canada and he's from Iceland originally and he's literally a man in the world he came here to our efforts and bipolar disease research he's currently associate director of depression center he's a cheer'd professor in the Department of Psychiatry he's a delightful person and a good friend and we're very excited to see about a black drop to he's been recently involved using cell phones and tracking you know and bipolar disease and you'll tell you about the atom you know you know cell phones and in tracking people's behaviors whether they be sleep behaviors or when they're up or where they're at or all these kinds of things could be a marvelous you know additional information to help us understand bipolar disease in its treatment and it's been a great leader in that and so we're just delighted to have thank you thank you very much Brian so I'm told that people kind of stand off to the left here and so I will do that with the clicker and I'm gonna take as little time as I possibly can just want to introduce the project because I have some colleagues say you John Gideon who's going to talk on his work and so he'll quorum was going to talk on his work and Emily Mora Provost is the faculty in the computer science and engineering in the College of Engineering that is supervising this work the clicker isn't clicking okay I'm good I just want to do one plug so we just work on here one plug for the pricked her friends so we have a lady Mimi beard who's coming to do the fraktur lecture on October 19th this is a lady who found her father's memoirs in a big box father was his name was Perry Baird he actually came through the University of Michigan in the 40s and so she published this book he wanted the moon it is a discussion on bipolar disorder and his experiences with the healthcare system in the 1940s her father died it's fascinating and so she's coming to talk on the 19th and it is our understanding and Alix that it's going to be made into a movie with this fellow Brad Pitt so the other line what I'm not going so we're gonna I'm just really gonna be this slide I should have edited that but really just going to talk about bipolar disorder a little bit with the impulsivity and the reactivity and to talk about it from a from an energy perspective and then these guys are going to talk about priori but firstly in terms of bipolar disorders so many of you know me in the you know from my research in bipolar I've thought a lot about how to start to boil down the phenotype and so a common thread here is energy so thinking through the manic state bipolar disorder has mania and depressions and think of the manic state is a high energy state and think of the depression as a low energy state and so visualizing that and we sort of see it in the classical paradigm there of this sine wave you know as such okay so I did put this this in so I'm just gonna talk about some reactivity and impulsivity and bipolar disorder and using the example from response inhibition and in impulsivity and suicidality so the average person with bipolar disorder even when they're well they are not like the person on the street in other words they have a tendency to be work probably means I have this genetic disorder oh it's good to yell it's it's it's green so I can see it the red one I cannot see and so we are studying the response inhibition with Patti Delvin in the in the labs there and we're using the p300 deflection at 300 milliseconds as a measure the way that they do this yes I can do that so P 300 and so when you put at P 300 300 milliseconds and after a stimulus or after a request to do something can measure that and it's the deflection electrophysiological deflection on the brain at the 300 millisecond time point and so it is thought to be related to a measure of impulsivity at p 300 and we can measure the go/no-go stimulus there at that that state and so this is what they're to do they're all they're asked to hit the button when you see the go stimulus for example at the M here and you don't hit the button when you see the no go stimulus at the W you can vary it up and you can look at when the go/no-go stimulus is either a neutral face or a happy face so the bipolar individuals when they are seeing a no-go stimulus so their neutral faces that go so every time you flash a go you see you hit the button when you see the no-go stimulus the happy face you don't hit the button and so what you see here is is that there is a significant deflection at the happy inhibition de Simmons the no-go when it's a happy face indicating that these individuals are relatively reactive even when they are Wow so there's this dynamic state in these individuals that reflects a an instability of mood and an instability of behavior instability over time and the reason I wanted to talk about that was it gives an excellent introduction into the ongoing variability over time of these individuals as they are being monitored so what you're looking at here is time periods one week time period tear each week this is over a course of a year an individual is filling out symptom severity measures so here we have the depression of the mania score and the red is a young mania standardized rating scale the Hamilton depression score here in the blue and so you can see an individual their moods are going up and down from both the depression perspective and their varying here in the context of the of the manic symptoms so someone who is less than five sorry yeah so they said so individuals vary in their amplitude in their period of their variation and so the mathematicians are playing with this to look for for patterns but what you can see is is that you're gonna things are going up and down all the time and so in trying to make sense of this in the context of this instability of mode in the instability of both the moods going up and down and their behaviors just printed it out and took it home to my wife who was a theoretical physicist and an oncologist and so in the her capacity of physics she said well what you've got going on here are a bunch of dynamic states so what's going on in these individuals with bipolar disorder is that their moods are going up and down all the time and so we then questioned whether we could use speech removed monitoring so the hypothesis was is that speech acoustics which would serve as a proxy measure for internal emotional and affective states very simple straightforward hypothesis speech of course is modulated by the internal state of the individual psychiatrist clinicians we ask about patients we listen to how they talk and the way they talk what they say how they say it is a very integral part of our assessment so when we get histories from the families typically they will say you know somebody who's just admitted to hospital for a manic episode and they would say you know hear it in their speech there was something different about them you know a week ago they were just they were just different you could hear it so then we ask the question well if a human being can start doing that can we can we devise an experiment to see whether or not we can identify features of speech over the course of time in individuals with bipolar disorder and use that data from acoustics to identify moods and predict moods so data that we have we have 50 individuals 10 healthy controls for the bipolar individuals we conditioned on having individuals rapid cycling we ask that they have the device and talk for at least 60 minutes per week and we have weekly assessments with a Hamilton depression rating scale a clinical clinician driven rating scale and the young mania rating scale so so the so with the assessments of it is is that we have weekly assessments as you saw on that curve people are going up and down all the time individuals with bipolar disorder they're unstable even when they're relatively a well as you saw with the ERP measure and we're getting data from them for one year so what we did the interview goes for about 20 minutes to 30 minutes depending on the nature of the symptoms so they talk for thirty minutes on the phone with the therapist once a week we use Padma that as Bob no that's not part of the six team is a the amount of talk that we got from them was it was not a problem that was plenty and so some people with so what I was good about to say so what we did was we buy them a mobile device a you know an s5 at the time we pay their data plan unlimited data plan unlimited talk unlimited use of the phone and we pay them to participate in the study and so there's a modest incentive you know for that and so they so that's there's not much to not like about the the deal so the project was that every conversation that the individual has is recorded on the device the incoming conversation Brian calls mean my speech is recorded Brian speech is not that would we have to adhere to the law you can't report incoming speech without me and without the person's permission thank you very much excellent point so that's it the way that we think about bipolar disorder is that you either manic or depressed right and so that's what the average average person thinks but in fact that's not really the case so you can have states where you have energized an elevated state of energy and deflated dysphoric mood so things can are going up and down here respect to fall of each other just regardless of what they you know what there's a depression state is the manic state can go come up so there it's almost as if they're behaving independently somebody's in a manic state they're more likely to talk and talk very quickly yeah but if somebody's in the depressive state they're less likely to interact with more like a real likely to close in how do you guys account for that enrichment so that's not entirely true because people when they're depressed and in here so that they would call up and they will complain or they'll talk to something and I'm really feeling depressed and sad and so that's the sort of classical kind of way of thinking about it and some for some people it's the case but not everyone is the case like that and so so you have someone and so the other so the other thing is that but I mean there's there's so much and so many opportunities so to look at all these different features that that you know is to ask particular questions like that now so speech you know analysis and depression is nothing new so this has been done before these are the papers that are there and you can go and look at them and read them and find it yes they found features of depression in the acoustics but what they haven't done is they haven't general dataset that is passive so the most of the data or the data out there to date is generated in the context of like our assessment cause it's a kind of a contrived in conversation or you record something and they use that data to analyze and associate that with moods anybody can you can recognize the actor there anybody know the movie what is it Sabrina thanks for that know me so here's the here's really the essence of what Pryor is doing and so what we're looking for this is very simplified kind of drawing and drawing drawing and drawing so we're looking to see if we can identify features of states and state changes that would generate an early warning sign and we're not at this stage in the and the experiment yet but can that early warning sign identify a need to intervene and so the point being if you can intervene timely one could mitigate if you will the manic episode or mitigate the depressed episode so this is the reason why we're doing this is to prevent so before I end I just want to describe for you the experiment that we're doing as I said individuals get the phone they use the phone the application records the outgoing speech of the individual sent to the science folks for feature extraction the hamdeen and young mania are there to identify the mood states mood state estimation and then the support vector machine system looks at the decision making process and determines whether there are no manic euthymic or depressed state and so that is as bout as far as I can fix and so I'm gonna turn it over to John Gideon from computer science and engineering to talk about his work on the project thanks Melvin should I wait no it's trying to fix the clicker I think also importantly for this all the audio that we record on their phone is encrypted immediately before it's uploaded and then when it's in the feature extraction process that's all done in an automated way by a computer okay so I'm gonna be mainly going into a bit more technical detail of one of the publications that is one of our more recent submissions back in March we looked at being able to predict mood from speech but in particular looking at the difficulties that come up with different variations and acoustic quality because like Melvin mentioned the recordings that we have are not in laboratory settings they're just from everyday use in the phone call so as Melvin mentioned speech patterns have been shown the reflect mood in the clinic but often these previous experiments we found that there in controlled environments like a laboratory setting and they often only have one single type of recording device usually like a laboratory or a very professional looking microphone which does not look at all like somebody's cell phone microphone in real-world recordings there are big variations and background noise and there's also variations in microphone quality so the idea is speech record in the real world has large variations in quality but this would make distributed mobile health systems like ones using cell phones infeasible without controlling for these differences so there go into a bit more detail about the current makeup of the database we have about 51 subjects enrolled that hat bipolar disorder there rolled for between 6 and 12 months and given one of the phones at this point we have about 3,600 hours of phone calls across 45,000 calls of this 45,000 calls a subset 1300 of them are those weekly clinical phone based assessments and when averages are about 15 minutes each these phone calls are a structured clinical interview so they'll be asked questions like had they had feelings of guilt over the past week and SAAM Nia anxiety weight loss and from these we can form the mania and depression rating scales of the Y Mars in the handi that I'm showing earlier of these assessments 23 of them were transcribed for validating algorithms later on and I'll bring those up in a second but in particular for this analysis I'm going into we only use the assessment audio so the idea is trying to take the phone call where they're calling up the clinician and predicting the mood of that rating that comes out of that assessments we're not using the rest of the personal calls on this analysis later on so how will be giving a presentation where we do start beginning to use some of these personal calls I think it's a terabyte it's probably like 50 or 100 gigabytes it's in uncompressed WAV format so but I think earlier an average how much people call I think it's about we saw that people are calling about 15 minutes a day on average so that's the idea of the order of magnitude day that we have so about at O of two or three hours a week the interview is just a clinical interaction how are you feeling today I'm feeling really great feeling really crappy what happens when my dog dies so it's an intervention like that they have a guided interview that they're following and so so it's asking about energy and so they'll say what so how's your anything the past week and we'll talk about it today Sagar you know some of the conversations when they're negative to say your and your energy a large fine no problem and then last question have you been sad or talking about sweet little say no so we coach them to interact and say you know what's that sadness about so we get some spontaneous speech but fair enough fair enough yeah to them and that's part of the clinical insights a global clinical assessment so if somebody's we've done a lot of thought into trying to figure out this relation between like the interview and the ultimate rating and the acoustics of it and try to control for that and how we form the audio the ideas but work on this assessment calls first we know that given people's personal calls just everyday random phone calls might not have as clear manifestation of moods so this is a good first step to find those biomarkers before moving on to personal calls since the interviews have been transcribed but you could do a double-blind study to do equivalent ratings just from the transcript and see if it's a bigger variation double-blind variation based on that conversations you know we're trying to do is to get a bunch of group of team of students to listen to some of the audio and sort of rate that you know rate that can you rate the effect and the moods of the depression level you know just based on listening to that but that is a lot of work you know they have two really good training and you know listening to have a standardized way of rating we're really interested in trying to get some sort of human baseline for that's just to see where we can go diagnosed with Bipolar how many subjects you have in our 10 I believe that our control so that we're not using the in this day we're just using individuals at hat by Paul okay this gives an overall distribution of the why Mars mania and Hampi depression source so as you can see there are some people that are have both in certain situations for this experiment we decided to break it down into discrete classes to help aid the techniques later on talking with the clinicians we came up with less than six ANBU skills less than or equal to six and those skills do Fineman or the healthy state greater than or equal to ten on the why mars scale is considered to be manic and then greater than equal to handy scale is concerned to be depressed the idea behind this is as this first experiment trying to come up with discrete classes see if we're able to differentiate between this this presence of mania versus euthymic or depressed versus assignment so if we just studied the participants had two different types of phones that they used the new samsung galaxy s 3 and S 5 and initially for the study this was not something that we particularly looked into but we upgraded participants into the s5 later on as new phones came out so about eight enough data on the s3 and included in the analysis there's a bit more for the s3 in terms of assessments because this was earlier similar participants for more data what is looking at comparisons between this client of Li is there's times as much clipping in the s3 versus the s5 is you can see it's costing because you can imagine like scissors going through and cutting off the peaks of the signal and essentially this is where the signal should have been more advisable but the microphone sensitivity was too low or tuned to two Hutchison's dudes in addition the s3 with six times as loud as the s5 like they do their microphone sensitivity issue there was a 3.9 decibel drop in estimated signal-to-noise ratio going from that's loud s3 which means that there's much more noise than the s3 versus the s5 so looking at all these issues we came up with a pre-processing pipeline to address each one individually first of all we looked into and implemented a technique called akbar which helps to decrypt a signal it looks at what the signal could have been and extrapolates it we also did deal with the loudness issue noise or bus segmentation - dividing this signal into the different audio sections controlling for the noise differences between types of phones just looking into the method originally found C bar which was a method that tries to extrapolate clipped regions of speech so here you can see a signal that's clipped off here in here the idea behind C bar is it tries to minimize pointyness which is acceleration also that the signal extrapolates at least be honest amount because the one thing that we do know from points here is that this should be at least higher than that area where they are clipped off so that's what C butter does it extrapolates it's it's not going to get it exactly back where it could be because ultimately it's not going to be able to because that data is lost but this gives an idea of what the signal could have looked like and it's a better representation than just leaving these flat C bar actually is very slow the Ron speech and because we have huge amounts of phone calls it was impractical to run on so the people that made C but also made our bar which is a fast approximation to C bar which we ended up using than our pre-processing pipeline and to give an idea of what signal looks like once passed through this you can see that before and after of a signal going through this d clipping method construction it is better to pass this into our later algorithm to future extraction then the clip speech next we did the normalization followed by noise robust segmentation so the idea behind this paper by Sajid II at all was that we use five different measures of speech activity so what we have is we have a long conversation lots of which had lots of which has silence in between when the other person's talking or when both of them are silent or just some background noises so this does is it filters out all that and just produce his segments of speech which we won't look into the different sources that this paper considered was harmonicity clarity prediction gained periodicity and perceptual spectral flux these were combined together using principal component analysis to just make one signal for the likelihood of speech being present in a region we then smoothed it with a hanging window and normalized by the 5th percentile and standard deviation which allowed for the signal to be comfortable across different phone calls once we reach this point we use those 25 or there's 23 different transcripts I had previously mentioned to come up with the threshold and the amount of minimum silence that we should use nor the best optimize the creation of segments comes in the transcripts we already have those segments well-defined once we applied this is the only included segments that were longer than two seconds and cemented within those to find two seconds with one second overlap and that's for the future extraction which I'll go into in a bit but this gives an idea of what's actually going on so the future extraction will be performed on these two second sub segments nestled on the pipeline we did feature extraction which is feature extraction off level statistics and then normalization you actually mentioned this earlier manic and depressive speech have rhythm related symptoms and particularly romantic speeches more frequent it's quicker quicker and it's louder well depression has a slowing of speed sometimes slurring of speech and difficulty articulating so in this paper we explored mainly rhythm features because they are symptomatic across both types of nudes we use to costs in two second sub-segments behind this constant two seconds so segment was just control for the futures in using a consistent size segmented in which to extract them additionally the paper I previously reference showed that two-second sub-segments are long enough to have enough rhythm to be able to form a good measurement of rhythm but they're short enough that they don't have so much variation that a measurement of rhythm is difficult to attain over these two second sub segments we extract the audio envelope we extract seven rhythm statistics over each of these audio envelopes and then form together using thirty-one statistics over all of these statistics a 217 call level feature so this means that for each assessment call there is one two hundred and seventeen dimensional representation of that acoustically we normalize either globally about subjects roughly means that we would take all thunk calls across all individuals and subtract their means and divide by the standard deviation means that we would just do that on subject by subject basis which could potentially allow for subjects to be more comparable because it zero resound all their means individually and I show an analysis of doing both those different types so finally we have SVM classification or support vector machine which is a type of machine learning analysis as I mentioned previously we consider the binary cases of mania versus your thymic and the thymic versus depressed we use this participant independent testing which means that said we have a 15 subject to the mania test we leave one of them out and we train a system using 14 we then test using the one that was left out to see how well it can predict that one that was left out so it gives an idea of the generalizability of the system additionally in order to be including this up in the study we ensure that participants have at least six phone calls if we switch at least two or thermic and two or Marion Prust we as participant independent validation and match the testing and invalidation we determine which of these features are the best use of this 217 we also experimented using a multi task support vector machine which essentially what that does is it includes as a second task the phone device so while learning the process knows whether the audio clip came from an s3 or an s5 and it can take that into account when doing the learning procedure to potentially make calls from different devices not have as much weight as a call from the same device for example we use the performance measure of area under the receiving operating characteristic curve shown here random classification will be point 5 what perfect classification would be one intuitive thought behind it you see is given two samples of given two predictions what is your likelihood of ranking them correctly so if you were perfectly random then it would be 50% every time perfect it would be one so here are our results from this analysis number one at the top here you can see our baseline which was point five seven AUC Romania and point six four for depression once we added in these different types of pre-processing methods d clipping normalization by subject and multitask learning we saw significant increase in the performance of mania AZ and possibility for this is when looking at the audio between the s5 within the s5 we found significantly more clipping in manic than depressed calls and this is kind of intuitive because individuals may speak more loudly in a manic state than a depressed state so that's why it's potentially having the best benefits for mania significantly improved both mania and depression one possible reason for this is the subjects use the same phone the entire time that they're within the study so what you're normalizing by subject you're also implicitly normalizing by device occlusion for this study the results demonstrated our ability to counter variations in recording device quality such as clipping loudness and noise using these different methodologies we've got significantly better than baseline performance by using these methodologies but we did find it wasn't an overall comprehensive solution so we really need to look into the different types of methodologies that we can use in order to find the best method for certain types of tests hopefully in the future we could use this as an ability to increase subject comparability we're performing analysis on protocols following the assessment calls but that's all for my presentation are there any questions relative to this presentation before in the month you mean trying to learn different that's something that we've considered also potentially device specific process so how I was particularly interested in building subject specific models but you're talking about doing something specific pre-processing decisions to specific location yeah it's exactly so why we were collecting so there are you know there's bounding bounding with opportunities to experiment so I'm going to call so come up his project and so that was really good [Music] so the big lesson learned is the data science conundrum that you know 85% of the energy went into the process pre-processing of the data so you can analyze it right huge you know cost centers for the whole field right now [Music] [Music] hello I'm so head quorum and a postdoc working on the spirit of dr. McKinney's and dr. promise I'm also working in the primary project and in this presentation I'm going to present my work published and recent entry speech conference and I'm really happy that you asked that question because my work is completely in the same direction of your concern my work is about applying subject specific information into previous population general models that John explained most mood prediction systems usually assume that the symptomology of patients and it can be modeled using using a model for all subjects they train a population general model for all subjects but this population general model cannot capture subject specific information we know that subjects are unique and there are a lot of subject specific information that we can use in our modeling approaches so the main idea of this work is to incorporate subject specific information into previous population general models to improve the performance of previous systems we also use primary database I don't explain primary database join John explained that in this work our focus is just on detecting depression versus atomic depression this is the distribution of our database as John mentioned and at the time of preparing the paper the database contained 20 contained I think around 40,000 calls and among these calls we defined three classes depressed you time weak and exclusive excluded depressed is defined as a score of 10 or higher for hamdi and lower than 10 for y mrs atomic is defined as the score of 6 or lower for boos hamdi and why Mars and at the time of prepaying database you can see the distribution of this database the rule of paper is to detect these two regions we want to train a classifier that could separate these two classes of the population general system we used reading features as just John mentioned for to capture subject specific information we use the feature set called a lecture a vector is a successful representation of space viability as you know speech contains many sources of variability such as age gender speaker or emotion or mood and it has been proved that a victory can capture speech variability and so we can use that in mood detection and we found a victory that we found that a vector can capture your subject specific information as well in a victory each utterance is represented by its gaussian mixture model mean super vector and the gaussian mixture model or GMM means super vector of each utterance is modeled using this equation in this equation M of you as the GMA means the vector of all trans you M is the GM means wave vector of the global Moodle or universal background model the T is total wild matrix and W is a vector but the point is that in this equation you can see the difference between M of U and M of the difference between means per vector of these are trans and means per vector of the universal model Universal model this difference lies in the low dimensional space spanned by the columns of total wealth matrix it means that I vector if we want to explain if we want to define a vector in simple words we can say that a vector is low dimensional representation of difference between model of these are trends of the Global global global model so it captures it captures that vibe speech variability in a low dimensional space and makes a low dimensional space we mean a 400 dimensional space because inner speech contains a lot of variability so what an interesting point about a vector is that we could use personal calls to train our victory structure because for I make sure we have a lot of unsupervised algorithms to train this structure so we can use personal calls and we're lucky that we have a lot of personal calls to capture a speech variability from that this is the overall block diagram of our victories right now I will explain that really briefly we have personal calls and I assessment calls first which tracked frame level features for familiar features we use em FCC ffcc is a well-known set of features for speech processing to extract that first species to be applied 25 millisecond framing to speech after that for each frame MFC see features were extracted kept small mean variance normalization is applied to msec features after that the unfound delta delta feature is very excited and finally in speaker and or voice activity detector is applied over the interactive FCC features after extracting FCC features for all personal calls an assessment call we train the total Vyra two matrix and actually the victor extractor over the personal calls and finally we extract our vectors for all assessment calls then we use assessment calls and then the normalized assessment confusing wccn technique within class kiwanis normalization techniques after that we train a classifier I mean the SVM classifier to detect depression versus autonomy the point of this structure is to we can use personal calls to obtain our feature extraction method useful feature session and after that we use assessment calls to train them our esteem structure in this Estonia four different systems were trained for reading features a population general SVM as John mentioned were trained for Iowa choose a subject specific SVM is trained for and also we used to fusion systems a feature fusion and decision see fusion focus feature fusion we first combine features together and after that applies vm for decision fusion every first apply SVM's enough there that combine them svms force using the linear equation that we can see here in this equation lambda is the ways of population genomics is demand in decisions fusion we used three different decision fusion algorithms constant hard soft is fixed it is equal to 0.5 it means that population general system and subject specific system have the same effect on the final system force for heart is the left one population general or subject specific system the best one for soft fusion we select an a weight between 0 and 1 2 combined population general and subject to specific system here you can see how we divide our database into a test and train set for operation general rhythm system in this figure each node represent the data sample 3 partners represent subject ID regions represent test which test retest samples and blue regions represent train samples as you can see for population general SVM the beauty of one subject is left out and the data for all other subjects is considered for train and we test on the data that has been left out for example different iterations of operations you know general system will be something like this for subject specific classifier one one data sample is left out and is left out full test and all other data samples for that subject is considered or for train as you can see in this figure and for feature fusion we use only two samples one one data sample is left out for tests and all other death data samples for all for this subject and all other subjects are considered for training our model and to train our SVM we have some parameters cosine gamma as we am and that should be they should be defined you should be defined through the cross validation mechanism for technique we use two different kernel size linear and radial basis function for feature selection we use weighted information gain I just named them to one foot class validation and as the cross-validation measure unweighted accuracy is used Omega die QC gives the same way to different classes for EMS course signed the commonly used sign distance of Sciences to hyperplane we used and actually our databases to of balance to handle this problem we used weighted information gain and a you AR and also weighted and also we deplete all for the data samples or waited to handle unbalanced problem this is these are our results as you can see the first two rows represent the our baseline radium and I vector features and all fusion systems are the results for our fusion systems are represented in red grouse two measures uar an a you see were used for for the test results of two comparative test results you AR or on weighted average recall is automated accuracy and AUC is area under the kids are ROC as you can see for a population general system and subject specific system are quite similar to each other from the point of you a our population general is easily do better for a you see some of these specific but is a little better but the variance of them are quite different the variance of a victor is much higher than the variance of rhythm it means that rhythm can work more consistent for different subjects and it is reasonable because for a victory we just use data from one subject to data is smaller so yes this high variance actually makes sense and truly the three few genres or fusion results are better than the baselines but three of them are significantly better than the baseline the based on reading features and rhythm system one interesting point is that soft decision fusion works well and it also provide really small variance it means that it is consistent over different subjects and it provides an estate stable system this is the results for different subjects results of rhythm population general system i've actually specific system the soft decision fusion and the a way that we use to combine these two systems the number of autonomy number of the present number of personal calls as you can see for IV true for numbers or lower than chance but a lot of numbers are better than read them and so our victory is not really stable but when we combine a victory to rhythm we have really better accuracy what it's interesting point is that decision fusion selects better system when the different systems are different from each other for example when a victor is a much better than rhythm it selects I later when rhythm is better than a victory 70 reasons but in some cases when I vacuum rhythm are similar to each other it actually combines both of them and our performance and for a conclusion we showed that it is important to capture both subjective specific and population general information we use redevelop terms of decision fusion algorithm to combine these two systems and to Train a vector we use personal cars and also these results show that we are able to have a population general system for initial users and after that improve the performance of that system gradually by capturing more data for that patient for future fusion one one important problem of Pacific system is that we don't have enough data so if we use similar subjects in our classifiers we can output may be we can now perform these results and thank you for listening do you guys have any plans in the future [Music] first it took us about two years one and a half years or so to get this through the IRB we got it through the IRB because we specifically are not planning to listen to the conversations so if folks knew that we were going to be analyzing what they said that they would be less we can can we can use the content of the assessment cause for example to do that there are many different strategies and things we can do for example and when somebody's in an episode we can just in going forward could you call in and just you know give us a piece of your mind if you will or there'll be some way of someone to call in and share with us what's going on or have a conversation and particularly when they're in a manic state or depressed state yeah so what we want to do now though is we have the system downloadable from Google Play and the data can be bumped into the Amazon Cloud so we want to get as much data as we can we do want to go forward getting clinical assessments in the current concern in a similar manner and potentially with daily assessments of the individuals you know after a three minute call so the real question becomes which calls should we analyze and in listening to the assessment calls is really fascinating because you have a 15-minute call where should you drop in on that call to assess the you know the features of it and so it it turns out some people and when we were listening to them at the end of a sentence their voice drops off in the context of a depressive episode or in other people you know you hear in the beginning of you know the sense of awe you know or going up like that so the human brain has the capacity to integrate that data and sort of you get to know how a person sounds in a particular mood State and so the real challenge here is to determine from a computational man what part of the sentence what part of the speech you should actually go down on and in analyze fact something specific differences to because every specific you have differences in the acoustics video also differences in the way that people manifest their symptoms so as was mentioned earlier some individuals might interact less with the world when they were depressed but some might instead go on social media and post a lot on there for example as their way of it expressing their symptom ology so we got IRB permission to you know to capture all the strokes on the phone so every time you stroke the phone or do anything on the phone we capture that all the I'll the you know the gyrometer the accelerometer and you know a GPS coordinates you know in the amount of data in and I don't even the battery degradation rate you know would be significant so the whole concept of you know psychomotor activity is captured in the context of the device the other thing that that we're particularly excited about this for is is that and why bipolar disorder is such a thing it's most fascinating illness in the history of humanity essentially because the variability of it is profound so can we use the information gathered from bipolar disorder to identify features that would predict sickness behavior so we've talked with people in cancer for example and so that you know when somebody is surviving from cancer could you monitor them with priori over time and then determine you know when something shifts off the baseline that is that a predictor either of mood state variation or recurrence of cancer likewise with cardiovascular diseases I guess one thing that what's the catch of June's question first yeah I just wanted to mention that I'm actually new in this field and I was working in other fields of speech passing for example I worked on speaker verification speech recognition or fields of speech processing the originals for example for speech recognition we have the accuracy of 90 something 95 something 96 something but here you can see our accuracy is 70 something the best actually so I just want to mention that the field is too young is to me and it's really valuable if we can obtain something in this area also so how make a comment and there the question at the end so law enforcement keeper occurred our fingerprints and now was a DNA DNA matching is very accurate science ought to be only of other type of match I read a fascinating article about facial recognition so if you build a database of people's pictures but the real life you know collected images it's hard to match to database of a million suspects so this article is about public release law enforcement in UK are now hiring human who who are great at recognizing faces and great and remembering then the performance for that it's way better than support vector machine and everything basically the premise of that how effort is to say fingerprint can be matched to your database the faces of suspect cannot be much too reliable by machine to a database I guess my question is how well do you see the voice can be matched to database a voice is potentially even different mood states after that person really interesting question for a variety reasons it goes back to the whole concept of you know what you know a with sensitivity and specificity and so that you know I'm a psychiatrist okay I am not all that worried about specificity you know in the sense of and so my colleagues in the computer science and engineering said well we want to be really accurate they want to be accurate they really are striving for accuracy and so he was playing in we were 90% and there were only 70% this is really an intolerable situation but for a psychiatrist and a clinician this is not bad because really what I'm interested in is this sensitivity is in being able to predict when somebody's running into an episode and so I'm interested you know in in almost in Zona Markov model as something as the you know you could get a hit from day to day and so you're getting a suggestion the sensitivity is hitting 70% that there's a likelihood of a depression going on and that goes on for four or five days that is important too because that would trigger this early warning light and then we would need specifically the clinician the person as they say in Scotland the person that would actually interview the the you know or interact with the person with the with a patient and determine if there's something to be worried about and so when the light goes off the goes off for somebody up in you know the U P and and we're sitting here with a big board and we call a clinical person up there I need to check on you know Joe blows up and the U P and they nurse there or the social worker or whomever calls them up talks to the guys I went on a big drunk this weekend and you know but I'm fine now and everything you know I'm feel bad about it but that's what happened or it could be you know listen I wind up I just went gambling and I get this and so you got to get the context of it as well so that yeah but that's the dad you see but so the whole purpose of this then you know Brian is to be able to triage the individual is for end or intervention so if we could use so you want to be able to use the health system in the most efficient way that we possibly can and so if we could monitor you know as many individuals technologically and triage those individuals for care that would be the holy grail of this system I think also kind of going off the it depends a lot on defining your context because when you're looking at like a criminal database trying to match images within that it might not be a human performance but for example Facebook actually has better than human performance on recognizing faces within your own friend groups they can recognize faces that are completely obscured or let out better than individuals can but that's within a certain context of that platform so a lot of this is about recognizing the context of where you're working and trying to work within that problem so I was going to say give an example on the same 100 along the same lines there was a face recognition competition when their different universities participated in it and the first place actually that Russian team for actually spoken to an education competition and then they they spin it off on a startup and they what they did they did a very deep neural network traded on that there is a brush Russian social network vk.com and they have millions of people's a crush on Facebook so and then they are open so they train their model on all of those images and publish an app mobile app that you can download you can take a picture or friend a person on the street and it will find it if this person is on the great woman Russian Facebook it will find them and so people started doing that they would start taking pictures from a random people on subway in the bastion on the street behind them find their profiles on the Facebook and that works very well as you said this it's you know in the 90s like that I you see is like 90 to 95 and then company when this when they spin DUP they would start up out of that they got a contract from Russian law enforcement and police actually now working with them to use that for our law enforcement and the question for you guys you are very limited in terms of like your data set because it's not very big that's why you stuck pedestrians you know because we fight characters there might not make many instances you do consider transfer learning may be looking a bigger speech datasets that already out there and then there are pre-trained models that you can take and you know flag your samples in there and see if you can take like those or presentations yeah yeah yeah I'm particularly interested in looking at potentially mid-level futures that are trained on external data sets like there's some reasonably large emotion datasets that we can use that we can get a measure of how emotional the person is through a conversation and that is likely tied to their mood patterns as well so we could extract those sorts of more direct features and then use the more limited data set to figure out how the Tytos to mood so yeah looking into that because I think I saw Emily politely publish a paper and they did like deeper model and some more intimate for speech assessment and they had the relatively large for hours our big problem is that we have a lot of data but we don't have much labeled data so we're trying to figure out how to bridge that gap right now yeah logical and in week we got people you know downloading the app and bumping it up into the into the cloud so we could say one of the strategies that we're thinking of saying so why don't you call us you know talk to me when you're feeling in edit so they're calling us and you know and I'm grading my mood as this and so they give us a label on the rating and said you know I'm really feeling shitty and I want to tell you how what a real jerk they were at the University of Michigan and didn't give it anything and I couldn't get an appointment so you kind of what you're suggesting or saying that this kind of database good because we what we want to be able to generate it for that personalized system for the for the individuals so that they could call in in various of your moods and then the system could learn what they sound like and so that they would get better at recognizing when when the wheels fall off is that if you have large labeled data said that there's already out there and other people using it for emotional abuse or something like that you can pre train your model on that large data set right and then use those fix those learn weights and use it on your smaller and weakly labeled data to get representations the problem with the data sets that are out there and you can correct them if I'm wrong is what I've seen a lot of actors you know and so that they get people that you know say well I want you to walk into a room and pretend you really angry and so people will get on the hood you know that and there will be speech you know examples a PT knows that you know really putting contrived emotions on it so yeah person who's better so the question I would have is said if we got the patient's themselves and so we got people that are in our study it would take a longer we would be collecting data but would that be I think it would be more valid babe I mean there there are data says that they build by showing people said or you know videos that they make them say I am P or angry or something we actually look at the tools and a presentation like that collected the data like that but it's of course it's not as I guess as intense as patient situation maybe not maybe modality is not the same of that emotion that they invoke in them I mean at least gives you a starting point from there and to be able to probably conserve like transfer learning you talked about yeah you could start there and then try to transfer the knowledge into the field is more natural I just want to add something to your point yeah actually it is completely to we can use all our databases and define some mid-level features because you know the distance between speech and mood is really you know large if we can define some mid-level features here for example emotion or just I'm working on another mid-level features for example if we can capture how much this all trance is different from the the how much this all turns is different from that subject if you have a model for that subject and you can capture for example if you get that to a speech a verification system what's the score of that a speaker verification is it really similar to himself or not that all transits did is similar to himself own it can be another mid-level official but you're right we should use another mid-level features to actually handle this low amount of data base that's my way more complicated model way yeah for example yeah for speech verification systems they used neural nets version they extract these are vectors or deep neural networks and they're they are actually better than human beings the systems are more accurate than human beings but the point is that they have very big databases and listen ya know when this first needs needs a lot of database but if it has not been proved yet you know we cannot say that neural network won't work for small we cannot say that but usually they use a lot of data to train that so we'd like to see happen you know to get this data in a way or in a manner that could be presented you know through the you know through - or in some manner so the student you know wakes up in the morning comes up with an idea in Coweta then can access at least elements of this data system to run some experiments problematic thing with it of course is that there's a there's a you know from the audio I mean we just have the raw audio now we don't do anything with it in terms of its content but we do have the capacity we constructed so we can't make it available and you know even internally because of the IRB issues so if you know if somebody wants to come in you know and play in the sandbox you know there's there are more than enough things that could be could be done so talk to these guys about that super this has been an excellent presentation it's great to have you back yeah thank you thank you very much for the I'm just collaboration thank you