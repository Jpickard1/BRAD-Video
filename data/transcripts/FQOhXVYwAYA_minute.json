[
    {
        "start": 0.0,
        "text": "what development great thanks you I'm very excited to talk to you all today I'm gonna be telling you about some work I did you mentioned recently partly overlapping with my postdoc and the the we named our algorithm lagger and this is a picture of the liger but you get if you cross a lion and a tiger and I'm excited for this talk because I'm going to get to focus on some of the algorithmic details which I don't usually get to talk about because there are a lot of biologists in the audience and usually these things get relegated to the the end of the supplement so I'm excited to get to share a little bit about the the actual inner workings of the approach and the the heart of the approach is non-negative matrix factorization and so I'm going to share some of the things that I've learned about the spectrum of options that are available when you're implementing a non-negative matrix factorization approach to the overall problem that "
    },
    {
        "start": 64.439,
        "text": "we're interested in solving with this algorithm is moving from qualitative definition of cellular identity which is the traditional way biologists think about cell identity to a quantitative definition so qualitative definition of cell types has traditionally been based on things like morphology so how does the cell look under microscope we can see that there are clearly different stall shapes and these are likely different cell types surface markers painstakingly characterizing a handful of surface markers which the presence or absence decides whether a cell is a type A or B or phenotypic properties where you can do some kind of measurement on the cell like if you're thinking about neurons the phenotypic property of interest could be the firing pattern and so based on some ad hoc combination of these qualitative properties biologists have traditionally defined cell types but now with the with the invention of "
    },
    {
        "start": 126.36,
        "text": "various single cell genomic technologies we have the opportunity to revisit the definition of cell type and properties of cell identity from a quantitative perspective using all these different types of omics which have been initially developed in bulk and now we could perform them on a single cell level so we can measure expression in single cell RNA seek is by far the most widely used method for measuring single cell properties you can also do single cell methylation sequencing and attack seek which gives you comfort and accessibility measurements you can also do hi-c and most recently people have started doing spatial transcriptomics where you have a 2d or 3d coordinate for the cell along with its gene expression profile and so the kind of grand goal that I set out to feed was to try to "
    },
    {
        "start": 186.62,
        "text": "leverage all these different types of measurements across a range of biological settings to move toward a quantitative definition of cellular identity and in order to accomplish this we want several properties we want to be able to identify both similarities and differences in corresponding cells across conditions species in tissues and part of the reason for this is because we want to be able to understand the distinction between sort of the invariant properties of the cell type that makes it that cell type across settings but we also want to understand the main modes of variation in which particular cell type in vary for example in response to treatments or environmental conditions or rows we also want to be able to capture continuous and discrete modes of cellular variation so the traditional way in which people have thought about cell identity is cells occupying discrete types but it looks increasingly like in many cases "
    },
    {
        "start": 247.939,
        "text": "some identity is more of a continuous phenomenon where cells can kind of transition between stable minima continuously we also want to be able to distinguish technical confounders for biological signals because one of the main challenges with single cell data is a large amount of technical noise due to uncontrolled variation across experiments as well as just the inherent challenges of measuring quantities from an individual cell and additionally we want to be able to incorporate these multiple kinds of data which is very challenging because the data each have their own intrinsic technical biases which are completely different and they often measure completely different features for example gene expression is a completely different type of measurement than chromatin accessibility we expect them to be correlated in some way but there are different modalities and so integrating them is challenging so that's kind of the space in which I'm "
    },
    {
        "start": 308.48,
        "text": "working here until last year during my postdoc with Evan and also into this year I'm still working on this we developed an approach called logger linked inference of genomic experimental relationships and the name came from a liger which is what you get if you read a line in the tiger and the this is kind of a metaphor for what we're doing with the approach which is blending different kinds of data together and we released a preprint on the bio archive last fall and just within the last couple of weeks the paper was accepted by self and so this is our graphical abstract from the salt paper so basically what we did is we developed this approach logger which uses integrative non-negative matrix factorization and we investigated several different ways of performing data integration we did it across individuals to identify human subject variation across species to any great amounts of human cells and then we also integrated different modalities including gene expression facial trick "
    },
    {
        "start": 370.09,
        "text": "transcriptomic data and genomic data and bragger outputs a set of meta-gene factors which allow us to identify both similarities and differences in cells across datasets and also perform multi omec integration so this is kind of the summary of paper and now I'm going to walk you through sort of how I developed this approach and some of the decisions that I made and alternative approaches can see and also give you an overview of the what I learned about non-negative matrix factorization in the process so a necessary first step for a lot of single-cell data processing is performing dimensionality reduction and this is helpful for two reasons first as a simply computational and practical reason the reducing the number of dimensions and the data allows you to operate more efficiently on it so rather than working with say a 10,000 by 20,000 "
    },
    {
        "start": 432.349,
        "text": "dimensional matrix we can now work with 10,000 by 20 dimensional matrix and there are a number of techniques that are used to perform dimensionality reduction including PCA and I see a map and all of these approaches can also be thought of as denoising the data or extracting the most dominant signals that you can use summarize the structure data so in addition to just computational efficiency dimensionality reduction allows us to denoise the data and account for some of these biases and qualitative data and so on a first step for a lot of single cell approaches is to do dimensionality reduction and a common choice for dimensionality reduction is PCA principal component analysis and PCA is an optimal approach under certain assumptions and it produces a low dimensional representation that has certain properties including that it is each of "
    },
    {
        "start": 492.559,
        "text": "the components are mutually orthogonal nmf is an alternative approach which makes different assumptions about the data and produces a representation that has some different properties in particular the factors that you get after performing nmf are not necessarily orthogonal and PCA is globally optimal a globally optimal solution to the optimization problem by which it's formulated whereas nmf the objective function is a non convex problem and so you can't hope to obtain a globally optimal solution all you can hope for is a locally optimal solution and so pretty when people do enema they have to perform all restarts and take the best objective however there are some cases in which the orthogonality restriction of PCA is actually a limitation and nmf is since it doesn't have this limitation sometimes it can better capture distinct factors "
    },
    {
        "start": 553.08,
        "text": "whereas PCA might live naturally together and here's the geometric example of how this could be the case so if you have this point cloud' to principal components would be these green arrows note that there are THOG '''l but neither of them really captures the actual directions of variations of data so both of them kind of capture a combination of these two axes of variation which a different approach that doesn't have your thuggin allottee restriction like ica or and MF but actually better capture and this turns out to be an important property which I'll come back to but um another advantage of n MF is that it yields interprete belated watts or is the 4th organ ality is just a simple mathematical definition that this essentially means that the axes are perpendicular in multiple dimensions so "
    },
    {
        "start": 613.38,
        "text": "the well yeah the axes are the principal components that you have clear from the data that's correct so the question is how do those axes relate to the actual underlying biological signals ortho I'll come back in a second so another benefit of n MF is that it yields interpretable factors so this is a figure from the original n MF paper in nature doing the difference between the way n MF and pca represent faces when you do dimensionality reduction on base images you can see that this is an example reconstructed image using the PCA and the n MF low dimensional representation you can see the reconstructions are essentially identical but what's different is that if you look at the dimensions themselves the pixels here "
    },
    {
        "start": 676.29,
        "text": "are colored by the intensity of their loading on each dimension you can see that with the nmf factors the factor each factor kind of represents a part of the face so you can see mouth here and here you can see some some eyelashes or eyelids here whereas if you look at the principal components it's really not easy to understand what each individual principal component represents it's some kind of ballistic property of the faces the other thing that makes it hard to understand the nmf are the principal components is that individual components can contribute either positively or negatively to the reconstruction and that's why they're some of them are colored black and some of them are colored red and so it makes it really hard to understand how you're actually reconstructing this face in terms of the individual principal components whereas with nmf you can clearly see for example this factor right here corresponds to "
    },
    {
        "start": 736.37,
        "text": "chin and so the office way up at the top here here's a mouth bacteria and that one's actually activated so the idea is that an advantage of nmf is that you can actually point to how a particular factor contributes to the reconstruction of hydrogen allele so now if we imagine doing nmf on gene expression data we have a matrix that has n cells and g genes and we want to decompose it in such a way that we have K factors which we use to reconstruct the data and we're gonna identify the game loading values so we can think of each of these rows of the W matrix as being a meta-gene and each of the columns in this matrix as being a expression of that meta gene in a particular cell "
    },
    {
        "start": 797.59,
        "text": "yeah yes that would be called I think it's called semi nmf yes good question I don't think that there's anything in the optimization that would require that to be the case so my guess would be no but one thing I should mention mathematically about this is that the solution is not unique so for any H and W if you multiply H by a matrix and then multiply W by the inverse of that matrix you get another solution that has the same objective function value so that probably means that in your in your case "
    },
    {
        "start": 860.26,
        "text": "the matrices will not be okay so here I'm showing one example of the advantage of this interpreter ability property I'll show some others later in the talk this is one of the things that got Evan and myself excited about these factorization approaches here I'm showing factor xi from this is a zebrafish heart regeneration data set and you can see that this so this is a two-dimensional representation of the data and I'm coloring each cell by the loading value on this factor and you can see that this factor loads on all the cell clusters and within each cluster it has kind of different values for some cells than others and if we look at the the top-loading genes on this factor they're all mitochondrial genes and we know that the difference in the amount "
    },
    {
        "start": 921.28,
        "text": "of mitochondrial genes per cell is a common source of technical variation this tends to vary between batches and it's an uncontrolled variable even within batches and so the fact that we have a factor that indicates the amount of sort of mitochondrial gene expression here allows us to either use or not use that source of variation when we're clustering themselves and we've observed in a lot of cases that if we identify a technical factor like this and then remove it before clustering we get clusters that more accurately represent biological differences among selves because for example this this group of cells right here if we just naively perform clustering using all the factors this might show up as a separate cluster whereas if we remove this factor then it turns out that these cells are really not very different from these cells biologically and they get merged into a single more correct cluster so this is one advantage of having interpretable factors genes here not necessarily "
    },
    {
        "start": 990.589,
        "text": "always but it's it's pretty stable yeah but be surprised by logic it's true you might be yeah so in some cases the mitochondrial content of the stones would actually be an important variable but in this case we actually we found that removing the escape from that for a number of other independent reasons look much more okay so that's nmf in general and in order to integrate data from multiple data sets we use the modified "
    },
    {
        "start": 1051.789,
        "text": "version of nmf called integrative n enough and the main difference between i n MF and regular n MF is that we now have multiple data sets which share a set of genes but have different potentially different numbers of cells and we're going to force the meta-gene matrix to be a sum of two different matrices one which is shared across data sets and one which is data set specific and this has several advantages so by having this shared matrix across data sets this causes each factor to essentially have the same biological interpretation across data sets because there's a common set of genes that define this factor across the data sets but at the same time we can also identify the data set specific ways in which these factors of variation change and this has two advantages one is that it allows us to account for sort "
    },
    {
        "start": 1112.899,
        "text": "of nuisance variation for example if we have two different batches where we expect identical cell composition and we do PCA and we find that the cells separate by batches these data set specific terms can allow us to identify for each factor what is the nuisance variable that makes this factor different at the same time it also allows us to actually identify these data set specific signals themselves which in a lot of cases if there are biological differences among your data sets these factor differences can give you insights into the biological differences immediately and before we try this we tried something simpler which was to just simply stack our data sets dosac easy mate receipts and perform regular nmf on them and we found that if we did this we didn't get the effect we wanted because the resulting factors contain some factors where the "
    },
    {
        "start": 1174.7,
        "text": "the signal was essentially is it from data set a or for they sent me and the rest of the factors had some mingling of Sheriden distinct sources variation so explicitly separating out these data sets specific effects allows us to much more accurately identify corresponding factors of variation constants and so we use this integrative non-negative matrix factorization and vigor and we developed an additional approach that performs robust joint clustering using a sharing factory neighborhood graph across data sets and this allows us to be able to robustly identify shared and data set specific population so now I'm going to tell you some about nmf and how we actually implemented this the objective function for regular nmf is Polly we want to minimize the squared for beanie it's norm between the matrix and "
    },
    {
        "start": 1235.22,
        "text": "its reconstructed HW subject to the constraint that H and W are both positive here's the objective for integrative nmf it's essentially the same except we have the addition of a data set specific term and we have a shared meta-gene matrix the shared across all data sets we also introduced a regularization term which minimizes the data set specific effects that's great subject to the constraints that HD and W are all non-negative and so this optimization problem is non convex and there have been several different approaches developed to solving it so the first approach that is developed by Li and sung in the nature paper is called multiplicative update algorithm and this algorithm is easy to derive and easy to implement the way you derive it is you essentially write out the first order optimality conditions for this "
    },
    {
        "start": 1295.85,
        "text": "optimization problem which are defined by the KKT conditions you take an optimization and then once you do that you can actually solve for H the individual elements of H and W in closed form but the problem is that H and W occur on both sides of the equations you have to iterate these updates the way that you do it this algorithm is you initialize H and W with random values and then you iterate until convergence update each element of H following way update each element of W using this equation and insulting versions and you can show based on the kk k-- KKT conditions that these updates do not increase the objective function value so this approach works but there's two main drawbacks one is the speed of convergence and two is that there's no theoretical convergence and the only thing we know about the solution is that "
    },
    {
        "start": 1358.46,
        "text": "the updates do not increase the objective function generation so we don't actually have a guarantee that the that the optimization strategy converges to the local rate so people after the multiplicative updates develop new approaches based on block coordinate descent so gradient descent is a very commonly used way of optimizing these sorts of objective functions and yeah ideas essentially it you start at a random point and at each step in the optimization you compute the gradient and then take a step in the direction of the gradient this morning and you repeat that coordinate descent is different in that you only move along one coordinate at a time during the optimization and the gradient descent is often more well behaved but for certain types of objective functions including the one that we're working on coordinate descent can work just as well and it has the "
    },
    {
        "start": 1418.64,
        "text": "additional advantage that it's often easier to solve the optimization problem along one coordinate at a time holding the others fixed as opposed to trying to solve it for all coordinates at once and in our case we can actually break the optimization problem into blocks and optimize blocks of coordinates at a time holding the others fixed so this is a diagram from a paper a very helpful working paper by Kim and parks where they review block coordinate descent algorithms for nmf and non-negative tensor factorization and so you can think of blocks of variables here as being either the entire matrix in the entire matrix h or individual factors which are columns of w and rows of H or individual elements of each of these matrices and and then you can perform ROC coordinate descent by taking steps along one block well thank you go to sleep and there's a really helpful "
    },
    {
        "start": 1482.66,
        "text": "classic theorem in non linear optimization are sorry non convex optimization from Vegas which is that a block coordinate descent algorithm is guaranteed to converge parameters lie in a closed convex set which parts do and if the block coordinate descent subproblems are convex so because we have this rubinius norm loss if we hold W or H six problem is convex and the other and the block coordinate descent algorithm empirically converges very fast or the NMS problem so now I'm going to walk you through some of the different strategies we tried so the first one that we tried the first block for data set is an alternating least squares algorithm and essentially what you do here is you fix W and then you use non-negative least squares regression do identify the optimal age given that W and then you repeat and this is quite easy to implement if you "
    },
    {
        "start": 1545.299,
        "text": "have a non-negative least squares solver and it has the advantage that you solve each subproblem exactly and it's fairly straightforward to do this because it's just a least squares problem the the thing that makes it a little difficult is the non negativity constraint and so here are the updates that you would have to iterate for integrative non-negative matrix factorization you can see that it's it's not too difficult to write out the objective and it's actually quite similar to what you would do for regular and in that it just involves stacking so the challenge and the limitation of this approach is that the non-negative least squares of problems it's difficult to solve them all efficiently because you essentially have to solve a different subproblem for each of the that's a W but it turns out that there's sufficient overlap among the subproblems you can actually leverage the overlap in "
    },
    {
        "start": 1606.2,
        "text": "a smart way using a strategy called block principle pivoting which was developed by Professor Park at Georgia Tech and with block principle pivoting you can leverage the commonalities among regression sub problems to solve quickly and you still have the property that you're you're solving the optimization exactly for this for the block that you are updating at that point another popular strategy for optimizing the nmf objective is called hierarchical alternating least squares and it's called hierarchical because again you hold W fixed and solve for H but you do it hierarchically where you solve for each of the factors iteratively and so essentially you're dividing the variables it did for IMF we're dividing the variables into two D plus 1 times K subproblems vector blocks instead of "
    },
    {
        "start": 1667.04,
        "text": "matrix blocks and this for the case of vector blocks you can actually calculate just using the derivatives in closed form what the optimal update is for each of the factors and if you do this or I nmf you get these updates here and the the bracket plus means we're going to take the max between X and epsilon where epsilon is a small positive value and this both enforces the non negativity constraints and avoids numerical problems with factors that are all 0 and so if you look at the form of these updates you can see that they will be fairly straightforward to code and most of the work actually goes into computing these matrix products here so if you're using a language like MATLAB or R or Python that has highly optimized an "
    },
    {
        "start": 1727.97,
        "text": "acreage routine it's usually pretty efficient though you can actually go up further and accelerate the house algorithm even more by using a clever trick which is that we update each block up to a times in a row which allows reuse of the matrix products here which are actually fixed for the entire block and the theorem of bertsekas still holds in the case where you update a fixed number of times each coordinate before updating next as long as each coordinate gets updated within a fixed number of iterations the block coordinate descent algorithm will still converge and so it's actually possible to calculate this optimal value of a empirically based on the sparsity pattern of your matrix and "
    },
    {
        "start": 1789.75,
        "text": "the relative sizes of the matrices and I also tried out this approach but the matrices that that we're using our sufficiently sparse that it really didn't help very much so the the algorithm we ended up going with was the ALS algorithm and actually this is the algorithm that's preferred by park as well as their implementation of n enough and it's it's hard to beat just because since you're solving the the coordinate updates exactly on each iteration it really decreases the objective function quickly so here's a benchmark showing how the multiplicative updates approach prepares with the ALS approach for different values of K on two different single cell data sets and that this PBMC data set has about 12,000 cells and the pancreas data set has "
    },
    {
        "start": 1852.36,
        "text": "about 10,000 and so you can see that we're converging much more rapidly and in a lot of cases the objective value that we're achieving is than what the ultimate about dates emergency and it also converges in what I would consider to be reasonable amount of time for the datasets overanalyzing which is around two to three four minutes okay so another type of nmf algorithm which i didn't know about but discovered versus this project is online and that algorithms this is a really interesting area online approaches essentially address the problem where you have data that's continually coming in and you can't store it all but you want to compute some result essentially using the whole data set and it's they're called online algorithms because this is a common problem with online web applications where you have data "
    },
    {
        "start": 1913.0,
        "text": "constantly coming in and have to update your analysis of the data so there's a really interesting field that tries to develop online algorithms and there's actually a number of online algorithms for performing an event so two key advantages in these approaches one is that you can avoid storing the entire data set in memory and the other is that you can refine the results as data continuously arrive so um I thought this would be a nice feature to have vigor because we can continually update our definition of so identity as new data sets arrive and also we're getting to the point now where we're analyzing data sets that have hundreds of thousands of millions of cells and storing all memories is annoying so rather than implementing something that's a true online algorithm I first tried sort of a smart initialization strategy just to see how this would work and it actually "
    },
    {
        "start": 1974.32,
        "text": "turned out to work quite well the idea was that since we know the meta-gene the identity of the meta genes from initial run through the data we can use those meta genes to come up with a smart initialization for new data points by simply solving this non negative least squares problem and then we can use this initial value for our H on the new cells and just restart the optimization at that point using the initialization from the previous version of the data set and then we can just iterate those updates until convergence we can also use a similar strategy to update the number of factors on the fly and so there's two different cases you have to consider depending on whether you're increasing or decreasing number of factors and so if we're increasing the number of factors we can we can "
    },
    {
        "start": 2041.6,
        "text": "initialize the new factors using these updates based on the existing factors and then we just restart the optimization using these visualizations or if we want to decrease k then we first compute this Sigma value that essentially tells us how much each factor contributes to reconstruction and we picked the k2 factors with the largest Sigma value and then simply restart the optimization and when we benchmark these approaches they actually work quite well and you can see that especially in the case of adding new data the approach converges extremely fast in less than 1/4 of the time that's required to recompute from scratch using all of the data we also I didn't mention this but it's also straightforward to "
    },
    {
        "start": 2101.94,
        "text": "show that you can change the value of the regularization parameter lambda 2 to try a different constraint on the similarity of data sets much more efficiently than you could from restarting the factorization from scratch ok so the this was a initial online approach but not a true online approach because it's mostly just zatia strategy even though it in practice it gives you a similar fact so the way that you can perform nmf online is to identify a surrogate function and and then optimize it using stochastic gradient descent and someone showed in 2010 that if you use this surrogate function or the nmf objective that converges almost surely to the optimal value of n enough as T approaches so the "
    },
    {
        "start": 2164.26,
        "text": "nice thing about this approach it's kind of a neat trick of the updates for the meta-gene matrix depend only on the sell factors and data they depend only through two matrix products a and B and you can compute these matrix products incremental as new data arrives so essentially when you get new data you just form this matrix product on the new data and then add it to the accumulated matrix product from the previous data and I I tried out the implementation of this online learning algorithm from the 2010 paper I tried it out on some single-cell data and it performs quite well so this is a batch nmf using the house optimization strategy and the red line is the online @ MF and you can see that it converges in less than a sixth of the time that's required for the batch algorithm and in addition it has "
    },
    {
        "start": 2224.98,
        "text": "the the online version has the advantage that you don't have to store the data in memory so another another trick you can do with n MF is missing data imputation and n MF is actually really good at this because of its parts base representation so a simple extension that allows imputation in n MF is to define a weight matrix where a IJ is 1 if the data is observed at 0 otherwise and then just perform optimize this objective where we take the element wise matrix product between the egg and the loss and you can optimize the subjective using any of the strategies that I presented and then at the end you have a matrix where you can impute you can compute the missing elements of the matrix using an objective that was trained only on the observed elements of the data so I haven't implemented this yet for IMF but "
    },
    {
        "start": 2285.17,
        "text": "it's something that um a couple other moves you can make with n MF is to use a different loss so instead of taking the simple Euclidean distance between your reconstruction and the data you can use different versions like the KL divergence or the eat Icarus Saito divergence and people have found this to be better for count data in some cases and this to be better for audio spectrum another trick is you can actually write the n MF objective probabilistically and this is what David Bly and his graduate students did in 2015 when they developed an approach called hierarchical Poisson factorization the idea of hpf is your observed counts are produced by a Poisson distribution where the rate parameter is determined by the vector product of a row vector and the column vector and then you can further put "
    },
    {
        "start": 2346.06,
        "text": "distributional assumptions on these row and column vectors using a gamma distribution and this gamma Poisson model actually gives you something where the account for in the individual gene across cells is negative binomial a distributed which matches well with how people think about the distributional properties of single cell organs and somebody recently implemented hbf for single cell RNA seek data MSB systems biology we've tried this out and it it works pretty well but it's extremely slow because I estimating the posterior distribution under these assumptions for a large matrix is quite challenging and very slow and in practice we also notice that it tends to they possibly because of the uh the Bayesian nature of optimization so now "
    },
    {
        "start": 2406.5,
        "text": "I'm I'm just gonna go through quickly some of the things that we use this for and give you a sense for what the liger algorithm do we uh we generated a data set from the mouse edge nucleus which is this tiny little region inside the brain and this had never been sequenced using single-cell RNA seek and we found a crazy number of cell types we have actually never seen a region in the brain that has this many distinct cell types and it may be partly because this region has kind of an integrative role in integrating signals from all throughout the brain um and the thing that's really interesting about the Fed nucleus is that it's the most sexually dimorphic region in the brain we did we sequenced male and female mice and compared the gene expression from the mics and the a lot of the cell types come from the principal nucleus of the bed nucleus which is also the most amorphic region within the bed nucleus and if we look at the managing factor "
    },
    {
        "start": 2468.63,
        "text": "that loads most strongly on this region we can identify the male and female specific genes that are specific that are dimorphic specifically within this population by looking at the data set specific factors and the the genes that we found here there were some known hits there's I think 12 genes that somebody painstakingly validated as dimorphic in this region but we also found a bunch of new ones including some very clear dimorphic genes like et al for a CDR 1c so again the interpretability of the factors is he here because it allows us to identify not only that cell types are shared between male and female but specifically how each of those cell types differs we also did a related analysis in the substantia which is the part of the brain that produces dopamine and it goes wrong in Parkinson's disease and we were able to identify "
    },
    {
        "start": 2530.9,
        "text": "corresponding cells across human owners he had seven human donors and putting the data from these different samples together was actually quite challenging because there are so many uncontrolled factors of variation so this was frozen brain Bank tissue and we had like three males and four females ages between 20 and 70 and cause of death everything from heart attack to brain trauma and so we were able to put these together using the shared factors that logger at first and we were also able to identify some human subject differences among these patients I'm using the data set specific factors of migrant we found a couple of interesting things one was that there was the patient 5828 had a lot of genes related to cell activation upregulated and it turned out that this was the patient who "
    },
    {
        "start": 2591.78,
        "text": "had died from head trauma and this is the genes that we found as being games at specific are known genes that are related to response to traumatic brain injury we also noticed some genes upregulated related to unfolded protein response and also a toe e which is a gene that's commonly seen in a number of neurodegenerative diseases and it turned out that 5840 had a post mortem diagnosis of cerebral amyloid angiopathy which is a neurodegenerative disease related to amyloid accumulation so it was interesting that we were able to identify these datasets specific differences from the dataset specific factors we were also able to take the substantial Niagra sauce or human and map them to mouths substantially ourselves to identify corresponding cell "
    },
    {
        "start": 2651.84,
        "text": "types and we were hoping that we would find some cell types that were human specific but we didn't really we actually found some cell types that were mouth specific but it turned out that they were related to anatomical and dissection different so for example we found this population of astrocytes was mouth specific and it turns out that this particular subtype of astrocytes only located in a certain part of the brain that's very much closer to the substantial Niagra announcement on so the the cross species analysis essentially showed that the subtypes are quite similar between thousand we were also able to integrate single-cell RNA seek with spatial transcriptomic data and we use data from a protocol called star map which can measure up to a thousand cells a thousand genes per cell with spatial resolution and so we we were able to "
    },
    {
        "start": 2714.05,
        "text": "using the shared factors that logger produces we were able to identify which clusters from dropsy correspondent to which clusters from star map and we're actually able to plot with spatial distributions of the cell types using the coordinates of the star map data and when we did this one thing that we found which was unexpected and interesting is that there's a subtype of astrocytes which has this spatial distribution according to the star map data which we confirmed with its c2 staining from the Allen Brain atlas and it turns out that this is the same subtype of astrocytes that we saw else and this helped us to understand why because this part of the brain here is very much closer to the substantia in Dobson so once we had this mapping from dropsy to star map "
    },
    {
        "start": 2774.81,
        "text": "we were able to impute the spatial gene expression patterns of genes not measured by star map because star map the only measure about a thousand years or so and we we were able to confirm using by holding out some genes and then feeding them that we could correctly impute the spatial patterns of these genes and we were also able to do this for for complex spatial patterns like this PSG which is a marker of endothelial cells and this is a strip of vasculature that apparently was left in them we were also able to do multi modal integration with RNA and methylation data and this was the most challenging type of integration that we did because it's not clear exactly what the relationship between methylation impression is but we were able to identify corresponding cell types by assuming that the gene body methylation for a gene is negatively "
    },
    {
        "start": 2835.59,
        "text": "correlated with this expression and doing this we identified a set of joint clusters that matched very closely with the annotations of the RNA and methylation that had already been published and then also helped us to clarify a couple of the methylation populations which are hard to identify because most of the previous studies that identify cell types have looked at either protein or RNA expression so if you just have methylation for a cell it's sometimes hard to figure out what cell type it is but by mapping the RNA data onto the methylation data or vice-versa we were able to identify for example MDL 3 mouse deep layer 3 is actually Foxtrot and also there is a discrepancy between clusters layer 5b which the methylation analysis thought that it was actually from layer 6 it "
    },
    {
        "start": 2895.619,
        "text": "turns out the reason for this and this annotation was that these cells have very low overall levels of methylation and none at the common layer 5 markers from expression actually we're another thing that we were able to do with the methylation data was actually identify cell types with better resolution than was possible with the methylation alone do we zoomed in on this bot we pulled out just these MGE cells and did a second level of clustering and were able to identify a total of 12 clusters whereas before I think they had identified two in the methylation analysis alone and when we look at these clusters we can confirm that they're not just sort of spurious computed structure by comparing the methylation and expression for these genes some of the markers of these clusters and noting that they have extremely anti-correlated "
    },
    {
        "start": 2957.32,
        "text": "expression and methylation we were able even able to identify this type of selves here MGE flow which are called toddled cells and they represent point 1 percent of all the cells in the dataset so the fact that we were able to identify these and the methylation data was was pretty surprising and we also we didn't include this in the paper but we're now working on integrating RNA and single-cell attack seek data which is even more challenging because the attacks the data is extremely sparse you usually only get about 10,000 reads per cell and so in addition the relationship between accessibility and expression is fairly weak correlation it's challenging but we have some initial results that it's possible this is a mouse cortex RNA and ataxia configuration another application that I'm actively working on is using lagger to integrate data across "
    },
    {
        "start": 3019.43,
        "text": "different time points and development and using this to study cardiac regeneration in zebrafish and human cortical development and we can also use vigor to integrate drop seek data and slide seek slide seek is a new experimental technique that my post on mentor evan usually published that gives you unbiased transcriptome wide spatial measurements and using logger we can identify the spatial locations in these or clusters in the drop seek data with that I will end and acknowledge the folks in the macaques collab who did the actual experiments valina helped some with coding the liger r package and Marta and RP from Steve McGarrett slab thank you I have a question of that's like you're required to have the same "
    },
    {
        "start": 3080.96,
        "text": "set of genes yes it requires that you have corresponding injures across campuses so following up on that for the kind of attack and RNA integration great kind of which way have you found is best to unify the feature set is it like just taking promoter accessibility or is it taking accessibility or pathology body or yeah I'm actually working on I've seen that out right now I have two data sets and in one data set the gene cluster motor is the best and the other promoter only sees the best but it's not it's not entirely clear why that's the case good so did you do any Studies on one stick we don't get done the same versus on one of imputation works I haven't actually tried it for oh so the only imputation we did was this spatial and "
    },
    {
        "start": 3143.32,
        "text": "so we investigated several ways is this what you're asking about the half station of tradition yeah so just to be clear I haven't actually implemented the weighted and performs imputation on missing data here we did the imputation by identifying for each facial cell it's and nearest drops each cells and then just simply taking the average and that was good enough to get this but we assess this in a couple ways so we tried leaving out one gene at a time and recomputing the factorization and then imputing that missing gene and we compared that to using all of the data and imputing genes that were used in the alignment and we found that the results were essentially identical so we so leaving out genes didn't really change the results at all but comparing the measured genes with "
    },
    {
        "start": 3204.9,
        "text": "their imputed versions dude that we were able to do pretty well another thing we tried was we used a sort of a dumb imputation strategy which was to take the joint clusters and then just simply set the imputed value to being the expression average across the cluster and we found that the mean square error was actually significantly higher when we did that compared when we looked at the twenty here's neighbors across datasets many questions putting in a picture I think a technique when the problem is open there's a few hour delay of the genes right so I've looked "
    },
    {
        "start": 3265.74,
        "text": "at I guess three data sets now do of the data sets are mature fully differentiated cells where there's essentially steady-state no differentiation expected for those cases I haven't seen any examples where the PAC is decoupled from the RNA but I also missed up and I observed that in this data set there's intermediate population which is kind of between two sulfates and that was significantly less correlated with expression than any of the other populations so for that for that case essentially lagger just says these populations are different but yeah that's that's definitely something interesting to look at more the other questions all right [Applause] "
    }
]