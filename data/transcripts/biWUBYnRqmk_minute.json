[
    {
        "start": 0.56,
        "text": "awesome uh thanks so much uh Marcy for the introduction and uh nice to see you all and uh great to be presenting in the tools and Technology seminar um so the topic of my talk is you know ranking and reting improves out of distribution robustness and as you might have seen from the name I do not have a concrete medical application in mind but I think many of the techniques that I present could be used used for uh in medical contexts and I would love to hear from you guys regarding that okay so uh yeah so I think like before I get started I just want to give a little bit of uh overview of my past research to see what kind of work do I do so I know this is the tools and Technology seminar and my research is on people and technology so a little bit related to tools and Technology okay so and especially I built uh machine learning "
    },
    {
        "start": 61.12,
        "text": "and deep learning methods to understand people and Technology uh so in some of my past research what I've done is I've looked at this large clickstream data of what articles people read how much time they spent reading on it to understand what are uh the stable and the evolving interests of people so that we could uh like do so that we could construct better user profiles and we could personalize uh content for them on like you know New York times or Boston Globe or Wall Street Journal um and so that they have a better better user experience online and recommendations are one part of it okay um and uh yeah so there's some uh like in this particular work like we built a neural network which combine the Simplicity of Matrix factorization uh plus the flexibility of neural network to give to give a stateof the art performance on building this efficient and interpret trajectories "
    },
    {
        "start": 123.759,
        "text": "um then another work you know uh we did an experiment so here the idea is um that if you want to Target like let's say think about your uh like any online sub online newspaper say uh in this case Boston Globe right and newspapers as you all know like nowadays have subscription based models right so you have to be a subscriber in order to continue reading but what happens is that you know people lose interest and they cancel their subscription so in this experiment what we're trying to see is how to proactively Target discounts to people uh so that they would not cancel their subscription uh okay so now unfortunately the problem is uh that we um that the that whether somebody cancels or not or whether somebody continues to be a subscriber is only "
    },
    {
        "start": 185.28,
        "text": "observed in the long term right you know uh but you have to Target them discount now so we have to you have to Target an intervention right now with the hope that in the long term maybe year two years five years from now you know uh they they continue to be your subscriber so think about that like a good application of that is also in a medical context where you want to Target the treatment you know like some dose of a medicine right now but the outcome would only be observed 5 years later to see whe the person survived or what's their quality of life right so how could we use shortterm uh information about the person to Target discount to Target discounts in this case that are better in the long term okay that that that improves some long-term outcomes so we developed uh like an approach that uses surrogate variables to Target discounts to people like personalized discount personalized treatments um um so to say and you know we come came up with an "
    },
    {
        "start": 247.079,
        "text": "optimal policy that is what discount would Target to which person so that uh so that the cancellation of subscriptions of people are minimized over over like a long period of time which in this case was three years okay um so yeah so like this was some of my recent work um uh that I did and and um you know uh there are some key assumptions parried in here right okay so there are some key assumptions bed in my past work on content analytics um so first of all um uh like we assume stationarity we assume that people's preferences and their content consumption patterns uh stay stable over time okay we also assume that the that there is a stationarity in the user panel we track an existing set of users over time "
    },
    {
        "start": 309.199,
        "text": "and we assume that no new people come uh in our panel over time right and finally a third assumption that we make is the group proportion good group proportion stational that is the relative proportion of people that are in a training set versus in a testing St set this this uh stay more or less constant right so for example the number of people commenting on an article from India or China uh stays the same uh in the training versus the test set right and um so basically all of these three assumptions they are related to this idea of a distribution share and the idea is that we want to build machine learning model that are robust to out of distribution data and in the next few slides I will show like how distribution out of distribution uh data could creep into such a scenario and like how we could mitigate "
    },
    {
        "start": 374.52,
        "text": "that okay so distributional uh robust uh machine learning is a key uh like I should say a foundation of Equitable machine learning right so what Equitable machine learning says is that we want to ensure fairness in our model outcomes we want that all patients uh receive the same outcome uh receive the same treatment uh like irrespective of their race or gender or the demographics right so we want to ensure a fairness in its outcomes right and um however out of distribution uh uh data could lead the models to perform poorly on under represented people in the data and and uh as I will show some examples what happens is that such models perform poorly from people who are minority groups okay so what we want is that a model which is trained on say patients from an "
    },
    {
        "start": 435.08,
        "text": "arbor which is mostly white and Asian population uh it should also do well when we deploy that model on patients in Detroit which is mostly black population yeah right so we want to build models that are robust uh across these distribution shifts in the in the populations now let's think about U more conrete um definition of our problem so robust machine learning and Equitable machine learning are very active areas of research and uh there are like several different ways in which the uh like there there could be distribution shifts but for this work you know we focus on what we call group distribution shifts okay and uh even a further subset of that we uh uh like we zero in on two particular types of group distribution shaps so the first uh scenario is like imagine like imagine this example where "
    },
    {
        "start": 496.68,
        "text": "the training data contains examples of one types of camera uh but on the test data you see a totally new type of camera for which there are no examples in the training data okay the second type is population shift this was the example that I gave earlier uh in which a model is trained on patients in Anarbor which has certain demographic makeup but on but while testing it is deployed in an area which is a completely different demographic makeup so how do we ensure that models uh trained in one place generalize well to other um uh like other like other domain other type of test data or could we fix them like could we train these models better or maybe we could fix them post talk uh to perform better do we have a second yeah so I think like let's try the US let's try sorry everybody hold on one moment having some in room technical difficulties no no so okay so so let's "
    },
    {
        "start": 558.0,
        "text": "yeah so this was right yeah so let try that oh there we go good job Mark okay so um uh so so this is a high level overview of the problem that we are trying to solve um and I just wanted to narrow down the scope of the problem that we're trying to solve because distribution shifts can be of so many different types so before we proceed further are there any questions about the setup right is this something that you can relate with with your own domain that you're working with like can you see examples of such a case there and does this setup uh like the scope of the setup makes sense to "
    },
    {
        "start": 621.079,
        "text": "everybody so any any questions any thoughts before we proceed further okay um so uh let's see the let's see the uh like the formal problem definition and setup okay so we assume that we are given data to us uh that we are given uh XS uh like X and Y pairs you know the features and the output labels and we're also given group identities right and let's assume that we are there are some n examples in our data and what we want to do is the statistical inference task is we want to infer the model parameters of some model Theta right and a data is split into a training data you know dra Trin and D test and we only have access to the training data right so just to go "
    },
    {
        "start": 681.12,
        "text": "back to the earlier example that I gave we do not have access to if our model is trained on Anarbor hospitals we do not have access to Detroit data at all so we cannot even look at 1% of it and change the predictions of our model uh so that it predicts well on that population composition right so we just have access to the data that we have like from an arbor and uh like the models uh uh the model parameters are estimated via the simple ERM simple empirical loss minimization where you assume some loss function you know your favorite model could be uh like a neural or non neural network or what have you that you just want to minimize the loss over all these uh n examples and to estimate to to get the like the maximum likelihood estimate uh of these parameters Theta okay and then you want to make a predictions why had based on these Theta parameters that you've "
    },
    {
        "start": 743.72,
        "text": "estimated so U uh at least the purpose of the uh models uh for the scope of the things that I would discuss here the the Baseline features or the model that we'll consider is a pre-trained b model so B is a precursor to some of the llms that all of you hear about these days okay so uh B is an encoder model which takes in input text and gets you and outputs uh representations real value representations of uh of those of those Texs you know you could you could use any other model here are like whatever I'm going to say in the next few slides is still applicable to to a different model like you know robota or uh like Excel net and so on so but we just chose bird because like it's probably the most well used model for for for such a context and it's at least parsimonious to uh estimate uh uh like you know B edings for this type of data okay and "
    },
    {
        "start": 806.8,
        "text": "the model performance okay is evaluated using worst group accuracy right so this is in contrast to standard machine learning where you look at mean squared error or classification error or mean absolute error or something like that so in this case uh so so the mean error hides the worst group accuracy right like so you could still build a model on Anor patients it would do well if you look at the mean uh like uh like the like the average accuracy on patients in Detroit right but what it's hiding under the hood is that it performing like far subar on the uh on the on the people that that belong to the minority group right so we want our our model to do well on the um uh uh like you know on the worst performing DRS you know which are typically uh in most cases are the minority groups also in the data okay so now you know now that I've described the problem uh and like you know this simple empirical risk minimization framework and I've "
    },
    {
        "start": 868.759,
        "text": "described our uh evaluation metric let's see how is this uh problem currently solved you know what's the state of the art okay so the stateof the art like there's this idea about um you know Minimax robustness right so what people what we want to do is we want to train the model uh to minimize the error of the worst performing growth okay so you want to take a Max over all the groups uh G belonging to capital G so take the max loss of all the groups and find the parameters that minimize the max loss right the simple minia Max loss that you want to do and as I said like our evaluation metric is the worst group accuracy it's a natural um objective to minimize right like and as you know uh most of machine learning uh uh like is about convex learning objectives right or something that could be easily optimized right like so so "
    },
    {
        "start": 928.839,
        "text": "that's what people thought about you know thinking about convex surrogates of this non-differentiable uh objective function that can be optimized easily okay so and there have been many approaches that people have uh proposed on this Minimax framework to solve the group distributional robustness framework and there are some approaches like you know drro distributional robust uh optimization group Dr and all these approaches at a high level what they try to do is try to find like a convex approxim imation to this objective function and trying to solve that right now a key assumption made by these approaches is that U uh uh that the that the low worst error worst group error on the training data would be would mean would imply that on the test data also we would get low worst group error right which could be a very very strong assumption because implicitly you are assuming that the worst group on "
    },
    {
        "start": 989.759,
        "text": "training data is similar to the worst group in testing data okay like a priori without any knowledge that's a very strong assumption to make right but the fact that people have been doing these things is because like most of the machine learning research proceeds we have an objective function that we can easily optimize you know how would you know what we cannot see on the test data so why not based on the information that we have is just uh minimize the the worst groups accuracy right so that's why there a justification of what these people are doing and why these approaches have been uh popular in this uh uh in this pafic area there so uh before I uh describe our model in detail you know I'm going to describe a key intuition behind an improved solution right now what are what is some of the blueprints of a better approach uh like approach that could perform better right and then we will see on test on like some actual "
    },
    {
        "start": 1051.08,
        "text": "empirical data whether that approach actually performs better or not so the key idea is that instead of focusing only on the worst performing group at training time consider several per several poorly performing groups okay right so now going back to the previous slide where I said that these models uh rigidly assume that the worst group in training data is similar to the worst group in testing data what we are seeing is you know we should relax it a little you know uh it turns out that we could take some combination of several poorly performing groups in training data and they should be similar to some of the worst performing groups in test data so basically moving from one to like uh K bestest okay so now um why is this a good idea as I as I just briefly alluded to you know the worst group might not be similar to the worst group on the test data leading to to out of distribution uh uh generalization problems and uh "
    },
    {
        "start": 1114.44,
        "text": "like this point is even more Salient for group distribution shift you know where unseen groups arrive at test time or group proportions change yeah so in terms of uh like you know U uh like statistical terms what we are what we are proposing here is to perform a type of smoothing a soft minax approach rather than taking the worst group and you know ensuring the worst group is similar in the training and test we just take some kind of weighted average of several worst performing groups and hope that that they are similar to several worst performing groups in the test data right so we are quote unquote performing smoothing okay a tried and tested idea in statistics and machine learning and we'll see how this performs uh um on the several data sets that we have so now uh sure like this is a that we should perform smoothing is a high level "
    },
    {
        "start": 1175.72,
        "text": "Idea Idea okay but how do we operationalize this how do we come up with an algorithm that would that would do this type of smoothing okay and for that in order to come up with an algorithm we take inspiration from the information retriever literature and use this idea uh uh of discounted cumulative gain okay so the idea is so so discounted cumulative gain uh uh to to differentially rewe the different groups during the training process okay so those of you are not um familiar uh with discounted cumulative gain so it's like used for ranking search results so the search results that you get on Google or Bank uh like there are criteria to to like wait those results based on relevance okay and what dcg does is um it it does the logarithmic waiting of those results okay um yeah so uh so our key idea is that we should uh use like a simple objective "
    },
    {
        "start": 1236.72,
        "text": "function you know uh like a discounted cumulated gain to differentially Reade the various groups during the training process and then uh like hopefully the worst performing groups will have higher say in the model parameters and it and it would generalize better um so now now like why did we choose uh like dcg so the so the idea the reason why we chose dcg and not any other functional form is because it's widely used uh in the information retrieval literature it has strong precedents you know people have proven strong theory about it and it's very very easy it's very very easy to understand and also it has a logarithmic drop off you know from like so it would wait the worst performing group in a certain way and the second wor worst performing group a log of that the third one log of log of that and so on so it's like so it has a logarithmic Decay um and so it ensures that uh we do not uh "
    },
    {
        "start": 1299.76,
        "text": "like have a very very long tail of weights right so that we do not uh uh so that we do not upweight even the best performing groups so it is a sh drop off but so it ensures that instead of one we would um upweight like maybe five or 10 groups but not all of them like if we have 100 groups yeah so here is here is our here is our iterative algorithm okay um basically you know um uh like as I'm sure all of you um have uh trained some kind of neural network model you know so there is Epoch wise training you know you make a Passover the data you do gradient updates and then you move again and so on right so it's a so we try to integrate this idea of BCG into this Epoch wise uh training over the data okay so we iterate over the training data and we and uh we compute accuracy uh of all the groups in our "
    },
    {
        "start": 1360.919,
        "text": "data right so um in the first Epoch we rank all the groups based on their accuracy in the previous Epoch so the first Epoch sure there is no previous group but second Epoch converts we rank the groups based on their accuracy in the previous Epoch and then we do this logarithmic weighting so the groups that perform the worst you know that will get weighted the highest and the group that got the second worst it will get weighted slightly lower and so on and with a with a standard logarithmic Decay right and see uh in the numerator and here is like is a is a hyper parameter you know that is chosen by cross validation it just chooses it just decides uh how much uh like how many groups we should wait and what should be the Decay okay and uh C also decides like you know as you can see that we only um um like upweight groups up to up to up "
    },
    {
        "start": 1423.279,
        "text": "to ranks given by Capital C and so it's like an implicit threshold also you know to decide like how many groups we yeah um so so so what our model does is like it minimizes the following objetive function that here is our loss function like a standard loss function okay and then uh we uh weit each of the groups based on that waiting function that I described on the previous slide right simple uh discounted cumulative gain inspired uh waiting function and we do this uh iterative training so now this process of like looking at the model errors and upweighting certain examples is very similar so those of you who know about stage- wise learning method like you know Ada boost or like its cousins it's very similar to the idea that boosting does right so what boosting also does is like it looks at the errors of certain examples and upgrate certain errors right of course it has an exponential "
    },
    {
        "start": 1485.08,
        "text": "loss function and it has its own upweighting criteria but at a high level like you know this reting based idea is similar to boosting style um boosting style uh approaches okay and another key difference is like you know in our case we do this waiting per group we upweight all the uh all the examples all the observations in a group as opposed to some of these stagewise learning approaches which are observation based you know which would upweight which would compute error of each observation and it would upweight uh like each of them by a certain amount okay so uh like this default idea that I said said it updates all the sample in a group but we could we could be even smarter about this you know we could only choose uh to update the misclassified examples from a group right so if let's say there's a there's a there's a group containing a certain demographic Group which is minority and "
    },
    {
        "start": 1545.64,
        "text": "for which we get very very high errors um so maybe we should uh uh like why upate all the examples in that group maybe we could upate only the misclassified examples from the group and the objective function changes slightly but at a high level the blueprint of the algorithm uh stays the same now so that was the algorithm so now let's try to understand why it works right like you know why um I've given you intuition that instead of uh doing Minimax we are doing soft Mini Max right so instead of assuming that the worst group in training is similar to the worst group in test we are assuming some weighted combination of worst groups in training is similar to some uh combination of worse groups in the test right but why why does this approach work so in order to understand um uh like why some of these models um some of "
    },
    {
        "start": 1606.0,
        "text": "these Baseline models I should say that do not are that are not distributionally robust that they perform well on average but worse on some these under represented groups in the data is due to spous featur features okay so the spous features are um are are features that are correlated with the label but their correlation would switch labels you know maybe they're positively correlated on training data but negatively correlated in test data right so for example in our case the uh the uh ex uh like the empirical scenario is we have reviews from users and we're trying to predict what rating they would get right like is it a good is it a festar review or a fourar review or something so in that case like an easy spous feature to see is the writing style uh uh of people right so so because like a like a vanilla model like a vanilla off the shell model it does so well on average "
    },
    {
        "start": 1666.48,
        "text": "but does poorly on some of these minority groups is that it has lashed on to the writing style of people it has seen that when somebody doesn't have doesn't form like fully grammatical sentences uh or like you know uh like punctuation is bad uh that person also writes really bad reviews but this is a feature which is not generalizable because on the test data that correlation might not be present like you know there might be uh like you know like all well written responses right now you could you could generalize it to like other examples you know like in medical domains where it could be like where it could be studious features that would Graphics of people that they are present with with um One sign in training data but with the reverse sign on the test data or like uh like no correlation of the test data um so um um what uh like what our approach you "
    },
    {
        "start": 1728.039,
        "text": "know uh uh like Dru discounted rank upating uh like what we are doing is we are assuming that these spous features are present in all the groups right so uh like again like this goes with the spirit of smoothing uh that we assume that they are present in all the groups to varing degrees and hence uh we Reade all the groups proportional to their group errors to find robust features that are invariant to these previous features right so that's the Crux of our of our approach you know that U that that we assume that the serious features are scattered all over right and it's just the extent is different you know the spous features might be present to a smaller extent in groups that perform uh like that are majority groups and for which the model performs really really well right uh and we take this agnostic approach that we do not know which groups have the most serious features compared to the other "
    },
    {
        "start": 1788.2,
        "text": "approaches that people have proposed where they strictly assume that you know the worst performing group on the training data has to contain the serious features okay so before I proceed further there any questions Yeah question oh okay in your two examples one you had uh no notion of the um it was totally out of distribution um like it was a different camera type Al together not inter train set in your second example it was just the proportion proportion was different but I'm curious in your method are you assuming that you have access to the group label uh at test time can you can you make decisions based on like the group that of the person that you're trying to make predictions for yeah so I think like you know uh so the so like "
    },
    {
        "start": 1848.559,
        "text": "the like the results that you will see so they deal with more of the second type which is the subpopulation ship that all the groups are there but their proportion change right so the first type of example that I gave the new type of camera that's like the more extreme type in that case like you know our approach could still work but like you know it's it doesn't work as well because you know we do not assume that we have any knowledge of the uh uh of the of the test group memberships I guess so then the F question is um why do you have to have a single model that is applied to all instances equally could you train separate models for each group separate models for each groups like the performance on a group you could pick the model that would be the most performant for it I see so then how would you combine those models well you're you're given an instance to score and you pick you know "
    },
    {
        "start": 1908.6,
        "text": "what group they are so you pick the model and you score them I don't know what you mean by combined okay so you were saying I see [Music] uh yeah I think like okay so such a I think like it's definitely uh like a reasonable Baseline to try you know that that you could learn a separate model for each group um but I think the problem with that is you know like in this case if we have we typically have 500 groups so if especially if you're training a bird style model to learn and you learn a separate model for each group so now you're now you have like a data uh positive problem you know like you only have thousand examples to fit a model right sure uh so you have to be really careful about how you regularize the model to ensure that all those 500 models that you train on each of the groups that they perform uh that they "
    },
    {
        "start": 1970.799,
        "text": "that they generalize well so now you like might have circumvented some of the uh like the distributional shift problem but there is a problem on how to uh fit a such a rich model on data which is one 1 divided 500 the size of the of the actual data okay thank you make sense yeah yes so I think we are like still possible to General model all the available data and then for each subgroup you five tune the general model based on a small said and in that case you know you are still using kind of using all the data that small subset so how would you do that like how would you so I mean I know uh these days with large language models you know we use the word fine tune a lot but uh in this case like think about 500 groups so this is not an llm domain that we're "
    },
    {
        "start": 2032.84,
        "text": "dealing with so we don't have trillions of examples so we have about 500,000 examples let's say in the training data and like there are say 10,000 groups so each group is a user like as I will show in that example so like people write reviews on Amazon right like you know you buy different products uh and all my reviews would be like I would be a group right and then in the test data like uh people are only present either on training or test so uh my data would only present in the training data but there could be a new person in the test data whom we we would have to perform well on right so so so this idea about fine tuning like if you could maybe shed some more light like how what exactly do you have in mind like how would you fine tune because at least the way I understand about fine tuning and like training a general model and then fine tuning on some small data it doesn't seem like we would have enough data to do "
    },
    {
        "start": 2094.56,
        "text": "that so for example 100,000 data General 500 TR model on the 500,000 uh data inst and then uh for each subgroup you TR and you start with a general model you still still train it further but using a much smaller uh step size and then you train this to a VI then okay so you were saying that train one General model just as the Baseline that we are using here do and then you take a given subr and you make some gradient gradient steps uh on "
    },
    {
        "start": 2155.8,
        "text": "that yeah just make we smaller and but but still it but how will it address the um like the speedio feature problem right so what if I mean even if all the groups had equal predictive power and had similar distribution of superious features uh sure this such kind of an approach could work but I don't think it takes into account that some of those groups could have higher like you know prevalence of spous features than others so I'm happy uh in a TR of time I'm happy to talk afterwards you know but I think uh let's let's move ahead so I think there was one more question yeah I I think just seeing some examples would be great and then you know because it's you know it's a little high level and abct perfect perfect yeah I think that that sounds good thanks "
    },
    {
        "start": 2217.2,
        "text": "okay so um yeah so like speed features is one one one story like why this why this approach works and you know this also has connections to causal interest okay and that's uh like assume that you have some data which coming from uh some unknown distribution and each group uh data point belongs to uh some group okay and the training and testing data are sample from this unknown distribution um uh and we assume that there is very little overlap between the dra Trin and D test okay right so if you if you assume that you know then out of distribution generalization can be seen as a sample selection problem right so it's like a like certain examples are more likely uh to be included into training data versus testing data so it's like a selection bias issue okay um and you know because what you would have wanted to do was um "
    },
    {
        "start": 2279.24,
        "text": "uh like the X train and white uh X train and X test were sampled uniformly at random from this underlying distribution but uh assume they are not right that that there is some uh like uh feature maybe some demographic or writing style based on which uh a data point is more likely to be in the training data than in the testing data okay and uh so if you uh like and and so what are some of the common approaches to deal with such kind of a selection bias issue uh so as we know like for observational uh data you know there are approaches such as propensity score matching uh or um um like ipw or like you know its uh cousins you know that maybe you could have W robust version of ipw uh you could use such kind of approaches uh to address these issues so like turns out with some algebra you could see our approach is just inverse propensity weighting but "
    },
    {
        "start": 2339.359,
        "text": "where the propensity weights are the group errors right so the groups that have higher errors we are waiting them higher we are giving them higher propensity weights compared to the groups that have low errors okay and this ipw connection also shows that we don't have to uh like constrain ourselves to just the group errors that we could move to other properties like we could have maybe have uh like like a broader array of features on which we could rank ropes so for example we could rank based on group size maybe the groups that are really really big are somehow uh contain lots of serious features uh and you know need to be dealt with separately or how similar are the groups to other features or the entropy of the labels and so on right and um uh so now let me show you some of the some of the results and then then I'll so this is like part of some of the on ongoing work work where where we are using multiple features uh as "
    },
    {
        "start": 2399.68,
        "text": "opposed to just a group error uh to rank the groups and like you know uh uh like do it in our like discounted rank upating uh setup okay so um so like the data that we have uh like there are three common data sets that people use so one is the Amazon data set you know where there Amazon reviews and you want to rate the reviews on a scale of you know one to five uh IMDb you know people rate movies uh and there is y right again people rate restaurants there and uh there are number of groups in the training data and the testing data what's the average group size right so they're about uh in in in Amazon a given person has about 75 reviews right and uh in in a group for IMDb like an average person had 25 reviews okay uh for y it had about 100 so as I said like a group here is a a person but it doesn't have to be like you know it could be group could be like "
    },
    {
        "start": 2461.319,
        "text": "a demographic feature like you know you could construct a group based on some demographic attributes why we constructed a group based on a person because these are some of the data sets that people use in this literature so we wanted to stick to that and not uh you know change the goal posts uh uh to show the efficacy of our approach okay what is out of distribution here mean so the out of distribution here is on the on the test data uh like the reviews are from a different person potentially a different movie that they have rated that they haven't then and that person wasn't in the training no it was not yeah so let's say I was in the training data and you know you were in the test data and so the idea is that there will be some serious features you know maybe my writing style is significantly different from other people and that will not generalize well yeah so uh here are some like so if you were to run a baseline model which is simple empirical RIS minimization you know take any state of the art model how would it do on these data sets right so "
    },
    {
        "start": 2522.88,
        "text": "this is the average accuracy okay so uh so you can see the average accuracy is like you know in the 60s and 70s it's does reasonably well this is the uh 10th percentile group accuracy so basically if you were to rank groups in terms of their errors like what's the accuracy of the 10th the the the group at the 10th percentile and the red one one is the accuracy of the worst performing Dr okay so now you could see um uh that the the like moving from training to test this is this is slight drop off on the average accuracy right but what's what this average accuracy is masking inside it is like a big discrepancy in the worst group accuracy right so note that the worst group here it's not the same as the worst group here because I'm in the training data but I'm not in the test data so it's just some other person we don't know who that is uh but like "
    },
    {
        "start": 2583.44,
        "text": "whatever that whoever that person is the accuracy was 12% okay and and and again you could see a big drop off from 26 to 15 and 41 to8 so the goal of our approach and like any other approach in this is to improve these numbers without hurting the average accuracy much yes oh yeah I got a question so how we get this aage gr accuracy so is it weighted based on the gr uh grp size or no so this is no there is no waiting this is like simple uh like you train any model that is agnostic to group structure okay and then you compute the accuracy and this is just the average for each group yeah if that was your question oh yeah just so each group has a we one Bic right so that again each yes each group has a weight of one so it's like what's the average of each group you know so so that's why so so the reason for that is the bigger groups should not have yeah "
    },
    {
        "start": 2644.64,
        "text": "they should not drive this right yeah but this is like any offthe shelf method that you that you run like how would it perform right it's totally agnostic to group structure it doesn't do anything with it okay sorry one more question yeah how skewed are the groups do you have power users and some longtail very few users so in this particular data set like you know they in uh like the data set is created in such a way that all the users have 75 groups sorry all the users have at least 75 reviews at least at least uh some people have more like you know some people have two 300 but like there is not a strong uh like power law Behavior here right but yeah there are also not people with two reviews so that's again that's how the data set was created there are again like as part of this ongoing research we're trying to come up with new data sets ourselves because we see that there are some limitations in the setup uh that is there like it might not be fully represented yes might be a stupid question but you "
    },
    {
        "start": 2705.76,
        "text": "in the number of groups you have two entries what are the difference so it's training and testing training and testing yes so there are 1252 people in training 1334 in testing and so on yeah thank you okay so now uh like let's see like now we've seen the Baseline you know these were the like the like the baselines right so now let's see uh how some of these other approaches performed so this was the approach that I talked about like the Minimax approach right that that just does a convex uh uh like surrogate of the Minimax objective and tries to minimize that right so it turns out that you know that on some data sets it performs really badly but but in some of them it does reasonably well like you know where the worst group in training might be similar to the worst group in testing okay and this was another approach just trained twice uh like again you know which uh like this is also reting based approach but like they just do very similar to what boosting "
    },
    {
        "start": 2766.68,
        "text": "does you know so that's why I made the connection to boosting and then there could be uh other approach that we could maybe just do this boosting style idea that I said like as we are doing Epoch wise upating but let's just upweight the worst performing growth okay so this is different than this because this has this is a more principal convex learning objective that they're trying to optimize whereas this is more Epoch wise learning and in each Epoch you upweight the worst performing group we wanted to see how this would perform and does reasonably well and as you can see like this simple approach like you know it's competitive or like better than some of the more complex approaches that people have proposed and uh this is our this is our approach so as you can see like you know the worst group accuracies are significantly significantly improved than any of the uh uh Baseline methods the 10th percentile is also slightly improved but sure it's not a big difference uh in compared to those "
    },
    {
        "start": 2828.359,
        "text": "approaches and the average accuracy is slightly worse than what the what the simple empirical risk minimization was doing right so which shows like you know and people have already observed that there is this tradeoff like even with this so much literature on fairness and you're trying to enforce fairness on different groups that you hurt the average person or the average accuracy a little bit in order uh to ensure fairness or in other words that there's a cost like a slight cost of fairness right but that's what we see here like you know there's like a slight drop off in accuracy on in DB it's kind of similar but again these are like some idos synes of these data sets also and we definitely need more data sets and more evaluations uh like to understand these things okay again like it's a Apples to Apples comparison all the approaches had access to the bird fine tune for the word because it's the input text and how do we encode the uh input text into some representation so everybody use the word fine tune there a "
    },
    {
        "start": 2890.96,
        "text": "perfect and all the hyper parameters were tuned on some held out data there was like a perfect Apples to Apples comparison like in every way between these different approaches okay so uh so uh so in conclusion um you know like robust ml is important for ML safety and and in Equitable ML and we proposed a simple approach uh Rank and Reade and it uses you know distributional uh and it and it uses discounted rank up waiting uh for it for like addressing group distribution ships okay and some of the key strengths of our approach are like you know there is uh like it shows strong empirical performance and a priori it does not require require a knowledge of serious features or the associated dag right so uh now there's a separate set of approaches not for this distributional robust problem but in general in dist uh in like uh robust machine learning where people use causal "
    },
    {
        "start": 2951.4,
        "text": "infes techniques but they assume that they know what the superious features are they know what the dag is now which could be good in some context but it could be limiting right like you know in this case we don't know is like writing style the only serious feature or there could be others we don't know what the dag structure is right like so our approach is agnostic to that it just takes several worst performing groups and up weits them in a simple strategy yeah and and it turns out like it has connections to inverse propensity waiting and our approach is exactly inverse propensity waiting with uh with the propensity weights are given by the group errors okay uh so so like you know some ongoing work that we are doing is like uh rather than just using the group error use bunch of other group attributes to rank those things okay so then this would be more generalization that that that we would have a more like broader range of propensity weights rather than just "
    },
    {
        "start": 3012.359,
        "text": "based on the group errors okay so we're looking at things like you know group size you know entropy of labels you know how random is like you know because there are some people who would write reviews and they would just randomly hit a three or a four or a five right or there are some people who would write uh like a oneline review uh the movie was really good and give it a two star because there's like a personto person variation right and and somebody would write a very scathing review and like still give it a four stars right so so there's a personto person idiocracies in the way they review things um and so like entropy of labels you know or like you know like or the current group gradients could be like other featur to look at that and we have like a simple upating based objective a natural Next Step could be to think about a convex objective function uh like like you know that is based on the ranking loss for this problem okay you know hopefully we will get better convergence of results "
    },
    {
        "start": 3073.16,
        "text": "in that way rather than doing this Epoch wise uh Epoch wise updating you know um and and also like we assume that the group structure is given to us exogenously but it might not be you know might you might have to infer groups by some kind of clustering or like you know by some other means like you know which could be endogenous so uh how how to how would such approach work like you know the group structure is not uh like you know given given to us like provided to us okay so this is a joint work with like you know professor shabum and uh like Yuan Leu and bhan Zang our PhD second year PhD students in our program and um this paper is uh currently in review and you know there's some ongoing work as I said uh on like generalizing it to other criteria for ranching that's it uh and thank you so much and I'll uh stop for "
    },
    {
        "start": 3133.64,
        "text": "questions yes um you don't have to slide 18 for a minute oh my I just had a question like reting there's kind of like a whole for option okay in the Baseline everything's considered equally and then of course the common case dominates it other groups and then like on the other side there's the St waiting where you only wor performance and then here's is kind of somewhere in the middle um where you look at the average ERA within a group and you do some log B I'm curious did you explore like instead of like a log function based on the ranking just actually use like multiplying that average grou accuracy instead did you try that that's that's a great question so so the way so the reason we use ranking instead of actual numbers is because of calibration issues right so like actual group errors could be very very close to each other right like you know some group errors could be "
    },
    {
        "start": 3194.48,
        "text": "let's say on some scale number they could be five versus 4.5 versus three so in terms of the waiting they would be very very similar right so why like ranking uh like ensures better separation between the different ranks I guess what I'm trying to say is like if the loss functions are so close to each other yes then like if the ranks differ um then like basically just a small perturbation could shift the ranks and maybe those two classes should be weighted relatively equally kind of what the performance is looking like true I mean like we we we did try like non-ranking based approaches in fact that's how we were initially trying to do this it's just that like ranking is just more robust to like you know calibration miscalibration issues right that's why we stuck to uh we stuck to ranking right so so yeah your your point "
    },
    {
        "start": 3255.68,
        "text": "is well taken that even if the group errors are similar group erors are similar but but ranking still uh like puts a relative prefence in terms of the group weights right so the group weights would should should still be clearly separated right otherwise like what we were finding was that everything was kind of moving together like you know if the group one of the group pars was five and there was 4.7 and know 4.3 in the bigger scheme of things like this was very similar to the Baseline right we are essentially operating everything the same uh so it was trying to like be more Stark right like in terms of the way we are doing aerating but yeah that's a good question yeah this is more question about the data set okay um I know that for a bunch of reviews there's sometimes concerns about Bots or people say that again so situations where you have Bots or people V on paying other ones toie bomb or various "
    },
    {
        "start": 3316.64,
        "text": "bombie what so as in a bunch of people for whatever reason you have your payment or events decide to um collaborate or trying to reduce the actual score of the contct oh I see oh so so you are saying okay how do we address BS uh or like you know some kind of strategic yes so so yeah so that's a great uh so that's a big concern right like know like you know strategic reviews like you know people do like it's done all the time so what what so you know these data sets at least two of them came from this paper from Stanford uh like on this reverse machine learning Benchmark and what they did like you know they they describe all the details and this these were some of the things that we were worried about that what if there is there are strategic reviewing like you know that you just give one star to the reviews of a given restaurant or a different a different product so they build some classifier to to detect some kind of fake reviews or you know like reviews that might have been strategic in some way and and and "
    },
    {
        "start": 3377.92,
        "text": "remove those right and and like also this idea that each person like you know was only kept to 75 you know so they have good justification for it in terms of like ensuring that the model runs that and the data set is representative but still it's not too trivial right so so yeah so so some of those issues were definitely addressed but I think there could be still some of these more issues because they still built a classifier to weave those things up yes some GRS are more close for other say that again so here some groups are inic to other for example right RWS Chinese and singaporeans have the same mistake yes so uh more put some way for groups that are "
    },
    {
        "start": 3440.4,
        "text": "more similar to have a lower weight for groups that have uh they are similar to other fols yeah so that's a that's a good question and uh did I have it uh I think I had it somewhere but one of the things that we are looking at is that maybe the waiting should also take into account similarity of Grows Right so other than so like currently we're just looking at looking at errors in an IID way we're looking at independently for each group what's the error and upweight based on that but that maybe somehow consider the similarity or the network structure of the groups for for these issues that there could be correlation uh in the review patterns of some groups right you know like even if each person uh each group is an individual person people from same country could write singar types of reviews so like you know have uh like a separate criteria for "
    },
    {
        "start": 3501.16,
        "text": "reranking like separate feature on which we rank based on that I think we are right about at the hour so you can probably wrap up I do as want pass along uh there was a a comment of appreciation in the chat for Thought thanking you for Thought talk saying that they look forward to thank you so much and "
    }
]