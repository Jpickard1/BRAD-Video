[
    {
        "text": "well I know that people are still coming",
        "start": 282.779,
        "duration": 5.551
    },
    {
        "text": "but I think we should get started so um",
        "start": 285.58,
        "duration": 5.16
    },
    {
        "text": "it's a great pleasure to welcome you",
        "start": 288.33,
        "duration": 5.589
    },
    {
        "text": "bill here and we're really looking",
        "start": 290.74,
        "duration": 6.899
    },
    {
        "text": "forward to a seminar so Jane just gave",
        "start": 293.919,
        "duration": 5.911
    },
    {
        "text": "me a 40 page CV will be also I'm placing",
        "start": 297.639,
        "duration": 5.101
    },
    {
        "text": "quite a challenge here so I'm gonna",
        "start": 299.83,
        "duration": 6.48
    },
    {
        "text": "shorten it there is a very nice summary",
        "start": 302.74,
        "duration": 7.259
    },
    {
        "text": "of Bill's achievements on the is MV",
        "start": 306.31,
        "duration": 5.699
    },
    {
        "text": "website because it's also a good time to",
        "start": 309.999,
        "duration": 6.39
    },
    {
        "text": "congratulate him on on is B is and be",
        "start": 312.009,
        "duration": 6.87
    },
    {
        "text": "innovator a word that he just received",
        "start": 316.389,
        "duration": 4.08
    },
    {
        "text": "was just announced a few weeks ago I",
        "start": 318.879,
        "duration": 3.57
    },
    {
        "text": "guess you haven't received it yet but",
        "start": 320.469,
        "duration": 3.051
    },
    {
        "text": "it's coming",
        "start": 322.449,
        "duration": 4.5
    },
    {
        "text": "so using that very short summary that",
        "start": 323.52,
        "duration": 8.049
    },
    {
        "text": "they posted so Bo Nobel is a professor",
        "start": 326.949,
        "duration": 6.39
    },
    {
        "text": "of Genome Sciences at University of",
        "start": 331.569,
        "duration": 3.84
    },
    {
        "text": "Washington and also professor of",
        "start": 333.339,
        "duration": 5.7
    },
    {
        "text": "computer science as well and he received",
        "start": 335.409,
        "duration": 9.57
    },
    {
        "text": "his PhD degree in well less than 20",
        "start": 339.039,
        "duration": 13.37
    },
    {
        "text": "years ago and 1998 in short postdoctoral",
        "start": 344.979,
        "duration": 9.84
    },
    {
        "text": "fellowships in fellowship and David",
        "start": 352.409,
        "duration": 7.151
    },
    {
        "text": "how's the lab at UC Santa Cruz and moved",
        "start": 354.819,
        "duration": 12.77
    },
    {
        "text": "to New York three years later and remain",
        "start": 359.56,
        "duration": 11.609
    },
    {
        "text": "there since the time so it was really",
        "start": 367.589,
        "duration": 5.89
    },
    {
        "text": "internationally recognised his work on",
        "start": 371.169,
        "duration": 4.98
    },
    {
        "text": "developing machine learning methods for",
        "start": 373.479,
        "duration": 5.43
    },
    {
        "text": "all sort of high throughput data pretty",
        "start": 376.149,
        "duration": 5.82
    },
    {
        "text": "omics to genomics including start",
        "start": 378.909,
        "duration": 6.81
    },
    {
        "text": "modeling and today we'll hear his recent",
        "start": 381.969,
        "duration": 10.26
    },
    {
        "text": "work thanks Alexei and thank you all for",
        "start": 385.719,
        "duration": 7.891
    },
    {
        "text": "coming",
        "start": 392.229,
        "duration": 3.87
    },
    {
        "text": "I've enjoyed visiting with people here",
        "start": 393.61,
        "duration": 3.809
    },
    {
        "text": "today and it seems like there's a lot of",
        "start": 396.099,
        "duration": 4.231
    },
    {
        "text": "exciting stuff going on it's also nice",
        "start": 397.419,
        "duration": 5.041
    },
    {
        "text": "to be back in the Midwest it wasn't I",
        "start": 400.33,
        "duration": 5.16
    },
    {
        "text": "grew up outside Chicago think of I think",
        "start": 402.46,
        "duration": 4.41
    },
    {
        "text": "of Michigan at the place I always went",
        "start": 405.49,
        "duration": 5.28
    },
    {
        "text": "for vacation nice to be here",
        "start": 406.87,
        "duration": 5.609
    },
    {
        "text": "I'm gonna talk today about a couple",
        "start": 410.77,
        "duration": 4.23
    },
    {
        "text": "different projects as the Lex I",
        "start": 412.479,
        "duration": 4.771
    },
    {
        "text": "mentioned my background is in machine",
        "start": 415.0,
        "duration": 3.81
    },
    {
        "text": "learning and I've made a career out of",
        "start": 417.25,
        "duration": 3.15
    },
    {
        "text": "sort of trying to apply machine learning",
        "start": 418.81,
        "duration": 3.3
    },
    {
        "text": "method to develop new machine learning",
        "start": 420.4,
        "duration": 3.079
    },
    {
        "text": "method for making sense out of",
        "start": 422.11,
        "duration": 5.309
    },
    {
        "text": "biological data so I've got a collection",
        "start": 423.479,
        "duration": 6.851
    },
    {
        "text": "of three different projects I'm actually",
        "start": 427.419,
        "duration": 4.291
    },
    {
        "text": "going to focus on the first and the",
        "start": 430.33,
        "duration": 2.97
    },
    {
        "text": "third and just briefly mention the",
        "start": 431.71,
        "duration": 5.13
    },
    {
        "text": "second one and they all sort of share",
        "start": 433.3,
        "duration": 5.19
    },
    {
        "text": "one aspect which is that there's a",
        "start": 436.84,
        "duration": 3.81
    },
    {
        "text": "latent space that's involved which",
        "start": 438.49,
        "duration": 4.109
    },
    {
        "text": "realistically it's not like I went in",
        "start": 440.65,
        "duration": 3.329
    },
    {
        "text": "thinking oh I've got to find different",
        "start": 442.599,
        "duration": 3.871
    },
    {
        "text": "ways to fill latent spaces or like",
        "start": 443.979,
        "duration": 4.321
    },
    {
        "text": "afterwards I looked back and said oh",
        "start": 446.47,
        "duration": 4.86
    },
    {
        "text": "look he's doing kind of similar in this",
        "start": 448.3,
        "duration": 7.02
    },
    {
        "text": "way so I'm gonna dive in by telling you",
        "start": 451.33,
        "duration": 6.42
    },
    {
        "text": "a little bit about consortium that I've",
        "start": 455.32,
        "duration": 4.529
    },
    {
        "text": "been involved in since its inception in",
        "start": 457.75,
        "duration": 4.289
    },
    {
        "text": "2003 and probably most of you are",
        "start": 459.849,
        "duration": 4.201
    },
    {
        "text": "familiar with encode stands for the",
        "start": 462.039,
        "duration": 4.591
    },
    {
        "text": "Encyclopedia of DNA elements and it was",
        "start": 464.05,
        "duration": 4.08
    },
    {
        "text": "meant to be a follow-on to the human",
        "start": 466.63,
        "duration": 3.629
    },
    {
        "text": "genome project where the human genome",
        "start": 468.13,
        "duration": 4.62
    },
    {
        "text": "project sort of gave us the book and",
        "start": 470.259,
        "duration": 4.5
    },
    {
        "text": "encode is supposed to help us media",
        "start": 472.75,
        "duration": 4.02
    },
    {
        "text": "right in a sense that encode is trying",
        "start": 474.759,
        "duration": 3.931
    },
    {
        "text": "to find what are the things in the",
        "start": 476.77,
        "duration": 3.989
    },
    {
        "text": "genome that actually do stuff the the",
        "start": 478.69,
        "duration": 5.729
    },
    {
        "text": "functional elements encode is in many",
        "start": 480.759,
        "duration": 7.53
    },
    {
        "text": "ways just a data generation consortium",
        "start": 484.419,
        "duration": 7.021
    },
    {
        "text": "but the idea is not just to generate the",
        "start": 488.289,
        "duration": 5.43
    },
    {
        "text": "data but also to interpret it and the",
        "start": 491.44,
        "duration": 3.99
    },
    {
        "text": "kind of data that being generated",
        "start": 493.719,
        "duration": 4.29
    },
    {
        "text": "essentially measures different kinds of",
        "start": 495.43,
        "duration": 5.01
    },
    {
        "text": "biochemical activity along the genome so",
        "start": 498.009,
        "duration": 4.53
    },
    {
        "text": "you have for example measures of gene",
        "start": 500.44,
        "duration": 4.819
    },
    {
        "text": "transcription most notably RNA seek",
        "start": 502.539,
        "duration": 5.37
    },
    {
        "text": "measurements the measurements of protein",
        "start": 505.259,
        "duration": 5.53
    },
    {
        "text": "binding like transcription factor chip",
        "start": 507.909,
        "duration": 5.43
    },
    {
        "text": "seek you have measurements of DNA local",
        "start": 510.789,
        "duration": 5.271
    },
    {
        "text": "packing density DNA's or attack seek",
        "start": 513.339,
        "duration": 5.791
    },
    {
        "text": "measurements and all of those together",
        "start": 516.06,
        "duration": 6.339
    },
    {
        "text": "are carried out in many different cell",
        "start": 519.13,
        "duration": 6.269
    },
    {
        "text": "lines and and tissues so this is just a",
        "start": 522.399,
        "duration": 4.981
    },
    {
        "text": "picture from another consortium the road",
        "start": 525.399,
        "duration": 4.051
    },
    {
        "text": "map consortium showing you there's a",
        "start": 527.38,
        "duration": 4.59
    },
    {
        "text": "variety of different tissue type that",
        "start": 529.45,
        "duration": 3.87
    },
    {
        "text": "you can imagine doing this kind of",
        "start": 531.97,
        "duration": 3.69
    },
    {
        "text": "experiment in but of course the space is",
        "start": 533.32,
        "duration": 4.44
    },
    {
        "text": "even larger than this because you can",
        "start": 535.66,
        "duration": 4.65
    },
    {
        "text": "have lots of different perturbations you",
        "start": 537.76,
        "duration": 4.319
    },
    {
        "text": "can do CRISPR manipulations of cell",
        "start": 540.31,
        "duration": 3.42
    },
    {
        "text": "lines and so on so",
        "start": 542.079,
        "duration": 4.26
    },
    {
        "text": "a large space of possible kinds of cells",
        "start": 543.73,
        "duration": 4.979
    },
    {
        "text": "and then a large space of possible",
        "start": 546.339,
        "duration": 4.321
    },
    {
        "text": "measurements to perform in each of those",
        "start": 548.709,
        "duration": 6.391
    },
    {
        "text": "cells if you're thinking of this like a",
        "start": 550.66,
        "duration": 6.33
    },
    {
        "text": "computer scientist or an electrical",
        "start": 555.1,
        "duration": 3.78
    },
    {
        "text": "engineer or a statistician it's helpful",
        "start": 556.99,
        "duration": 4.11
    },
    {
        "text": "to say well okay there's all these",
        "start": 558.88,
        "duration": 4.59
    },
    {
        "text": "assays but realistically they are all",
        "start": 561.1,
        "duration": 5.4
    },
    {
        "text": "involved short lead sequencing and you",
        "start": 563.47,
        "duration": 5.01
    },
    {
        "text": "essentially summarized everything as a",
        "start": 566.5,
        "duration": 4.889
    },
    {
        "text": "measurement along the genome that has to",
        "start": 568.48,
        "duration": 4.38
    },
    {
        "text": "do roughly with how many sequences",
        "start": 571.389,
        "duration": 4.2
    },
    {
        "text": "covered each base at the genome so you",
        "start": 572.86,
        "duration": 4.68
    },
    {
        "text": "can represent it all is just one vector",
        "start": 575.589,
        "duration": 4.201
    },
    {
        "text": "it's a long vector because the genome",
        "start": 577.54,
        "duration": 4.5
    },
    {
        "text": "long but this is just a sort of showing",
        "start": 579.79,
        "duration": 4.859
    },
    {
        "text": "a short piece of 50 kilobases well that",
        "start": 582.04,
        "duration": 4.02
    },
    {
        "text": "bars 50 kilobases",
        "start": 584.649,
        "duration": 4.831
    },
    {
        "text": "of signal for three different assays so",
        "start": 586.06,
        "duration": 5.48
    },
    {
        "text": "these are measurements of histone",
        "start": 589.48,
        "duration": 3.9
    },
    {
        "text": "modifications three different kinds of",
        "start": 591.54,
        "duration": 3.82
    },
    {
        "text": "histone modifications in three different",
        "start": 593.38,
        "duration": 4.319
    },
    {
        "text": "cell types where the value at each",
        "start": 595.36,
        "duration": 4.65
    },
    {
        "text": "position shows the level of modification",
        "start": 597.699,
        "duration": 6.481
    },
    {
        "text": "as measured at that position so the",
        "start": 600.01,
        "duration": 6.389
    },
    {
        "text": "point of the first project is that this",
        "start": 604.18,
        "duration": 5.19
    },
    {
        "text": "is a big space of possible data so",
        "start": 606.399,
        "duration": 6.841
    },
    {
        "text": "there's some some types of assays that",
        "start": 609.37,
        "duration": 6.12
    },
    {
        "text": "were performed sort of systematically",
        "start": 613.24,
        "duration": 4.05
    },
    {
        "text": "across many many different cell phones",
        "start": 615.49,
        "duration": 4.079
    },
    {
        "text": "and then there are some popular cell",
        "start": 617.29,
        "duration": 4.38
    },
    {
        "text": "lines that were characterized in great",
        "start": 619.569,
        "duration": 4.2
    },
    {
        "text": "depth with many many different kinds of",
        "start": 621.67,
        "duration": 4.62
    },
    {
        "text": "assays but there's a lot of purple in",
        "start": 623.769,
        "duration": 5.31
    },
    {
        "text": "here also which are all of the assay",
        "start": 626.29,
        "duration": 5.039
    },
    {
        "text": "cell type combinations that have not yet",
        "start": 629.079,
        "duration": 6.901
    },
    {
        "text": "been performed so this slide is actually",
        "start": 631.329,
        "duration": 6.63
    },
    {
        "text": "old this is from the original data set",
        "start": 635.98,
        "duration": 4.049
    },
    {
        "text": "that we performed our initial analyses",
        "start": 637.959,
        "duration": 4.081
    },
    {
        "text": "on so when I say it's big it's true it's",
        "start": 640.029,
        "duration": 3.06
    },
    {
        "text": "big but it's bigger now",
        "start": 642.04,
        "duration": 2.88
    },
    {
        "text": "so there's actually more with more than",
        "start": 643.089,
        "duration": 4.521
    },
    {
        "text": "10,000 experiments in encode currently",
        "start": 644.92,
        "duration": 5.729
    },
    {
        "text": "but this is a picture of sort of a junk",
        "start": 647.61,
        "duration": 6.01
    },
    {
        "text": "of the data where you can see in yellow",
        "start": 650.649,
        "duration": 4.981
    },
    {
        "text": "the experiments that have been performed",
        "start": 653.62,
        "duration": 3.329
    },
    {
        "text": "and purple the ones that have not yet",
        "start": 655.63,
        "duration": 6.569
    },
    {
        "text": "and about 5% of this matrix is complete",
        "start": 656.949,
        "duration": 7.44
    },
    {
        "text": "or at least of the full matrix is",
        "start": 662.199,
        "duration": 3.621
    },
    {
        "text": "complete",
        "start": 664.389,
        "duration": 5.201
    },
    {
        "text": "the problem is if you are a particular",
        "start": 665.82,
        "duration": 6.27
    },
    {
        "text": "biologist studying a particular kind of",
        "start": 669.59,
        "duration": 4.51
    },
    {
        "text": "cell type and maybe you're interested in",
        "start": 672.09,
        "duration": 4.56
    },
    {
        "text": "a particular transcription factor you go",
        "start": 674.1,
        "duration": 4.41
    },
    {
        "text": "to encode project org and you say",
        "start": 676.65,
        "duration": 3.6
    },
    {
        "text": "alright I'm gonna go get all this data",
        "start": 678.51,
        "duration": 3.48
    },
    {
        "text": "that the tax payers paid millions of",
        "start": 680.25,
        "duration": 3.69
    },
    {
        "text": "dollars for and it's gonna tell me does",
        "start": 681.99,
        "duration": 3.72
    },
    {
        "text": "my transcription factors bind there or",
        "start": 683.94,
        "duration": 3.18
    },
    {
        "text": "is there a histone modification",
        "start": 685.71,
        "duration": 3.27
    },
    {
        "text": "occurring at the position that I'm",
        "start": 687.12,
        "duration": 4.47
    },
    {
        "text": "interested in in the genome and lo and",
        "start": 688.98,
        "duration": 5.04
    },
    {
        "text": "behold they didn't bother to do it now",
        "start": 691.59,
        "duration": 5.13
    },
    {
        "text": "you can email the encode investigators",
        "start": 694.02,
        "duration": 4.77
    },
    {
        "text": "and lobby to have them do the experiment",
        "start": 696.72,
        "duration": 4.62
    },
    {
        "text": "or you can do it yourself but maybe",
        "start": 698.79,
        "duration": 4.53
    },
    {
        "text": "those sell that particular type of cell",
        "start": 701.34,
        "duration": 3.39
    },
    {
        "text": "is hard to get or you don't have the",
        "start": 703.32,
        "duration": 3.57
    },
    {
        "text": "facilities to do it it would be nice if",
        "start": 704.73,
        "duration": 4.14
    },
    {
        "text": "you could try to use a computer to",
        "start": 706.89,
        "duration": 4.14
    },
    {
        "text": "predict what would happen if you did do",
        "start": 708.87,
        "duration": 4.8
    },
    {
        "text": "that experiment so that was the project",
        "start": 711.03,
        "duration": 4.65
    },
    {
        "text": "that Tim Durham tackled so Tim is a",
        "start": 713.67,
        "duration": 5.73
    },
    {
        "text": "graduate student in my lab and he was he",
        "start": 715.68,
        "duration": 4.98
    },
    {
        "text": "worked together with max another",
        "start": 719.4,
        "duration": 3.36
    },
    {
        "text": "graduate student Jeff Halbert who the",
        "start": 720.66,
        "duration": 3.54
    },
    {
        "text": "research scientists in lab and Jeff",
        "start": 722.76,
        "duration": 3.05
    },
    {
        "text": "films who's an electrical engineering",
        "start": 724.2,
        "duration": 5.16
    },
    {
        "text": "and the project was motivated in part by",
        "start": 725.81,
        "duration": 6.58
    },
    {
        "text": "Jeff Albert's participation before he",
        "start": 729.36,
        "duration": 5.43
    },
    {
        "text": "started in my lab in the Netflix prize",
        "start": 732.39,
        "duration": 7.32
    },
    {
        "text": "challenge so Jeff pointed out that",
        "start": 734.79,
        "duration": 7.44
    },
    {
        "text": "recommender systems have been extremely",
        "start": 739.71,
        "duration": 4.95
    },
    {
        "text": "successful in many different settings",
        "start": 742.23,
        "duration": 4.05
    },
    {
        "text": "and in particular they were useful in",
        "start": 744.66,
        "duration": 3.57
    },
    {
        "text": "the Netflix challenge so this was a",
        "start": 746.28,
        "duration": 4.11
    },
    {
        "text": "challenge in which Netflix handed out a",
        "start": 748.23,
        "duration": 5.04
    },
    {
        "text": "database of movies and users and asked",
        "start": 750.39,
        "duration": 3.9
    },
    {
        "text": "people to predict",
        "start": 753.27,
        "duration": 2.79
    },
    {
        "text": "here's a user I'll give you some of",
        "start": 754.29,
        "duration": 3.3
    },
    {
        "text": "their preferences now predict what other",
        "start": 756.06,
        "duration": 3.3
    },
    {
        "text": "kinds of movies they like on a scale of",
        "start": 757.59,
        "duration": 5.49
    },
    {
        "text": "one to five and the idea is that let's",
        "start": 759.36,
        "duration": 6.15
    },
    {
        "text": "say that Tim is a particular user and",
        "start": 763.08,
        "duration": 6.36
    },
    {
        "text": "this is a particularly you'd like to you",
        "start": 765.51,
        "duration": 7.89
    },
    {
        "text": "know matrix decomposition approach you",
        "start": 769.44,
        "duration": 6.99
    },
    {
        "text": "would take the you decompose this matrix",
        "start": 773.4,
        "duration": 5.22
    },
    {
        "text": "into two matrices each with the latent",
        "start": 776.43,
        "duration": 5.4
    },
    {
        "text": "dimension of Cade latent dimension of K",
        "start": 778.62,
        "duration": 6.63
    },
    {
        "text": "values where the row here represents",
        "start": 781.83,
        "duration": 5.79
    },
    {
        "text": "sort of features of the movie and the",
        "start": 785.25,
        "duration": 4.65
    },
    {
        "text": "row the column here represents features",
        "start": 787.62,
        "duration": 4.77
    },
    {
        "text": "of the user then you can say you know",
        "start": 789.9,
        "duration": 4.62
    },
    {
        "text": "for instance this value might represent",
        "start": 792.39,
        "duration": 4.14
    },
    {
        "text": "how much is this movie about cycling and",
        "start": 794.52,
        "duration": 4.11
    },
    {
        "text": "this value might be",
        "start": 796.53,
        "duration": 4.049
    },
    {
        "text": "does Tim like movies about cycling and",
        "start": 798.63,
        "duration": 4.5
    },
    {
        "text": "as you can tell from his sporty gear he",
        "start": 800.579,
        "duration": 4.891
    },
    {
        "text": "does like cycling and so then you can",
        "start": 803.13,
        "duration": 3.81
    },
    {
        "text": "predict that he's probably gonna like",
        "start": 805.47,
        "duration": 5.729
    },
    {
        "text": "this so the idea of relatively",
        "start": 806.94,
        "duration": 6.329
    },
    {
        "text": "straightforward idea in this first",
        "start": 811.199,
        "duration": 4.771
    },
    {
        "text": "project was to try to do this but use",
        "start": 813.269,
        "duration": 5.461
    },
    {
        "text": "instead of a matrix use a tensor so the",
        "start": 815.97,
        "duration": 4.849
    },
    {
        "text": "tensor a version of matrix decomposition",
        "start": 818.73,
        "duration": 6.06
    },
    {
        "text": "was described back in 1970 that's called",
        "start": 820.819,
        "duration": 6.671
    },
    {
        "text": "parrot back but the basic idea is",
        "start": 824.79,
        "duration": 3.96
    },
    {
        "text": "exactly the same as what I just",
        "start": 827.49,
        "duration": 3.24
    },
    {
        "text": "described to you except that instead of",
        "start": 828.75,
        "duration": 4.5
    },
    {
        "text": "movies and users we have cell types say",
        "start": 830.73,
        "duration": 5.339
    },
    {
        "text": "types and genomic position and so what",
        "start": 833.25,
        "duration": 5.16
    },
    {
        "text": "you get are these three different sub",
        "start": 836.069,
        "duration": 4.95
    },
    {
        "text": "matrices or the different matrices that",
        "start": 838.41,
        "duration": 4.619
    },
    {
        "text": "are produced during the decomposition",
        "start": 841.019,
        "duration": 4.711
    },
    {
        "text": "and when you take a sort of generalized",
        "start": 843.029,
        "duration": 5.49
    },
    {
        "text": "scalar product of the corresponding rows",
        "start": 845.73,
        "duration": 4.289
    },
    {
        "text": "or column from each of those matrices",
        "start": 848.519,
        "duration": 4.291
    },
    {
        "text": "and combine them then that gives you a",
        "start": 850.019,
        "duration": 5.221
    },
    {
        "text": "prediction for a particular cell type of",
        "start": 852.81,
        "duration": 4.35
    },
    {
        "text": "particular assay type at a particular",
        "start": 855.24,
        "duration": 5.339
    },
    {
        "text": "position in the genome the optimization",
        "start": 857.16,
        "duration": 5.64
    },
    {
        "text": "this is the original pair of fact paper",
        "start": 860.579,
        "duration": 5.06
    },
    {
        "text": "the optimization it looks like this",
        "start": 862.8,
        "duration": 4.68
    },
    {
        "text": "mathematically it looks a little",
        "start": 865.639,
        "duration": 3.43
    },
    {
        "text": "daunting but it's actually relatively",
        "start": 867.48,
        "duration": 3.39
    },
    {
        "text": "straightforward all you're doing is",
        "start": 869.069,
        "duration": 3.901
    },
    {
        "text": "summing over all the different possible",
        "start": 870.87,
        "duration": 3.93
    },
    {
        "text": "combinations that I just described and",
        "start": 872.97,
        "duration": 4.739
    },
    {
        "text": "saying how close what's the difference",
        "start": 874.8,
        "duration": 5.519
    },
    {
        "text": "between the value I get when I predict",
        "start": 877.709,
        "duration": 6.75
    },
    {
        "text": "at this position and the actual value so",
        "start": 880.319,
        "duration": 6.96
    },
    {
        "text": "here's the actual training set value and",
        "start": 884.459,
        "duration": 4.831
    },
    {
        "text": "here's the value you get when you do",
        "start": 887.279,
        "duration": 3.901
    },
    {
        "text": "this scalar product with some additional",
        "start": 889.29,
        "duration": 4.71
    },
    {
        "text": "factors built into it and you take the",
        "start": 891.18,
        "duration": 4.829
    },
    {
        "text": "mean squared error between those and you",
        "start": 894.0,
        "duration": 3.42
    },
    {
        "text": "put in some terms to do some",
        "start": 896.009,
        "duration": 4.58
    },
    {
        "text": "regularization to prevent overfitting",
        "start": 897.42,
        "duration": 5.279
    },
    {
        "text": "this turned out to be challenging for",
        "start": 900.589,
        "duration": 4.571
    },
    {
        "text": "many reasons the in particular it's a",
        "start": 902.699,
        "duration": 4.38
    },
    {
        "text": "fairly large data set and so we wanted",
        "start": 905.16,
        "duration": 3.599
    },
    {
        "text": "to do it in the cloud so there was a",
        "start": 907.079,
        "duration": 4.351
    },
    {
        "text": "whole large part of the paper about this",
        "start": 908.759,
        "duration": 4.591
    },
    {
        "text": "is about how you get this to work in a",
        "start": 911.43,
        "duration": 4.589
    },
    {
        "text": "cloud computing setting and we did it",
        "start": 913.35,
        "duration": 5.4
    },
    {
        "text": "both on Amazon Cloud and Azure first or",
        "start": 916.019,
        "duration": 5.161
    },
    {
        "text": "mostly for logistical reasons having to",
        "start": 918.75,
        "duration": 4.319
    },
    {
        "text": "do with both funding and getting around",
        "start": 921.18,
        "duration": 5.25
    },
    {
        "text": "bugs but it ended up working quite well",
        "start": 923.069,
        "duration": 5.341
    },
    {
        "text": "now I'm not going to show you all the",
        "start": 926.43,
        "duration": 3.779
    },
    {
        "text": "results there because I want to focus on",
        "start": 928.41,
        "duration": 4.08
    },
    {
        "text": "the next project which goes",
        "start": 930.209,
        "duration": 4.171
    },
    {
        "text": "beyond this and I'll convince you that",
        "start": 932.49,
        "duration": 3.69
    },
    {
        "text": "that one worked even better and that's",
        "start": 934.38,
        "duration": 6.17
    },
    {
        "text": "the sort of the key take-home question",
        "start": 936.18,
        "duration": 4.37
    },
    {
        "text": "so so know it's funny Tony and I were",
        "start": 947.03,
        "duration": 6.58
    },
    {
        "text": "hired at Columbia like basically the",
        "start": 951.57,
        "duration": 4.47
    },
    {
        "text": "same year but I know Tony and we",
        "start": 953.61,
        "duration": 3.72
    },
    {
        "text": "co-advised the student but I don't know",
        "start": 956.04,
        "duration": 2.73
    },
    {
        "text": "anything about what you're talking about",
        "start": 957.33,
        "duration": 19.67
    },
    {
        "text": "yeah I'd like to hear okay oh really",
        "start": 958.77,
        "duration": 18.23
    },
    {
        "text": "okay great and I should I meant to say",
        "start": 977.27,
        "duration": 4.06
    },
    {
        "text": "actually please do interrupt if there's",
        "start": 979.86,
        "duration": 3.69
    },
    {
        "text": "other questions I'd rather stop and get",
        "start": 981.33,
        "duration": 4.92
    },
    {
        "text": "clarifications and so on then then power",
        "start": 983.55,
        "duration": 11.01
    },
    {
        "text": "through and lose oh yeah yeah I see",
        "start": 986.25,
        "duration": 11.79
    },
    {
        "text": "interesting okay so what happened next",
        "start": 994.56,
        "duration": 6.12
    },
    {
        "text": "was that Jacob Schreiber joined my lab",
        "start": 998.04,
        "duration": 5.16
    },
    {
        "text": "and he developed a piece of software all",
        "start": 1000.68,
        "duration": 4.14
    },
    {
        "text": "of his software is named after fruits",
        "start": 1003.2,
        "duration": 4.68
    },
    {
        "text": "and avocado apparently is a food even",
        "start": 1004.82,
        "duration": 3.99
    },
    {
        "text": "though I kind of think of it as a",
        "start": 1007.88,
        "duration": 2.85
    },
    {
        "text": "vegetable so it shows up in your salad",
        "start": 1008.81,
        "duration": 4.8
    },
    {
        "text": "and so he developed a software called",
        "start": 1010.73,
        "duration": 5.49
    },
    {
        "text": "avocado that is sort of the predicted",
        "start": 1013.61,
        "duration": 4.74
    },
    {
        "text": "version 2.0 and this was done in",
        "start": 1016.22,
        "duration": 4.71
    },
    {
        "text": "collaboration with him and Jeff so the",
        "start": 1018.35,
        "duration": 6.33
    },
    {
        "text": "basic concept here was first of all",
        "start": 1020.93,
        "duration": 6.09
    },
    {
        "text": "Jacob likes deep learning right so it's",
        "start": 1024.68,
        "duration": 3.81
    },
    {
        "text": "coming into it with that sort of plier",
        "start": 1027.02,
        "duration": 3.66
    },
    {
        "text": "he's like okay I want to find some way",
        "start": 1028.49,
        "duration": 4.53
    },
    {
        "text": "to apply deep learning but we already",
        "start": 1030.68,
        "duration": 4.59
    },
    {
        "text": "have this successful infrastructure the",
        "start": 1033.02,
        "duration": 5.37
    },
    {
        "text": "key drawback a one key drawback to the",
        "start": 1035.27,
        "duration": 4.62
    },
    {
        "text": "way that we did this with the tensor",
        "start": 1038.39,
        "duration": 3.33
    },
    {
        "text": "decomposition was you're sort of",
        "start": 1039.89,
        "duration": 3.6
    },
    {
        "text": "treating everything independently but",
        "start": 1041.72,
        "duration": 4.38
    },
    {
        "text": "more more to the point you're also just",
        "start": 1043.49,
        "duration": 4.68
    },
    {
        "text": "doing this relatively straightforward",
        "start": 1046.1,
        "duration": 4.14
    },
    {
        "text": "generalized scalar product to combine",
        "start": 1048.17,
        "duration": 4.89
    },
    {
        "text": "the latent factors and maybe that should",
        "start": 1050.24,
        "duration": 5.13
    },
    {
        "text": "be done in some nonlinear way but turns",
        "start": 1053.06,
        "duration": 4.11
    },
    {
        "text": "out there's an existing set of tools",
        "start": 1055.37,
        "duration": 4.83
    },
    {
        "text": "called deep tensor factorization that do",
        "start": 1057.17,
        "duration": 5.46
    },
    {
        "text": "allow you to do to a nonlinear setting",
        "start": 1060.2,
        "duration": 5.37
    },
    {
        "text": "where you essentially take your a say",
        "start": 1062.63,
        "duration": 3.78
    },
    {
        "text": "positions",
        "start": 1065.57,
        "duration": 3.24
    },
    {
        "text": "type things multiply them all together",
        "start": 1066.41,
        "duration": 5.97
    },
    {
        "text": "and then take the resulting vector and",
        "start": 1068.81,
        "duration": 6.18
    },
    {
        "text": "put it through a deep neural network and",
        "start": 1072.38,
        "duration": 4.97
    },
    {
        "text": "that's called the tensor factorization",
        "start": 1074.99,
        "duration": 4.95
    },
    {
        "text": "Jacob made a slight modification of that",
        "start": 1077.35,
        "duration": 4.21
    },
    {
        "text": "idea with the idea that we don't",
        "start": 1079.94,
        "duration": 4.77
    },
    {
        "text": "necessarily want to assume that we have",
        "start": 1081.56,
        "duration": 5.43
    },
    {
        "text": "to multiply the the values together",
        "start": 1084.71,
        "duration": 4.53
    },
    {
        "text": "first we want to actually allow the",
        "start": 1086.99,
        "duration": 4.17
    },
    {
        "text": "multiplication to be generalized also",
        "start": 1089.24,
        "duration": 4.11
    },
    {
        "text": "and so rather than multiplying first and",
        "start": 1091.16,
        "duration": 4.41
    },
    {
        "text": "then putting the local ID values through",
        "start": 1093.35,
        "duration": 4.95
    },
    {
        "text": "he just concatenated them and this has",
        "start": 1095.57,
        "duration": 4.65
    },
    {
        "text": "this the nice side benefit that we no",
        "start": 1098.3,
        "duration": 4.4
    },
    {
        "text": "longer have to have the same latent",
        "start": 1100.22,
        "duration": 4.83
    },
    {
        "text": "dimensionality or the different kinds of",
        "start": 1102.7,
        "duration": 4.15
    },
    {
        "text": "factors so you could have for example",
        "start": 1105.05,
        "duration": 4.89
    },
    {
        "text": "more factors for cell type than you have",
        "start": 1106.85,
        "duration": 5.46
    },
    {
        "text": "the genomic position and that turned out",
        "start": 1109.94,
        "duration": 6.42
    },
    {
        "text": "to be quite helpful so in particular for",
        "start": 1112.31,
        "duration": 6.03
    },
    {
        "text": "the genomic position this is where the",
        "start": 1116.36,
        "duration": 4.53
    },
    {
        "text": "bulk of our parameters are of this model",
        "start": 1118.34,
        "duration": 4.53
    },
    {
        "text": "because even though we're doing this at",
        "start": 1120.89,
        "duration": 3.75
    },
    {
        "text": "one point we're doing this a 25 base",
        "start": 1122.87,
        "duration": 4.53
    },
    {
        "text": "pair resolution there's still a lot of",
        "start": 1124.64,
        "duration": 4.56
    },
    {
        "text": "positions even if we're only looking at",
        "start": 1127.4,
        "duration": 5.01
    },
    {
        "text": "the pilot regions of the genome which is",
        "start": 1129.2,
        "duration": 5.82
    },
    {
        "text": "the 1% of the genome that encode focused",
        "start": 1132.41,
        "duration": 4.53
    },
    {
        "text": "on initially",
        "start": 1135.02,
        "duration": 6.57
    },
    {
        "text": "so the idea is could we solve the or",
        "start": 1136.94,
        "duration": 6.719
    },
    {
        "text": "address the independence problem that I",
        "start": 1141.59,
        "duration": 4.049
    },
    {
        "text": "alluded to earlier while also reducing",
        "start": 1143.659,
        "duration": 5.01
    },
    {
        "text": "our parameter space and the idea is well",
        "start": 1145.639,
        "duration": 5.22
    },
    {
        "text": "we know that the different kinds of",
        "start": 1148.669,
        "duration": 4.38
    },
    {
        "text": "phenomena happen at different scales",
        "start": 1150.859,
        "duration": 3.57
    },
    {
        "text": "you're looking for a particular",
        "start": 1153.049,
        "duration": 3.69
    },
    {
        "text": "transcription factor maybe that does",
        "start": 1154.429,
        "duration": 4.56
    },
    {
        "text": "bind at a 25 base pair resolution or",
        "start": 1156.739,
        "duration": 3.36
    },
    {
        "text": "something close to that",
        "start": 1158.989,
        "duration": 2.28
    },
    {
        "text": "whereas if you're looking at some",
        "start": 1160.099,
        "duration": 3.241
    },
    {
        "text": "nucleosomes sized thing that's a",
        "start": 1161.269,
        "duration": 4.321
    },
    {
        "text": "different or even histone modifications",
        "start": 1163.34,
        "duration": 4.11
    },
    {
        "text": "happen at the level of nucleosomes a",
        "start": 1165.59,
        "duration": 3.959
    },
    {
        "text": "little bit more like 250 base pairs and",
        "start": 1167.45,
        "duration": 4.109
    },
    {
        "text": "then you might have gene size or larger",
        "start": 1169.549,
        "duration": 4.71
    },
    {
        "text": "elements and so what we did was divide",
        "start": 1171.559,
        "duration": 5.37
    },
    {
        "text": "our factors into some that are defined",
        "start": 1174.259,
        "duration": 6.24
    },
    {
        "text": "at 25 base pair resolution some at 250",
        "start": 1176.929,
        "duration": 4.98
    },
    {
        "text": "and summit-five Killah base pair",
        "start": 1180.499,
        "duration": 3.78
    },
    {
        "text": "resolution and so this reduces the total",
        "start": 1181.909,
        "duration": 3.75
    },
    {
        "text": "number of parameters in the model by",
        "start": 1184.279,
        "duration": 4.08
    },
    {
        "text": "about a factor of 5",
        "start": 1185.659,
        "duration": 5.07
    },
    {
        "text": "you could ask how do you come up with 25",
        "start": 1188.359,
        "duration": 4.26
    },
    {
        "text": "to 50 and 5 KB and those were",
        "start": 1190.729,
        "duration": 3.78
    },
    {
        "text": "essentially just chosen based on our",
        "start": 1192.619,
        "duration": 4.44
    },
    {
        "text": "prior knowledge you also ask how'd you",
        "start": 1194.509,
        "duration": 5.431
    },
    {
        "text": "come up with 15 35 and 50 that was done",
        "start": 1197.059,
        "duration": 4.98
    },
    {
        "text": "essentially through a hyper parameter",
        "start": 1199.94,
        "duration": 5.099
    },
    {
        "text": "search so we have to choose the hyper",
        "start": 1202.039,
        "duration": 5.07
    },
    {
        "text": "parameters of the model how many layers",
        "start": 1205.039,
        "duration": 4.59
    },
    {
        "text": "how many neurons per layer and then how",
        "start": 1207.109,
        "duration": 4.471
    },
    {
        "text": "many factors for each of these things we",
        "start": 1209.629,
        "duration": 3.6
    },
    {
        "text": "have seven different hyper parameters",
        "start": 1211.58,
        "duration": 3.809
    },
    {
        "text": "and we just do a random search over a",
        "start": 1213.229,
        "duration": 4.62
    },
    {
        "text": "grid of these hyper parameters and",
        "start": 1215.389,
        "duration": 4.441
    },
    {
        "text": "essentially measure the mean squared",
        "start": 1217.849,
        "duration": 4.02
    },
    {
        "text": "error so this is just a plot of a",
        "start": 1219.83,
        "duration": 3.51
    },
    {
        "text": "histogram of all those mean squared",
        "start": 1221.869,
        "duration": 3.48
    },
    {
        "text": "errors for different parameter",
        "start": 1223.34,
        "duration": 4.049
    },
    {
        "text": "combinations trained on that one percent",
        "start": 1225.349,
        "duration": 4.8
    },
    {
        "text": "of the genome the two bars here show the",
        "start": 1227.389,
        "duration": 5.01
    },
    {
        "text": "performance of chrome impute and",
        "start": 1230.149,
        "duration": 4.681
    },
    {
        "text": "predicted so mean squared error right so",
        "start": 1232.399,
        "duration": 4.921
    },
    {
        "text": "smaller is better and so we chose the",
        "start": 1234.83,
        "duration": 3.87
    },
    {
        "text": "model that worked the best on the",
        "start": 1237.32,
        "duration": 5.789
    },
    {
        "text": "validation down here so this gives us a",
        "start": 1238.7,
        "duration": 7.26
    },
    {
        "text": "performance increase of 4.4 percent",
        "start": 1243.109,
        "duration": 4.8
    },
    {
        "text": "decrease in means great error compared",
        "start": 1245.96,
        "duration": 4.409
    },
    {
        "text": "to predicted and a 17 point 5 percent",
        "start": 1247.909,
        "duration": 5.37
    },
    {
        "text": "decrease compared to the first method",
        "start": 1250.369,
        "duration": 5.31
    },
    {
        "text": "and developed in this field by Noah",
        "start": 1253.279,
        "duration": 4.441
    },
    {
        "text": "scalice and Jason Ernst called chrome",
        "start": 1255.679,
        "duration": 4.37
    },
    {
        "text": "impute",
        "start": 1257.72,
        "duration": 2.329
    },
    {
        "text": "this is just an aside to let you know",
        "start": 1260.48,
        "duration": 4.35
    },
    {
        "text": "what if you look at the results from",
        "start": 1262.609,
        "duration": 3.841
    },
    {
        "text": "that hyper parameter search a slightly",
        "start": 1264.83,
        "duration": 3.9
    },
    {
        "text": "different way you can see that the",
        "start": 1266.45,
        "duration": 4.169
    },
    {
        "text": "neural network parameters the the",
        "start": 1268.73,
        "duration": 4.079
    },
    {
        "text": "topology parameters are by far the most",
        "start": 1270.619,
        "duration": 5.091
    },
    {
        "text": "important each of these plots is just",
        "start": 1272.809,
        "duration": 5.31
    },
    {
        "text": "marginalizing over one of the of the",
        "start": 1275.71,
        "duration": 4.059
    },
    {
        "text": "seven hyper parameters that we were",
        "start": 1278.119,
        "duration": 3.51
    },
    {
        "text": "searching for so these are the five",
        "start": 1279.769,
        "duration": 3.48
    },
    {
        "text": "different values for a given hyper",
        "start": 1281.629,
        "duration": 5.221
    },
    {
        "text": "parameter and these are the histogram of",
        "start": 1283.249,
        "duration": 6.39
    },
    {
        "text": "mean squared error values for all runs",
        "start": 1286.85,
        "duration": 4.98
    },
    {
        "text": "where that that value was say for",
        "start": 1289.639,
        "duration": 5.28
    },
    {
        "text": "example and the main observation is that",
        "start": 1291.83,
        "duration": 6.63
    },
    {
        "text": "these are relatively flat whereas these",
        "start": 1294.919,
        "duration": 6.651
    },
    {
        "text": "show a marked trend as you get more",
        "start": 1298.46,
        "duration": 7.73
    },
    {
        "text": "larger models you're getting improved",
        "start": 1301.57,
        "duration": 4.62
    },
    {
        "text": "okay so practically speaking what this",
        "start": 1306.519,
        "duration": 4.99
    },
    {
        "text": "means is that you can look at real data",
        "start": 1309.919,
        "duration": 4.801
    },
    {
        "text": "like this and then you can try to impute",
        "start": 1311.509,
        "duration": 5.941
    },
    {
        "text": "it and you can see that qualitatively",
        "start": 1314.72,
        "duration": 4.829
    },
    {
        "text": "all three of these different methods",
        "start": 1317.45,
        "duration": 6.51
    },
    {
        "text": "seem to perform well in the sense that",
        "start": 1319.549,
        "duration": 7.411
    },
    {
        "text": "they capture the main qualitative",
        "start": 1323.96,
        "duration": 5.429
    },
    {
        "text": "features of the data we can tell you",
        "start": 1326.96,
        "duration": 5.25
    },
    {
        "text": "that quantitatively that we do have an",
        "start": 1329.389,
        "duration": 5.25
    },
    {
        "text": "improvement in in the lowest one which",
        "start": 1332.21,
        "duration": 7.049
    },
    {
        "text": "is the avocado imputation there's a",
        "start": 1334.639,
        "duration": 6.12
    },
    {
        "text": "bunch of metrics that I'm not going to",
        "start": 1339.259,
        "duration": 3.721
    },
    {
        "text": "go through so I'm just gonna skip this",
        "start": 1340.759,
        "duration": 3.841
    },
    {
        "text": "slide because otherwise you'll stare at",
        "start": 1342.98,
        "duration": 2.85
    },
    {
        "text": "it and try to figure out what they all",
        "start": 1344.6,
        "duration": 1.47
    },
    {
        "text": "mean",
        "start": 1345.83,
        "duration": 3.689
    },
    {
        "text": "and instead I'll focus on the subsequent",
        "start": 1346.07,
        "duration": 5.37
    },
    {
        "text": "part where we tried to zoom in a little",
        "start": 1349.519,
        "duration": 3.72
    },
    {
        "text": "bit more to ask what we thought were",
        "start": 1351.44,
        "duration": 3.78
    },
    {
        "text": "kind of the interesting questions which",
        "start": 1353.239,
        "duration": 4.38
    },
    {
        "text": "is if you think about when are you gonna",
        "start": 1355.22,
        "duration": 5.87
    },
    {
        "text": "use an imputation",
        "start": 1357.619,
        "duration": 3.471
    },
    {
        "text": "method like this well you really want to",
        "start": 1361.21,
        "duration": 6.429
    },
    {
        "text": "be able to do is find Peaks most of the",
        "start": 1363.859,
        "duration": 5.31
    },
    {
        "text": "time if you're looking for Satan or",
        "start": 1367.639,
        "duration": 5.581
    },
    {
        "text": "histone modifications or even gene",
        "start": 1369.169,
        "duration": 5.82
    },
    {
        "text": "expression values you know you'd like to",
        "start": 1373.22,
        "duration": 3.569
    },
    {
        "text": "be able to predict where there's large",
        "start": 1374.989,
        "duration": 5.461
    },
    {
        "text": "Peaks in the data and most of the time",
        "start": 1376.789,
        "duration": 5.49
    },
    {
        "text": "what's interesting to you are Peaks that",
        "start": 1380.45,
        "duration": 4.64
    },
    {
        "text": "are sort of unusual right if you look at",
        "start": 1382.279,
        "duration": 5.551
    },
    {
        "text": "27 different cell types and you see a",
        "start": 1385.09,
        "duration": 4.63
    },
    {
        "text": "peak at this position every time and",
        "start": 1387.83,
        "duration": 4.169
    },
    {
        "text": "then someone predicts in the 28th cell",
        "start": 1389.72,
        "duration": 3.36
    },
    {
        "text": "type that there's going to be a peak",
        "start": 1391.999,
        "duration": 2.131
    },
    {
        "text": "there you're not",
        "start": 1393.08,
        "duration": 3.209
    },
    {
        "text": "gonna be super excited if you like no",
        "start": 1394.13,
        "duration": 4.26
    },
    {
        "text": "death right but if it's like half the",
        "start": 1396.289,
        "duration": 4.081
    },
    {
        "text": "time peak half the time not and you're",
        "start": 1398.39,
        "duration": 3.659
    },
    {
        "text": "pretty good at predicting in the next",
        "start": 1400.37,
        "duration": 3.32
    },
    {
        "text": "one whether it is that's a hard problem",
        "start": 1402.049,
        "duration": 4.411
    },
    {
        "text": "right so what we decided to do was",
        "start": 1403.69,
        "duration": 6.07
    },
    {
        "text": "segregate all of the peaks according to",
        "start": 1406.46,
        "duration": 5.91
    },
    {
        "text": "how frequently they occur so here's a",
        "start": 1409.76,
        "duration": 4.68
    },
    {
        "text": "picture just showing you know a peak is",
        "start": 1412.37,
        "duration": 3.78
    },
    {
        "text": "present in this number of cell types so",
        "start": 1414.44,
        "duration": 3.78
    },
    {
        "text": "you have like all the ones that are that",
        "start": 1416.15,
        "duration": 4.71
    },
    {
        "text": "are constitutive and those get evaluated",
        "start": 1418.22,
        "duration": 4.02
    },
    {
        "text": "separately from all the ones that are",
        "start": 1420.86,
        "duration": 3.449
    },
    {
        "text": "super rare and the ones in the middle",
        "start": 1422.24,
        "duration": 4.4
    },
    {
        "text": "are sort of the hardest ones to look to",
        "start": 1424.309,
        "duration": 6.181
    },
    {
        "text": "identify and so we looked at this three",
        "start": 1426.64,
        "duration": 5.26
    },
    {
        "text": "way we looked at the mean squared error",
        "start": 1430.49,
        "duration": 3.36
    },
    {
        "text": "separately in each of these different",
        "start": 1431.9,
        "duration": 4.8
    },
    {
        "text": "kinds of peaks and then also recall and",
        "start": 1433.85,
        "duration": 5.309
    },
    {
        "text": "precision where we had a argument in the",
        "start": 1436.7,
        "duration": 4.29
    },
    {
        "text": "paper about how we set the threshold for",
        "start": 1439.159,
        "duration": 5.581
    },
    {
        "text": "that and so here's the results what you",
        "start": 1440.99,
        "duration": 7.29
    },
    {
        "text": "can see is that here what we've done in",
        "start": 1444.74,
        "duration": 5.52
    },
    {
        "text": "is supposed to be over there number of",
        "start": 1448.28,
        "duration": 5.19
    },
    {
        "text": "cell types these regions are peak in so",
        "start": 1450.26,
        "duration": 4.94
    },
    {
        "text": "you've got some of these things are",
        "start": 1453.47,
        "duration": 4.11
    },
    {
        "text": "constitutive some of them are extremely",
        "start": 1455.2,
        "duration": 4.27
    },
    {
        "text": "rare Peaks that only happen in new cell",
        "start": 1457.58,
        "duration": 3.329
    },
    {
        "text": "types and these ones in the middle are",
        "start": 1459.47,
        "duration": 3.36
    },
    {
        "text": "the harder ones where it's sometimes",
        "start": 1460.909,
        "duration": 4.531
    },
    {
        "text": "there and sometimes not and if you look",
        "start": 1462.83,
        "duration": 4.68
    },
    {
        "text": "at the mean squared error you can see",
        "start": 1465.44,
        "duration": 4.44
    },
    {
        "text": "there's an improvement systematically",
        "start": 1467.51,
        "duration": 5.159
    },
    {
        "text": "between avocado relative to compute and",
        "start": 1469.88,
        "duration": 6.48
    },
    {
        "text": "predicted then we also can do recall and",
        "start": 1472.669,
        "duration": 5.671
    },
    {
        "text": "precision and here you can actually",
        "start": 1476.36,
        "duration": 4.319
    },
    {
        "text": "include the real experimental data as",
        "start": 1478.34,
        "duration": 5.73
    },
    {
        "text": "well because of that threshold and what",
        "start": 1480.679,
        "duration": 5.851
    },
    {
        "text": "you can see is that in terms of recall",
        "start": 1484.07,
        "duration": 4.5
    },
    {
        "text": "it looks like chrome imputed actually",
        "start": 1486.53,
        "duration": 4.08
    },
    {
        "text": "does really well compared to the other",
        "start": 1488.57,
        "duration": 4.53
    },
    {
        "text": "two but in terms of precision it does",
        "start": 1490.61,
        "duration": 5.28
    },
    {
        "text": "much worse so what this is telling us is",
        "start": 1493.1,
        "duration": 4.85
    },
    {
        "text": "that chrome imputed is sort of over",
        "start": 1495.89,
        "duration": 6.21
    },
    {
        "text": "predicting Peaks or and alternatively",
        "start": 1497.95,
        "duration": 5.62
    },
    {
        "text": "you can think of it as these other two",
        "start": 1502.1,
        "duration": 4.23
    },
    {
        "text": "methods are under predicting so you're",
        "start": 1503.57,
        "duration": 4.979
    },
    {
        "text": "getting a trade-off here precision and",
        "start": 1506.33,
        "duration": 4.65
    },
    {
        "text": "recall between the chrome imputing",
        "start": 1508.549,
        "duration": 4.321
    },
    {
        "text": "method and the two methods that we've",
        "start": 1510.98,
        "duration": 3.9
    },
    {
        "text": "developed and so of course that's a",
        "start": 1512.87,
        "duration": 4.169
    },
    {
        "text": "trade off but as an end user you'd want",
        "start": 1514.88,
        "duration": 4.47
    },
    {
        "text": "to know about and sort of help you to",
        "start": 1517.039,
        "duration": 5.451
    },
    {
        "text": "interpret the peaks that get predicted",
        "start": 1519.35,
        "duration": 6.14
    },
    {
        "text": "yeah",
        "start": 1522.49,
        "duration": 3.0
    },
    {
        "text": "and it gets easier and easier to say",
        "start": 1534.88,
        "duration": 4.62
    },
    {
        "text": "this is just there in all the time the",
        "start": 1536.95,
        "duration": 4.23
    },
    {
        "text": "ones that are easy are the ones that are",
        "start": 1539.5,
        "duration": 6.17
    },
    {
        "text": "always there are always not there okay",
        "start": 1541.18,
        "duration": 7.23
    },
    {
        "text": "okay so then the other half of this",
        "start": 1545.67,
        "duration": 5.59
    },
    {
        "text": "avocado project was to say okay we've",
        "start": 1548.41,
        "duration": 4.62
    },
    {
        "text": "done all this learning we now have these",
        "start": 1551.26,
        "duration": 4.2
    },
    {
        "text": "latent representations maybe they can be",
        "start": 1553.03,
        "duration": 4.65
    },
    {
        "text": "used in a transfer learning setting to",
        "start": 1555.46,
        "duration": 5.61
    },
    {
        "text": "help us do other things well so maybe",
        "start": 1557.68,
        "duration": 5.04
    },
    {
        "text": "those latent factors provide a useful",
        "start": 1561.07,
        "duration": 4.92
    },
    {
        "text": "summary so for example if you just take",
        "start": 1562.72,
        "duration": 6.21
    },
    {
        "text": "the latent factors on the cell type axis",
        "start": 1565.99,
        "duration": 5.67
    },
    {
        "text": "and you reduce them so they were I",
        "start": 1568.93,
        "duration": 4.56
    },
    {
        "text": "honestly don't remember let's say they",
        "start": 1571.66,
        "duration": 3.45
    },
    {
        "text": "were 50 dimensional you know what number",
        "start": 1573.49,
        "duration": 3.0
    },
    {
        "text": "we came up with let's say they're 50",
        "start": 1575.11,
        "duration": 2.55
    },
    {
        "text": "dimensional we reduce into two",
        "start": 1576.49,
        "duration": 5.28
    },
    {
        "text": "dimensions using t-sne and project here",
        "start": 1577.66,
        "duration": 5.97
    },
    {
        "text": "in this picture and what you can see is",
        "start": 1581.77,
        "duration": 3.93
    },
    {
        "text": "that if you color them according to sort",
        "start": 1583.63,
        "duration": 5.85
    },
    {
        "text": "of the canonical region of the body that",
        "start": 1585.7,
        "duration": 6.18
    },
    {
        "text": "for instance all of the blood cell types",
        "start": 1589.48,
        "duration": 5.22
    },
    {
        "text": "cluster together and so on so that was",
        "start": 1591.88,
        "duration": 4.62
    },
    {
        "text": "encouraging it said the latent factors",
        "start": 1594.7,
        "duration": 4.17
    },
    {
        "text": "seemed to capture something real about",
        "start": 1596.5,
        "duration": 4.95
    },
    {
        "text": "the biology but for a more quantitative",
        "start": 1598.87,
        "duration": 4.77
    },
    {
        "text": "approach we decided to try to use the",
        "start": 1601.45,
        "duration": 6.18
    },
    {
        "text": "genomic factors these are 110 values for",
        "start": 1603.64,
        "duration": 5.91
    },
    {
        "text": "each 25 base pair region of the genome",
        "start": 1607.63,
        "duration": 4.47
    },
    {
        "text": "try to solve a number of sort of",
        "start": 1609.55,
        "duration": 4.8
    },
    {
        "text": "canonical prediction tasks in genomics",
        "start": 1612.1,
        "duration": 4.95
    },
    {
        "text": "and so we used a gradient boosting",
        "start": 1614.35,
        "duration": 4.95
    },
    {
        "text": "classifier just because that's a fairly",
        "start": 1617.05,
        "duration": 5.19
    },
    {
        "text": "robust and widely used classifier the",
        "start": 1619.3,
        "duration": 4.41
    },
    {
        "text": "claim here is not that this is going to",
        "start": 1622.24,
        "duration": 3.39
    },
    {
        "text": "produce the state-of-the-art method but",
        "start": 1623.71,
        "duration": 4.02
    },
    {
        "text": "mostly to show that these factors are a",
        "start": 1625.63,
        "duration": 4.05
    },
    {
        "text": "useful way to represent the genome and",
        "start": 1627.73,
        "duration": 4.23
    },
    {
        "text": "we do cross-validation across",
        "start": 1629.68,
        "duration": 4.17
    },
    {
        "text": "chromosomes and leave one out fashion",
        "start": 1631.96,
        "duration": 5.67
    },
    {
        "text": "and we compare five different kinds of",
        "start": 1633.85,
        "duration": 6.81
    },
    {
        "text": "inputs you can either just take the",
        "start": 1637.63,
        "duration": 6.09
    },
    {
        "text": "roadmap data directly and put it into",
        "start": 1640.66,
        "duration": 5.97
    },
    {
        "text": "the gradient boosting classifier so this",
        "start": 1643.72,
        "duration": 4.56
    },
    {
        "text": "is the data set that we used initially",
        "start": 1646.63,
        "duration": 4.92
    },
    {
        "text": "or you can take the imputed data so this",
        "start": 1648.28,
        "duration": 5.7
    },
    {
        "text": "would be the road map data plus all the",
        "start": 1651.55,
        "duration": 5.1
    },
    {
        "text": "imputations so extending it to be a much",
        "start": 1653.98,
        "duration": 4.41
    },
    {
        "text": "longer vector where you have all the",
        "start": 1656.65,
        "duration": 4.26
    },
    {
        "text": "imputed values as well and those can be",
        "start": 1658.39,
        "duration": 4.02
    },
    {
        "text": "imputed by any one of these three",
        "start": 1660.91,
        "duration": 4.55
    },
    {
        "text": "methods or you can just take the 110",
        "start": 1662.41,
        "duration": 6.03
    },
    {
        "text": "avocado Laden factor and our hypothesis",
        "start": 1665.46,
        "duration": 5.65
    },
    {
        "text": "that those are an information-rich way",
        "start": 1668.44,
        "duration": 4.65
    },
    {
        "text": "to represent all the information in",
        "start": 1671.11,
        "duration": 4.83
    },
    {
        "text": "those other vectors in a way that's sort",
        "start": 1673.09,
        "duration": 6.72
    },
    {
        "text": "of amenable to machine learning so the",
        "start": 1675.94,
        "duration": 7.32
    },
    {
        "text": "first experiment we did was predicting",
        "start": 1679.81,
        "duration": 6.03
    },
    {
        "text": "gene expression so what we're doing is",
        "start": 1683.26,
        "duration": 4.8
    },
    {
        "text": "we're taking RNA seek data from the",
        "start": 1685.84,
        "duration": 4.02
    },
    {
        "text": "roadmap project and ten different cell",
        "start": 1688.06,
        "duration": 5.1
    },
    {
        "text": "types and we're inputting five histone",
        "start": 1689.86,
        "duration": 6.0
    },
    {
        "text": "marks in promoter regions and predicting",
        "start": 1693.16,
        "duration": 4.47
    },
    {
        "text": "positions with RNA seek expression",
        "start": 1695.86,
        "duration": 3.15
    },
    {
        "text": "greater than point five so this is a",
        "start": 1697.63,
        "duration": 3.66
    },
    {
        "text": "fairly standard way to frame this",
        "start": 1699.01,
        "duration": 4.43
    },
    {
        "text": "prediction task as a classification task",
        "start": 1701.29,
        "duration": 5.01
    },
    {
        "text": "and actually I wrote ten but that's how",
        "start": 1703.44,
        "duration": 4.33
    },
    {
        "text": "an older version of this slide there's",
        "start": 1706.3,
        "duration": 5.97
    },
    {
        "text": "obviously many more than ten so type so",
        "start": 1707.77,
        "duration": 6.6
    },
    {
        "text": "and we've just sorted them according to",
        "start": 1712.27,
        "duration": 4.41
    },
    {
        "text": "the performance of the best of actually",
        "start": 1714.37,
        "duration": 4.8
    },
    {
        "text": "of the of the baseline so the majority",
        "start": 1716.68,
        "duration": 5.37
    },
    {
        "text": "baseline is just if you took the average",
        "start": 1719.17,
        "duration": 5.22
    },
    {
        "text": "among all the training set cell types",
        "start": 1722.05,
        "duration": 4.94
    },
    {
        "text": "and use that as your prediction for this",
        "start": 1724.39,
        "duration": 6.18
    },
    {
        "text": "and for each cell type you can see all",
        "start": 1726.99,
        "duration": 5.77
    },
    {
        "text": "of the different colors here and what",
        "start": 1730.57,
        "duration": 4.05
    },
    {
        "text": "hopefully you can tell is that there's a",
        "start": 1732.76,
        "duration": 4.32
    },
    {
        "text": "lot of green up near the top there I'm",
        "start": 1734.62,
        "duration": 4.35
    },
    {
        "text": "sorry a lot of red is what you want the",
        "start": 1737.08,
        "duration": 3.75
    },
    {
        "text": "red is up near the top there and it's",
        "start": 1738.97,
        "duration": 3.75
    },
    {
        "text": "hard to see in the picture so I put some",
        "start": 1740.83,
        "duration": 6.57
    },
    {
        "text": "statistics avocado improves over just",
        "start": 1742.72,
        "duration": 7.17
    },
    {
        "text": "using the epigenomic measurements so the",
        "start": 1747.4,
        "duration": 5.37
    },
    {
        "text": "raw data in all cell types and by an",
        "start": 1749.89,
        "duration": 5.28
    },
    {
        "text": "average at point one four four here this",
        "start": 1752.77,
        "duration": 4.53
    },
    {
        "text": "is the average precision so averaging",
        "start": 1755.17,
        "duration": 5.28
    },
    {
        "text": "over all the recall axes and it even",
        "start": 1757.3,
        "duration": 5.22
    },
    {
        "text": "performed better than the full roadmap",
        "start": 1760.45,
        "duration": 3.87
    },
    {
        "text": "compendium this means taking all of the",
        "start": 1762.52,
        "duration": 3.39
    },
    {
        "text": "different data not just from your cell",
        "start": 1764.32,
        "duration": 3.54
    },
    {
        "text": "type but from every other cell type and",
        "start": 1765.91,
        "duration": 4.8
    },
    {
        "text": "putting it into the classifier in 36 out",
        "start": 1767.86,
        "duration": 6.65
    },
    {
        "text": "of 47 cell 5 so this seemed like a",
        "start": 1770.71,
        "duration": 6.45
    },
    {
        "text": "promising evidence that this was a nice",
        "start": 1774.51,
        "duration": 7.44
    },
    {
        "text": "way to represent the genomic axis",
        "start": 1777.16,
        "duration": 6.53
    },
    {
        "text": "we also looked at a number of other",
        "start": 1781.95,
        "duration": 3.51
    },
    {
        "text": "tasks I didn't include them all we have",
        "start": 1783.69,
        "duration": 3.329
    },
    {
        "text": "promoter enhancer interactions",
        "start": 1785.46,
        "duration": 4.709
    },
    {
        "text": "predicting replication timing and now",
        "start": 1787.019,
        "duration": 5.341
    },
    {
        "text": "Jacobs currently looking at predicting",
        "start": 1790.169,
        "duration": 5.941
    },
    {
        "text": "exons but we did this one was frequently",
        "start": 1792.36,
        "duration": 6.83
    },
    {
        "text": "interacting regions this is a feature of",
        "start": 1796.11,
        "duration": 6.87
    },
    {
        "text": "hi-c data that essentially corresponds",
        "start": 1799.19,
        "duration": 6.28
    },
    {
        "text": "to it often corresponds to the edges of",
        "start": 1802.98,
        "duration": 4.62
    },
    {
        "text": "particular kinds of topological domains",
        "start": 1805.47,
        "duration": 4.799
    },
    {
        "text": "in hi-c data essentially what the way it",
        "start": 1807.6,
        "duration": 4.799
    },
    {
        "text": "works is that you look for sort of a",
        "start": 1810.269,
        "duration": 3.061
    },
    {
        "text": "cross-shaped",
        "start": 1812.399,
        "duration": 3.15
    },
    {
        "text": "region like this where you see",
        "start": 1813.33,
        "duration": 5.49
    },
    {
        "text": "particular density within these these",
        "start": 1815.549,
        "duration": 5.01
    },
    {
        "text": "bars and that means that the thing at",
        "start": 1818.82,
        "duration": 3.75
    },
    {
        "text": "the center of the cloth is a frequently",
        "start": 1820.559,
        "duration": 3.75
    },
    {
        "text": "interacting region there's actually a",
        "start": 1822.57,
        "duration": 3.599
    },
    {
        "text": "statistical model that tries to account",
        "start": 1824.309,
        "duration": 4.141
    },
    {
        "text": "for biases to identify these frequently",
        "start": 1826.169,
        "duration": 4.5
    },
    {
        "text": "interacting regions we just downloaded",
        "start": 1828.45,
        "duration": 5.76
    },
    {
        "text": "the fire annotations from this paper at",
        "start": 1830.669,
        "duration": 6.21
    },
    {
        "text": "all from cell reports and then asked can",
        "start": 1834.21,
        "duration": 4.11
    },
    {
        "text": "we plug it into our gradient boosting",
        "start": 1836.879,
        "duration": 9.211
    },
    {
        "text": "classifier and make predictions so",
        "start": 1838.32,
        "duration": 9.959
    },
    {
        "text": "that's an open question the question was",
        "start": 1846.09,
        "duration": 3.78
    },
    {
        "text": "if there's more than two tags coming",
        "start": 1848.279,
        "duration": 4.62
    },
    {
        "text": "together it's not clear what the fire is",
        "start": 1849.87,
        "duration": 6.24
    },
    {
        "text": "actually correspond to I think that is",
        "start": 1852.899,
        "duration": 4.98
    },
    {
        "text": "one model because there's it's",
        "start": 1856.11,
        "duration": 3.24
    },
    {
        "text": "frequently and there's so much",
        "start": 1857.879,
        "duration": 3.061
    },
    {
        "text": "interaction happening that there may be",
        "start": 1859.35,
        "duration": 4.799
    },
    {
        "text": "you know lots of interaction that's",
        "start": 1860.94,
        "duration": 4.05
    },
    {
        "text": "right that's right",
        "start": 1864.149,
        "duration": 4.11
    },
    {
        "text": "for our purposes all we know is they we",
        "start": 1864.99,
        "duration": 5.669
    },
    {
        "text": "have a clear definition of them and so",
        "start": 1868.259,
        "duration": 4.081
    },
    {
        "text": "we'd like to be able to say where else",
        "start": 1870.659,
        "duration": 5.73
    },
    {
        "text": "might they be along the genome and so",
        "start": 1872.34,
        "duration": 6.539
    },
    {
        "text": "this is showing similar kinds of plots",
        "start": 1876.389,
        "duration": 4.14
    },
    {
        "text": "and thoughts again it's the mean average",
        "start": 1878.879,
        "duration": 3.961
    },
    {
        "text": "precision which is a really weird but I",
        "start": 1880.529,
        "duration": 4.561
    },
    {
        "text": "guess the fairly standard term mean and",
        "start": 1882.84,
        "duration": 3.809
    },
    {
        "text": "average it's average because it's",
        "start": 1885.09,
        "duration": 3.839
    },
    {
        "text": "averaged across all the different recall",
        "start": 1886.649,
        "duration": 4.321
    },
    {
        "text": "values and then it's mean because it's",
        "start": 1888.929,
        "duration": 6.811
    },
    {
        "text": "mean over a different chromosome let's",
        "start": 1890.97,
        "duration": 5.73
    },
    {
        "text": "see no I'll get it over different",
        "start": 1895.74,
        "duration": 3.87
    },
    {
        "text": "degenerate positions anyway I tried",
        "start": 1896.7,
        "duration": 4.679
    },
    {
        "text": "continue the average precision and the",
        "start": 1899.61,
        "duration": 4.439
    },
    {
        "text": "others license or dr. but the point is",
        "start": 1901.379,
        "duration": 4.951
    },
    {
        "text": "that we're again comparing the same",
        "start": 1904.049,
        "duration": 5.911
    },
    {
        "text": "kinds of things here and the avocado",
        "start": 1906.33,
        "duration": 6.78
    },
    {
        "text": "latent factors in nearly every case well",
        "start": 1909.96,
        "duration": 5.26
    },
    {
        "text": "in every case the outperform using any",
        "start": 1913.11,
        "duration": 5.71
    },
    {
        "text": "the imputed values and today in nearly",
        "start": 1915.22,
        "duration": 5.76
    },
    {
        "text": "every case they also outperform using",
        "start": 1918.82,
        "duration": 4.92
    },
    {
        "text": "the poll all the real data from the",
        "start": 1920.98,
        "duration": 5.34
    },
    {
        "text": "roadmap compendium so this is a much",
        "start": 1923.74,
        "duration": 5.179
    },
    {
        "text": "more concise way to represent the genome",
        "start": 1926.32,
        "duration": 6.479
    },
    {
        "text": "and it captures it a lot that's in there",
        "start": 1928.919,
        "duration": 6.221
    },
    {
        "text": "what you can also see is that if you",
        "start": 1932.799,
        "duration": 4.711
    },
    {
        "text": "take the full roadmap compendium it",
        "start": 1935.14,
        "duration": 4.38
    },
    {
        "text": "doesn't do as well as that this is data",
        "start": 1937.51,
        "duration": 4.98
    },
    {
        "text": "from all the other cell types so you're",
        "start": 1939.52,
        "duration": 4.74
    },
    {
        "text": "trying to predict whether there's a fire",
        "start": 1942.49,
        "duration": 4.319
    },
    {
        "text": "here by looking at what's going on in in",
        "start": 1944.26,
        "duration": 4.289
    },
    {
        "text": "other cell types which is a little bit",
        "start": 1946.809,
        "duration": 3.99
    },
    {
        "text": "counterintuitive until you remember that",
        "start": 1948.549,
        "duration": 4.801
    },
    {
        "text": "that contains a bunch of other kinds of",
        "start": 1950.799,
        "duration": 4.411
    },
    {
        "text": "measurements that weren't necessarily",
        "start": 1953.35,
        "duration": 3.9
    },
    {
        "text": "available in the cell type that you're",
        "start": 1955.21,
        "duration": 4.38
    },
    {
        "text": "looking at in particular ctcf and some",
        "start": 1957.25,
        "duration": 4.08
    },
    {
        "text": "of the other factors that are really",
        "start": 1959.59,
        "duration": 4.17
    },
    {
        "text": "informative are often not available in",
        "start": 1961.33,
        "duration": 5.31
    },
    {
        "text": "the blue bar but are available in the",
        "start": 1963.76,
        "duration": 6.419
    },
    {
        "text": "green bar so what we're currently",
        "start": 1966.64,
        "duration": 6.419
    },
    {
        "text": "working on now besides making the data",
        "start": 1970.179,
        "duration": 6.391
    },
    {
        "text": "making the model available and getting",
        "start": 1973.059,
        "duration": 6.151
    },
    {
        "text": "the paper published is to think of other",
        "start": 1976.57,
        "duration": 4.44
    },
    {
        "text": "ways in which this kind of latent",
        "start": 1979.21,
        "duration": 3.24
    },
    {
        "text": "representation of the genome can be",
        "start": 1981.01,
        "duration": 4.32
    },
    {
        "text": "useful and so if you have ideas for",
        "start": 1982.45,
        "duration": 4.68
    },
    {
        "text": "where this might fit in to your own",
        "start": 1985.33,
        "duration": 3.99
    },
    {
        "text": "research we'd be happy to hear it the",
        "start": 1987.13,
        "duration": 3.24
    },
    {
        "text": "other thing that we're doing is",
        "start": 1989.32,
        "duration": 2.67
    },
    {
        "text": "organizing a challenge which was",
        "start": 1990.37,
        "duration": 4.2
    },
    {
        "text": "announced last week so this is being",
        "start": 1991.99,
        "duration": 4.86
    },
    {
        "text": "organized by myself I'm Chokin dodgy",
        "start": 1994.57,
        "duration": 4.589
    },
    {
        "text": "Manolis Kellis and Zhi ping Leng as part",
        "start": 1996.85,
        "duration": 6.449
    },
    {
        "text": "of the encode so this is a we've put",
        "start": 1999.159,
        "duration": 7.77
    },
    {
        "text": "together sort of a data cube a tensor of",
        "start": 2003.299,
        "duration": 5.271
    },
    {
        "text": "data that's got a lot of missingness",
        "start": 2006.929,
        "duration": 4.471
    },
    {
        "text": "we've got some prize money for our",
        "start": 2008.57,
        "duration": 6.52
    },
    {
        "text": "winners and the challenge has two",
        "start": 2011.4,
        "duration": 5.79
    },
    {
        "text": "different phases January 28th May 14th",
        "start": 2015.09,
        "duration": 5.13
    },
    {
        "text": "and June 3rd August 14th with the idea",
        "start": 2017.19,
        "duration": 5.7
    },
    {
        "text": "that wants to get all the experiments",
        "start": 2020.22,
        "duration": 4.35
    },
    {
        "text": "done through great a paper about the",
        "start": 2022.89,
        "duration": 4.68
    },
    {
        "text": "result a key aspect of this is that we",
        "start": 2024.57,
        "duration": 5.58
    },
    {
        "text": "have truly prospective validation in the",
        "start": 2027.57,
        "duration": 5.01
    },
    {
        "text": "sense that data generation labs within",
        "start": 2030.15,
        "duration": 5.07
    },
    {
        "text": "encode have agreed to perform additional",
        "start": 2032.58,
        "duration": 5.04
    },
    {
        "text": "experiments which have not yet been done",
        "start": 2035.22,
        "duration": 4.29
    },
    {
        "text": "right so that when you when we get their",
        "start": 2037.62,
        "duration": 3.36
    },
    {
        "text": "predictions we'll be able to test them",
        "start": 2039.51,
        "duration": 3.81
    },
    {
        "text": "on test sets that really no one has ever",
        "start": 2040.98,
        "duration": 4.819
    },
    {
        "text": "seen before",
        "start": 2043.32,
        "duration": 2.479
    },
    {
        "text": "so that's the end of contact one am i",
        "start": 2046.149,
        "duration": 6.6
    },
    {
        "text": "doing pretty good alright so I want to",
        "start": 2048.069,
        "duration": 7.61
    },
    {
        "text": "briefly tell you about another project",
        "start": 2052.749,
        "duration": 5.191
    },
    {
        "text": "with someone who you probably know",
        "start": 2055.679,
        "duration": 4.39
    },
    {
        "text": "because he's sitting right there but I",
        "start": 2057.94,
        "duration": 3.359
    },
    {
        "text": "didn't want to give you tons of detail",
        "start": 2060.069,
        "duration": 2.73
    },
    {
        "text": "because he can tell you all about it but",
        "start": 2061.299,
        "duration": 3.931
    },
    {
        "text": "I thought does fit into the same kind of",
        "start": 2062.799,
        "duration": 5.28
    },
    {
        "text": "some of the same ideas come out of this",
        "start": 2065.23,
        "duration": 5.669
    },
    {
        "text": "so now we're talking about single cell",
        "start": 2068.079,
        "duration": 4.35
    },
    {
        "text": "hi-c data this is a project that",
        "start": 2070.899,
        "duration": 3.78
    },
    {
        "text": "basically gia did with some help from to",
        "start": 2072.429,
        "duration": 4.021
    },
    {
        "text": "June and gokon and we're helpful in in",
        "start": 2074.679,
        "duration": 5.39
    },
    {
        "text": "making sense of some of the data formats",
        "start": 2076.45,
        "duration": 7.199
    },
    {
        "text": "so having talked to a lot of you I know",
        "start": 2080.069,
        "duration": 5.71
    },
    {
        "text": "a lot of you already know all this but",
        "start": 2083.649,
        "duration": 3.36
    },
    {
        "text": "just to be sure we're all on the same",
        "start": 2085.779,
        "duration": 4.02
    },
    {
        "text": "page single cell hi-c is a sort of a",
        "start": 2087.009,
        "duration": 5.82
    },
    {
        "text": "single cell variant of the hi-c assay",
        "start": 2089.799,
        "duration": 4.47
    },
    {
        "text": "that measures the three-dimensional",
        "start": 2092.829,
        "duration": 3.361
    },
    {
        "text": "confirmation of DNA in the nucleus so",
        "start": 2094.269,
        "duration": 4.111
    },
    {
        "text": "the idea is you're gonna isolate a",
        "start": 2096.19,
        "duration": 4.919
    },
    {
        "text": "single cell you do this like ligation",
        "start": 2098.38,
        "duration": 4.8
    },
    {
        "text": "phil and restriction enzyme digestion",
        "start": 2101.109,
        "duration": 4.2
    },
    {
        "text": "and cross-linking and so on and what you",
        "start": 2103.18,
        "duration": 5.399
    },
    {
        "text": "get out is a very sparse matrix in which",
        "start": 2105.309,
        "duration": 6.24
    },
    {
        "text": "the rows and the columns correspond to",
        "start": 2108.579,
        "duration": 6.21
    },
    {
        "text": "positions along the genome and values in",
        "start": 2111.549,
        "duration": 6.45
    },
    {
        "text": "the matrix correspond to accounts of how",
        "start": 2114.789,
        "duration": 5.04
    },
    {
        "text": "frequently you observe contacts between",
        "start": 2117.999,
        "duration": 5.04
    },
    {
        "text": "those positions so in this one each of",
        "start": 2119.829,
        "duration": 5.19
    },
    {
        "text": "these boxes corresponds to one",
        "start": 2123.039,
        "duration": 3.96
    },
    {
        "text": "chromosome you have lots of intra",
        "start": 2125.019,
        "duration": 4.56
    },
    {
        "text": "chromosomal contacts and dark and then",
        "start": 2126.999,
        "duration": 4.83
    },
    {
        "text": "the inter chromosomal contacts are sort",
        "start": 2129.579,
        "duration": 4.23
    },
    {
        "text": "of on a lighter lighter shade here with",
        "start": 2131.829,
        "duration": 5.881
    },
    {
        "text": "fewer contacts one of the big questions",
        "start": 2133.809,
        "duration": 6.601
    },
    {
        "text": "is if you look at the RNA seek",
        "start": 2137.71,
        "duration": 4.889
    },
    {
        "text": "literature a lot of what people are",
        "start": 2140.41,
        "duration": 4.97
    },
    {
        "text": "interested in with single cell analysis",
        "start": 2142.599,
        "duration": 6.091
    },
    {
        "text": "is stuff like this this is the monocle",
        "start": 2145.38,
        "duration": 5.739
    },
    {
        "text": "software produced by cold trap now in",
        "start": 2148.69,
        "duration": 5.089
    },
    {
        "text": "our department and it's trying to",
        "start": 2151.119,
        "duration": 6.091
    },
    {
        "text": "reconstruct differentiating the",
        "start": 2153.779,
        "duration": 5.59
    },
    {
        "text": "trajectory of development in a",
        "start": 2157.21,
        "duration": 3.95
    },
    {
        "text": "population of differentiating cells",
        "start": 2159.369,
        "duration": 4.74
    },
    {
        "text": "automatically from the data so this is a",
        "start": 2161.16,
        "duration": 4.99
    },
    {
        "text": "projection down to 2d and then followed",
        "start": 2164.109,
        "duration": 4.2
    },
    {
        "text": "by this kind of tracing step there are",
        "start": 2166.15,
        "duration": 3.719
    },
    {
        "text": "other kinds of projection that you can",
        "start": 2168.309,
        "duration": 4.2
    },
    {
        "text": "do just doing things like T as an e it's",
        "start": 2169.869,
        "duration": 4.351
    },
    {
        "text": "just showing tissue type clustering in",
        "start": 2172.509,
        "duration": 4.8
    },
    {
        "text": "some single cell data as well and so the",
        "start": 2174.22,
        "duration": 4.799
    },
    {
        "text": "question is could we do a similar kind",
        "start": 2177.309,
        "duration": 2.641
    },
    {
        "text": "of thing",
        "start": 2179.019,
        "duration": 5.011
    },
    {
        "text": "a single-cell hi-c data so one of the",
        "start": 2179.95,
        "duration": 6.12
    },
    {
        "text": "things that gherkin helped with I",
        "start": 2184.03,
        "duration": 3.99
    },
    {
        "text": "mentioned gherkin was one of the people",
        "start": 2186.07,
        "duration": 4.11
    },
    {
        "text": "involved here he had been involved in a",
        "start": 2188.02,
        "duration": 4.62
    },
    {
        "text": "previous study run through encode in",
        "start": 2190.18,
        "duration": 5.13
    },
    {
        "text": "which our lab sort of coordinated for",
        "start": 2192.64,
        "duration": 4.32
    },
    {
        "text": "other labs who are developing ways of",
        "start": 2195.31,
        "duration": 4.53
    },
    {
        "text": "measuring the reproducibility of hi-c",
        "start": 2196.96,
        "duration": 5.58
    },
    {
        "text": "data so they were measurements for how",
        "start": 2199.84,
        "duration": 4.8
    },
    {
        "text": "similar is this hi-c data set to this",
        "start": 2202.54,
        "duration": 4.8
    },
    {
        "text": "other hi-c data set and so what we did",
        "start": 2204.64,
        "duration": 4.95
    },
    {
        "text": "was said well maybe those will work for",
        "start": 2207.34,
        "duration": 5.16
    },
    {
        "text": "single-cell as well so what Jia did was",
        "start": 2209.59,
        "duration": 5.22
    },
    {
        "text": "took one of those existing methods",
        "start": 2212.5,
        "duration": 5.7
    },
    {
        "text": "called hi-c rep and then combined it",
        "start": 2214.81,
        "duration": 4.95
    },
    {
        "text": "with a standard dimensionality reduction",
        "start": 2218.2,
        "duration": 3.98
    },
    {
        "text": "technique multi-dimensional scaling and",
        "start": 2219.76,
        "duration": 5.91
    },
    {
        "text": "took some data where this was published",
        "start": 2222.18,
        "duration": 6.22
    },
    {
        "text": "data in which we actually had labels or",
        "start": 2225.67,
        "duration": 5.07
    },
    {
        "text": "where the cells were in the cell cycle",
        "start": 2228.4,
        "duration": 5.67
    },
    {
        "text": "you can see g1 and cyan earliest mid s",
        "start": 2230.74,
        "duration": 6.24
    },
    {
        "text": "and late s and so on and this was in the",
        "start": 2234.07,
        "duration": 4.83
    },
    {
        "text": "sort of test set that we initially",
        "start": 2236.98,
        "duration": 3.9
    },
    {
        "text": "pulled out early evenly sampled from",
        "start": 2238.9,
        "duration": 4.29
    },
    {
        "text": "each of the cell cycle phases and here's",
        "start": 2240.88,
        "duration": 5.64
    },
    {
        "text": "from the phone data set and this might",
        "start": 2243.19,
        "duration": 5.67
    },
    {
        "text": "not seem so impressive to you except if",
        "start": 2246.52,
        "duration": 4.47
    },
    {
        "text": "you hadn't tried a bunch of other stuff",
        "start": 2248.86,
        "duration": 4.71
    },
    {
        "text": "the way we did and nothing else gave us",
        "start": 2250.99,
        "duration": 4.83
    },
    {
        "text": "much structure and then suddenly I see",
        "start": 2253.57,
        "duration": 4.68
    },
    {
        "text": "rep that's pretty convincingly cell",
        "start": 2255.82,
        "duration": 4.5
    },
    {
        "text": "cycle even if I didn't color it you",
        "start": 2258.25,
        "duration": 3.45
    },
    {
        "text": "could pretty much see that there's a",
        "start": 2260.32,
        "duration": 4.05
    },
    {
        "text": "circle going on there right so he",
        "start": 2261.7,
        "duration": 5.7
    },
    {
        "text": "accantus this was presented last year at",
        "start": 2264.37,
        "duration": 5.76
    },
    {
        "text": "the isnb but the point is this is a very",
        "start": 2267.4,
        "duration": 5.85
    },
    {
        "text": "nice way to take single-cell data and",
        "start": 2270.13,
        "duration": 5.49
    },
    {
        "text": "project it down into a lower dimensional",
        "start": 2273.25,
        "duration": 5.15
    },
    {
        "text": "space a latent space in which the",
        "start": 2275.62,
        "duration": 4.92
    },
    {
        "text": "relationships in this lower dimensional",
        "start": 2278.4,
        "duration": 4.48
    },
    {
        "text": "space and now clearly reflect at least",
        "start": 2280.54,
        "duration": 4.08
    },
    {
        "text": "for this data set the underlying biology",
        "start": 2282.88,
        "duration": 5.63
    },
    {
        "text": "of of cell cycle variation",
        "start": 2284.62,
        "duration": 3.89
    },
    {
        "text": "okay so now I'm going to change gears",
        "start": 2288.96,
        "duration": 6.25
    },
    {
        "text": "again and as promised in the title I'm",
        "start": 2291.79,
        "duration": 5.85
    },
    {
        "text": "changing from genomics to proteomics I",
        "start": 2295.21,
        "duration": 4.41
    },
    {
        "text": "know there are some people here who do",
        "start": 2297.64,
        "duration": 5.22
    },
    {
        "text": "proteomics that's that's nice I spend",
        "start": 2299.62,
        "duration": 5.66
    },
    {
        "text": "about half my time thinking about mass",
        "start": 2302.86,
        "duration": 6.39
    },
    {
        "text": "protein mass spectrometry and so I want",
        "start": 2305.28,
        "duration": 5.14
    },
    {
        "text": "to tell you about some of the work we've",
        "start": 2309.25,
        "duration": 3.03
    },
    {
        "text": "been doing on gleans",
        "start": 2310.42,
        "duration": 4.59
    },
    {
        "text": "so first just in case some of you don't",
        "start": 2312.28,
        "duration": 4.47
    },
    {
        "text": "do proteomics or haven't heard so much",
        "start": 2315.01,
        "duration": 3.9
    },
    {
        "text": "about it just to remind you that the key",
        "start": 2316.75,
        "duration": 4.98
    },
    {
        "text": "idea here is that you have some complex",
        "start": 2318.91,
        "duration": 5.64
    },
    {
        "text": "sample whatever the sample is it's got",
        "start": 2321.73,
        "duration": 5.01
    },
    {
        "text": "proteins in it and as you know proteins",
        "start": 2324.55,
        "duration": 4.2
    },
    {
        "text": "are complicated looking right they've",
        "start": 2326.74,
        "duration": 3.36
    },
    {
        "text": "got all these fancy structures",
        "start": 2328.75,
        "duration": 3.66
    },
    {
        "text": "associated with them and biochemically",
        "start": 2330.1,
        "duration": 4.17
    },
    {
        "text": "proteins are kind of hard to deal with",
        "start": 2332.41,
        "duration": 4.17
    },
    {
        "text": "especially in high throughput so the way",
        "start": 2334.27,
        "duration": 4.86
    },
    {
        "text": "that shotgun proteomics works is that",
        "start": 2336.58,
        "duration": 4.29
    },
    {
        "text": "you take these proteins and you first",
        "start": 2339.13,
        "duration": 4.47
    },
    {
        "text": "cut them up into pieces using something",
        "start": 2340.87,
        "duration": 5.82
    },
    {
        "text": "like trypsin and the trypsin gives you",
        "start": 2343.6,
        "duration": 6.57
    },
    {
        "text": "or somewhat deterministic cleavages of",
        "start": 2346.69,
        "duration": 6.12
    },
    {
        "text": "the proteins and these peptides are",
        "start": 2350.17,
        "duration": 4.17
    },
    {
        "text": "other things that are subject to mass",
        "start": 2352.81,
        "duration": 4.08
    },
    {
        "text": "spectrometry you put them through a",
        "start": 2354.34,
        "duration": 5.1
    },
    {
        "text": "device this is a picture of a device in",
        "start": 2356.89,
        "duration": 8.55
    },
    {
        "text": "our department and then out comes a",
        "start": 2359.44,
        "duration": 8.46
    },
    {
        "text": "bunch of observations they are spectra",
        "start": 2365.44,
        "duration": 5.52
    },
    {
        "text": "which really are just basically vectors",
        "start": 2367.9,
        "duration": 5.43
    },
    {
        "text": "again and I'll tell you a little bit",
        "start": 2370.96,
        "duration": 4.62
    },
    {
        "text": "more about the spectra in a bit actually",
        "start": 2373.33,
        "duration": 3.24
    },
    {
        "text": "I'm not going to tell you that much",
        "start": 2375.58,
        "duration": 3.0
    },
    {
        "text": "about them except to say each of these",
        "start": 2376.57,
        "duration": 5.91
    },
    {
        "text": "represents one canonically one peptide",
        "start": 2378.58,
        "duration": 7.38
    },
    {
        "text": "species and the height of the peaks",
        "start": 2382.48,
        "duration": 6.18
    },
    {
        "text": "corresponds to different fragmentation",
        "start": 2385.96,
        "duration": 5.46
    },
    {
        "text": "events along the peptide backbone but",
        "start": 2388.66,
        "duration": 4.11
    },
    {
        "text": "then you put it through some fancy",
        "start": 2391.42,
        "duration": 5.55
    },
    {
        "text": "computer and ideally outcomes and",
        "start": 2392.77,
        "duration": 6.54
    },
    {
        "text": "identification for each of the observed",
        "start": 2396.97,
        "duration": 4.41
    },
    {
        "text": "spectra you can say this is the peptide",
        "start": 2399.31,
        "duration": 3.9
    },
    {
        "text": "and I think was responsible for",
        "start": 2401.38,
        "duration": 5.28
    },
    {
        "text": "generating so the project I want to tell",
        "start": 2403.21,
        "duration": 6.51
    },
    {
        "text": "you about is gleans Lemes is a recursive",
        "start": 2406.66,
        "duration": 6.0
    },
    {
        "text": "acronym gleans is a learned embedding",
        "start": 2409.72,
        "duration": 5.01
    },
    {
        "text": "for annotating mass spectra that infant",
        "start": 2412.66,
        "duration": 4.95
    },
    {
        "text": "li and it was developed by Damon May a",
        "start": 2414.73,
        "duration": 6.03
    },
    {
        "text": "PhD student in my lab and also in",
        "start": 2417.61,
        "duration": 8.37
    },
    {
        "text": "collaboration with Jeff film so one",
        "start": 2420.76,
        "duration": 10.35
    },
    {
        "text": "motivation for this project is that most",
        "start": 2425.98,
        "duration": 7.98
    },
    {
        "text": "protein journals require that your data",
        "start": 2431.11,
        "duration": 5.64
    },
    {
        "text": "once you've finished it that you stick",
        "start": 2433.96,
        "duration": 4.95
    },
    {
        "text": "it into a public repository so some of",
        "start": 2436.75,
        "duration": 4.23
    },
    {
        "text": "the common repositories are things like",
        "start": 2438.91,
        "duration": 4.75
    },
    {
        "text": "massive chorus Clyde and so on",
        "start": 2440.98,
        "duration": 5.44
    },
    {
        "text": "but unfortunately or the field this",
        "start": 2443.66,
        "duration": 5.34
    },
    {
        "text": "repository submission is usually the end",
        "start": 2446.42,
        "duration": 5.19
    },
    {
        "text": "of the process so you go along you",
        "start": 2449.0,
        "duration": 4.95
    },
    {
        "text": "generate your data you analyze it you",
        "start": 2451.61,
        "duration": 4.5
    },
    {
        "text": "write your paper you deposit your data",
        "start": 2453.95,
        "duration": 5.7
    },
    {
        "text": "publish ended story and what would be",
        "start": 2456.11,
        "duration": 5.85
    },
    {
        "text": "nice is if we could move this up a",
        "start": 2459.65,
        "duration": 4.35
    },
    {
        "text": "little bit more genomics is it much more",
        "start": 2461.96,
        "duration": 4.4
    },
    {
        "text": "advanced in this respect than proteomics",
        "start": 2464.0,
        "duration": 5.22
    },
    {
        "text": "where sometimes you actually make use of",
        "start": 2466.36,
        "duration": 4.87
    },
    {
        "text": "public data to make sense of your own",
        "start": 2469.22,
        "duration": 4.8
    },
    {
        "text": "data and this happens some in proteomics",
        "start": 2471.23,
        "duration": 4.29
    },
    {
        "text": "but we think it could happen more and",
        "start": 2474.02,
        "duration": 2.82
    },
    {
        "text": "we're all working on ways to facilitate",
        "start": 2475.52,
        "duration": 4.8
    },
    {
        "text": "that so there have been some efforts to",
        "start": 2476.84,
        "duration": 6.6
    },
    {
        "text": "do this kind of thing so most of the",
        "start": 2480.32,
        "duration": 5.19
    },
    {
        "text": "repositories out there provide an",
        "start": 2483.44,
        "duration": 4.77
    },
    {
        "text": "annotated collections of spectra so",
        "start": 2485.51,
        "duration": 5.55
    },
    {
        "text": "those allow you to search against the",
        "start": 2488.21,
        "duration": 5.31
    },
    {
        "text": "search your data against those annotated",
        "start": 2491.06,
        "duration": 7.08
    },
    {
        "text": "spectra and so some examples of those",
        "start": 2493.52,
        "duration": 8.31
    },
    {
        "text": "are NIST massive and pride cluster in",
        "start": 2498.14,
        "duration": 5.07
    },
    {
        "text": "particular if you look at the pride",
        "start": 2501.83,
        "duration": 3.33
    },
    {
        "text": "repository this is one of the more",
        "start": 2503.21,
        "duration": 4.53
    },
    {
        "text": "widely used repositories they did a",
        "start": 2505.16,
        "duration": 5.67
    },
    {
        "text": "clustering of all pride spectra in 2015",
        "start": 2507.74,
        "duration": 4.53
    },
    {
        "text": "this is one of the few attempts to",
        "start": 2510.83,
        "duration": 3.3
    },
    {
        "text": "actually cluster all of the spectra as",
        "start": 2512.27,
        "duration": 3.75
    },
    {
        "text": "opposed to just clustering the spectra",
        "start": 2514.13,
        "duration": 4.5
    },
    {
        "text": "or which we know what the generating",
        "start": 2516.02,
        "duration": 6.45
    },
    {
        "text": "peptide was and the challenge here is",
        "start": 2518.63,
        "duration": 5.46
    },
    {
        "text": "that the clustering is pretty expensive",
        "start": 2522.47,
        "duration": 4.05
    },
    {
        "text": "and you have to sort of redo it if you",
        "start": 2524.09,
        "duration": 4.02
    },
    {
        "text": "want to start over again so we were",
        "start": 2526.52,
        "duration": 2.94
    },
    {
        "text": "trying to figure out a way that we could",
        "start": 2528.11,
        "duration": 3.51
    },
    {
        "text": "do something akin to clustering but that",
        "start": 2529.46,
        "duration": 5.64
    },
    {
        "text": "was a little more flexible and I'm not",
        "start": 2531.62,
        "duration": 5.52
    },
    {
        "text": "sure why but it certainly is the case",
        "start": 2535.1,
        "duration": 4.2
    },
    {
        "text": "that this was done in 2015 and there's",
        "start": 2537.14,
        "duration": 4.98
    },
    {
        "text": "no evidence been done again maybe maybe",
        "start": 2539.3,
        "duration": 4.5
    },
    {
        "text": "because it's a pretty expensive thing to",
        "start": 2542.12,
        "duration": 4.95
    },
    {
        "text": "do so our concept was maybe we could use",
        "start": 2543.8,
        "duration": 6.42
    },
    {
        "text": "the concept of latent embedding to embed",
        "start": 2547.07,
        "duration": 6.06
    },
    {
        "text": "all of our spectra into a learned space",
        "start": 2550.22,
        "duration": 4.89
    },
    {
        "text": "if we can use a machine learning",
        "start": 2553.13,
        "duration": 4.02
    },
    {
        "text": "approach to embed the spectra into a",
        "start": 2555.11,
        "duration": 4.38
    },
    {
        "text": "learned space then if you stick a new",
        "start": 2557.15,
        "duration": 5.64
    },
    {
        "text": "spectrum into that space you can just",
        "start": 2559.49,
        "duration": 6.15
    },
    {
        "text": "see who's close to you and if they have",
        "start": 2562.79,
        "duration": 4.62
    },
    {
        "text": "labels associated with them then you can",
        "start": 2565.64,
        "duration": 4.699
    },
    {
        "text": "adopt their label",
        "start": 2567.41,
        "duration": 6.99
    },
    {
        "text": "so the question is you know would this",
        "start": 2570.339,
        "duration": 7.27
    },
    {
        "text": "be helpful well ideally if we do this",
        "start": 2574.4,
        "duration": 5.099
    },
    {
        "text": "embedding approach sure it might take a",
        "start": 2577.609,
        "duration": 4.381
    },
    {
        "text": "while to train it but once it's trained",
        "start": 2579.499,
        "duration": 4.86
    },
    {
        "text": "it'll hopefully be super fast to just",
        "start": 2581.99,
        "duration": 4.2
    },
    {
        "text": "stick new spectra into the embedded",
        "start": 2584.359,
        "duration": 4.861
    },
    {
        "text": "space and so you don't have to relearn",
        "start": 2586.19,
        "duration": 5.49
    },
    {
        "text": "the embedding function all you have to",
        "start": 2589.22,
        "duration": 3.93
    },
    {
        "text": "do is click the spectrum in and then",
        "start": 2591.68,
        "duration": 5.28
    },
    {
        "text": "look and look and see who's closed so to",
        "start": 2593.15,
        "duration": 5.609
    },
    {
        "text": "do this we needed a way to get spectra",
        "start": 2596.96,
        "duration": 4.379
    },
    {
        "text": "into a machine learning system it's",
        "start": 2598.759,
        "duration": 4.83
    },
    {
        "text": "turned out to be kind of a pain because",
        "start": 2601.339,
        "duration": 4.561
    },
    {
        "text": "of the way spectra just the way they act",
        "start": 2603.589,
        "duration": 5.791
    },
    {
        "text": "they're there they're not easily",
        "start": 2605.9,
        "duration": 5.459
    },
    {
        "text": "summarized abaut as a simple vector",
        "start": 2609.38,
        "duration": 4.59
    },
    {
        "text": "they're really bags of Peaks you know",
        "start": 2611.359,
        "duration": 6.541
    },
    {
        "text": "that's where each peak is a is a mass to",
        "start": 2613.97,
        "duration": 6.059
    },
    {
        "text": "charge ratio and an intensity and the",
        "start": 2617.9,
        "duration": 3.48
    },
    {
        "text": "master charge ratios have to be",
        "start": 2620.029,
        "duration": 3.33
    },
    {
        "text": "represented quite precisely and there's",
        "start": 2621.38,
        "duration": 4.08
    },
    {
        "text": "also a precursor mass which is the mass",
        "start": 2623.359,
        "duration": 4.111
    },
    {
        "text": "of the intact peptide divided by the",
        "start": 2625.46,
        "duration": 4.74
    },
    {
        "text": "charge and that's also important",
        "start": 2627.47,
        "duration": 5.7
    },
    {
        "text": "actually very important so we spent a",
        "start": 2630.2,
        "duration": 4.53
    },
    {
        "text": "while and I'm not going to go through",
        "start": 2633.17,
        "duration": 4.589
    },
    {
        "text": "much detail thinking about different",
        "start": 2634.73,
        "duration": 5.67
    },
    {
        "text": "ways to represent spectra or input to a",
        "start": 2637.759,
        "duration": 4.02
    },
    {
        "text": "machine learning system when we came up",
        "start": 2640.4,
        "duration": 3.27
    },
    {
        "text": "with this well you have a couple of",
        "start": 2641.779,
        "duration": 3.48
    },
    {
        "text": "experimental parameters that are",
        "start": 2643.67,
        "duration": 4.349
    },
    {
        "text": "provided as input so the mass accuracy",
        "start": 2645.259,
        "duration": 5.82
    },
    {
        "text": "and the fragment mass accuracy of the of",
        "start": 2648.019,
        "duration": 5.461
    },
    {
        "text": "the device that generated the spectrum",
        "start": 2651.079,
        "duration": 5.46
    },
    {
        "text": "you have peptide information about the",
        "start": 2653.48,
        "duration": 4.529
    },
    {
        "text": "intact peptide and then you have",
        "start": 2656.539,
        "duration": 3.57
    },
    {
        "text": "fragment information about what the",
        "start": 2658.009,
        "duration": 3.99
    },
    {
        "text": "locations are of all the peaks in the",
        "start": 2660.109,
        "duration": 4.5
    },
    {
        "text": "spectrum and so these three pieces of",
        "start": 2661.999,
        "duration": 6.211
    },
    {
        "text": "information oh and I should mention we",
        "start": 2664.609,
        "duration": 6.301
    },
    {
        "text": "also use similarity to a collection of",
        "start": 2668.21,
        "duration": 5.309
    },
    {
        "text": "reference spectra where we basically use",
        "start": 2670.91,
        "duration": 4.619
    },
    {
        "text": "this the pattern of similarity as",
        "start": 2673.519,
        "duration": 3.99
    },
    {
        "text": "another set of features to represent the",
        "start": 2675.529,
        "duration": 5.431
    },
    {
        "text": "spectrum and then those three sets of",
        "start": 2677.509,
        "duration": 6.48
    },
    {
        "text": "features are each processed in a deep",
        "start": 2680.96,
        "duration": 5.25
    },
    {
        "text": "neural network where they they each get",
        "start": 2683.989,
        "duration": 4.02
    },
    {
        "text": "processed separately through some layers",
        "start": 2686.21,
        "duration": 5.159
    },
    {
        "text": "and then jointly thereafter so many of",
        "start": 2688.009,
        "duration": 4.921
    },
    {
        "text": "you probably familiar with these kind of",
        "start": 2691.369,
        "duration": 4.591
    },
    {
        "text": "terms from deep neural networks but just",
        "start": 2692.93,
        "duration": 4.5
    },
    {
        "text": "in case you're not you have some",
        "start": 2695.96,
        "duration": 3.629
    },
    {
        "text": "convolutional layers which are designed",
        "start": 2697.43,
        "duration": 5.61
    },
    {
        "text": "to sort of scan across a region of data",
        "start": 2699.589,
        "duration": 5.731
    },
    {
        "text": "a data object and find local patterns",
        "start": 2703.04,
        "duration": 4.29
    },
    {
        "text": "therein and then you have fully",
        "start": 2705.32,
        "duration": 4.35
    },
    {
        "text": "connected layers that take a bunch of",
        "start": 2707.33,
        "duration": 4.62
    },
    {
        "text": "information and consider it jointly and",
        "start": 2709.67,
        "duration": 5.25
    },
    {
        "text": "so this is a picture we have 500",
        "start": 2711.95,
        "duration": 5.04
    },
    {
        "text": "reference spectrum similarities we have",
        "start": 2714.92,
        "duration": 4.23
    },
    {
        "text": "the bin fragment intensities and then we",
        "start": 2716.99,
        "duration": 5.85
    },
    {
        "text": "have the precursor feature we handle",
        "start": 2719.15,
        "duration": 6.12
    },
    {
        "text": "these these convolutional e the the",
        "start": 2722.84,
        "duration": 4.83
    },
    {
        "text": "first two sets convolutional e and then",
        "start": 2725.27,
        "duration": 4.32
    },
    {
        "text": "only connected for the precursor",
        "start": 2727.67,
        "duration": 5.37
    },
    {
        "text": "features partly because there's no real",
        "start": 2729.59,
        "duration": 5.279
    },
    {
        "text": "ordering of the precursor features so a",
        "start": 2733.04,
        "duration": 3.3
    },
    {
        "text": "convolution wouldn't really make sense",
        "start": 2734.869,
        "duration": 4.531
    },
    {
        "text": "and then we combine all of these through",
        "start": 2736.34,
        "duration": 5.46
    },
    {
        "text": "another their concatenated here and then",
        "start": 2739.4,
        "duration": 5.219
    },
    {
        "text": "combine down here again there's hyper",
        "start": 2741.8,
        "duration": 4.86
    },
    {
        "text": "parameters involved though for example",
        "start": 2744.619,
        "duration": 4.291
    },
    {
        "text": "there's a 30 to 32 dimensions is the",
        "start": 2746.66,
        "duration": 3.78
    },
    {
        "text": "number of dimensions we chose for the",
        "start": 2748.91,
        "duration": 4.199
    },
    {
        "text": "final embedding there's also sizes for",
        "start": 2750.44,
        "duration": 5.34
    },
    {
        "text": "these internal layers and so on and in",
        "start": 2753.109,
        "duration": 4.26
    },
    {
        "text": "the paper we have some description of",
        "start": 2755.78,
        "duration": 3.68
    },
    {
        "text": "how we chose all those hyper parameters",
        "start": 2757.369,
        "duration": 5.091
    },
    {
        "text": "yeah",
        "start": 2759.46,
        "duration": 3.0
    },
    {
        "text": "so you're talking about whether your",
        "start": 2777.62,
        "duration": 4.02
    },
    {
        "text": "pre-training each of these pieces and",
        "start": 2779.84,
        "duration": 3.63
    },
    {
        "text": "then what are you talking about how many",
        "start": 2781.64,
        "duration": 9.48
    },
    {
        "text": "layers to put in yeah so it's always a",
        "start": 2783.47,
        "duration": 10.47
    },
    {
        "text": "question this is a common yes so the",
        "start": 2791.12,
        "duration": 4.89
    },
    {
        "text": "question is how do there's a lot more",
        "start": 2793.94,
        "duration": 3.78
    },
    {
        "text": "parameters than the ones I just alluded",
        "start": 2796.01,
        "duration": 3.99
    },
    {
        "text": "to some of them are implicit like how",
        "start": 2797.72,
        "duration": 4.56
    },
    {
        "text": "many layers should I put in and how many",
        "start": 2800.0,
        "duration": 5.52
    },
    {
        "text": "nodes in each layer and then also should",
        "start": 2802.28,
        "duration": 5.34
    },
    {
        "text": "I do the training piecewise I've trained",
        "start": 2805.52,
        "duration": 4.02
    },
    {
        "text": "part of this first and what type of",
        "start": 2807.62,
        "duration": 6.3
    },
    {
        "text": "parameters should I use for the learning",
        "start": 2809.54,
        "duration": 8.22
    },
    {
        "text": "procedure when when it's hyper",
        "start": 2813.92,
        "duration": 5.75
    },
    {
        "text": "parameters of the learning itself I",
        "start": 2817.76,
        "duration": 5.16
    },
    {
        "text": "would I wouldn't say we cover them",
        "start": 2819.67,
        "duration": 5.77
    },
    {
        "text": "exhaustively we try to do a give a",
        "start": 2822.92,
        "duration": 4.26
    },
    {
        "text": "discussion of that but it doesn't do",
        "start": 2825.44,
        "duration": 3.48
    },
    {
        "text": "justice to everything Damon did of",
        "start": 2827.18,
        "duration": 4.74
    },
    {
        "text": "trying things out ahead of time we the",
        "start": 2828.92,
        "duration": 4.5
    },
    {
        "text": "nice thing is that we did all of this",
        "start": 2831.92,
        "duration": 3.24
    },
    {
        "text": "development and an independent set of",
        "start": 2833.42,
        "duration": 4.44
    },
    {
        "text": "data from what we'll report here so at",
        "start": 2835.16,
        "duration": 4.64
    },
    {
        "text": "least we don't have to worry about the",
        "start": 2837.86,
        "duration": 4.68
    },
    {
        "text": "the results that we report being over",
        "start": 2839.8,
        "duration": 5.5
    },
    {
        "text": "fit but this is a common challenge in",
        "start": 2842.54,
        "duration": 5.04
    },
    {
        "text": "the literature is that it's it's typical",
        "start": 2845.3,
        "duration": 5.04
    },
    {
        "text": "for deep learning development it",
        "start": 2847.58,
        "duration": 5.49
    },
    {
        "text": "contained many many iterations of trying",
        "start": 2850.34,
        "duration": 4.95
    },
    {
        "text": "things out and to my frustration as a",
        "start": 2853.07,
        "duration": 4.14
    },
    {
        "text": "leader often none of those details are",
        "start": 2855.29,
        "duration": 4.44
    },
    {
        "text": "given I think we did a reasonable job",
        "start": 2857.21,
        "duration": 4.74
    },
    {
        "text": "but there may be probably more detail we",
        "start": 2859.73,
        "duration": 4.68
    },
    {
        "text": "could have given as reviewers when we",
        "start": 2861.95,
        "duration": 11.57
    },
    {
        "text": "put up with so much yeah so we are",
        "start": 2864.41,
        "duration": 10.1
    },
    {
        "text": "let me just show you the next slide",
        "start": 2873.52,
        "duration": 2.58
    },
    {
        "text": "because I think that Atlantis so this is",
        "start": 2874.51,
        "duration": 3.63
    },
    {
        "text": "what the actual network looks like you",
        "start": 2876.1,
        "duration": 4.59
    },
    {
        "text": "take the previous picture and you stick",
        "start": 2878.14,
        "duration": 5.04
    },
    {
        "text": "it in here there's actually two copies",
        "start": 2880.69,
        "duration": 5.84
    },
    {
        "text": "of it so this is called a Siamese",
        "start": 2883.18,
        "duration": 3.35
    },
    {
        "text": "configuration so there's two copies but",
        "start": 2886.56,
        "duration": 5.47
    },
    {
        "text": "they have the same parameters this is",
        "start": 2890.74,
        "duration": 3.51
    },
    {
        "text": "network this network in this network had",
        "start": 2892.03,
        "duration": 4.47
    },
    {
        "text": "tied parameters all those convolutions",
        "start": 2894.25,
        "duration": 3.15
    },
    {
        "text": "everything I showed you in the previous",
        "start": 2896.5,
        "duration": 4.35
    },
    {
        "text": "slide sit here they output 232",
        "start": 2897.4,
        "duration": 5.61
    },
    {
        "text": "dimensional vectors which we then",
        "start": 2900.85,
        "duration": 3.72
    },
    {
        "text": "compute the Euclidean distance between",
        "start": 2903.01,
        "duration": 4.7
    },
    {
        "text": "and we do this kind of contrastive law",
        "start": 2904.57,
        "duration": 6.63
    },
    {
        "text": "so mathematically what you're seeing",
        "start": 2907.71,
        "duration": 6.76
    },
    {
        "text": "here is this is the label on a pair of a",
        "start": 2911.2,
        "duration": 7.23
    },
    {
        "text": "pair of spectra plus one means they were",
        "start": 2914.47,
        "duration": 6.09
    },
    {
        "text": "generated by the same peptide minus one",
        "start": 2918.43,
        "duration": 3.24
    },
    {
        "text": "means they were generated by different",
        "start": 2920.56,
        "duration": 4.41
    },
    {
        "text": "peptides this is the distance in the",
        "start": 2921.67,
        "duration": 5.01
    },
    {
        "text": "embedded space that came out of these",
        "start": 2924.97,
        "duration": 5.37
    },
    {
        "text": "two two networks and then this is some",
        "start": 2926.68,
        "duration": 6.66
    },
    {
        "text": "pre-specified margin and so the concept",
        "start": 2930.34,
        "duration": 5.61
    },
    {
        "text": "is that if these two peptides have the",
        "start": 2933.34,
        "duration": 4.98
    },
    {
        "text": "same label and they're farther apart",
        "start": 2935.95,
        "duration": 4.98
    },
    {
        "text": "than your pre-specified margin then this",
        "start": 2938.32,
        "duration": 4.89
    },
    {
        "text": "will try to push them together and if",
        "start": 2940.93,
        "duration": 3.39
    },
    {
        "text": "they have different labels and they're",
        "start": 2943.21,
        "duration": 2.61
    },
    {
        "text": "closer than that arjun it's going to try",
        "start": 2944.32,
        "duration": 3.84
    },
    {
        "text": "to push them apart and it just",
        "start": 2945.82,
        "duration": 3.87
    },
    {
        "text": "iteratively does that over and over",
        "start": 2948.16,
        "duration": 3.81
    },
    {
        "text": "again this is not something that we",
        "start": 2949.69,
        "duration": 4.29
    },
    {
        "text": "develop this is I should have a citation",
        "start": 2951.97,
        "duration": 4.91
    },
    {
        "text": "you know a decade ago it's very",
        "start": 2953.98,
        "duration": 4.98
    },
    {
        "text": "relatively commonly used kind of",
        "start": 2956.88,
        "duration": 5.53
    },
    {
        "text": "architecture okay so the first thing",
        "start": 2958.96,
        "duration": 5.55
    },
    {
        "text": "that comes out is an embedding it looks",
        "start": 2962.41,
        "duration": 4.41
    },
    {
        "text": "like this if you take into 32 dimensions",
        "start": 2964.51,
        "duration": 4.68
    },
    {
        "text": "and you project to two dimensions with",
        "start": 2966.82,
        "duration": 6.81
    },
    {
        "text": "t-sne so this is 70,000 embedded spectra",
        "start": 2969.19,
        "duration": 6.96
    },
    {
        "text": "were colored by precursor mass-to-charge",
        "start": 2973.63,
        "duration": 4.83
    },
    {
        "text": "ratio you can see that there is some",
        "start": 2976.15,
        "duration": 6.0
    },
    {
        "text": "structure there if you instead color by",
        "start": 2978.46,
        "duration": 5.76
    },
    {
        "text": "charge state you can see it slightly",
        "start": 2982.15,
        "duration": 4.59
    },
    {
        "text": "different structure question is is this",
        "start": 2984.22,
        "duration": 5.37
    },
    {
        "text": "useful so one thing that you can do is",
        "start": 2986.74,
        "duration": 5.73
    },
    {
        "text": "zoom in a little bit more closely so",
        "start": 2989.59,
        "duration": 8.7
    },
    {
        "text": "what we did was we randomly chose some",
        "start": 2992.47,
        "duration": 8.16
    },
    {
        "text": "of these globs so each of these things",
        "start": 2998.29,
        "duration": 4.71
    },
    {
        "text": "is a little glob we randomly chose some",
        "start": 3000.63,
        "duration": 4.02
    },
    {
        "text": "of those blobs and then looked to see",
        "start": 3003.0,
        "duration": 4.77
    },
    {
        "text": "what was in there or actually no it's",
        "start": 3004.65,
        "duration": 4.32
    },
    {
        "text": "sorry we did this the other way around",
        "start": 3007.77,
        "duration": 3.71
    },
    {
        "text": "we",
        "start": 3008.97,
        "duration": 5.27
    },
    {
        "text": "chose all of the spectra in a randomly",
        "start": 3011.48,
        "duration": 5.94
    },
    {
        "text": "chosen charged state and about a1 Dalton",
        "start": 3014.24,
        "duration": 6.569
    },
    {
        "text": "precursor mass bin and then we colored",
        "start": 3017.42,
        "duration": 4.98
    },
    {
        "text": "them one color and then we randomly",
        "start": 3020.809,
        "duration": 3.54
    },
    {
        "text": "chose another charge state and one",
        "start": 3022.4,
        "duration": 4.56
    },
    {
        "text": "Dalton Aspen and did another color we",
        "start": 3024.349,
        "duration": 3.75
    },
    {
        "text": "didn't do too many colors because you",
        "start": 3026.96,
        "duration": 3.089
    },
    {
        "text": "can't see that many colors but we chose",
        "start": 3028.099,
        "duration": 3.681
    },
    {
        "text": "for this one",
        "start": 3030.049,
        "duration": 4.231
    },
    {
        "text": "226 spectra in total and there's about",
        "start": 3031.78,
        "duration": 4.329
    },
    {
        "text": "what is it six or seven different colors",
        "start": 3034.28,
        "duration": 3.87
    },
    {
        "text": "and what you can see is that this seems",
        "start": 3036.109,
        "duration": 4.771
    },
    {
        "text": "to be just the main features the charge",
        "start": 3038.15,
        "duration": 5.459
    },
    {
        "text": "state and and the masked seem to",
        "start": 3040.88,
        "duration": 5.699
    },
    {
        "text": "relatively place these things on the",
        "start": 3043.609,
        "duration": 5.25
    },
    {
        "text": "plot so now let's zoom in on this",
        "start": 3046.579,
        "duration": 4.831
    },
    {
        "text": "particular one the one that's here if we",
        "start": 3048.859,
        "duration": 4.921
    },
    {
        "text": "zoom in on it it actually opens up you",
        "start": 3051.41,
        "duration": 4.02
    },
    {
        "text": "can't really see it I was gonna try to",
        "start": 3053.78,
        "duration": 3.299
    },
    {
        "text": "get Damon to send me it better there's",
        "start": 3055.43,
        "duration": 3.33
    },
    {
        "text": "some dots here which are hard to see and",
        "start": 3057.079,
        "duration": 5.03
    },
    {
        "text": "then these ones over here as well so",
        "start": 3058.76,
        "duration": 5.91
    },
    {
        "text": "what's going on here well if we color",
        "start": 3062.109,
        "duration": 4.421
    },
    {
        "text": "those dots according to what peptide",
        "start": 3064.67,
        "duration": 3.96
    },
    {
        "text": "generated them you can see that all of",
        "start": 3066.53,
        "duration": 4.019
    },
    {
        "text": "these were generated by the same peptide",
        "start": 3068.63,
        "duration": 3.689
    },
    {
        "text": "these are a mixture of different",
        "start": 3070.549,
        "duration": 3.931
    },
    {
        "text": "peptides and a mixture but you can see",
        "start": 3072.319,
        "duration": 3.961
    },
    {
        "text": "this some of the same colors show up",
        "start": 3074.48,
        "duration": 5.339
    },
    {
        "text": "together but then the question is what's",
        "start": 3076.28,
        "duration": 6.21
    },
    {
        "text": "going on or what do we do with this so",
        "start": 3079.819,
        "duration": 3.381
    },
    {
        "text": "there's a number of different",
        "start": 3082.49,
        "duration": 3.51
    },
    {
        "text": "possibilities we the first thing we",
        "start": 3083.2,
        "duration": 4.72
    },
    {
        "text": "considered was just trying to do some of",
        "start": 3086.0,
        "duration": 3.72
    },
    {
        "text": "that kind of grouping that I described",
        "start": 3087.92,
        "duration": 4.1
    },
    {
        "text": "previously like can we go find",
        "start": 3089.72,
        "duration": 5.25
    },
    {
        "text": "collections of nearby spectra that all",
        "start": 3092.02,
        "duration": 5.65
    },
    {
        "text": "belong together we considered it a",
        "start": 3094.97,
        "duration": 4.29
    },
    {
        "text": "couple of different ways to do this and",
        "start": 3097.67,
        "duration": 4.5
    },
    {
        "text": "ended up with a relatively simple greedy",
        "start": 3099.26,
        "duration": 6.569
    },
    {
        "text": "algorithm so what we do is we find the",
        "start": 3102.17,
        "duration": 6.389
    },
    {
        "text": "1,000 nearest neighbors of each spectrum",
        "start": 3105.829,
        "duration": 4.351
    },
    {
        "text": "so it turns out this you can do quite",
        "start": 3108.559,
        "duration": 3.81
    },
    {
        "text": "fast we use a library from Facebook",
        "start": 3110.18,
        "duration": 5.81
    },
    {
        "text": "called base which is spelled F a is s",
        "start": 3112.369,
        "duration": 6.391
    },
    {
        "text": "that allows you to rapidly you can take",
        "start": 3115.99,
        "duration": 4.78
    },
    {
        "text": "these 32 element vectors and take tons",
        "start": 3118.76,
        "duration": 3.87
    },
    {
        "text": "of them and find your thousand nearest",
        "start": 3120.77,
        "duration": 4.95
    },
    {
        "text": "neighbors quite quickly then you sort",
        "start": 3122.63,
        "duration": 6.239
    },
    {
        "text": "all the spectra and you sort them first",
        "start": 3125.72,
        "duration": 6.42
    },
    {
        "text": "by the neighbor count within some",
        "start": 3128.869,
        "duration": 6.631
    },
    {
        "text": "pre-specified distance range tau and",
        "start": 3132.14,
        "duration": 5.82
    },
    {
        "text": "then by mean distance to their neighbors",
        "start": 3135.5,
        "duration": 3.56
    },
    {
        "text": "and",
        "start": 3137.96,
        "duration": 4.37
    },
    {
        "text": "for each spectrum in order if it's not",
        "start": 3139.06,
        "duration": 5.61
    },
    {
        "text": "already a spoke you call it a spoke and",
        "start": 3142.33,
        "duration": 4.56
    },
    {
        "text": "you and you connect it to all of the",
        "start": 3144.67,
        "duration": 5.46
    },
    {
        "text": "hubs around it so here for example this",
        "start": 3146.89,
        "duration": 6.33
    },
    {
        "text": "guy got connected to a bunch of peptide",
        "start": 3150.13,
        "duration": 5.25
    },
    {
        "text": "spectra that we're near it then you go",
        "start": 3153.22,
        "duration": 4.47
    },
    {
        "text": "and grab the next cluster and then the",
        "start": 3155.38,
        "duration": 6.93
    },
    {
        "text": "next cluster and so on and you continue",
        "start": 3157.69,
        "duration": 6.51
    },
    {
        "text": "doing that and then at the very end",
        "start": 3162.31,
        "duration": 4.38
    },
    {
        "text": "sometimes you have to do some combining",
        "start": 3164.2,
        "duration": 4.35
    },
    {
        "text": "of adjacent hub-and-spoke communities",
        "start": 3166.69,
        "duration": 3.81
    },
    {
        "text": "if the hubs are too close together and",
        "start": 3168.55,
        "duration": 3.96
    },
    {
        "text": "that's only because the first step was",
        "start": 3170.5,
        "duration": 3.96
    },
    {
        "text": "an approximation you just you got the",
        "start": 3172.51,
        "duration": 3.75
    },
    {
        "text": "thousand nearest neighbors and maybe",
        "start": 3174.46,
        "duration": 4.11
    },
    {
        "text": "there's more than that within a distance",
        "start": 3176.26,
        "duration": 4.5
    },
    {
        "text": "of town that's usually a very small",
        "start": 3178.57,
        "duration": 6.18
    },
    {
        "text": "number so then we wanted to ask how well",
        "start": 3180.76,
        "duration": 7.23
    },
    {
        "text": "does this kind of community generation",
        "start": 3184.75,
        "duration": 7.17
    },
    {
        "text": "process work so we went in to test data",
        "start": 3187.99,
        "duration": 6.36
    },
    {
        "text": "or we hadn't used this during training",
        "start": 3191.92,
        "duration": 4.95
    },
    {
        "text": "and we asked how well does it do at",
        "start": 3194.35,
        "duration": 4.98
    },
    {
        "text": "making pitayas that are sort of pure",
        "start": 3196.87,
        "duration": 6.24
    },
    {
        "text": "they have one peptide in them and and",
        "start": 3199.33,
        "duration": 6.48
    },
    {
        "text": "not too many not too many communities",
        "start": 3203.11,
        "duration": 5.28
    },
    {
        "text": "with multiple peptides in them and so",
        "start": 3205.81,
        "duration": 3.78
    },
    {
        "text": "you'd like to be in the upper left",
        "start": 3208.39,
        "duration": 4.44
    },
    {
        "text": "corner of this kind of law if we you",
        "start": 3209.59,
        "duration": 5.07
    },
    {
        "text": "started by just doing k-means clustering",
        "start": 3212.83,
        "duration": 3.75
    },
    {
        "text": "which is seems like a reasonable thing",
        "start": 3214.66,
        "duration": 4.29
    },
    {
        "text": "to try right lots lots of people use",
        "start": 3216.58,
        "duration": 3.75
    },
    {
        "text": "k-means for lots of things",
        "start": 3218.95,
        "duration": 4.56
    },
    {
        "text": "turns out this is really slow there",
        "start": 3220.33,
        "duration": 4.83
    },
    {
        "text": "probably are some speed ups that we",
        "start": 3223.51,
        "duration": 3.12
    },
    {
        "text": "could make it go faster but it's not",
        "start": 3225.16,
        "duration": 3.05
    },
    {
        "text": "going to go as fast as the simple",
        "start": 3226.63,
        "duration": 3.41
    },
    {
        "text": "nearest neighbor thing that we",
        "start": 3228.21,
        "duration": 5.38
    },
    {
        "text": "implemented but more importantly it",
        "start": 3230.04,
        "duration": 5.89
    },
    {
        "text": "makes too many multi peptide communities",
        "start": 3233.59,
        "duration": 4.65
    },
    {
        "text": "if you think about it k nearest neighbor",
        "start": 3235.93,
        "duration": 4.74
    },
    {
        "text": "or most clustering algorithm really are",
        "start": 3238.24,
        "duration": 5.1
    },
    {
        "text": "designed to handle to try to cluster",
        "start": 3240.67,
        "duration": 5.43
    },
    {
        "text": "things a much larger scale than what",
        "start": 3243.34,
        "duration": 4.62
    },
    {
        "text": "we're interested in we want to find very",
        "start": 3246.1,
        "duration": 5.75
    },
    {
        "text": "dense small clusters and so this",
        "start": 3247.96,
        "duration": 6.06
    },
    {
        "text": "basically puts you way over here",
        "start": 3251.85,
        "duration": 5.8
    },
    {
        "text": "depending on what value of k whereas",
        "start": 3254.02,
        "duration": 5.46
    },
    {
        "text": "this hub-and-spoke method that we",
        "start": 3257.65,
        "duration": 3.78
    },
    {
        "text": "developed looks much better these are",
        "start": 3259.48,
        "duration": 4.59
    },
    {
        "text": "just for different values of tau tau",
        "start": 3261.43,
        "duration": 6.42
    },
    {
        "text": "equals point one nine five in that space",
        "start": 3264.07,
        "duration": 7.49
    },
    {
        "text": "that we defined using the neural network",
        "start": 3267.85,
        "duration": 6.02
    },
    {
        "text": "we chose the tally equals point or nine",
        "start": 3271.56,
        "duration": 4.65
    },
    {
        "text": "five because that means that about 1% of",
        "start": 3273.87,
        "duration": 4.35
    },
    {
        "text": "the communities that we came up with our",
        "start": 3276.21,
        "duration": 5.25
    },
    {
        "text": "multi peptide community so the nice",
        "start": 3278.22,
        "duration": 5.54
    },
    {
        "text": "thing is that this is not that expensive",
        "start": 3281.46,
        "duration": 5.88
    },
    {
        "text": "embedding a 5.6 million spectra takes",
        "start": 3283.76,
        "duration": 7.69
    },
    {
        "text": "about 20 minutes and then the thousand",
        "start": 3287.34,
        "duration": 5.64
    },
    {
        "text": "nearest neighbors search takes about",
        "start": 3291.45,
        "duration": 3.42
    },
    {
        "text": "seven and a half hours for all of those",
        "start": 3292.98,
        "duration": 4.59
    },
    {
        "text": "5.6 million spectra and then the actual",
        "start": 3294.87,
        "duration": 4.32
    },
    {
        "text": "Huggins Bach method takes just a few",
        "start": 3297.57,
        "duration": 5.04
    },
    {
        "text": "minutes after that so the question is",
        "start": 3299.19,
        "duration": 5.73
    },
    {
        "text": "does this help us so what we would like",
        "start": 3302.61,
        "duration": 4.62
    },
    {
        "text": "to do ideally is say okay we've embedded",
        "start": 3304.92,
        "duration": 4.53
    },
    {
        "text": "these spectra some of them have labels",
        "start": 3307.23,
        "duration": 5.01
    },
    {
        "text": "can we now propagate additional labels",
        "start": 3309.45,
        "duration": 4.95
    },
    {
        "text": "on top of the ones that we that we had",
        "start": 3312.24,
        "duration": 5.1
    },
    {
        "text": "initially I'm giving away the answer",
        "start": 3314.4,
        "duration": 6.75
    },
    {
        "text": "here but we get of the 1.6 million in",
        "start": 3317.34,
        "duration": 6.03
    },
    {
        "text": "our data set of 5.6 million that were",
        "start": 3321.15,
        "duration": 3.96
    },
    {
        "text": "identified we can get an additional",
        "start": 3323.37,
        "duration": 4.02
    },
    {
        "text": "about 8% and I'll show you how that",
        "start": 3325.11,
        "duration": 5.43
    },
    {
        "text": "happens so the most obvious thing to do",
        "start": 3327.39,
        "duration": 6.27
    },
    {
        "text": "is you say well I look at my communities",
        "start": 3330.54,
        "duration": 5.58
    },
    {
        "text": "and if this community contains only",
        "start": 3333.66,
        "duration": 5.73
    },
    {
        "text": "peptides with a given only spectra with",
        "start": 3336.12,
        "duration": 5.88
    },
    {
        "text": "a given peptide label or unlabeled",
        "start": 3339.39,
        "duration": 5.88
    },
    {
        "text": "spectra then I can just assign the label",
        "start": 3342.0,
        "duration": 5.31
    },
    {
        "text": "to the additional peptide in that",
        "start": 3345.27,
        "duration": 3.78
    },
    {
        "text": "community and that immediately gives us",
        "start": 3347.31,
        "duration": 4.28
    },
    {
        "text": "about 40,000 additional spectra the",
        "start": 3349.05,
        "duration": 6.18
    },
    {
        "text": "other thing that we can do is find sort",
        "start": 3351.59,
        "duration": 5.62
    },
    {
        "text": "of try to characterize what is often",
        "start": 3355.23,
        "duration": 3.75
    },
    {
        "text": "called the dark matter of proteomics",
        "start": 3357.21,
        "duration": 4.74
    },
    {
        "text": "which is all of the spectra that you can",
        "start": 3358.98,
        "duration": 4.8
    },
    {
        "text": "identify successfully when you do a",
        "start": 3361.95,
        "duration": 3.63
    },
    {
        "text": "standard database search to try to",
        "start": 3363.78,
        "duration": 6.66
    },
    {
        "text": "identify the things that are the problem",
        "start": 3365.58,
        "duration": 6.75
    },
    {
        "text": "in general with this dark matter is that",
        "start": 3370.44,
        "duration": 4.77
    },
    {
        "text": "a lot of it is non peptide junk right",
        "start": 3372.33,
        "duration": 5.28
    },
    {
        "text": "that they're they're noise or other",
        "start": 3375.21,
        "duration": 4.83
    },
    {
        "text": "kinds of contaminants and so the idea",
        "start": 3377.61,
        "duration": 5.01
    },
    {
        "text": "here is that if we can embed this",
        "start": 3380.04,
        "duration": 5.82
    },
    {
        "text": "learned peptide and better and a lot of",
        "start": 3382.62,
        "duration": 5.22
    },
    {
        "text": "different spectra all show up near each",
        "start": 3385.86,
        "duration": 4.79
    },
    {
        "text": "other and maybe that will help us to",
        "start": 3387.84,
        "duration": 5.43
    },
    {
        "text": "focus in on the ones that are likely to",
        "start": 3390.65,
        "duration": 5.59
    },
    {
        "text": "be less junkie right more more likely to",
        "start": 3393.27,
        "duration": 5.46
    },
    {
        "text": "be real and so what we were able to do",
        "start": 3396.24,
        "duration": 5.31
    },
    {
        "text": "is then focus on the the communities",
        "start": 3398.73,
        "duration": 4.91
    },
    {
        "text": "that don't have any label",
        "start": 3401.55,
        "duration": 4.85
    },
    {
        "text": "and doing what's called a tight target",
        "start": 3403.64,
        "duration": 7.11
    },
    {
        "text": "decoy search so because we the because",
        "start": 3406.4,
        "duration": 8.82
    },
    {
        "text": "of the way the spectra work we can we",
        "start": 3410.75,
        "duration": 6.84
    },
    {
        "text": "can define a much more fine-grained",
        "start": 3415.22,
        "duration": 5.37
    },
    {
        "text": "mass-to-charge ratio which reduces the",
        "start": 3417.59,
        "duration": 4.86
    },
    {
        "text": "number of candidates you have to search",
        "start": 3420.59,
        "duration": 4.02
    },
    {
        "text": "when you when you do the database search",
        "start": 3422.45,
        "duration": 4.92
    },
    {
        "text": "and so this allows you to essentially",
        "start": 3424.61,
        "duration": 4.74
    },
    {
        "text": "reduce your multiple hypothesis burden",
        "start": 3427.37,
        "duration": 4.71
    },
    {
        "text": "and get better statistical power and the",
        "start": 3429.35,
        "duration": 5.25
    },
    {
        "text": "result is that by by using that",
        "start": 3432.08,
        "duration": 5.07
    },
    {
        "text": "information you get about another 40,000",
        "start": 3434.6,
        "duration": 5.28
    },
    {
        "text": "spectra and then finally you can do an",
        "start": 3437.15,
        "duration": 5.52
    },
    {
        "text": "open modification search which means you",
        "start": 3439.88,
        "duration": 5.64
    },
    {
        "text": "can essentially allow for lots of",
        "start": 3442.67,
        "duration": 5.01
    },
    {
        "text": "different kind of post translational",
        "start": 3445.52,
        "duration": 5.07
    },
    {
        "text": "modifications on the peptides on the the",
        "start": 3447.68,
        "duration": 5.49
    },
    {
        "text": "remaining unidentified communities and",
        "start": 3450.59,
        "duration": 4.17
    },
    {
        "text": "that gives you an additional 40,000",
        "start": 3453.17,
        "duration": 5.1
    },
    {
        "text": "spectra so in the end you get about as I",
        "start": 3454.76,
        "duration": 5.52
    },
    {
        "text": "mentioned about oh sorry",
        "start": 3458.27,
        "duration": 5.28
    },
    {
        "text": "and then finally that wasn't there",
        "start": 3460.28,
        "duration": 5.16
    },
    {
        "text": "there's an additional step of doing an",
        "start": 3463.55,
        "duration": 4.05
    },
    {
        "text": "anima done on database search to find an",
        "start": 3465.44,
        "duration": 5.43
    },
    {
        "text": "additional 5,000 spectra yeah so that",
        "start": 3467.6,
        "duration": 6.33
    },
    {
        "text": "allows us to increase the number of",
        "start": 3470.87,
        "duration": 6.65
    },
    {
        "text": "identified spectra by about 8% which is",
        "start": 3473.93,
        "duration": 6.45
    },
    {
        "text": "if you were considering doing this to",
        "start": 3477.52,
        "duration": 4.84
    },
    {
        "text": "something like all of the data in the",
        "start": 3480.38,
        "duration": 4.08
    },
    {
        "text": "massive repository as of a couple months",
        "start": 3482.36,
        "duration": 3.99
    },
    {
        "text": "ago there'll be about fifteen point",
        "start": 3484.46,
        "duration": 4.02
    },
    {
        "text": "three million additional spectra",
        "start": 3486.35,
        "duration": 4.5
    },
    {
        "text": "identified and the other thing that we",
        "start": 3488.48,
        "duration": 3.93
    },
    {
        "text": "show in the paper I'm not showing you",
        "start": 3490.85,
        "duration": 3.21
    },
    {
        "text": "the evidence here is that the ones that",
        "start": 3492.41,
        "duration": 3.09
    },
    {
        "text": "we can't identify",
        "start": 3494.06,
        "duration": 3.09
    },
    {
        "text": "they're enriched for low quality",
        "start": 3495.5,
        "duration": 5.25
    },
    {
        "text": "spectrum so that suggests that perhaps a",
        "start": 3497.15,
        "duration": 5.34
    },
    {
        "text": "lot of those remaining ones that we",
        "start": 3500.75,
        "duration": 4.26
    },
    {
        "text": "can't identify really are are",
        "start": 3502.49,
        "duration": 5.43
    },
    {
        "text": "problematic in some way so that's where",
        "start": 3505.01,
        "duration": 3.72
    },
    {
        "text": "we are currently",
        "start": 3507.92,
        "duration": 3.48
    },
    {
        "text": "the future directions for this are",
        "start": 3508.73,
        "duration": 5.76
    },
    {
        "text": "numerous so we obviously one thing is",
        "start": 3511.4,
        "duration": 4.77
    },
    {
        "text": "we've only demonstrated this sort of",
        "start": 3514.49,
        "duration": 3.63
    },
    {
        "text": "proof of principle on a relatively small",
        "start": 3516.17,
        "duration": 4.38
    },
    {
        "text": "database so our small small set of",
        "start": 3518.12,
        "duration": 4.65
    },
    {
        "text": "spectra so we want to scale up to 100",
        "start": 3520.55,
        "duration": 4.35
    },
    {
        "text": "million 200 million spectra and really",
        "start": 3522.77,
        "duration": 5.22
    },
    {
        "text": "see what happens when we do that and to",
        "start": 3524.9,
        "duration": 5.85
    },
    {
        "text": "see what what might happen we did a down",
        "start": 3527.99,
        "duration": 5.34
    },
    {
        "text": "sampling experiment by sort of randomly",
        "start": 3530.75,
        "duration": 5.97
    },
    {
        "text": "tossing out some of our spectra this is",
        "start": 3533.33,
        "duration": 4.019
    },
    {
        "text": "not actually",
        "start": 3536.72,
        "duration": 2.579
    },
    {
        "text": "retraining the model we're using a fixed",
        "start": 3537.349,
        "duration": 4.621
    },
    {
        "text": "embedding but just asking if you start",
        "start": 3539.299,
        "duration": 4.591
    },
    {
        "text": "making smaller and smaller sets of",
        "start": 3541.97,
        "duration": 4.44
    },
    {
        "text": "spectra by tossing out experiments from",
        "start": 3543.89,
        "duration": 6.659
    },
    {
        "text": "our repository what percentage of the of",
        "start": 3546.41,
        "duration": 7.619
    },
    {
        "text": "the spectra don't have any labeled",
        "start": 3550.549,
        "duration": 5.49
    },
    {
        "text": "neighbour within the distances point out",
        "start": 3554.029,
        "duration": 5.911
    },
    {
        "text": "that tile distance before and that seems",
        "start": 3556.039,
        "duration": 6.091
    },
    {
        "text": "to be it's not really surprising it goes",
        "start": 3559.94,
        "duration": 3.839
    },
    {
        "text": "up as you get more and more spectra",
        "start": 3562.13,
        "duration": 3.57
    },
    {
        "text": "right but the point is it's not like",
        "start": 3563.779,
        "duration": 5.101
    },
    {
        "text": "it's really flattened off so the Idaho",
        "start": 3565.7,
        "duration": 5.49
    },
    {
        "text": "is that it continues to go up and we get",
        "start": 3568.88,
        "duration": 4.77
    },
    {
        "text": "much more identifications as we get out",
        "start": 3571.19,
        "duration": 3.96
    },
    {
        "text": "you know orders of magnitude more",
        "start": 3573.65,
        "duration": 5.399
    },
    {
        "text": "spectra going into the database the",
        "start": 3575.15,
        "duration": 5.609
    },
    {
        "text": "other thing that we're curious about is",
        "start": 3579.049,
        "duration": 4.56
    },
    {
        "text": "to explore this idea from natural",
        "start": 3580.759,
        "duration": 4.681
    },
    {
        "text": "language processing that maybe the",
        "start": 3583.609,
        "duration": 3.93
    },
    {
        "text": "latent space encodes some interesting",
        "start": 3585.44,
        "duration": 6.389
    },
    {
        "text": "semantics the word to vac is a common",
        "start": 3587.539,
        "duration": 7.23
    },
    {
        "text": "popular way to do embedding of words in",
        "start": 3591.829,
        "duration": 5.371
    },
    {
        "text": "natural language and this is an example",
        "start": 3594.769,
        "duration": 4.05
    },
    {
        "text": "from the word to that paper where they",
        "start": 3597.2,
        "duration": 3.779
    },
    {
        "text": "were able to show that if you embed the",
        "start": 3598.819,
        "duration": 4.68
    },
    {
        "text": "word King into a latent space you",
        "start": 3600.979,
        "duration": 4.89
    },
    {
        "text": "subtract the vector from man and you add",
        "start": 3603.499,
        "duration": 4.381
    },
    {
        "text": "the vector for woman that you end up",
        "start": 3605.869,
        "duration": 5.041
    },
    {
        "text": "near the vector for Queen right this is",
        "start": 3607.88,
        "duration": 5.369
    },
    {
        "text": "a semantic relationship it's captured by",
        "start": 3610.91,
        "duration": 5.03
    },
    {
        "text": "would be back similar for lots of",
        "start": 3613.249,
        "duration": 6.05
    },
    {
        "text": "superlatives and other participle so",
        "start": 3615.94,
        "duration": 6.339
    },
    {
        "text": "you're hoping that we might be able to",
        "start": 3619.299,
        "duration": 4.42
    },
    {
        "text": "see similar kinds of things that we look",
        "start": 3622.279,
        "duration": 4.44
    },
    {
        "text": "for for example amino acid substitution",
        "start": 3623.719,
        "duration": 5.34
    },
    {
        "text": "or a post translational modification or",
        "start": 3626.719,
        "duration": 5.971
    },
    {
        "text": "even mass changes and then the longer",
        "start": 3629.059,
        "duration": 6.02
    },
    {
        "text": "term idea would be that we could",
        "start": 3632.69,
        "duration": 5.19
    },
    {
        "text": "hopefully make this work in conjunction",
        "start": 3635.079,
        "duration": 5.68
    },
    {
        "text": "with some of these existing repositories",
        "start": 3637.88,
        "duration": 6.06
    },
    {
        "text": "to to better enable community-wide",
        "start": 3640.759,
        "duration": 6.24
    },
    {
        "text": "services for understanding and",
        "start": 3643.94,
        "duration": 5.129
    },
    {
        "text": "interpreting mass spec data as it's",
        "start": 3646.999,
        "duration": 4.05
    },
    {
        "text": "generated so that you could quickly and",
        "start": 3649.069,
        "duration": 4.081
    },
    {
        "text": "efficiently upload your data into a",
        "start": 3651.049,
        "duration": 4.05
    },
    {
        "text": "repository and get back some feedback",
        "start": 3653.15,
        "duration": 4.709
    },
    {
        "text": "about what your experiment resembled",
        "start": 3655.099,
        "duration": 4.62
    },
    {
        "text": "what other experiments resemble it what",
        "start": 3657.859,
        "duration": 3.601
    },
    {
        "text": "peptides you were not able to identify",
        "start": 3659.719,
        "duration": 4.85
    },
    {
        "text": "that could be identified and so on so",
        "start": 3661.46,
        "duration": 4.29
    },
    {
        "text": "that",
        "start": 3664.569,
        "duration": 2.351
    },
    {
        "text": "the whole story and I think I'm just",
        "start": 3665.75,
        "duration": 3.99
    },
    {
        "text": "about out of time but I'm very grateful",
        "start": 3666.92,
        "duration": 4.26
    },
    {
        "text": "that you're here to hear my story and",
        "start": 3669.74,
        "duration": 14.73
    },
    {
        "text": "I'm happy to answer quick okay well",
        "start": 3671.18,
        "duration": 14.42
    },
    {
        "text": "since I have it",
        "start": 3684.47,
        "duration": 3.33
    },
    {
        "text": "one question is so you mentioned that",
        "start": 3685.6,
        "duration": 4.51
    },
    {
        "text": "the problem with clustering of spectra",
        "start": 3687.8,
        "duration": 4.41
    },
    {
        "text": "is that it's quite expensive to cluster",
        "start": 3690.11,
        "duration": 4.22
    },
    {
        "text": "them again as the data sets get bigger",
        "start": 3692.21,
        "duration": 4.29
    },
    {
        "text": "and you mentioned that you may have an",
        "start": 3694.33,
        "duration": 4.06
    },
    {
        "text": "advantage here that you can embed",
        "start": 3696.5,
        "duration": 4.29
    },
    {
        "text": "spectra using the same model that you",
        "start": 3698.39,
        "duration": 5.07
    },
    {
        "text": "develop using a smaller dataset so at",
        "start": 3700.79,
        "duration": 5.73
    },
    {
        "text": "what point you do have to retrain your",
        "start": 3703.46,
        "duration": 5.37
    },
    {
        "text": "underlying models as you continue adding",
        "start": 3706.52,
        "duration": 4.38
    },
    {
        "text": "embedding spectrum that's a good",
        "start": 3708.83,
        "duration": 4.89
    },
    {
        "text": "question so I think the answer will have",
        "start": 3710.9,
        "duration": 4.65
    },
    {
        "text": "to be determined empirically I mean what",
        "start": 3713.72,
        "duration": 3.48
    },
    {
        "text": "we would do is have some performance",
        "start": 3715.55,
        "duration": 3.15
    },
    {
        "text": "measures",
        "start": 3717.2,
        "duration": 3.75
    },
    {
        "text": "you know how accurate the model how",
        "start": 3718.7,
        "duration": 4.47
    },
    {
        "text": "accurately the model is recovering known",
        "start": 3720.95,
        "duration": 5.79
    },
    {
        "text": "peptides my hope would be that once you",
        "start": 3723.17,
        "duration": 6.51
    },
    {
        "text": "reach a certain threshold so let's",
        "start": 3726.74,
        "duration": 4.74
    },
    {
        "text": "imagine you've got a billion spectra in",
        "start": 3729.68,
        "duration": 3.93
    },
    {
        "text": "there probably adding another billion is",
        "start": 3731.48,
        "duration": 3.6
    },
    {
        "text": "not going to get you that much more it's",
        "start": 3733.61,
        "duration": 2.82
    },
    {
        "text": "at a certain point it's not going to",
        "start": 3735.08,
        "duration": 7.92
    },
    {
        "text": "become as important so I want to thank",
        "start": 3736.43,
        "duration": 9.18
    },
    {
        "text": "you for demonstrating the multi-omics",
        "start": 3743.0,
        "duration": 5.37
    },
    {
        "text": "approach to the seminar we are proud",
        "start": 3745.61,
        "duration": 6.51
    },
    {
        "text": "that 12 years ago 1350 14 years ago we",
        "start": 3748.37,
        "duration": 5.22
    },
    {
        "text": "started here with a National Center for",
        "start": 3752.12,
        "duration": 6.16
    },
    {
        "text": "NIH on integrative",
        "start": 3753.59,
        "duration": 8.99
    },
    {
        "text": "bioinformatics analysis demonstrated I",
        "start": 3758.28,
        "duration": 5.92
    },
    {
        "text": "want to ask you specifically about the",
        "start": 3762.58,
        "duration": 3.48
    },
    {
        "text": "universal spectrum identifier",
        "start": 3764.2,
        "duration": 5.13
    },
    {
        "text": "alright Deutsch and colleagues including",
        "start": 3766.06,
        "duration": 7.26
    },
    {
        "text": "the yupo protein standards initiative",
        "start": 3769.33,
        "duration": 6.0
    },
    {
        "text": "have been working on this I believe it's",
        "start": 3773.32,
        "duration": 4.26
    },
    {
        "text": "ready for publication this year are you",
        "start": 3775.33,
        "duration": 3.45
    },
    {
        "text": "aware of that do you think it's",
        "start": 3777.58,
        "duration": 5.7
    },
    {
        "text": "something to be useful in your work it's",
        "start": 3778.78,
        "duration": 7.26
    },
    {
        "text": "a search tool starting with the spectrum",
        "start": 3783.28,
        "duration": 11.22
    },
    {
        "text": "instead of the sequence been some recent",
        "start": 3786.04,
        "duration": 14.88
    },
    {
        "text": "deep learning and a couple other recent",
        "start": 3794.5,
        "duration": 9.0
    },
    {
        "text": "advances in that area so I'll be",
        "start": 3800.92,
        "duration": 19.74
    },
    {
        "text": "interested hi so in your avocado model",
        "start": 3803.5,
        "duration": 20.82
    },
    {
        "text": "what would cause your model to perform",
        "start": 3820.66,
        "duration": 6.09
    },
    {
        "text": "better or worse on Wheldon one cell type",
        "start": 3824.32,
        "duration": 4.2
    },
    {
        "text": "versus another is it just the",
        "start": 3826.75,
        "duration": 3.45
    },
    {
        "text": "availability of information or is there",
        "start": 3828.52,
        "duration": 10.8
    },
    {
        "text": "some other factor but it also has to do",
        "start": 3830.2,
        "duration": 11.37
    },
    {
        "text": "it not just how much data you have in",
        "start": 3839.32,
        "duration": 5.1
    },
    {
        "text": "that cell type but how much data you",
        "start": 3841.57,
        "duration": 6.27
    },
    {
        "text": "have in what kinds of data that is right",
        "start": 3844.42,
        "duration": 5.25
    },
    {
        "text": "some some of these types of data are",
        "start": 3847.84,
        "duration": 3.72
    },
    {
        "text": "more informative than others but often",
        "start": 3849.67,
        "duration": 3.93
    },
    {
        "text": "it's also the case that a cell type",
        "start": 3851.56,
        "duration": 3.9
    },
    {
        "text": "access is not really just a linear",
        "start": 3853.6,
        "duration": 3.9
    },
    {
        "text": "access right it's more like a dendrogram",
        "start": 3855.46,
        "duration": 4.83
    },
    {
        "text": "so if you have if you're looking at a",
        "start": 3857.5,
        "duration": 4.71
    },
    {
        "text": "blood cell type then you've got 14 other",
        "start": 3860.29,
        "duration": 3.93
    },
    {
        "text": "blood cell types then it's going to be",
        "start": 3862.21,
        "duration": 3.72
    },
    {
        "text": "much easier for you because you've got",
        "start": 3864.22,
        "duration": 3.21
    },
    {
        "text": "lots of neighbors in the cell type",
        "start": 3865.93,
        "duration": 3.81
    },
    {
        "text": "spaces well even if you have relatively",
        "start": 3867.43,
        "duration": 4.44
    },
    {
        "text": "small amounts of data so it is",
        "start": 3869.74,
        "duration": 4.29
    },
    {
        "text": "challenging to characterize like a",
        "start": 3871.87,
        "duration": 4.62
    },
    {
        "text": "priority to say for this prediction I",
        "start": 3874.03,
        "duration": 3.63
    },
    {
        "text": "think we're going to do well and for",
        "start": 3876.49,
        "duration": 2.7
    },
    {
        "text": "that one we're not because we don't",
        "start": 3877.66,
        "duration": 2.97
    },
    {
        "text": "really understand that some of the",
        "start": 3879.19,
        "duration": 3.45
    },
    {
        "text": "structure that's me",
        "start": 3880.63,
        "duration": 15.119
    },
    {
        "text": "I was wondering if it might be possible",
        "start": 3882.64,
        "duration": 16.53
    },
    {
        "text": "to formulate the proteomics task as kind",
        "start": 3895.749,
        "duration": 5.28
    },
    {
        "text": "of a regression problem instead of an",
        "start": 3899.17,
        "duration": 4.29
    },
    {
        "text": "embedding problem where you have these",
        "start": 3901.029,
        "duration": 4.861
    },
    {
        "text": "spectrum peptide pairs and you just try",
        "start": 3903.46,
        "duration": 4.02
    },
    {
        "text": "to predict the peptide directly from the",
        "start": 3905.89,
        "duration": 6.619
    },
    {
        "text": "spectrum two different questions I",
        "start": 3907.48,
        "duration": 6.779
    },
    {
        "text": "thought you were going one way and then",
        "start": 3912.509,
        "duration": 3.34
    },
    {
        "text": "you learn another way so you can",
        "start": 3914.259,
        "duration": 5.07
    },
    {
        "text": "definitely you can frame it as a",
        "start": 3915.849,
        "duration": 6.42
    },
    {
        "text": "classification task or a regression if",
        "start": 3919.329,
        "duration": 4.53
    },
    {
        "text": "you have a peptide spectrum pair and you",
        "start": 3922.269,
        "duration": 4.32
    },
    {
        "text": "want to know how good is this pairing so",
        "start": 3923.859,
        "duration": 5.16
    },
    {
        "text": "the most common way to do it is and this",
        "start": 3926.589,
        "duration": 4.74
    },
    {
        "text": "is what Alexa did first was to sort of",
        "start": 3929.019,
        "duration": 5.431
    },
    {
        "text": "say here's a peptide in a spectrum this",
        "start": 3931.329,
        "duration": 5.01
    },
    {
        "text": "is a true match you're the peptide in a",
        "start": 3934.45,
        "duration": 4.829
    },
    {
        "text": "spectrum this is a false match right so",
        "start": 3936.339,
        "duration": 5.101
    },
    {
        "text": "that's one way to do it but you I think",
        "start": 3939.279,
        "duration": 3.121
    },
    {
        "text": "you were going a different direction",
        "start": 3941.44,
        "duration": 3.299
    },
    {
        "text": "yeah so the way you're using the",
        "start": 3942.4,
        "duration": 4.619
    },
    {
        "text": "embedding is you're saying I want to",
        "start": 3944.739,
        "duration": 4.59
    },
    {
        "text": "assign a peptide to this spectrum so I'm",
        "start": 3947.019,
        "duration": 3.631
    },
    {
        "text": "going to look at the assignments of my",
        "start": 3949.329,
        "duration": 4.23
    },
    {
        "text": "neighbors in Leighton space but seems",
        "start": 3950.65,
        "duration": 4.439
    },
    {
        "text": "like you could also just try to directly",
        "start": 3953.559,
        "duration": 3.901
    },
    {
        "text": "predict the peptide from the spectrum",
        "start": 3955.089,
        "duration": 4.081
    },
    {
        "text": "given a bunch of true assignments oh I",
        "start": 3957.46,
        "duration": 4.47
    },
    {
        "text": "think so yeah that's the those are the",
        "start": 3959.17,
        "duration": 4.23
    },
    {
        "text": "kind of thing that we just alluded to so",
        "start": 3961.93,
        "duration": 3.78
    },
    {
        "text": "you can for instance the most recent",
        "start": 3963.4,
        "duration": 5.129
    },
    {
        "text": "papers trainer a deep neural network to",
        "start": 3965.71,
        "duration": 5.159
    },
    {
        "text": "say I'm gonna take as input a spectrum",
        "start": 3968.529,
        "duration": 5.46
    },
    {
        "text": "and output a peptide doing that is hard",
        "start": 3970.869,
        "duration": 5.87
    },
    {
        "text": "right because you've got this weird",
        "start": 3973.989,
        "duration": 5.79
    },
    {
        "text": "secretly your sequence over a twenty",
        "start": 3976.739,
        "duration": 5.921
    },
    {
        "text": "dimensional discrete space so there's",
        "start": 3979.779,
        "duration": 4.171
    },
    {
        "text": "some bells and whistles to make that",
        "start": 3982.66,
        "duration": 14.869
    },
    {
        "text": "work any other questions if not let's",
        "start": 3983.95,
        "duration": 13.579
    },
    {
        "text": "so the question was how where a project",
        "start": 4009.62,
        "duration": 5.32
    },
    {
        "text": "like encode in 40 nucleon going in the",
        "start": 4012.78,
        "duration": 4.14
    },
    {
        "text": "next five years and I would really like",
        "start": 4014.94,
        "duration": 5.73
    },
    {
        "text": "to know we don't know for sure I think",
        "start": 4016.92,
        "duration": 7.02
    },
    {
        "text": "so 40 nucleon is just in its first phase",
        "start": 4020.67,
        "duration": 5.58
    },
    {
        "text": "and it is a what's called a common bond",
        "start": 4023.94,
        "duration": 3.9
    },
    {
        "text": "initiative which means it's funded by",
        "start": 4026.25,
        "duration": 4.92
    },
    {
        "text": "the jointly by the NIH not associated",
        "start": 4027.84,
        "duration": 4.89
    },
    {
        "text": "with a particular Institute and those",
        "start": 4031.17,
        "duration": 6.0
    },
    {
        "text": "can only go for two phases so obviously",
        "start": 4032.73,
        "duration": 6.3
    },
    {
        "text": "there's no official announcement yet but",
        "start": 4037.17,
        "duration": 3.12
    },
    {
        "text": "we're three and a half years in and",
        "start": 4039.03,
        "duration": 3.27
    },
    {
        "text": "we're all hoping that there will be an",
        "start": 4040.29,
        "duration": 5.34
    },
    {
        "text": "announcement soon in the second phase if",
        "start": 4042.3,
        "duration": 5.97
    },
    {
        "text": "there is I would guess that I mean it's",
        "start": 4045.63,
        "duration": 5.19
    },
    {
        "text": "a 40 nucleon but it's been very 3d so",
        "start": 4048.27,
        "duration": 5.01
    },
    {
        "text": "far there's not that much time happening",
        "start": 4050.82,
        "duration": 4.92
    },
    {
        "text": "it's a little bit but I think the next",
        "start": 4053.28,
        "duration": 4.56
    },
    {
        "text": "phase will have to be much more time",
        "start": 4055.74,
        "duration": 7.65
    },
    {
        "text": "oriented it's also 40 nucleon in",
        "start": 4057.84,
        "duration": 9.72
    },
    {
        "text": "particular is focused on imaging and and",
        "start": 4063.39,
        "duration": 6.54
    },
    {
        "text": "sort of sequencing based technologies",
        "start": 4067.56,
        "duration": 5.82
    },
    {
        "text": "and getting them to work together has",
        "start": 4069.93,
        "duration": 5.67
    },
    {
        "text": "been a big focus of the 40 nucleon but",
        "start": 4073.38,
        "duration": 4.04
    },
    {
        "text": "it's not done yet it's not even really",
        "start": 4075.6,
        "duration": 4.62
    },
    {
        "text": "diamond it started yet we're working",
        "start": 4077.42,
        "duration": 4.42
    },
    {
        "text": "hard to make that happen but it's going",
        "start": 4080.22,
        "duration": 4.38
    },
    {
        "text": "to run into the next things yeah",
        "start": 4081.84,
        "duration": 5.04
    },
    {
        "text": "encode is an entirely different beast",
        "start": 4084.6,
        "duration": 4.95
    },
    {
        "text": "right I think encode is really mature by",
        "start": 4086.88,
        "duration": 8.63
    },
    {
        "text": "now or in you know and I don't you know",
        "start": 4089.55,
        "duration": 8.64
    },
    {
        "text": "the initial phase was essentially",
        "start": 4095.51,
        "duration": 4.57
    },
    {
        "text": "technology development the pilot phase",
        "start": 4098.19,
        "duration": 3.87
    },
    {
        "text": "the second phase was a scale up to the",
        "start": 4100.08,
        "duration": 3.99
    },
    {
        "text": "whole genome the third phase was hey",
        "start": 4102.06,
        "duration": 3.63
    },
    {
        "text": "this is working pretty let's pretty well",
        "start": 4104.07,
        "duration": 2.46
    },
    {
        "text": "let's just keep going",
        "start": 4105.69,
        "duration": 3.06
    },
    {
        "text": "you know it wasn't it was less clear",
        "start": 4106.53,
        "duration": 4.56
    },
    {
        "text": "what the mandate was and I think that's",
        "start": 4108.75,
        "duration": 4.35
    },
    {
        "text": "also true the fourth place except that",
        "start": 4111.09,
        "duration": 5.22
    },
    {
        "text": "it's clear that the data has value right",
        "start": 4113.1,
        "duration": 6.57
    },
    {
        "text": "so moving from primaries from cell lines",
        "start": 4116.31,
        "duration": 6.69
    },
    {
        "text": "the tissues have been a big push I think",
        "start": 4119.67,
        "duration": 6.33
    },
    {
        "text": "moving toward more integrative kinds of",
        "start": 4123.0,
        "duration": 6.18
    },
    {
        "text": "analysis is also a big push and I think",
        "start": 4126.0,
        "duration": 4.98
    },
    {
        "text": "one of the directions that NIH seem to",
        "start": 4129.18,
        "duration": 5.38
    },
    {
        "text": "be going is pushing toward better",
        "start": 4130.98,
        "duration": 5.8
    },
    {
        "text": "tools for visualizing understanding and",
        "start": 4134.56,
        "duration": 4.44
    },
    {
        "text": "interpreting the data so you know can",
        "start": 4136.78,
        "duration": 3.87
    },
    {
        "text": "you make something you know the genome",
        "start": 4139.0,
        "duration": 4.2
    },
    {
        "text": "browser was a great thing right at the",
        "start": 4140.65,
        "duration": 3.63
    },
    {
        "text": "beginning of the human genome project",
        "start": 4143.2,
        "duration": 3.3
    },
    {
        "text": "though we may have moved beyond that and",
        "start": 4144.28,
        "duration": 5.28
    },
    {
        "text": "what other kinds of what's the what's",
        "start": 4146.5,
        "duration": 5.22
    },
    {
        "text": "the blast of the 2020 you know what's",
        "start": 4149.56,
        "duration": 3.57
    },
    {
        "text": "the way that we're gonna interact with",
        "start": 4151.72,
        "duration": 3.36
    },
    {
        "text": "the genome in the future",
        "start": 4153.13,
        "duration": 3.54
    },
    {
        "text": "in a way that takes into account the",
        "start": 4155.08,
        "duration": 2.88
    },
    {
        "text": "fact that we now have 10,000",
        "start": 4156.67,
        "duration": 2.85
    },
    {
        "text": "measurements every single day stare at",
        "start": 4157.96,
        "duration": 11.16
    },
    {
        "text": "the Dino that's right",
        "start": 4159.52,
        "duration": 11.64
    },
    {
        "text": "so a lot of a lot of the browsers now",
        "start": 4169.12,
        "duration": 9.36
    },
    {
        "text": "allow you to see the 3d structure all",
        "start": 4171.16,
        "duration": 9.69
    },
    {
        "text": "right if normal question let's think",
        "start": 4178.48,
        "duration": 3.2
    },
    {
        "text": "bill again",
        "start": 4180.85,
        "duration": 4.389
    },
    {
        "text": "[Music]",
        "start": 4181.68,
        "duration": 3.559
    }
]