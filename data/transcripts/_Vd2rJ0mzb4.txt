and so I'm but then the statistics online computational resource so car it was really a neat thing it's that you've got followers everywhere and so Evo is now an associate professor in the School of Nursing you know oversees all the informatics there he's a PI with me and jagged Ishaan bd2k which looks like it's going somewhere and he's one of our leaders in informatics definitely in computational and statistical work at the University of Michigan all right well it's a pleasure to be here so today my goals are to show you some of the stuff that we've developed over the past seven years that has computational aspects to it modeling aspects to it educational aspects but itself that's why the title is statistics online accommodation resource infrastructure for technology hence transdisciplinary research and and science and so to begin with here is roughly what what I plan to cover today three kind of complementary topics the first one is going to be on the software what is soccer and I'll show you three different four different aspects of it and then we'll shift gears a little bit and talk about interdisciplinary research I know many of you are very engaged in informatics projects and computation projects in running projects or give you a few flavors and maybe even do challenging problems and then at the end if there's interest in time I'll be happy to do a live demo because my philosophy is it's not online it doesn't exist okay so everything that I do is your online or you didn't happen so here are some of the links you know that and there is a few other ones that I may show you here so personal about soccer I will show you some stem stands for science technology engineering and math education it's a big initiative mostly led by the National Science Foundation started with a decade ago so so it's very well known so what we've tried to do is we've tried to integrate concepts theory practice applications biology engineering math stats and so forth to so complex problems okay and I'll show you some of these things I'll show you schematics I guess some of the datasets many of the research projects that we worked on have generated derived data that they always post online make it available for students or educators it's important to integrate in their curriculum and so forth and I'll show you some of our computational tools which are really platform agnostic we developed only in Java or html5 so these are against the most robust things that you can have across platforms browsers ISPs and so forth and there may be a little bit on the on the learning resources so that's not a little bit about the science interactive why do we need you know biologists to stick to mathematicians stick to engineers to computational and informatics scientists and so forth so you know if you think about translation of science there is three big fields right basic sense and sciences Medical Sciences and the computational sciences these things all need to come together to solve health care challenges for example and here are some of the interactions like your basic sign from basic science you provide theoretical models and optimization into the computational sciences and then composition science generate tools services and so forth that can be used to validate some of the models you know you've all seen PDE based models you've seen all kinds of our linear modeling and programming problems and so forth so these come in vacuum typically but they can be validated and so forth they have Medical Sciences provide a very rich collective driving challenges of driving motivational challenges and they have the feedback between computational science citizen and obviously biological constraints oftentimes need to find their place as model restrictions when you do the analytical model representation and we will do the implementation of it as a software city to make sure these restrictions are embedded in the model cause a noise models don't work too well so these are kind of schematic of the of the science interactive so a little bit about soccer so this is our page as OCR that UMich okay that's our page we just migrated we are about 80% done with the transition this is this resource has several hundreds of applets thousands of pages many hundreds of data set so it's fairly complex there is and because it's all interlinked like when I go in my classroom I typically have an able that I'll show you in a second so it needs to link to the services needs to link to the theater it needs to link to them data so it's kind of complex to transition one of these things so that's what it looks like and you can see on the table the main things that I'm gonna kind of show you through now the first thing is automatically translated everything that we do is 60 languages so then we see an English now you can have in Bulgarian another language is that some of you may be more comfortable with so here is an example the e-book we developed arguably the most popular probability and statistics electronic in book out there it's got over a million users since 2007 it's very broad scope and because it's eBook it's constantly evolving you know me and other colleagues of good new sections device sections new examples get plugged in so so that's the line beast right this is not a static static image then here here's a small snapshot of data it's categorized you know anything we have three types of data sets okay observed research to write data we have lots of data that we can simulate and if you eat there is interest afterwards I can show you we have probably the largest open source simulator engine in the world 80 different probability distributions you can sample from it's really complex you don't really click of a mouse you get the data copy pasted into anything you want so it's really drunk but in addition to simulator observe data and there is other data that we generate through our virtual experiments we have a very large collection of virtual simulators cards dyes anything that you can come up with spin and see no roulette wheels very large collective simulators that we have they generate data obviously and so forth so this is the data and now if you go to the top here is for example the software modeler it allows you once you have your data that's been where you come from you can actually fit in models directly in your browser you can select the model you can you just specify the parameters of the models from ability distribution models polynomials or more complex based function models like Fourier or wavelet representations of moms okay but you fit in a model and then and then again you either specify the parameters of the model or we use maximum likelihood to estimate some of these parameters and that's what comes out right so here's an example you know you're playing some there and you're like is this really two cohorts that are mixed together to generate this histogram I don't know well we plug in tomorrow and says model one wait 70% you know certain certain mean and instead of issues and tells you how distant these two these two moves of the distribution are it does come across a rough test don't test to find out if the model and the data agree right so this is all done in the in the browser as you can see this these are some of the last kind of components we have a very large collection of learning modules or activities so we go to one of these things and it kind of shows you every activity starts with with a bunch of goals and it has motivation data sets example it has snapshots at the actual tool that you're using and so forth so that's what soccer is and now we are now building here at University of Michigan a new career something called scientific methods for for health sciences education so I'm designing this with a bunch of my colleagues it's going to be a health science course series it's gonna start this fall with the first two forces hi Pepe and 51 so it's gonna do a broad spectrum of it's not gonna be just a page takes it's not gonna be just computation it's gonna mix techniques it's gonna mix data challenges and so forth so we're not going out to the whole thing but here's the course series for each of these into a description objective topics that are going to be covered in the next is apply inference so we're going to be talking about you know various parametric and nonparametric models obviously classification methods based in in units and so forth in the new modern aspect we're gonna cover whole spectrum of general linear model representations we will be talking about generalized estimating equations time series longitudinal data and so forth and the last one is going to be a little bit more open-ended it's gonna vary from year to year what exactly gets covered but here are examples of competences are we talking about obviously big day you know who are we talking about are we talking about classification methods causality and so forth this is a pre-sub container with a lot of are gonna be undergraduate quantitative level these four to six credits so you have def calc it was you know you should have taken some probability statistics you know you need to know what is a sigma you know how what's the mode you know things like this you value because we're not gonna be going anyways now let's talk a little bit about the multidisciplinary aspect of solution so three things that were touch on the first one is size of the data second one is the fact that you've got thousands of devices out there that wanna interact they generate data there is more digital data journalist out there in a human face of the earth okay well what's happening you know who's who's collecting who's managing all these data and then and then obviously there is there is devices and whatnot that and software tools that that you may want to deal with so just to give you a little bit of an example here the types of things that eventually come into play in one specific situation the situation of doing neuroimaging and genetic studies so here's an example when you redo your image in it I'll show you an example in a second exactly I will show you what kind of data but the typical things are how do you design your study how should the population is stratified to use block you know if you know certain effects are there how is this going to be so there's a lot of things that come from the epidemiology I guess or a study design and then when you have genetics obviously you have either dominant or recessive effects that you're looking for were you looking for edited effects you know those response effect and you want to combine some of these things oftentimes to increase the sample size because sample size is matter right if you have 1020 subjects and you doing genetics that stuff you you need thousands of individuals because the effects are statistically robust but oftentimes there Hanako in fact it is marginal mostly because it's not one or two genes it's typical networks that come into play and so forth so causality a big thing right when when I see two two things called vary or you know inversely varying or something you may want to know are these things is one influence in the other reason that there's an underlying latent variable that kind of controls both of these things sample size is obviously reported of negative findings okay we're gonna be there let's be honest you know you do a study your apology's are not proven right the typical don't publish only for these things in sting tournament get some track you know should there be a way for us to learn from the negative results and there is no journals of negative findings he won't be a Jew well there was this I don't know if it's been released yet into the public domain it was embargo but today maybe or there's some article by Francis Collins and Larry Tabak their principal the Associate Director at the NIH on the reproducibility of data that's in nature like you know I can send it around one thing zeebo they were talking about in the article is that NIH is actually going to put out lead and helping to develop the best practices and then help to ensure that the journals in their method session and all you know have a checklist of all these kinds of things that you know the experimental model and all this is gonna have to be much more explicitly laid out in the method sections of the papers coming up a sample size can you include the kind of results that you think you can from the sample size the experimental all this gonna stuff it's going to be required in your paper explicitly so and obviously things like type 1 and type 2 errors false positive false negatives and so forth so let me give you some some examples okay so the Alzheimer's disease neuroimaging initiative is a large national effort to collect you know thousands of individuals in 3 different populations their own elderly individuals the story focuses on dementia but I looking at H matched controls there are asymptomatic in looking at mountain cognitively impaired individuals MC is and they're looking at people with dementia at different stages and they're collecting they're collecting a large number of variables including brain scans you know longitudinal including genetics including biospecimen that they do variety of things and I just wanted you know this is a simple example arguably only about 36 cases or so but what we wanted to know is we wanted to look at the early onset dementia individuals so we were focusing on 55 to 65 year old okay and then we want to have a very very widget match in between our summers disease ad and model cognitively-impaired NCIS individuals and this please take McCallum that's a little bit of demographics here just to show that they're mostly different here in a mini mental state exam score which is a measure of cognitive dysfunction right up here that that's highlighted and and now what we are trying to do is we were trying to use tensor base before many techniques that they've developed and and genome-wide Association study for all of these individuals we have taken the 630,000 slips single morphisms after quality control desert reduced by half and now we're gonna be in the very same soccer stats very same libraries that we developed to provide an interface computational language in the browser using this for very high throughput analytics here on a box of a box of case when we do the analysis of the images and genetics data in dimensions so and we're using also the pipeline workflow is it says here and so forth so here carries you know just to be perfectly clear you know snip is one of these things where a senior takes to a team and then what we're looking for is we're looking for in a more simple of all cases we're looking at imaging biomarkers which can be the phenotype and this is kind of a response observed variable and then we have the genotype of an individual and remember that this is this is kind of going between the the loadings on on the a a allele you know x1 is can be can be classified in terms of the loading on the a from zero one and two and so forth and I what you're doing is you're finding a linear model and asking yourself are the effect sizes here these beta coefficients are in trivial or not if they're trivial then obviously gene of you know Association is weak if they're not trivial than that then you have something more interesting so schematic on what's happening here you know you lose the moment on on the on the x variable and genotype variable and then again if the beta coefficient is zero you have to put three clusters that don't show you any trend but then if beta is positive or negative then you have some kind of an indication that your response would be an imaging marker could be no marker and so forth may be associated somehow with the phenotype but now what we're doing now is we're going to be doing the statistics in three dimensions as shown in the corner here or sometimes on many folds and many folds are these currently linear spaces that don't have usual Euclidean geometry on them so if you have two points on the face of the earth the shortest pack is not the path connecting them because you gotta burn on the surface and the earth is a little bit flat so you gotta go on a geodesic surface on a geodesic curve to go from point A to point B to estimate the distance and doing statistics on some of these many cuts to three or higher dimensional manifolds is not very tricky because you have to have a computational way of tracking the metric on space coming in a second in a different example so all this is this is what's happening we have all the subjects they're already on set MC eyes or a DS and then we look globally personally their brains in 256 regions okay and we do some statistics to find out which region which regions show statistically significant differences in their morphometrics size dimensions and so forth and and some of these regions are indicated right up here and then what we do is we've got the genetics here for for all the 22 chromosomes and we obviously look at the minus log P inverse carries in the Manhattan still Manhattan part and these the big ones are obviously very small P values that indicate that so these beta coefficients the effect size that I showed you earlier seem to me significantly non-trivial and then what we do is we get some of these some of these we can't connect the grams okay so in the left hemisphere here you got the 20 or so snips that are most significant and then only on the left side you actually have the pile imaging markers and then when you look at their association so these ribbons the size the color the brightness are indicative of the Association so you know that that specific reference sequence right up here she occupies quite a bit of the angular momentum here in your connect around here is associated with this region 10 or so which which you can then map and find out exactly where these two populations are different therefore you can argue that maybe maybe the progression from mild severe dementia has something to do in this region for your region predefined we just have pretty fun prices so so we look at first of all we separate them into 56 and then we extract the 15 or so that that show the children what we call regional differences and then occasionally we can actually go ahead and do local differences to find out if you know that the difference is in the amygdala is the entire amygdala using the handle that tell the amygdala where is the difference because oftentimes some of these brain disorders and other problems manifest locally of necessarily globally now in terms of data size big data is okay so I summarized here for us the situations of a single individual and equals to one and when you have the genetic studies depend upon what your coverage is a whole genome can be almost 200 or more of a terabyte for a single individual right so promises in this quarter of a terabyte is obviously very significant it requires lots of RAM CPU time and so forth that's the genetics the imaging obviously depends on what particle and how is the scanner children and so forth but it's not a typical to have family to 512 gradient directions from your quad diffusion-weighted tensor imaging there will be acquired at a bunch of directions to look at diffusion of water molecules and find out the wiring of the brain the white matter to progress with the brain and these are you know density bytes and it is a no in fact at least in your imaging that's where every byte that you acquire raw from the scanner it's a tenfold increase in terms of process derived afterwards so you're talking about hundreds of gigabytes of data that needs to be managed store you know retrieved and so forth single individual now for larger studies cohort studies typically you know 10200 you know you essentially everything skills at least linearly if not if not more and therefore more institutional subject studies in thousands longitudinally there is no one you know here this is just one cross-sectional image so you can imagine how quickly these things get get out of control anyway now to be a little bit more specific about the growth the data when we talk about new images of genetics with generate these data table that kind of shows you for similar individuals single planet or graphic image we're thinking about optically or somehow acquiring a slice by slice very high-resolution images in 3d depending upon grayscale color and so forth the single image at a ten microns could be five petabytes you know obviously that's a extreme but we are into into the tens and hundreds of gigabytes at this point when we talk about new imaging globally how much data is acquired there is an exponential increase you as you can see here genomics they're the same kind of thing and obviously so these are predictions important thing is that the computational power increases about doubles about every 18 months what we have here in terms of increase of data is about 12 to 14 months double so we obviously in a collision course here we generated more data than we in our capability to managing data allows us to process so this is an interesting thing from the beginning a kind of perspective now two's okay there is many many examples and I'm sure you know a lot more than I do with this but for every category or class of software tools out there there are dozens if not hundreds of algorithms and for every one of those there are multiple implementations so it is an open source some of these are proprietary and so forth but when it comes down to selecting a tool that does a specific task like cortical brain extraction or informal genetic correction soudanese you've got multiple things that you can choose from how do you select the one that's most appropriate most efficient most reliable you know there's a lot of things that have to go into the into the navigation in a way somebody said several years ago Brian I use is part of the National centers for biomedical computing where we we have RDF descriptions of resources either stories bio sitemaps and these resources describe tools that there are their capabilities and most of the metadata including ok including how is to computationally in both so you don't include something like when is it is to build somewhere how can I use it how do I call it what's the syntax green punk in it you know and so forth so this language that we have in terms of power sitemaps allows us not only to navigate this universe of the resource and identify possible tools but but but it's also some of the two side additional description that allow you to actually find out computation not a human reading the code how to call these two and get a result or the type of the output and so forth you know we can do it selves a lot of good if we went back to that mapped it into trans mark so so now we have now we have a lot of a lot of data one of tools and lot of devices so what we engineered a few a few years ago is this pipeline computational graphics environment that allows you to design computational protocols and joint solutions starting with the data as is rather than an import and then every processing step is coded as a node in this graph a graphical representation and then you the designer the workflow can share it with somebody else they can open it on their client system and then connect to remote server and outsource the calculations calculations are almost never done locally now nowadays right we have powerful servers here and elsewhere including cloud services and so forth so but it is dead disconnect representation of clients and servers the user communicates with the client the client and the servers different services you can see different web cows and so forth communicate with each other there's obviously grief managers in the background and so forth that one can use to streamline the sharing and in the calculation so very roughly speaking what is the pipeline it's available online here it has the graphical user interface has a canvas or you can design one of these workflows and every workflow has three components on the top you've got the data inputs they can be local they can be from any node file system they can be from the cloud box Amazon whatever and then you have a body of the workflow where different things coming from different sources you see their iconography here indicates that these are not necessarily things that we won't here at Michigan or necessarily the NC bc0 different labs the tools are designed to fit completely different so completely different problems they can be made to interoperate once we have this meta language of describing I want what I mentioned in bio sitemaps what is the input what's the output what are the controls and what's the invocation protocol okay you can make these things interpret and then the product you've got same thing the results need to be stored somehow locally or remotely and so this is a java-based in fact now what you see is is redesigning the client you know six to ten million dollars spending it over all over a decade or so a massive massive at least more than yeah so two types of processes we've tried to despite my environment the first one is genomics computer the second one is new imaging obviously this is just a small collection can kind of illustrate some of the things that we have and and this is a library that that's available online you can kind of navigate keyword search there is tree hyperbole all kinds of ways to look at the available resources and resources within the pipeline of our three times in a day no resources individual modules or these individual data filters or end-to-end weight-loss there are multiple things strung together to form a processing protocol so you can kind of look at some of these things here is here is tree paste paste hierarchy you know the previous one that we just looked at a second ago which was the rectory cool as I kind of the trees organized to the right now here you can search or something you you find one of these two so something that you're interested in using Java Web Start technology you can start it directly off the web by you know kind of clicking on a specific node it shows you schematic of the workflow and then you have right up here the options say download open or run now when you retrieve it is just XML which is retrieves an XML instruction set for you okay and then you can write as a guest you don't even need an account we have a server that suppose I think 40 cores it does get cast calculations if you can try this online if you have Java Web Start technology and then this thing runs and they generate some outputs notice the metadata annotation here for every module 2 you have citations PubMed ID you click on it it gives your publication it reported at - so you - if you want to make it available to everybody and you wanna get cited you put the proper PubMed ID anybody who uses that should technically speak inside your publication there are minimum obviously we can't guarantee that the mandate is your licensing website you know everything is meditating encoded for every one of these modules so yeah can you tell us a little bit the effort and the amount of time to put in to sort of keep up with the changing landscape of these tools especially in these next-gen sequencing area it's two or three versions a year yeah so with reference data and changes in the tools like mass mappers there's probably a new version of is there a versioning time period yeah so this is an interesting play and right up here you can't see what it says package yeah version so you can control that now let's be honest this is very tough okay because it's not only the versioning it's where was the package built in which compiler compile it because if you use two different compilers you're not guaranteed to get the same results so it's really complex but we have thought about it and under one of these additional additional paths that you have here at the bottom there's a whole collection of metadata additional that you can store for every tool that you view so it's not ambiguous to anybody what what operating system was used on version of the operating system optimization or without optimization company you know there's a lot of things that you can store in metadata now we obviously don't have the manpower to keep track of all these but they these the schema is there for people to keep track of their things now when it comes to the library I'll show you in a second in the pipeline keeping track of all the different pressures is very very difficult just like your system administrators here I'm sure have a heck of a time trying to keep different versions straight and when you type MATLAB which original MATLAB shows up this is a complex thing and I don't think there is a unique solution but we have many tools that have version for versafine we actually keep them that way and myself for four so far same modules at least it'll be important for this reproducibility thing because you're gonna want to run the analysis and get the editors give the readers and reviewers the ability to run the analysis just the way you ran on the versions of the data and software that you ran yeah she's gonna be a requirement yeah yeah he's with protocol provenance the data provenance he's the protocol the entire thing so did somebody I mean I was at the conference just earlier in the month and I'm asking this guy you know he reported some really interesting tools and I'm like can we use your too and he was I just go to my paper look at my methods and you can implement it and like yeah very good idea I'm pretty sure I can you know we've all seen method sections in papers right it's a very terrorist rudimentary there's no way to implement and get anything close to what they've reported by looking at the method section of a paper right so anyways conceive got things like bowtie in there how do people actually move that much data into the system interesting so when it comes down to one of these really huge data sets by Trinity and all kinds of our things were literally hundreds of gigabytes of their own SS we need to be honest there you either have to have a robust connection or what you got a group of suitcases I mean we've had some of these situations we move the suitcase in the data center or put the hard drive slot I mean and that's the fastest way to get it how are you allocating resources like Jim's shop could say hey I really like this let's do all of our X Gen analysis on an evil system this is the next series of our existing hardware our data doesn't have to go to USC you know it can run here it's different we we once we you know amazing connections cannot be reliable for over five hours and let's be honest you know all kinds of interruptions do happen and when you have one of these want to gigabytes multiple cases you're most likely not going to be able to transfer you can start the process but you can't disconnect you know it's just for that we're kind of Gnostic because we allowed we'd occurred cloud infrastructure to connect now how reliable and then volume size it's an issue but the server can also be local to a site my server very powerful which that it's got essentially more powerful in terms of RAM than the USC one but not as powerful in terms of compute notes this is what the website looks like and if you go to Webster riderhood that's all you're starring and there is a great advantage that I may show you later we have an image on Amazon ec2 you can actually if you have your notes and you pay for something you can just use our pipeline image to to start a service there now this is another HIPAA warnings of any sort no hippo warnings because because we you know this is a framework now how you use it and how much you disseminate of your data that's up to you now just in terms of a disclaimer some of these connections let me see if there is one here there isn't one here but oftentimes if you connect raw DICOM files then have patient identifiable information in them when you connect one of these to something that requires analyze or miss the input automatically don't be a smartphone that pulls up here and it actually removes everything with the exception of six or seven things in the die comes and makes a file that they identified so we do these things for some cases but obviously can't rip the system by in place in that comparison whatnot so you know it's not this is just a framework she's not a HEPA regulator of any of any sort of the imagination so this is what the GUI looks like on the very top you can see multiple I can open multiple canvases these are multiple protocols here on the left hand side is where my libraries and let's see if I can well unfortunately it's not shown here but but if you have multiple versions of our package it will be listed with with multiple multiple times here each of these things can be expanded so so you see theta individual modules in workflows in workforce you can find informatics you've got imaging and so forth so that's the library here and every library is associated to the server we can have a whole sermon that has none of these libraries the pipeline is not including the tools unless you want them the distribution allows you to download all the tools but you don't have you doing a storeroom here you don't need an every step and then right up here where the main camera sees you is the input of the data here is different workflow processes puts are those data sinks Davis sources and data sinks and then when module complete they they turn green no he's wet wet wet this is 20 percent complete it says so when they're not completely different color you can inspect the output within each stage with the mouse you can look at me so here's a little bit about the status it tells you how long has the workflow being we can pause stop reset and all that business and in the right in the corner here how many connections you one client can connect to 15 different servers okay and happen and and and have processes running on over and here is the server utilization and here is how much of your motor you are utilizing at any one time so that's roughly speaking how the the front end of the application is and here is again just a little bit about the classes pre processing tools alignment tools the normal assembly basic and advanced quality control tools that we have a variant calling annotation copy number variation and so forth and this is worker you know some of my colleagues at University of California Irvine and USC so the people are doing making use of those things they have to know about all the parameters and all the parameters exposed because a lot of this stuff is really subtle yeah no absolutely and so if you take one of the workflows that we've published it's going to have these parameters preset you can modify you can live in the way with it now if you are using simply the module definition the mantra definition doesn't say where all parameters should be you have to have the knowledge of it's the parameters are option somebody quite so so you need to have a little bit of expertise if you're drafting something from scratch you need to have you know a little bit about the background how is this implemented what should be the module that falls in you know you have to have a little bit of expertise to design something but if you are if you're trying to replicate somebody else's results you just get the pipe XML file and you and you just replace are the datasource of the cup and you replace the data sink on the bottom and you run it now it's not guaranteed to work because maybe they they have a slightly different design than you you're putting half the data so it's gonna real until it hits a problem and it's gonna read module so Brown modules are gonna occur and you have to troubleshoot but at least you aren't going down into a 2,000 lines script the only computer can read and find out what's there here you know we have exact each of these modules has input stream output stream output results it reports will you exactly the invocation command line okay so you can see the parameters that there was involved with if you won't be so troubleshooting is much easier than going through scripting the family is so here is a protocol here's an example alignment of yeah that's a very good point we have something like this we have I know how sophisticated but we have to - we like to call intelligent engines for controlling the workflow the first one is to simply literally do a phrase search I want to do a task of this sort and it spills out for you a few little modules or workflows that have something to do with that now obviously we're not going where most of our things are wrong a second thing however which is very interesting is if you start designing a new workflow get this you put them on you you drive from the library module accounts put it in and then you right-click on the output of this module and you ask the system to predict what should come next and what it does is it looks at the past three months of executions how many times this module is all about module a module beyond what you see and based on this frequency of invocation and that specific stream line and suggests the next Vol'jin so it's got a semi-intelligent building and cheese is what we have one or a burner a list no it shows you a list but they're ranked okay right it shows you a list and they're difficult okay so yes so there's two types of users right so the first type of users are what we call expert users that know how to design a computational protocol end-to-end and they're the ones that would either use the existing module definitions or they'll define a new to themselves and draft the workflow and then and then they pipe their data through that's case number one case number two is really two systems and other people that do not necessarily have the expertise to design the workflow protocol but they want to use what they said was very often in practice let's be honest I will replicate the result of somebody else so you only redesign the workflow you are using the workflow with your own data or perhaps with their data so these are the type of users so in the living world somebody you know has to design the workflow for you so you have to get down and find out how to design them now do you have to stop here this was meant to show you guys how how some of the software tools come into play to generate really interesting stats now let's talk a little bit about shape okay because I'm I'm passionate about shape okay shameless you know in terms of sin listening so probably through sheer restructure right you look at this thing in you're like these are important things because important in metabolic activities heart motion we did a study another while ago where we looked at the shape of the heart and through all the it's cycles to find out if we can do essentially the principle component eyes on the shape not not on univariate measures okay and then when it comes to brain atrophy we've done a bunch of things okay we've explored it here you can mark McCormick's natively you can Marlo it as a genus 0 surfaces where it's called the surface of the tennis ball you can borrow it on a pyramidal shape mathematically you can flatten a Matisse core as a rectangle and so forth so there's a lot of interesting work that comes down to representation of high dimensional nonlinear spaces okay Spacey square measuring distances is not opinion now when it comes out to that I will show one example that one of my colleagues in and we did over the past few years so we had the following problem okay the hippocampus is a very interesting region in the brain right in the middle temporal lobe where my thumb's are right behind ears in the middle and they're stored they they look like small bananas if you want you can smoke bananas and it's evolved the memory so it's a very intricate object to study but when you extract two hippocampal from two individuals you can establish languages between it's difficult okay so how can you do statistics of things that don't fit together you can't okay so the knowledge is a very important thing so we designed this method that's based on partial differential equations and all of us know about linear decomposition eigen vectors eigen functions and so forth well imagine the same situation now where we applying this linear algebra or linear theoretical results on my phones okay on operators in the operator features is something called a low-pass filter my operator which is an extension of the laplacian okay the laplacian is for flat spaces two three four or five flats Euclidean spaces you know right and so it's basis now when you have a manifold however things are a little bit more complex you need to have the metric up on the manifold that tells you how to measure distance incorporating this and then you're gonna be looking you can think of this laplacian on the manifold has been an operator here and you're looking for functions okay and eigen values that satisfy this equation okay there's something very interesting happens when you go ahead and rank order the eigenvalues okay and correspondingly rank order the eigenfunctions that solve that composite okay something very interesting emerges because as you can see in this diagram this is that this is the first eigen eigen function next one is gonna be the second eigen function it's just one subject the third eigen function you can see how you can begin to get local the representation of the topology of the object now imagine a whole bunch of these guys now zebra are labeled here you know now this allows you to more or less establish what is the Hat they are the hippocampus which one is you know look at how different they are okay shameiess based on this now you can establish a one-to-one correspondence between the geometries of these boundaries the hippocampus and once you have that and they you're up to the races in terms of the stats so this was an interest in advance it was highly cited paper and so forth now one last example perhaps that I want to show you because it is this study that we did trying to look at the outsiders decision imaging initiative data and try to predict which ones of the mildly cognitively impaired individuals are likely to convert to dementia in 24 months so we're looking at all the 2000 2000 or so different cases and we want to match them obviously for everything that we can think of okay at baseline there they're measured twice based on time zero and 24 months later beer to think of extending their assess these guys converted dislike it okay so they're all mildly cognitive impaired at baseline but some of them are not develop dementia some of them are gonna remain stable for long converters the question is how can we predict only using the baseline information and which ones are going to convert and which ones are going to stay stable that's the goal of this study and there has been a lot of work on these people have used you know tissue loss various tests vitamin E glucose hypometabolism we have used structure morphometry before now in this step but these are kind of unit you know you can annular it's you know this comes from from locally from University Michigan yeah people kind of focus on one thing and they kind of beat it to death what we wanted to do is we wanted to go beyond that a little bit and try to look at different types of data collectively so this is the ultimate one Florida was designed and I'll kind of walk you slightly to bad process obviously this was a big image so it could here to paste anything all together so this is the complete end-to-end solution what comes up so here is my data coming from different sources here's my process in Salida we get data and then some image analysis of the regions and then here is a classification module a little bit of combination here of things and then assess what that how good is the classification again once the accuracy was a power to detect converters once sensitive you know all these things are done at the last module and then you get some kind of result so that's the end to end workflow now each of these things here in the middle is actually a group of more so that's a function call in the middle of the protocol you call a function you know this function can be call you another function you know but he's amassed that group modules okay so it's a complex workflow that then has you know the other one has slightly different expansion here so the first thing that that happens is in that specific demonstration the data came from from global online personal clouds never sits on a cloud we pull it out of you lately the internal data entries here and then going through the district's extraction module kind of generates so these because they may remember is just three-dimensional wrong I mean you know it's just raw data you have three emotional Mary's at every lattice location in space XYZ you have a univariate intensity measure gives the contrast of the image you know so we model all these extreme the coracle surfaces so these are topologically two dimensional manifolds and then in addition to that we go ahead and postulate the brain in three dimensions right up here first on the brain in three dimensions and then we generate the boundaries of the dish in 3d and we can do a whole bunch of derive material so this left middle frontal gyrus and I'm sorry you can't see it but but this is probably surface area there is a whole bunch you can see here but those driven and 30 of these imaging biomarkers that we divide for every case right okay and then what we do is we provision to combine some of the non imaging data there is genetics data there is clinical data and there is demographic data that needs to be fused collectively for joint processing and classification so this is an example where now you see here so the subject data April he knew early on to information is the genetic you know psychiatric inventory so this is cognitive test and let me see you in the window so he's a clinical dementia you know all kinds of other things are faced together and then at the end make we're appending this as you can see with ritalin and so with a 300 vector of imaging morphometrics now and we have all of our cases every case is aligned now this is the foundation for the hierarchical classification that comes in okay spire simply labels every cases one or two or whatever it is labels have a case and then at the end what we need to do we append this we append this at the very end because we want to find out you know how much agreement is there between the label and the actual state of the individual is either converter or non converter and ultimately you know in a nutshell here demographics their genetics clinical and extract the top few imaging biomarkers so for example it's the baseline the gyrus rectus and these are just the volumes okay so we have a table that's more or less complete and then at the end of the game this this module over there kind of gives me true positives false negatives two negatives and false positives and then it computes for me sensitivity specificity power to detect and accuracy of the classifier and then that needs to be fed back into someplace right typically doesn't stay on the server because as soon as you reset the workflow everything gets wiped out someone has a keeper after that so in this case we feed it back into Globus online and and here is the ultimate result that that kind of shows you how good is this thing we're not perfect and there's a whole bunch of false negatives false positives and so forth but if you look only at the seven most important your imaging markers your sensitivity to detect the individuals that are likely to convert is eighty-one percent specificity not as good but the power to that converters now you want accuracy seventy and then if you increase how many of the imaging biomarkers you take then you get a fairly decent prediction here now there's a large team that has worked on this you know I wanna acknowledge my colleagues of funding and so forth most of this work was done at UCLA before I drove out September so these are the credits and now you know that's a good place to stop I'll be happy if there is interest and time to maybe do some demos oh great [Applause] yeah so the pi pi server runs on top of the oracle greed engine or the sin of green engine that actually schedules and maintains the the job try theoretically it would be possible to just take the server and run it on a on HD and even he doesn't acquire hg but typically it works much better for high throughput to have a schedule require sure to pilot system across on the server and on the server and it does require any network [Music] yes yeah although workforce are typical supplements of these papers in now obviously you've gotta keep in mind that you won't be able to run it right away unless you have access to that server but if you download the pipeline is free right so how are you installing it you serve proper tools and you in place the locations of where these tools are in the workbook it most like it were roughly and obviously all these resources d connecting the pi PI server has 4520 nodes at USC and anybody can get an account there you just need to commit and I've already form so that's not really a hurdle per se although so even they not gonna share that data you know whatever itself that could be issues but so that's the concept you want to use more than forty we have forty course here in the School of Nursing and he said whether you said somewhere we could sign up and there's like forty four days but then if you will the project be like a hundred yeah well that you have two options option number one is to submit a collaborative application and running them at USC option number two is down dr. Brian he's got the power I think we should if you were to sell what's the difference between this and stubborn ah yeah so there are many alternatives I do have a table it was not hearing these slides there's the burner there is galaxy there's Chaplin there's no bunch of other things okay and galaxy is probably the closest competitor there are also Java based build a java-based but but you should look at it okay it will be very enlightening to try all these things to see all the differences the difference is that here with the pipeline you can design really complicated things that run and not just things that you wish fun and and and you and you can you can share these things and you know you can inspect every single at any one point in time you can inspect the state of any one of the modules or galaxy you can it's one thing you submitted and you can't close a session you gotta stake you wanna stay open you close your laptop it's done you know there's a lot of things it's not graphically it kind of says it's almost like a script with a graphical interface step one step two step three and you can kind of track but but it's not as interactive for example we can't branch there yeah sweet cheese we have ears it's a slightly more complicated language that's behind it then enables the interoperability in the workflow design then it's available is it that recursive calls no no you can't have loose no you cannot well let me take it back you can't have special type of loops but you cannot go from module ABC and then go back to a because because you don't need that you can do but there is a way to do it eration okay there is a notion of an iterator iterating module which you you typically want to skip out I guess the three steps you want to go over five times and the third imatra says do this thing end times and just four nine times and exits and things like parameter search like running across different parameter values like a grid search this is very interesting one of your data sources on the very top could be simply a list we do listening all the time because we have tools that we just don't know for this data what's the optimal setting for the stiffness of the more phenomena we just don't know so we don't know how bunch of them and because they run in parallel it takes the same time is there any one and you expect the results this you know randomized and once you pick the one that gives the best result then you go with it but basically to specify a whole bunch of parameters you open a data source in against 515 you know you mr. Hubbell you can have thousands of them and what once you plug that in the next module it's going to run 10,000 times with each of these parameters individually and it's going to complete and where you have to have a mechanism for selecting the best yeah so there is some of that and to be honest with you you know our philosophy has been mostly driven by open science so we tend not to include stuff or use tools that require special licenses because it's a horrible we can distribute it you know it just it's it makes our protocols non replicable necessary so we tend to avoid these things and we've been approached by other people and I know people have used that the pipeline to expand it and do other things with it but we don't necessarily play that game so do you but close the set of functions or could I take my SuperDuper function that I developed the pipeline itself is too agnostic none of the tools are part of the core the core is a superior Java the tools they can be written in any language they're building simplify understand systems so it's complaining about weight the distribution of particle DP s is stupid the pipeline server that you stole cloud or something we allow you as an option to download all the new imaging tools that we have and all the genomics tools that we have that's option some of the new imaging to take two days to install Python it says takes five ministers and you can run the pipeline without any of our library you can have your order like this you may or may not want to share some of your libraries it's good luck to you can view that workflow definitions and editor is absolute on the only crack i mean that's what i can show you there anything can be modified because to the group and if you're really technical you can actually open the XML file and edit it there you know I don't necessarily recommend that because you can goof up something very bad but you can open it you just xml your XML hierarchical structures so you can edit anything either through the GUI you can edit you know all the metadata after version package license from an IDs you're outside the inputs/outputs executable syntax extra metadata for the other invocation system specs and whatnot so you can edit any of these things the types of the inputs and the outputs you can modify once you have the XML and loaded in the cannabis you just communicate it's always only oh okay so is it they can be they can communities I think there's one of several types of lines that you can connect to modulate one is obviously piping day of type a Type B and the time just must be comparable for you to otherwise the line turns red it doesn't want to connect but one of these is just something called flow of control just call the next module when we were done don't pass anything to it so we just a flow of control city happens I'm done next guy can proceed I think I have a simpler question which one - spits out the vampire can write it or it can just type it is it possible not to write it it's not because okay now that's a very interesting question why don't we why doesn't the environment take the data stream and feed it directly in the next module without going through the file system and the answer is that's what the Verma and some of the other ones do it's a big bottleneck because you can't have the environment manage the data for a thousand users we do have thousands of users thousands of cases and thousands of gigabytes it's just intractable so that's why these environments Kepler just point examples you know everything I've seen on camera is a toilet it's an academic exercise they can't do any high to put analytics in camera simply because they insist on the the the software you use it needs to be compiled against the capital the capital libraries the environment is built with the libraries and the benefit is that you actually store you know you don't have to go to found system to see you send everybody knows about all the data you know it's a big benefit but in reality for big data setting it just doesn't work so for us we're abstracting we're seeing over here we don't care how they get saved on the you know the fact that could be delaying the data being safe you know whatever it is as Americans we we simply store references to data in the environment which is very lightweight now have to death but that's a good point okay I mean I you guys the rest of you you don't want to see it I won't be offended oh you know what don't judge me here [Music] Preston can the teases aren't having USC to support that shows that the you know part is a Carlos truth and burn know the whole Latin America all right on councilman is I mean we have clear signs when some of them are a private they're mostly not on the core developer they're on the application they're likely to use the pipe without necessarily talking fundamental talk so let's see you know I was talking to you guys about this web star let's see if it works okay [Music] so again I won't be easy way to do it Oh Alex didn't get one let me install that should we did you know we install will you ask for but we didn't actually I didn't know what to task okay that's okay okay if this thing doesn't work if he doesn't let me start I can pop in my computer and show the client I'm just now only the client now if you own the server you need to download what's called the distributed pipeline server DPS if you want to build the backend this is just a client so you mentioned yes yes you can grab all the videos is that okay okay so so where is it at the bottom there okay all right so this is this is my my client let's connect to here to see you okay that's right okay now I'm connected to a server right so okay so this is my connection now I have one session that I've started some time ago I can actually check check this session and notice a so ctrl + ctrl - allows you to zoom in and zoom out in this thing now let's see if this thing works if you bring the mouse over zoom out a little bit oh by the way this is that the smart line that I showed you guys because this is a nifty file compressed and this requires a file of different type so it automatically on the Flies it converts but if you bring a mouse over an output okay this I did this was not available now here is an output okay you see it kind of shows you the specific things you see how you go from this that looked like a dart here is a little bit different here and that's what it looks in this step and then relabeling is gonna make it some somewhat flat or something I'm thinking and now you can go ahead and explore now right-click on this execution log you see this is the exact command minor where that was called here's my output stream nothing really interesting error stream nothing interesting now I can actually see the output here I can pop it open in 3d okay and I can kind of scroll Brian did this is one of some of your beta by the way yeah so I'm kind of scrolling if ever I can picture the three-dimensional volume and now and now suppose now I'm thinking about this but I want to change the liver so I do control a copy control new paste it in and I have my new alternative version of the very same workflow and now I'm thinking okay this module you know it's got a whole bunch of optional parameters you know you see now this is not gonna validate for me because some of my things are not specified see that the error on this parameter needs to be output is missing whatever you know that I need to get my act together here and namely these three so if I were to this is a number and I just said they're some kind of a number here and this is what this one let me just take these guys and now this is going to be a perfectly valid work for its cause okay the reason why it's not is it see the input data tells me yeah look at this thing this this machine here doesn't have a see whatever okay so let's go ahead and pretend okay let's go and pretend now I want to fix that issue so I'm going to reset and when I say like I just look for cranium modules data okay so let me just grab some data what's going on let me just get this guy okay this guy C Drive my C Drive doesn't have that I'm gonna get rid of this and I'm going to try to do it understand correctly it's possible to actually stream that they have from the clients to the server absolutely yeah she puts a smart line for me yeah maybe I don't want the smartphone I can D say nobody for a while but you gotta be careful with that and then our control B to validate now it's gonna be happy okay now what I'm going to do is I'm going to put perhaps a why not why don't we put a new data sink here and I want the data to be to come back to me here okay see Dropbox Amazon whatever I wanted locally okay and now what's going on here I need to let me just call it the results and I want to connect this guy over here and now what I need to do is I need to specify a local host okay why don't we just dump it on my desktop why not it won't okay do you want to give it today I want to just call them No so a slightly more interesting example would have been when I should just stop so failure okay the first module fails so I hate to go into the output and say what's wrong in multiple images detected it's a file okay maybe you should have left in the smart line make sure this is kosher here and now I just wanted to show you suppose this guy I want a replicate here and I want to indicate branches okay so these could have been in that right what okay so image I can put in here you know say I can put this you just indicate some kind of a branching process let's see if that's any better so by the way in that specific case the data did come from the server itself because you never had pulled it out from my server library but if I had anything on this computer that's the first time you know I'm longing this computer obviously we don't have data now it reports the status now ready okay so that guy's done that that is happy two things that can run in parallel will run in parallel so the same situation if you had let me yours pardon me here I'll just stop this for a second and maybe the situation of having a whole bunch of data set so look let's go ahead and pretend we have our ten cases okay pretend we have taken case now they're the same subject here okay but it's not important look what's gonna happen now now this guy is going to run 10 times in parallel but it's gonna take the cinema amount of time and I can actually see here with my usage I'm using my strong view because I've got 10 cases the server is fairly underutilized 5% you know so I'm doing ok and you can see the server is over 75% you want to see the speedometer kind of dancing around as it jumps between modules I mean so it's picking a little bit here 20 jobs now 20 jumps because now remember I am at this stage I have 10 here and 10 here rising it so and all the beauty of all this is that now now see you later you know I'm leaving no I don't want to save it I'm living and I've disconnected I'm home I walk in my home and then I go back to same thing starting the same or the same client that he connected and it's going to show me what a pro now my server has all the junk to deal with right scheduleanywhere so I need to reconnect here obviously I'm at home ok I see the connections I recollect in authentication done those days it's checking the library I have to search Astoria but last time I had one here's my entitled work father I just said a second ago okay look this guy was waiting because I was disconnected and it couldn't type the data back to me right now there should be you must see the banner here I guess right up here there should be ten cases or something right up here on my desktop there there again now it was waiting for me to reconnect to Piper back into my desktop because you can be on a mobile device [Music]