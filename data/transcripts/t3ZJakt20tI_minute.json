[
    {
        "start": 0.0,
        "text": "hello everyone thank you so much for inviting me in this seminar so i'm arnold bescholer i did my phd initially in paris at university valley suite balisackley under the direction of kamiko and stefan dupa and i was working during my phd on biological invasions and it led to the development of these resources that i i will present today that is kedzal ketzel is a c plus plus library that aims to help programmers to develop new models to integrate distributional demographic and coalescence models and i defended like two years three years ago and i came to university of michigan under the direction of lacy knowles in the eve department ecology and evolutionary biology and now we are mostly trying to work on methodologies to disentangle "
    },
    {
        "start": 63.68,
        "text": "uh like what is considered as species-related genetic structure from what is actually like just partial structure of widespread populations so i will uh present you kedzal through through this presentation but first i would like to introduce you to the main concepts of this resource is the resource for environmental demogenetics so environmental geomod genetics aims to analyze geographic patterns of genetic diversity in order to inform the influence of environment on demographic so it raises several questions what data do we use typically to inform these processes how we formalize these models like that is like what statistical model we actually come with and how to extract the information from the genetic data that we have that is like the "
    },
    {
        "start": 125.36,
        "text": "question is what inference method can we apply and also how to implement the method that is like what tools are available and this is the question that is central to this presentation so to give you a better idea of an example of a concrete example it's the example of the asian hornet invasion in europe so this by villatina nigrito rocks or yellow lake hornet is causing some damages in [Music] western europe it was first encountered in south west france in 2004 and he had a very fast expansion after that um it presents economical and ecological impacts uh mostly because it's a predator of the honeybee and we know that colonies right now of honeybees are kind of "
    },
    {
        "start": 185.84,
        "text": "already impacted by pollution by disease by climate change and on top of that they have now to face a new predator so it's uh it's presenting like some problems uh we had access to a genetic data set that was um that was sampled a few years after the beginning of the invasion so 84 female genotype that's 22 loci uh 22 microcephaly so on the bottom left figure you have an idea of the process that is happening so the figure shows you in black the black dots are observed nest of this avilutina and the levels of colors give you an idea of the date the time at which the observations were made so this map is kind of old now the the population of espartina "
    },
    {
        "start": 247.12,
        "text": "has spread through all western europe it's found in portugal it's found in spain italy united kingdom germany even and to try to understand and inform this process what we use is the data set presented on the right so each line is an individual uh and for each individual you have like latitude and longitude at which it was it was sampled and you have like a list of loci these are the allelic markers so you have like two values that gives the number of repeats in the sequence so the all problem that is presented through this presentation is trying to inform the process on the left with the data on the right to do that you will need like a general framework modeling framework that i present here where the landscape is central that is it's partially explicit so across the landscape you will have "
    },
    {
        "start": 307.199,
        "text": "some data about environmental variables like rainfall or temperature or proxies of these variables and you will want to represent the growth process of the population conditionally to these variables that is like here what i call niche functions that is like what is the function that relates for example the temperature to the growth of the population we will see that instead of raw variables like temperature you can actually use proxy like suitabilities suitability maps is possible also because it's an explosive landscape sometimes you will want to specify very very detailed dispersal modes uh using these partial kernels these distributions relate the probability for an individual to disperse from point a to point b as a function of the geographic distance "
    },
    {
        "start": 368.24,
        "text": "between them so the the larger the distance is the lower the probability of dispersal is but there is concretely many ways to specify these distributions what happens is that for niche functions and dispersal functions you end up with parameters that are usually not known with pre precision and you just have some vague prior information about them and you want to better inform their values using genetic samples so what you do is informing a demographic process and conditionally to this demographic process you generate gene trees that establish like genealogical relationships between the gene copies of your sample so that's the main idea of the model we address here the main challenges are of course to choose the cell models for example this personal niche function "
    },
    {
        "start": 430.24,
        "text": "to specify them according to your biological case and it's not an easy task the second point is to estimate the parameters from the genetic data and for that you need a statistical inference method so the main problem when you tackle estimations based on such models is that embedding landscape heterogeneity totally complexifies the model and to the point where the likelihood function of the model becomes intractable or even impossible to impossible to to express that's why generally people rely on simulation based approaches like approximate by asian computation or abc if you are not familiar with abc i present you here a very short summary uh there are many ways to do that it's the simplest example uh but it will give you an id so it's based "
    },
    {
        "start": 491.199,
        "text": "on a simulation rejection algorithm so first you come with a prior distribution on your parameter space so i give you here like just an example case of a model where you just want to infer a monodimensional parameter so it's like the horizontal axis the only knowledge you have prior to the experience the only knowledge you have is that it's somewhere it should be somewhere between 0 and 1000 and since you don't have more knowledge you choose a uniform distribution to represent this prior and you sample some parameters that are like the right ticks on the your on the horizontal axis and for each parameter you will simulate data under your model and that's the the y prime that you that you simulate and then what you do is basically comparing the simulation to the observation you "
    },
    {
        "start": 552.399,
        "text": "come with some distance between observation and simulation and based on that you weight the parameters so the most basic way to do that is if the distance is greater than the threshold you just reject the parameter you say okay this parameter values leads to simulations or that are really too far from what i actually observe so i have no reason to to think it's the likely parameter value and by rejecting the parameters point like that you can update the posterior distribution like the prior distribution to a posterior distribution that gives you a better knowledge of what should be your parameter value conditionally to the data that you observed so that's an idea of the approximate biasing computation methods there are many different um variations on that uh to give you an idea of the complexity that is like the the data that you generally simulate "
    },
    {
        "start": 613.76,
        "text": "under the model we are talking about they are not monodimensional they are like very multi-dimensional these are genetic data sets with many individuals many loci and many different allele and so what people do is coming with big methods to reduce the dimensionality of these data sets [Music] although what happens is that rarely you have only one parameter to estimate most of the time you have multiple parameters it's a highly dimensional problem and so that raises another number of questions um the question i want to focus is what tools do you have to actually come with a simulation that is like you have parameters you have a model you have parameters you want to estimate but you need this simulation and for that you need some tools so if you are interested in that i advise you have a look at the yannick "
    },
    {
        "start": 674.399,
        "text": "author article 2020 they give a good description of the resources so they list several programs and i just selected a few here and they describe them so for example splash 3 is pretty popular it's a backward simulator that is it's based on coalescence processes coalescence allows you like to be a bit more efficient than for just simulating everything for once in time uh the level of interest is population based it's coding in c plus plus and it was developed by cura um you have also phylogeosine it's also population backward coalescence developed in java ibd sim is pretty popular too uh it's coded in c like the in c style by le blua and kothor and slim 3 is a very interesting resource in time they have like very efficient algorithms so it's like it's a really nice resource "
    },
    {
        "start": 734.88,
        "text": "and they came with an individual class that allows you to center the simulation on individuals and it's fairly recent um so what what the state of the art is right right now is that there is an abundance of simulation program but no library and that stage is highlighted by the some projects documentation for example a glib say that the underlying c plus plus like underlying their program uh has been designed with the aim of improving performance but it was at the cost of safety and intuitive design and so they recognized it's difficult to use the code components that they developed and they re-specified that in egg wrapper saying we actually strongly discouraged should try to use these code components same same thing for ms prime where they state the author state that "
    },
    {
        "start": 796.24,
        "text": "the low level code is written in c structure as a library but it's difficult to use it the interfaces change over time and it's undocumented so all the just to be clear all these resources are amazing resources like really good and easy to use and everything the problem is uh that the when you need a new simulation a new simulation tool without library the game is a complicated game so basically what you do is looking at your biological system coming with an idea of what the model should be to represent adequately this model and you compare that to the list of programs that is that are available and if you find a good fit you use the program so many people use splash or their ibdc and that's fine sometimes your biological model will present important deviations from "
    },
    {
        "start": 858.079,
        "text": "the programs in the list for example let's say that the kernel dispersion you want to represent is not a gaussian dispersion but in the programs nobody has implemented the dispersal you want what you would do is try to simulate data with the program on the list anyway and you make your analysis you write your article and in the discussion you say well in reality you know that the dispersal may not be cautioned but we could not like we could not develop a new tool and if your biological model that's the third case as really important deviations that make the simulation totally impossible to run practically uh then a new program would be required but since you don't have a library gathering the this amount of code that is required is just too much time consuming and so "
    },
    {
        "start": 918.079,
        "text": "you go back to two and you will try to force your biological model into a program or you forfeit and you use simpler models or more descriptive approaches but when you have a library the game is different if you have a closest feed in your in the list of available programs you use the program and you win and if you don't find the good fit you want you go to the library you gather all the components you need and you rapidly very rapidly you should be able to come with the new version of a program and run your simulations run your abc methodology and publish your data and and tada so that's why we created a new coalition's library when i was doing my phd because the the process we were studying with the asian hornet required this tool so that has been published "
    },
    {
        "start": 979.36,
        "text": "in 2019 in molecular ecology resources that's the kids alcohols library called for coalescence template library so it's a resource for sql express programmers and it integrates different models we will like come with a better description of that of course we like i needed to make some programming choices for the development so i chose c plus plus because i consider this language to be a good compromise between design and performances you can reach good levels of modularity and reusability without losing too much in terms of performances if you give as much information as possible to the compiler because it's a compiled language the compiler will come with ways to make your code run faster for example when you compute expressions mathematical expressions that are known "
    },
    {
        "start": 1040.559,
        "text": "at compile time like now modern c plus plus is able like to compute this expression like even before the program runs also what it's able to do is to give you like to identify bugs in the program not during run time but but during compile time and it's like really useful when a run can last very long it's frustrating when you have a bug that appears at the end of the simulation i chose to make a large use of generic programming template meta programming techniques because again it's a nice way to reach good levels of modularity and reusability without losing too much in terms of performance we will have like some example of that later and when i developed it of course i wanted to analyze my data set but i chose to focus on modularity and user experience that is the the code at the end of the day the "
    },
    {
        "start": 1102.24,
        "text": "code that you write using this library is still fairly complex because these are fairly complex model to to represent in the code but um the way you write the code is as simple as it's possible like i hope so like it's close enough to a good um a good code um so now just to uh to give you an idea of the capabilities of the library i will demonstrate in its use uh based on like the system i i have been studying the last two years during my postdoc and it's an australian wizard the name is ethero notre di noi but you like we don't care too much about that we can more about the tool um so i will present you how at the same time the mathematic mathematical aspects of the model that is the concepts "
    },
    {
        "start": 1163.36,
        "text": "uh that underlies the models and also the way that you can represent the concepts in the library using kedzal so first thing uh you want to represent the homogeneity processes through landscape in a landscape you need a landscape so you come with this concept of a discrete landscape that is a grid or lattice and you choose a number of environmental variables to embed in your model so here i chose to focus only on one variable and that's a suitability map as i said suitability maps are fairly popular in this kind of approaches because it's derived from species distribution models so you give to these models observations and a list of environmental variables and it will the model will use correlations between variables and observations to come with "
    },
    {
        "start": 1224.159,
        "text": "a map of the probability to observe your individuals in the landscape so the higher the suitability is the higher the likelihood that your species is actually present or lacks these environments in kedzal code the way to represent that is first to declare a single string to give the path to the suitability duotif file so geotiff is a geographic kind of a geographic raster so it's a specific format and then you declare a type the type of landscape you want to use you use a discrete landscape and you use two template arguments that are the string here that is like what id we want to have for the for this variable and integer here is the time of the time "
    },
    {
        "start": 1284.4,
        "text": "i'm using so generally you want to use generation time so it's an integer so you once you have the type you create an object landscape like an object planned and you give it the path to the to the file and sorry you give it the id that is like you want uh to access it by calling it suitability and the path to the file and then you give it the time that is represented that is its prison suitability it's like prison time so it's zero um what you can do uh is when you recall the bracket operator of the land object it will give you an object s here this object s is a callable is a function of space and time so it's very easy to use you can call s of one geographic coordinate and one time and it will return you a "
    },
    {
        "start": 1347.28,
        "text": "value so it's a very easy syntax and we will see later in the presentation that you can compose this function of time and space in more complex expressions something that is possible too and i had it here in common if you want to represent temporal eternity it's possible that is if you come with the suitability map for prison times but for ancient times another suitability map for example when climate was much colder or something uh it's possible what you need is like just construct the land objects by giving like an identifier for the first fit ability and the path to the first suitability map and you say that it represents suitability at time zero and then you give a second file here and you say that it represents fitability like 1000 generations in the past so it's possible um after that "
    },
    {
        "start": 1408.88,
        "text": "since you have represented your landscape you want uh to initialize the demography um what is usually done in this context is to consider an ancestral population that is not special that's right fisher population with like a number of upright individuals all deployed and it's assumed non-special but at the more recent time sorry at a more recent time you introduce a given number of individuals somewhere in the land landscape so you have two parameters here that's t 0 and x 0 a coordinate and time of introduction and then the following history will be especially explicit how you do that in the casal code you declare the core type the simulation cover uh so the like c plus plus is a type language so you will spend some time declaring types so here the "
    },
    {
        "start": 1468.96,
        "text": "type of the core is especially explicit and it uses geographic coordinates a type a type of time that is an integer and then these two template parameters are interesting it's the demographic policy and coalescence policy so what it means is uh so if you're familiar with policy-based design in c plus plus you would you would have like it would be clear so what it does is uh specifying the demographic algorithms that should be used and the quality some sex algorithm that should be used so it's a way to um to inject some specific behaviors into the library components and we will talk about that a bit later and then you initialize your core you build like your first score and you say uh you give it the three parameters that you need and then you set the size of the ancestral right fisher population size "
    },
    {
        "start": 1531.919,
        "text": "um and that's it then you need to parameterize the [Music] the special process for that you need to have an idea of the demographic growth processes uh you want to represent what is usually done is to focus on two quantities that are the growth rate and the carrying capacity so the growth rate or r sometimes is assumed being constant across the landscape uh as like for the demonstration we will make this assumption but the library doesn't care and for the carrying capacity uh what you what people do is that they define it as a function of the suitability map so what these folders were not familiar with the demographic logistic growth process this figure explains the impact of these quantities so you have the demographic process "
    },
    {
        "start": 1593.52,
        "text": "running through different generations in the horizontal axis and in the vertical axis you have the size of the population and the carrying capacity is the maximum number that your population can reach at this location so in high suitability locations um you want this carrying capacity to be quite high but in low suitability areas you want this carrying capacity to be lower and the growth rate uh just affects the the speed at which the population grows towards this plateau so the way to represent this concept in kedzel code is that first you create what is called a literal factory so the role like the the role of this little lead object is actually to transform the scalar uh here like 2.0 into a function of space and time so when you run the second "
    },
    {
        "start": 1654.0,
        "text": "line or equal leads of two uh r is now a callable like a function that you can call with the coordinate and a time argument again it will be very important when you we will like design more complex functions later of course most of the time when you design a simulator you don't want to hardcode the values of the parameters you usually read them in a configuration file and you build an option like an options object in the code based on this configuration file and you read the parameters when you want them and that's the meaning of the commenting line here when you complete on a value that is read as a double in a dictionary in an options dictionary so here you that's the way you would you would build the growth rate function um when you build your simulator "
    },
    {
        "start": 1714.64,
        "text": "the second value like the second quantity the carrying capacity uh we will play a bit more with that so we will complexify a little bit uh let's say that you want it to be scaled by the suitability value on continental continental cells and for the ocean cells of your landscape because you can have oceans and and sea and you want it to be null to to be zero most of the time but you want let's say you want to enable drifting like individuals drifting from the mainland through the ocean dying most of the time but sometimes reaching an island well if you want to do that it's easy and at least it's possible so you first accept the suitability function of time by accessing it to the lamp that's the the id behind the first line of code and then you declare "
    },
    {
        "start": 1774.88,
        "text": "k the carrying capacity function it's a lambda expression for those who are like familiar with c plus plus and you just capture random number generator and you capture the suitability that is that was defined just before and you want k to be an expression of space and time so it's the meaning of the second line here you have like two arguments is the coordinate and the time and here is some pseudo code if it's a notion say you want to return zero with probability 0.9 or 1 if not and if it's a continental cell you scale the suitability by the value 100. again all the values here don't have to be hard-coded and that's the concrete code you would type so if the suitability is less than the threshold like an epsilon threshold you built a bernoulli distribution with the probability here "
    },
    {
        "start": 1836.399,
        "text": "with the if and then you sample in the probability distribution if it gives you a true you return one and if not you return zero and if it's not the case then if you are not in an ocean cell you are in a continental cell and then you return the suitability scale by the value you wanted so just to be clear of all this code is surface level in the library that is that's basically the code that is in your in your main so it's almost like scripting your main you don't have to modify the components of the library so all of the code i'm presenting here is uh just directly accessible by the programmer you don't need to modify anything and if you're not happy with the model you just like record that like few lines and you totally changed your model based on these two quantities uh growth "
    },
    {
        "start": 1896.48,
        "text": "rate and carrying capacity you can come with the more complex expression that gives you the number of children so let's say you want your number of children to be stochastic you want to sample it in a poisson distribution that is parameterized differently for each location of the landscape for each time of the story so that's the sense of the g expression the g expression is the logistic growth that has an expression here this expression is fairly like it's used regularly in ecology the question is how you represent that in the code in a way that is modular well you define a function composition here so auto means that you the compiler will automatically deduce the type the type will be actually will be extremely complicated and you compose the function the different "
    },
    {
        "start": 1956.799,
        "text": "functions you want so population size here can be accessed through the library interface then you you transform the scholar one to a function of space and time you add it to the function r that is already a function of space and time we just created it like two slides ago and you do the same for the second part of the fraction and you use the carrying capacity function here so this is a very easy to write it then what you do is that you capture this expression within a lambda expression again you give some random number generator as arguments to coordinate the time you create the poisson distribution that you parameterize with the expression at this location at this time and you return the value of the dislike the value sampled in the description so now you have like kind of uh like a "
    },
    {
        "start": 2017.039,
        "text": "small angina with small simulation engine that represent the growth process and you can forward that inject that into the the other simulation engines of the library another aspect we did not tackle this dispersal so i will like there is plenty of mathematical description but we don't have to spend too much time on the details one way you may want to represent that is using um complicated dispersal kernels uh so you have like fat tail kernels gaussian kernels uh you have like at least 14 of them has have been listed in the instituted by nathan i can find the reference in the question if you want um and so these are complicated ways to represent them and here like let's do an easy way you just think uh of immigrants as being a fraction of population at a given site at a given location "
    },
    {
        "start": 2077.52,
        "text": "so you have just one parameter that is the immigrant rate you consider the four neighbors of the cell that is like north south west and east and you spread these individuals across these four locations like you spread the immigrants across these four locations in a mutinium multinomial law to represent this um dispersal concepts you just declare these two lines in kedzal that is like one the demographic policy i was saying that it's a way to specify at compile time the kind of algorithms that will be used in the simulation saying that it's mass-based allow you to use algorithms that conceptualize populations as being masses that can be split indefinitely so it's good when you have many individuals being split on but just of a small number of uh "
    },
    {
        "start": 2138.24,
        "text": "of coordinates so here you just have like four families four coordinates sometimes you will want to use kernels like dispersal patterns that spread few individuals on a very large continental scale for example when you have like long distance dispersal and in that case you will want to use a different demographic policy that is the strategy individual based strategy and again this will automatically change the algorithm that are used in the internals or the internals of the simulation engines and the second line is just a function that gives you like that's a way to get the four closest neighbor so it's just like it makes it a bit easier um if you don't do anything you will end up with like immigrants going in any duration "
    },
    {
        "start": 2199.68,
        "text": "with the same probability so for a number of ecological reasons it's not always true for example if you are on the shoreline you will not have one-fourth of the immigrants that will jump into the sea so you want this immigrant to not go there but try to go in nicer places around that's where the friction function comes in it's a way to represent friction through the landscape when a cell has a high friction value for example the ocean immigrants go there with very low probability but when a cell has low friction value immigrants tend to go there with high probability so it's a way to change the parametric parametrization of the multinomial law that you are using again defining this friction can be done using the suitability what you do is coming with an expression "
    },
    {
        "start": 2262.0,
        "text": "h that is again a lambda expression it's a function of just space if the suitability is less than a threshold that is like you are in a unsuitable area that maybe ocean or deserts or places that are not nice for your species then you return a high friction value like 0.99 the highest value would be one um and else you are in a suitable area a nice plate for your species you return one minus the suitability it's a common way to model the friction and then you build this partial kernel based on this information you have a facility function that is called make light neighbor immigration and you give the immigrant rate the friction expression and the function you want to use to find the neighbors and you have your dispersal engine so based on all of that "
    },
    {
        "start": 2324.48,
        "text": "is very straightforward then to simulate the demographic process you consider you want to consider the effective flow of migrants that is the number of individuals that actually spread from one place to another at a given time and these flows allow you to simulate the population size at every location at every time of the landscape you just um over the converging immigration flows and the way to simulate that in the code again is just like one line record the method expand demography on the simulation core and you give it the sampling time like 2021 it's the time at which you want to stop the simulation you give it the expression that was used to simulate the children and give it also like the the kernel expression and a random number generator because you want stochasticity "
    },
    {
        "start": 2386.16,
        "text": "so here i will give you like two examples uh i used uh for like the last two years in north australia i was playing with the simulation framework and it will highlight different processes so here you have uh across the landscape uh the levels of colors give you like the population size at the given time so you have this population like spreading through the landscape what you can see is that the way i parameterize the model it leads to very frequent events of colonization and extinction in areas that are not suitable for the species so all of these red dots are like just individual individuals spreading from sources populations to areas that are not good for them and they try to survive but they die and at some point they finally "
    },
    {
        "start": 2446.88,
        "text": "reach like a secondary area that is treated for their growth and that becomes a second population source and express again express migrants across the landscape so if you want a very stochastic population pattern through continental cells that's a way to do it we will have another idea of what the simulator can do with this simulation so this time i didn't really want like to express uh extinction fast extinctions and recolonization events that's why you have a very continuous spread across the landscape what is interesting and what i want you to focus on is like the small oscillations near the seashore on the shoreline these oscillations are actually that's the continental cell sending some migrants into the sea and they die with high probability "
    },
    {
        "start": 2507.2,
        "text": "but sometimes they can reach a bit further and that's why you have this delayed colonization of this island here that is like the mainland is colonized for a long time and after some delay you have this colonization so in terms of expected genetic patterns under this model because the island is geographically close from the mainland but still uh lowly like not very well connected you expect some genetic structure between mainland and islands and you can express this genetic structure using a model that is fairly mechanistic and that can be interesting sometimes uh so to connect these demographic processes to the genetic process like again like we like we focused on demography but the data you want to use or actually genetic data if you remember the micro satellite data set that we had "
    },
    {
        "start": 2569.2,
        "text": "so the process the model you will use is a coalescence model you just consider like the set of gene copies you have sampled at sampling time and then conditionally to the demographic flows and the demographic sizes that you simulated in the forward time story that we just showed that conditionally to that you can guide the coalition process so you can come with expressions of for a node in the landscape the probability that his ancestor is in another location in the landscape that's a just a function of the demographic quantities we were talking about and when you have two gene copies at the same location in the landscape you have an expression here that tells you the probability that they find a common ancestor so that's the way to simulate "
    },
    {
        "start": 2631.359,
        "text": "coalescence um like the way to represent these concepts in the kids alcohol or through coalescence policies uh here i will use a policy that generates new weak formulas because i i need that for my simulations so i want uh to generate trees but i want to focus on the distance in terms of in number of generations between like children nodes and parent nodes and i want to register the the name of each node of each leaf sorry uh and so i'm just defining my questions policy and then calling the coalesce to most recent common ancestor on the simulation core and i give it like the parameters i need that is like my sample the sampling time the function i'm using to get the positions of the nodes through through the landscape and to the function i'm using to get the "
    },
    {
        "start": 2691.839,
        "text": "name like the identifiers of the leaf nodes if it makes sense and then the some random generator because building the gene trees is a stochastic process uh so i will not say too much about this part because for a number of reasons that i will announce in the conclusion uh i see kedzel as a very flexible resource for building new demographic models that is like if you want to play with demography and you have reasons to believe that demographic processes are the main drivers of the genetic diversity of the biological model you study that's a good way to come with the simulation of these processes what is difficult to explain but that's true uh it's that it's easy to couple to quality and simulator also because kids al as the library is very abstract "
    },
    {
        "start": 2754.0,
        "text": "uh you can couple that with other resources and that's something i would like to do in the future um that is like to use other coalition simulators because right now i'm just like simulating new weak formulas it's useful for me but there are other resources outside that are really good at simulating sequences or necro satellites or based on gene trees so i would like at some point to couple it with the for example the tsk library developed by k layer and co-author that would allow for example like efficient generation of correctly t trees uh so that would be like just awesome for simulating sequences and stuff [Music] and it's open source on github so if you're interested have a look uh the you can find the documentation online uh pretty easily or like through the article i shared so thank you for your attention "
    },
    {
        "start": 2814.4,
        "text": "again thank you so much for inviting me and giving me the opportunity to present this resource just if you have some questions i would be happy to answer them so i know there is a comment i think in the chat i don't know if you're able to see the chat window um but someone's wrote although outside of my expertise very interesting talk especially enjoyed watching the island colonizations in the simulation so just a comment to you thank you hi marci this is alvin i had a question is that okay irving go ahead and ask yeah firstly beautiful talk i really enjoyed the way you tied all these components together i have a much more fundamental question i'm hearing this thing about biogeography versus phylogeography can you give us your sense from a expert standpoint what these are supposed to what these "
    },
    {
        "start": 2875.44,
        "text": "frameworks are supposed to be and what kind of questions one can uh aim to answer using these two different approaches okay um what my my sense of so i was confronted with this question when i came to my postdoc so um back in paris when i was working on the biological invasion process it was a very short process in time uh that is like it's a very recent uh process you have very low densities of individuals [Music] um and in terms of methodology that had serious implications um i'm not the only one that applied this method to these kind of questions like a stupenco thor uh they use this kind of method they use clutch i believe or variant of splash to "
    },
    {
        "start": 2936.079,
        "text": "study the invasion of uh the of the before marine music's a third uh toured in north australia actually and again it was a very recent process it was like few dozens of generations that is in terms of special and temporal scales very different from what you would find in other applications that focus on phylogeographic scales that is like thousands and thousands of years of history when where the climate change so much that you need to come with different species distribution models the further you go back in time the less information about the biological process you have also what it means at this phylogenic scale is that the process themselves that you're simulating can change for example the "
    },
    {
        "start": 2996.16,
        "text": "dispersal parameters can change just because of evolution it was actually shown i think on the before marinus case uh there was like a selection process uh the length of the leg of the toe changed in uh several several generations in and it was like changing affecting the dispersal model [Music] what what it means is that for the phylogeographic scales that is when you are focused about phylogeography um sometimes you will have not enough knowledge about the process and the process will be like too mechanistic if you are using this framework [Music] and you don't know really what happened "
    },
    {
        "start": 3057.68,
        "text": "thousands and thousand years ago so it will hinder your ability to apply this method uh if you're working on much more recent history that is more biogeography and that you're applying these coalescence-based methodologies sometimes you will end up making assumptions that do not hold at this temporal scale so be very careful about the assumptions around the scale of movement of the individuals or the the assumptions that revolve around the population sizes of the population um that's like we we published an article about this uh assumption in the case article in the case of library you have a way to to work at with very low densities of population and still be exact on your simulation that's my point of view on the question "
    },
    {
        "start": 3120.079,
        "text": "that's really helpful thank you can i ask you a quick follow-up question is how much of a stationarity assumption is typically made when you take these observations the reason i'm trying to ask this question is what if the observation point is actually in the middle of a giant flux in terms of a ecological uh you know perturbation and what if you just happen to observe things in the middle of that flux versus at the end of the flux how does one know when the physics has more or less converged to some notion of stability is it possible to infer that uh i'm not sure to understand your question can you can you try to rephrase it um so i guess it becomes a problem of inverse modeling like you have all these forward models for dispersal and the poisson assumption and the way these things are supposed to evolve the physics is sort of assigned right but i don't know how much "
    },
    {
        "start": 3183.44,
        "text": "of this physics is based on where in the temporal period you're observing the physics like is it happening is it after the system has converged to stability or is it in the middle of the flux and if so can we infer where we are in this temporal scale that's that's what i was trying to get okay or you you from what i understand you are asking about how to know if the parameters of interest are identifiable under the model yes yes yes so for that the abc methodology allows to test for that usually you would simulate the drops of data all across your prior distribution and that is like you simulate data for which you actually know the parameters that you use and you for each pseudo-data you try to estimate their parameters based on the methodology you used and sometimes for you will be able to "
    },
    {
        "start": 3244.319,
        "text": "identify correctly most of the parameters from the literature sometimes there are parameters you actually you don't have information about your parameters that you need to make the simulation work but you cannot hope to learn about these parameters using the genetic data that you had the biological context can greatly like affect that so for example the sample like the nature of your sample matters a lot when you have sampled distant points in the landscape like spread a few data points on the large landscape you will end up having like good information about like large scale processes for example the general dispersal patterns and you may end up like informing the dispersal parameters reasonably well but you lose the information that you would need to inform local "
    },
    {
        "start": 3305.839,
        "text": "processes like the landscape the impact of landscape eternity on the carrying capacity or the growth rate uh for that you need to kind of reschedule sampling having like different scales in your sampling for which you sampled few points very far to identify large scale processes and then try to oversample like small areas to try to identify other scales of the processes if it makes sense does it answer your question very much thank you so much thank you are there any other questions all right then i would like to thank uh our speaker for being with us today and i hope to see everyone here next week for next week's talk "
    }
]