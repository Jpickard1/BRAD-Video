like like oh the Marx Brothers originally where it's called vaudeville there's a whole circuit they travel you know they travel around in different towns because they actually everybody pronounces chuckles it's too close people pronounce it Chico you know some people are charismatic right some people are cheerful tools of technology some in our series I just sent around the sign-in sheet so as always please remember to sign in to help us with the pizza hopefully you saw in the promotional emails and I think I mentioned a few weeks ago but Network next week speaker October 26 and November 2nd I think those the dates there was a swap and the speakers from the original schedule so just in case you're interested in just one of those make note of who's actually speaking next week so today's speaker is Brent Griffin and he is assistant research scientist in electrical engineering and computer science and his research interests are actually in robotics and computers all right thank you well thank you for having me here it's an honor to speak for the tools and technology talk and today I'm going to be talking about tools for volumetric segmentation and grouping and I'm gonna have several applications that I'm gonna show you this message being applied to but thanks to some colleagues of mine professor Jason Corso and dr. Tom Yan who's a postdoc in our lab I'm going to be able to show you some applications that actually have to deal with medical imaging directly as well as some other medical applications and so while I've done a lot of computer vision type work and this kind of scenario I haven't personally done any research in the medical imaging field so if you have questions on these slides I'll do my best to answer but my hope is at least kind of advertised to you things I've been done by colleagues from North Campus and if you're more interested we can talk after the seminar and I can make sure you can get in contact with the right people if I can't answer questions that you have and just kind of as a brief intro to this idea of segmentation you see here there's this brain you have some initial imaging now you can do a manual annotation of this if you are looking for a tumor but the goal of doing some segmentation of a volume so you can actually turn these images even if there are lots of slices into a graph that's all interconnected pixel to pixel frame and this has applications in like a volume and the case of an MRI where you have actual frames lining up in slices in 3d also in video if you think of instead of an additional spatial axis being the difference between frames you have time that's playing out and so that's the additional volume for some segmentation applications and so to motivate this I'll show you an example of a brain tumor segmentation that was work done by Jason Corso earlier following that I'll give a more general introduction to this general segmentation and grouping and computer visions I'll talk about video object segmentation which is more work that I've done directly we're actually tracking objects throughout a video but the same mathematics applies to volume and then also at the very end I'll show different medical applications using computer vision that I think are pretty cool and some of this is from Tommy on the postdoc some of it is from collaborations that Jason's had one of the universities as well so in medical imaging segmentation of anatomy and pathology is important but it's also very difficult so some of the difficulties I'll get to in a second but you ideally want to be able to track volume shape and longitudinal variation and something like that tumor inside of your brain currently that's a very manual interactive process and so the goal is to be able to automate this to some extent so physicians have physicians and surgeons have more time to do other things rather than this very manual task and then also be able to track data for treatment and analysis now this is difficult because even if you have some kind of model for a tumor it's going to be highly variable in appearance size and shape you also have the pathology around the tumors in data form which I'll get into in a little bit more detail and this has been shown in Prior work to actually be impossible to identify with simple threshold igniters and so you need a lot of different modalities as the approach that Jason employed and at the time that you're doing this work there really no biophysical models that were yet No and the specific term I'll be talking about is sorry if I get the pronunciation completely wrong but glioblastoma multiforme brain tumor or just GBM and 40 percent of brain tumors across all ages are this type of cancer and Jason was kind of explaining to me a bit I actually grow so quickly that the interior of the tumor dies out and so there's a surface area that's active and growing rapidly meanwhile the interior of the tumor is actually kind of killing itself because there has no access to material and then you also have this edema or inflamed area around the brain because it's tumor is growing so quickly so this is common and it's also extremely fatal as far as the outcomes for patients who are diagnosed with this type of cancer so here are two examples I these are manual kind of annotations of MRI data and they're also t1 weighted and t2 weighted MRI images I'm sure some people here know much more about the differences between these but and some of them the tumor is very apparent and the t2 it seems like the edema is much easier to see and so you can use both of these images in order to kind of segment areas that you have from a scan as far as where is the tumor and where is this inflamed area for this live brain tissue and so this is what main or an automatic segmentation using this kind of volumetric segmentation approach looks like and so essentially we start with some initial image here and what happens is we end up grouping pixels together based on the data that's available for them so that's going to include both spatial information and in the case of this image some kind of intensity and so you have these initial group of segments inside of this image and this is also performed in a hierarchical manner which is why we talked about this segmentation pyramid and so what happens is not only are you grouping pixels together for these initial segments on top of that you can recursively segment those segments together and when you do this recursively you end up getting sometimes more meaningful segmentations so you can see here this all of the hierarchy you pretty much have the tumor isolated and the same is the case here but eventually what happens is you get the higher and higher hierarchies you end up kind of over segmenting and so now you're getting more area than just the tumor itself automated way that so there's some initial work on that I'm not going to be talking about it here but following some of this work developing these processes a previous student at Jason's came up with this hierarchy slicing and so he came up with some entropy definition and he slices through these hierarchies in such a way that it's minimizing some energy function that he's defined for this I think that is a very open challenge though and even for the method that I show the different hierarchies give you different results and kind of almost at an implementation level or the task that you're working on it seems like the best way to deal with it is to leverage some additional information to inform which hierarchy is the most meaningful or if you are going straight across hierarchies you need like some additional information to kind of help you figure out which hierarchy is the most beneficial but it is not you know this process is in place it's been used in a lot of places it may try feeding these into yes that's done very often and so in Jason's lab and in our groove you know a lot of people do working with images and feeding things in by a to 24 by to 24 every single pixel you know fine that's easier to make some inference on than say a volume of video and so oftentimes you can use something like this automated technique where you're going to be grouping things together as an initial processing step and then this is the information that you're going to feed in and it makes number of parameters that you need for learning much more attractable especially given the fact that you know if you look at image net you have you know out of million images that have been annotated there's no such data set this large and video and so anything you can do to make learning on video easier is a like an active area of research as well that's a good question and by the way if you guys have any questions at any point please do ask questions so I want to make sure that what I'm telling you is coming across and what you're more interested in as well here's just another example so before we had a really good segmentation of the tumor itself here what we find in the end is actually a very good segmentation a higher-level hierarchy of the edema area of the tumor but again because of this over segmentation the tumor kind of gets grouped in with the inflamed tissue if you kind of follow this along this may be like the optimal place as far as just segmenting out the tumor but this is again it's kind of showing a weakness of this approach with what you brought up that different hierarchies are good for different things and so how do you sort that out in the case of this brain tumor segmentation I know that they use models specific to this task so for example if you had a model that described characteristics of the tumor and you're trying to figure out if a neighboring segment wasn't edema there would be a prior that would tell you yes like the segment next to what you're identifying as a tumor should be the edema for example and so he was able to leverage this information and what you see in the end is these comparisons of manual segmentations of the tumor and the inflamed area and the automated process that was still kind of informed by the design of these models but again he's not manually going through and annotating these and so there are some problems like you can see here there's a scat in the middle of the edema that like a manual annotation wouldn't have but I think what's cool is okay so this is on the training set but even on the testing set I think he's even closer to the manual annotation of the tumor and inflamed area and he did develop a tool that can be used for this and so you can actually have an MRI scan apply the segmentation technique and actively look through the different hierarchy levels choose how transparent that overlay is with the image that you're working with and then you can actually manually select some areas of that segmentation that you're interested in and the thing I've talked about a lot is I've been showing you a lot of images but keep in mind that there are slices that are involved here as well and so when you select a voxel which is a collection of pixels in 3d space you actually end up with this 3d segmentation that you can help with them so getting volumetric information by selecting these annotations on a 2d image and more related to that is he also has done some work just identifying different structural elements from brain scans and so I could not tell you what the function of these different areas of the brain are but at this level of the segmentation he was able to pick out some structures in the brain as well as some finer detailed harder to find things like the hippocampus and another thing you can do is if you have this automated segmentation process you can also do some post-processing with things like graph shifts algorithm which started originally in 2d but can be applied in 3d as well and so what you see here is there's been some initial segmentation of these different areas of the brain and by applying this graph shift an algorithm it can go through and up having a much cleaner result that's actually much closer to the manual labeled example for this automated process so that kind of concludes am i trying to maybe convince you guys that segmentation does have some applications and medical imaging I'm going to talk more about a kind of a general segmentation method at this point does anyone have any guess what this object in the video might be what's that guinea pig that's a very good guess this one say dog yeah so this is actually a dog and in this case as you get into the higher levels of the hierarchy it becomes a bit easier than maybe kind of get a sense of that and so the point of this is to kind of show that ok there must be some kind of encoding in this segmentation because you're able to pick out this object from it and a lot of these are related to this space-time Gestalt principles so the brain doesn't really process things not pixel by pixel but cone by cone we actually group objects together and that's how we kind of estimate and understand the seasons that we're observing and so for example we have some idea of like space-time dynamics right so if I'm walking across this room I'm not just gonna suddenly be on the other side so there's some continuity to these objects that we're tracking additionally there should be a lot of pixels that are grouped together because again like my shirt consists of probably a lot of firing of these different cones in your eye but they're being grouped together so the idea is to segment these things together in a meaningful way such that when you see this video that shouldn't make sense you can still pick out things like this dog other important things are making sure that you preserve boundaries of objects an example of the tumor we saw he got the too high level of a hierarchy that might start to segment the tumor with other areas of the brain we also want to be able to do this in the way that's actually computationally feasible so I'll talk about one variation of some of these general math is this actually very efficient and ideally you don't have any more super block voxels than you need so the dog was actually difficult to pick out when it had a lot a lot of different segments for the same thing like the paw so we do want to use as few super pixels as possible so an example I'm going to walk through here's this video of someone snowboarding and so we're gonna represent this video be and the idea is that we're gonna form this lattice gamma and kind of like what I said towards the beginning the talk what we can do is we can actually line all these frames up where instead of having like a spatial Z dimension each neighboring piece of the lattice is actually just another frame in time from this we need to define some segmentation and we're going to minimize an energy function to do that given the volume and I'll talk in two slides about some options you can use for the energy function but essentially you're going to have these segments that are differentiated by color and the output of the algorithm and you can see that these reach across multiple frames as well as space within a single frame now the other part of this is a hierarchical approach which you've seen in the 2d sense but essentially we're going to recursively apply this algorithm not from one pixel level but group these things together in 3d space and if you keep applying this you'll eventually get to the point where you really only have like one or two segments and some of the assumptions of the model basically no segmentation is going to exist outside of the lattice the union of the segmentation realized is the lattice and there's no overlapping between the segment's so they're completely disjoint every segment and yeah as I've already kind of motivated the hierarchical sense of this is not always clear like which level of segmentation you should use like so you can see here there's this ladder in the background and that still has some meaning in the low levels but then as the dog becomes more meaningful you actually end up losing most of the ladder and so again with the edema or the tumor depending on what level of information you're looking for that kind of informs what level of the hierarchy is going to be most useful but no single level at this point really contains the right information for the entire scene so this idea of grouping pixels together based on some energy function has been around for a long time and there are a lot of variants on how you do this I hope you guys ever heard about graph cuts but here's like the 3d version that was being used earlier and grade tumor segmentation this is a version of the minimum spanning forest that's more like what was shown on the previous slide and I've also used this in a lot of work for a video object segmentation and then this is another method that I haven't heard about as much lately so I'd say primarily these two methods are maybe the most successful and the way that I can kind of say that in a meaningful way is because of this earlier work lib svx so a previous student of Jason's is basically built his ability to get a faculty job based on this work where he actually looked at all these different video segmentation methods and came up with some metrics for comparing them and what he found was the method that use the minimum spanning tree was basically the most effective and so you'll get accuracy here across the board even if you get to the point where you have much fewer number of super voxels compared to your initial segmentation this Dulce is pretty accurate as far as maintaining the correct boundaries whereas some of these other methods actually drop off pretty quickly and then the more graph shifts and spire technique is just right here so they have pretty similar performance and I found the same thing and some work that I'll talk about that I've done later but in addition to kind of evaluating all these methods I think one thing that he found was if you have a really large video this is actually very difficult to apply some of these segmentation techniques because they create this entire lattice and then you're grouping these pixels across this entire volume well if you have an HD video that's greater than a minute or several minutes this quickly blows up as far as a segmentation problem and so they usually have to kind of divide and conquer but then you have these very discrete disjoint segmentation methods that are informed completely by the Past so if you came up with a clever model that applies this temporal Markov assumption where essentially what he's saying is okay so if we're our very first frame we're gonna route these pixels together and this is the first level or hierarchy and then basically because we have no previous frame this is all we can do so this is pretty much the same as like a hierarchical graph based representation of an image but then as I move to the next room in the video I can be informed by at least the previous frame on the same hierarchy level as well as what's happening on the lower frame and so by doing this you can kind of keep this moving window of memory that's necessary and so this isn't quite at the speed of real-time sure if you had enough computers you can make your real-time but it's still computational feasible for you know videos of you know any kind of meaningful length of time so you'd have a ten minute video be processing on this the amount of processing that you spend per frame is going to be the same regardless of the overall length of the video and what happens is over time this does kind of converge on something meaningful given that there's some movement in the video or something that's structurally there and the lattice that it can kind of Montu so all this is showing is that when you get to the next frame you're just gonna kind of shift this window you're only ever using the very previous frame to inform the calculations that you're doing and you're only ever using a segment below the current hierarchy that you're processing for and here's an example of that segmentation method just so you can see kind of qualitatively how well it does this actually does a really good job at high levels of kind of segmenting out this snowboarder and maybe having some sense of the sky versus the ground and certainly if you dive into some of the lower levels of the hierarchy you're gonna have less kind of boundary kind of I guess cutting different pieces and together over segmenting but it's kind of less informative on a per super voxel level and every frame has the same segment size well so it ends up being some consistency at least at each level of the hierarchy but no matter what the lowest level the hierarchy in this approach at least is always just segmenting together the lowest level which is the pixels if you want to get into something that's combining the different areas of the hierarchy and you have to get into the interview slicing paper that's kind of a forward extension of this work but I didn't include but maybe I should have based on your guys's questions does that help okay this is just an example of comparing and what I was saying earlier about how videos have a lot of data in them which is motivating this reading process and just looking at kind of a qualitative and comparison of if you do things just frame by frame in which case you're kind of violating the Gasol idea of continuity from frame to frame so you have a lot of jumping because these are all processed in isolation you have this streaming technique which has a finite amount of computation per frame and only processes each frame once but satisfies this Gestalt time nudee because it's always being informed by the previous frame and trying to maintain boundaries according to that and in this case which I'm sure was cherry picked you have the version of the video that's actually segmenting based on the whole video and I think they're kind of more jumps here or a poorer segmentation but in general you do get a bit more performance on this although you take a big hit as far as computational cost goes okay so actually introducing that I want to talk about a little bit about work that I spend a lot more time on so if you have more questions with this would be much easier for me to answer and for disclosure I've only been a computer vision person for a little over a year I actually did my PhD in control and dynamics and robotics and what we had was a walking robot that had no eyes and we could design robust control methods such that this robot was able to walk outside walk over all kinds of terrain without being informed by that but as a selfish robot Isis I figured if I look at a scenario like this my robot would never be able to walk through here even though the terrain is not difficult there are all these objects moving through the environment that it's unaware of see they're gonna run to those Rover this biker as it's walking through additionally the sidewalk is in that tough there isn't many there's a much trained variation that the controller cannot handle but at the same time how do you know you're not about to walk into a car and so this is how I got interested in tracking objects and video and if we look at this motivating example of having a car drive through oftentimes a tool that people use to kind of inform understanding what's going on in the scene is they'll either use optical flow which are looking at the gradient of pixel intensities from frame to frame and as a person you can pick this out fairly well and you say yeah there's good agreement between this but you're also getting things like artifacts of where this gradient and the concrete is moving where the car has been if you look at the overall optical flow it's picking out the car well but it's taking up a lot of other artifacts now there's been other work in computer vision that's based on an individual frame just roughly this category of visual sailin see this particularly example uses a lot of different visual sailin see methods kind of together and the goal this is just given a single image pick out the object that's most important in the image and so if it was problem with picking out the object it does okay but it also picks up maybe a lot of the building because maybe that stands out a bit more being a yellow color it also picks up a lot of this car here which isn't even moving so individually each of these processing techniques has some flaws the way that we got around this and come up with a meaningful a to combine these pieces of information together is what I'm gonna call like distribution based scaling so if you look at the median deviation of optical flow what you'll find is you know in most videos a lot of that objects are pretty close to the median but ideally if there's one object that kind of stands out as far as moving through a video or being an active object it's gonna have a spike that's kind of outside of this interquartile range and so this is where they came up with for : weighted sailin see outliers all it is is taking two keys outlier test coming up with a way that processing video through it collecting how much of the intensities of either optical or visual sailin see is actually outside of this interquartile range and then based on how much that data is identifiable as an outlier scaling the data and saying okay we're catching on something that's an outlier it's isolated in the video this is a meaningful piece of data versus if you had something that had a lot of noise in it you basically wouldn't use this source of information because you're not really catching on to anything that's an outlier isolated everything kind of appears the same now the motivation for using the difference from the median is just that if you have a video that's moving with an object that's moving or if you have a video that's not moving an object is moving through it the median deviation of out to a flow is going to be salient for that object meaning if you're tracking an object it's not gonna have a lot of optical flow it stays in the center of the camera it's not really having a high gradient that everything else is moving so the majority of pixels have high optical flow so you take the absolute value of that and look at the median that vehicle that has basically no optical flow is actually fairly salient and in the case study habit camera that's not moving and the object is moving you won't have to necessarily do this meeting deviation but it's robust to either case so when we apply this we have an original video we have optical floral and we have this combination of motion and visual saliency outliers we're using these statistical measures to inform what we're tracking is a salient object and we get much less artifacts so basically where this color is being picked up before because it doesn't really have a significant meeting deviation optical flow it's kept out same thing with the street here it's still there a little bit but it's much more focused on the car as far as our confidence that this is the object that we care about in this video now this was combined with super voxels or the kind of general volumetric segmentation technique and the way that we use this information in combination is we had this initial estimate that was just based on these sailin see outliers then based on a super voxel that's through the whole volume we actually formed the consensus on a super voxel level so we actually call this technique super voxel gerrymandering you can kind of think of it like voting so basically within a super voxel you have this measure of intensity of house a lien as its object you form the consensus the nice thing is these super voxels actually keep boundaries fairly well even if they don't have as much information individually and so this is why I was talking about about using your application to inform how you should group super bottles together this is a picture of the foreground consensus we also have a consensus of the background and we can use that as a subtractive element and now we're essentially live in this video without you any human annotation this object of the car being left out and here's just a look at the process overall we're going to tax the video card a bit here but essentially you have your input video and a parallel process we generate the super voxels for this video from the motion and visual saliency outliers we can combine these two elements together we come up with a background consensus so the black pixels here are things that we are saying on the background the black pixels here are things that we're saying are the foreground we can basically use the background consensus to subtract from some of the noise in this initial estimate then we have this refined estimation and this is the output segmentation here so the red area is what we're saying this is the object that we're tracking in the video likewise taxing the video card again we can apply this to a lot of different cases so before we have this car moving here we have this dynamic object that's moving through a vehicle so the object itself is deforming from frame to frame and we're still able to track this fairly well even though throughout this parkour video you had things like handrails that he was jumping over occlusions as his legs are behind some object well we're able to stick with this object I'm trying to frame throughout the video and converge fairly quickly and there's a data set that we applied this to the Davis 2016 Davis Dave's 2016 data set and the whole point of this was just tracking the main object in the video for 50 different videos there are lots of different challenges that they had some videos you have problems where the object doesn't have a significant color difference some of the videos the object is becoming a clue in different parts of the video and so half of its going to be on one side of the tree the other half is gonna be on another side and basically just card measure is how much does your segmentation volume agree with the ground truth and when we looked at this method we came up with we essentially had the best unsupervised method for this Davis dataset and even with different variations of our algorithm we still had first second third and fourth place and the next best algorithm was essentially in fifth compared to the work that we were doing the other thing that we did was we use this as an opportunity to doing kind of an albatraoz study on the different hierarchy levels to help with this question of which hierarchy levels are most informative and what we found is at least in this kind of case where we have additional information that we can use the group segments together using the lowest level of the hierarchy where you had the best boundary kind of consistency with what the ground truth you'll get the best performance and then as you would get to the higher and higher levels of the hierarchy eventually you would eventually kind of drop off essentially because the super voxels that you're trying to form this consensus on our grouping background and foreground objects together the other thing that we found was that these complete video processing methods where they process the entire lattice at the same time do have better performance in the stream GBH version that I showed earlier but this is still computationally much better in terms of being able to have an arbitrarily long video and being able to apply this technique sorry are you trying to pick up the object have you challenged it with like something that comes in another person walking by and you have something that does like persistence through the whole like also the switch to the other thing yeah so that can certainly happen there are even examples in this data set so here's just one where you have like a simple occlusion for example and you can see we actually drop this object a bit when we move through these trees for example but thankfully it picks the object back up if you have other objects that are moving just as much as this one it could be having about doing that occlusion will actually pick up this object as well because if you're already kind of dropping this object you no longer have this kind of spatial temporal consistency constraint so you can go up onto a new object pretty easily or though if you had sort of two-stage thing that could then look across a greater set of frames and say this one's still here get rid of all the others so that's a great idea and when we did the super voxel consensus that actually is processed using the stream GBH but the super box will span across time and so we actually look at super voxels across the entire space we come up with a feature vector that describes each super voxel which can be things like histogram a gradients lav colors phase RGB color space there's really no limit on what features are used to describe these super voxels and then we use a local and a non-local consensus so for example if you have a super voxel that in your initial estimate never was regarded as the object but it looks almost identical in this feature space to all these other super voxels that are being identified as this foreground object it's still going to be included in this post process version and so that was kind of the trick it's kind of the thing that you're talking about you have this initial estimate then you go back and you refine it based on your findings over the whole video it's similar to what Jason was doing earlier with rashes he has this initial estimate and he's going back and refining it minimizing some cost function but when we did that that's kind of what gave us the boost and the jump over all the other methods applied before and yeah I here are some examples of results that work well here's an example with more occlusions but maybe here's an example that our method actually performs really poorly on and we just cannot track this goat it looks a lot like the rocks in terms of color the rocks are actually just a salient is the goat when you're using the pure image based visual conc standard and as far as optical flow goes I mean these rocks in the foreground are kind of moving around just as much as this go but this is a case where you had we're checking a cross or a bigger span of it that's the only thing that here's the problem with this approach if your initial estimate is so poor that you initially glommed onto something that was not the object then your consensus is not the object as well and so we actually get worse as we do this post-processing because like oh it's definitely these super voxels that look like rocks and we're dropping areas that were originally segmented for the go where some of these other techniques have used deep learning and they have this idea of object miss and that's been using things like image net to Train what these different objects aren't being able to classify them and you see that they perform very well in these cases but at the same time there are other cases where that really doesn't work highly because you're tracking an object that isn't necessarily well defined by your concept of object Ness yeah anyway there are a lot of trade-offs depending on which approach you're using I hope I didn't break the PowerPoint here here's a good example the object miss is actually very effective um you're not going to see the video moving but just kind of use your imagination here each of these ripples of water have a higher gradient of optical flow than this duck standing still itself so these tracking methods that were very reliant on where's the duck where's it go where are these objects I've been trained on does very well compared to our method so in addition to your question - this is about the Davis 2016 data set current work right now is dealing with the Davis 2017 data set which we did add some new kind of manually how do I wanna describe this like kind of trying to find more manual techniques and making meaningful information from this data and all first through fifth place was all deep learning based methods right so people are just throwing all this information into a neural network and then training it it doesn't matter if they understand the difference between ground truth or not this network can work while you're sleeping it will figure it out and Davis 2017 actually has a bit more challenge because it's actually tracking in one video say seven different dolphins moving through water I can't tell one dolphin from another so these dolphins are moving amongst each other and there are some of these techniques that were able to figure out and track an individual dolphin moving through even though it's occluded by other dolphins and maybe back up on it later so there are definitely more challenging example videos that are quickly ramping up and the community is getting more behind but in general from what I've seen in computer vision the video is often like an afterthought and now more and more people are becoming interested in there they're also seeing in media more and more people are using things like video as opposed to just messaging like I guess taking photos or something like that and it's actually related some work that I'm doing with DARPA where we're trying to create manipulation videos and what they want to be able to do is I'm actually detect if an image or a video has been manipulated and right now the results in the image space of that are very good but video is almost nowhere so recreated these manipulated videos but there's really no I can detect them anyway regardless how could you allow how good of a job that we do manipulating them so anyway a point being just that it's nice because this Davis data said just hard in 2016 and already in 2017 you see a lot of ramping up in the difficulty and the results that people are able to achieve on that so I think one will see and the near future is a lot better computer vision techniques and video in some cases that's way far behind but in case like this this video object segmentation like we're right in the middle of it ramping up is an active area of research and getting a lot of new results are you limited to doing it in the forward direction no does anything become easier some cases people backwards yeah so this is this is perfect for some work that I've done related to that because with this object tracking you can have a user input just say a bounding box and then pick a frame that they do that and then you erase this object from video now if you have a good segmentation of the object moving through space there's another really smart guy who's figured out how to in pay even dynamic objects like waves behind an object and there's some extent even people moving behind an object and so what we need to do is come with a really good segmentation now one problem that we had early on was this only forward-thinking type process because we would track that object from the initial frame and then move forward but oftentimes you might have an object that you're trying to remove from a video completely and initially that object isn't much in view so the segmentation that you have available at that point is very poor so initially you remove this object is really bad until this person almost moves completely into the video and then you can start tracking with it really well they get removed really well well the paradigm shift was well let's annotate this person in the middle and then let's propagate this technique forward and backwards our initial estimate got very good and then the post-processing then also was morning informed now you could probably come up with some way to do this iteratively and have like more processing of word backwards we just did a kind of a naive way we ran our algorithm this direction in this direction from here just kind of depends on the what extent you're going to code it up and then how much time you have computationally get more well I think that'd be up cool too like the tumor example as well like maybe you could do reprocessing of the volume of where the tumor is if you had an intermediate and kind of somewhere close to the end point annotation and then you just filled in the area in between and then you wouldn't need to go through and manually annotate and you'd have a fairly well-informed process for getting the entire volume accurately I'm gonna talk a little bit about Tommy ants work so he was using segmentation away where they actually had these Brainbow images and the goal was you have these pictures that are kind of outlining these different neurons in the brain and their different connections Israel way that we can track these connections from frame to frame and the way that he did this can't have a computer vision talk without talking about deep learning what is using deep learning based techniques where essentially they had the slices of images that were from this brain scan they would apply this pre-trained denoising technique and then use a tryout layer LS TM which I'll talk a little bit and two slides and then the output is essentially having this actual 3d volume where you're identifying these different neurons and then their connections throughout this these different slices so this is just talking a bit about this pre trained network I don't know how many people here use neural networks they're very popular in my lab I've been hesitant to use them to some extent because I believe in human intelligence and so the computer intelligence but after getting the floor wiped in the Davis 2017 challenge I'm now drinking the kool-aid as well but one thing that's nice is people have a lot of these networks and their weights are available so you can do things like grab this bbm 4d network that someone else is trained and it's not really good performance on you can almost just drop it into the problem that you're working on and so this is something that we're doing to make our segmentation better we're actually using some of these pre trained networks as opposed to classifying things as a cat or a dog we're sticking to the high level feature space that's like a vector of 4096 different values and then using that come up with these descriptions of super voxels as opposed to things just like RGB la be color strange things that we can think of manually they have a pre trained model is that so for a specific yeah so part of what you have to do is kind of pre process images and figure out a way to take what you're doing within your problem and first it to work with this other network so if you'll hold sudden want to do like higher res a lot more pixels that's the other so the other thing you can do too though is you can add intermediate networks that kind of take care of the process of transforming your input data that's not the right format to the correct format so you maybe have four layers that do this if you train on that problem given some output classification you're only dealing with weights associated with four layers you're still leveraging this these other sixteen layers or more of a network someone else trained or you can just freeze those so there ways that you can use deep learning to deal with that problem as well in my case I'm just manually picking out these segments reforming on the vehicle upgrades and taking out the mean difference for the areas that are not the segments for the input space so if you have a segment a super voxel for example that in one slice is 100 pixels by 100 pixels but your input is 224 by 224 essentially the nine segment area is being colored as close as possible to what's considered invisible by the network because usually what they do is a pre-processing technique is - let's say the mean RGB value of their entire training set so that value is essentially invisible to the network so there are manual tricks you can use and then there are also like these principle deep learning ways that you can transform your data to work for another previously trained network just to beat the dead horse a little bit more you can have the same input and if you're just trying to do a different classification problem just train their last layers are actually taking the output vector and translating it to these classifications that you've kind of defined in advance were most commonly used CMOS pre-trained networks the way convolution words they don't care about the image size that you fit in the only limitation that you have in a network on the input size is fully connected layers or average form that you can have so as you said chop it off the convolutional part doesn't care what the size of the image if it even was a train or look 244 but for an image that originally and you have 600 by 600 as you remove the fully connected layers in the end it will take 600 no problem and will apply all of those things and then you can have your own classifier in the end or some kind of adaptive they were in charge pooling at the bottom that doesn't care again America sizes so that used a lot another trick that people do instead of if you have set images that your new set of images into your fine union and it's different from the image in that they are different from the original ones people just before the network they sticking couple of normalizing layers they've also learned like question normalization before before the rest of the network and it starts sort of learning how to normalize that the new data submit so a couple of tricks that I think yeah this is really good we have a local deep learning expert which I am NOT yeah I guess maybe one of the most common layers is the convolutional layer which is part of the I guess the noising network that he was using along with some of the other features because we kind of talk about deep learning I won't go into details of this as much as LST M the difference with LST M is you're essentially taking input data from multiple time steps which is really useful in video or in this case the variable project where you have slices of these images spatially across like the z-axis and so now the output of this network becomes based on not just the current image the current slice but multiple slices that you were processing before and Tom Ian actually applied this using three layers of these LSD terms and when you look at the overall output it seems like at least qualitatively they got a pretty good segmentation of these objects in the volume these very small neurons through these rainbow images and then here's an example to where you have the raw image and then these different red dots are actually all the connections associated with this neuron here and if you're interested in this work definitely feel free to check this out and I can get you the name of the paper too after the talk as well another thing that Tom Yan's done is looking at where you have these multiple channels and some kind of like coming fluorescence imaging application and trying to unmix them so they can be more meaningful and images I think you just use kind of a k-means classification to do this but maybe some of these are a little more apparent than others where before maybe there was less distinction between these two channels but by using this on mixing process he's able to take data for specific channels and make them much more visible in an image that person to look at and then back to some of the work that Jason's had they've looked at when you have a camera available and you're performing surgery so I show this with pizza they can do things like try to automatically identify based on where they know these different tools are if there's bleeding going on these different things of classifying these activities during the surgical operation and this is just way out there but I actually saw this can come and give a talk and somehow this is real what they do is they play a video for a person in an MRI and then based on their brain scan a network predicts what the person is looking at so you're talking about video processing where you're not even processing the video you're literally processing a brain scan of a person that's watching the video and it's able to do better than these networks that were trained to process the video into like the semantic description of the object so anyway definitely check that out if that's something that's of interest to you I just liked it one more time thank Tom Yang and Jason for sharing some of those slides and sponsors who supported this work as well and I'm happy to answer any of any questions that you guys might have you guys did a good job of asking questions during the presentation so the stuff I'm working on has actually gone from more of the video object segmentation that I presented on towards the middle towards using perception to inform actions that a robot takes performing some task so I'm still very much a robot assist that has jumped in the computer vision I guess I for selfish reasons of recognizing limitations of just being a controls guy trying to do a lot of different automation tasks right you need something to actually inform what's going on in your environment and well that's a pretty popular task right now self-driving cars driving for them they're basically essentially doing the same regular camera at the flow of people different subjects that you want to avoid or for taking the relatives down terms of investment so listening pretty country and the good data that's still leaking let's go go on the bicycle world and some other ones or widely used so a lot of pre training models for you guys the thing I'm really excited about too because of automated vehicles is it it seems pretty clear to me that big car companies are like bought into this idea that it's gonna happen and you don't want to be beat you don't want to be the last manufacturer to have these and I think the other cool thing that seems to be recognized by potentially for but especially like Toyota or even Honda early on but I think now that you have more companies making these competitive investments is they're recognizing once we solve all these problems for automated vehicles here we have all this technology that can now be applied to things like home service robots so you actually have these companies are willing to fund these projects and take them seriously that maybe before there were less opportunities for it so that's also the reason why I'm able to work on these other kind of perception and directly type of robotics projects that are funded by the same people doing like automated vehicles at the same time so yeah I'm like a little kid when it comes to robots like I'm just very excited about so I compared to when I started my PhD is like what the heck am I going to do following walking robot work it's like oh like people are recognizing some value like an industry so this may actually take off but alright well thank you guys [Music]