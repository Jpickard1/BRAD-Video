[
    {
        "start": 0.08,
        "text": "welcome everyone thanks for joining us for today's tools and Technology seminar series as always there is an online audience so um if you're online and you have questions feel free to put those in the Q&A pod as you think of them they come up and we will pass them up to our speaker of course if you're in the room and you have questions just raise your hand as normal um we encourage you to ask them during the presentation you don't have to hold them all till the end um and if you do ask questions if you can just try to speak up a little bit that'll help the online audience hear the question in addition to the answer um yeah I think that's actually all I have to say right now so I'm pleased to introduce our speaker today we have Anna K who's an MD PhD student in computer science and engineering in Stella us lab thank you so today I wanted to give a brief talk about feature learning this is mainly just about the research in general and if there's two minutes left I'll talk about my research at the end so just a structure of what's going "
    },
    {
        "start": 60.92,
        "text": "to happen first is Introduction of the problem why we care about feature learning what makes a good feature then we'll talk about what makes feature learning hard and specifically focus on model bias and one strategy to deal with it then I'll do a brief aside on visual explainability so how do we understand what a network is paying attention to and finally talk about some self-supervised training so representation learning how are we going to get features when we don't really have a lot of annotations so in general here is a model um you take some input X it goes through the model and outputs some output y inside the model any really there's many representations you can pick just any intermediate form of the data can be counted as a representation but often times we're focusing on the very last representations kind of in the end here so why do we want representations well oftentimes we want the models that give representations to be reusable and so we can adjust them for new data types fine-tune them Etc so this is really Foundation models we hope that develop good representations of "
    },
    {
        "start": 121.56,
        "text": "data some properties of representations that you might hear about that could be useful to maintain is invariance so it just happens after you have some input and it changed in some way but the actual model feature or representation hasn't changed and that's because you hope that it recognizes these two cats for example that are symmetric as the same thing so maybe you don't want the feature to change or you want it to change but in a predictable way so if you predictably slid the cap to the side you want equivariant so you want the feature to predictably change and like shift along the axis so again reminder we just want good models that are good feature extractors and we want them to be adaptable to new tasks or unseen inputs here's just some examples of foundation models that exist in biology that all focus and use feature learning in some way so we have Alpha fold Transformer base but now in version three I believe they added diffusion we also have esm2 and esm full Transformer based and then there's this new model called bio clip which concatenates or or which Associates "
    },
    {
        "start": 182.76,
        "text": "images of different species with their class or their with their tonomy so now let's talk about model bias so model bias is what makes feature learning hard and because it happens because models aren't perfect so if you give a model an input with data points that are kind of skewed so you have some classes or some types of data that are more prevalent than others well then the model is most likely going to neglect everything in this tail and really just focus on representing well the more frequent dat was so we however believe that performance on rare classes is as important as performance on frequent classes and so we want to have a we want to have a technique that improves performance on rare classes without significantly hurting performance on frequent classes just as an example of bias let's look at clip so clip is a model that has learned to associate just by scraping off the internet a bunch of images and text it has learned to associate an image with a text and if want to use clip to predict something new about a new data set like "
    },
    {
        "start": 243.04,
        "text": "have it act as a classifier uh we can do as follows so this is an example from imag net 1000 or IM at 1K that's a data set that has a number of images for 10,00 different classes each its balance so the same different classes have the same number of images and so what we do is we have the name of the class um oh we have we have the name of the class here we uh we created into into a sentence a photo of a blank we pass pass it through the text and we have some representation or some feature for the text we also have the same thing similar thing for image and we pass the image through a network that creates an emcod an embedding or a feature for the image and then we look at the similarity between the text and the image and whichever similarity is bigger that's the class that we want to assign this image to but if we try this on imagenet and remember we have the same number of images per class we actually see that the number of predictions that we get per class is really variable so some classes we barely predict any images existing them at all in other classes we "
    },
    {
        "start": 303.199,
        "text": "have like thousands of them so there's this really big model bias and why does this happen well because the training data for clip just hasn't learned some Niche words or it's not as used to associate some Niche class words with images it's more likely going to associate some other words with those images and then you can see how the Precision and the recall also track with us so what do we do about how do we solve model bu so so there's some techniques such as data balancing which can happen just you resample your less frequent classes you can augment them when you resample so either maybe changing the color or changing them slightly or kind of Shifting away in a small radius you could also do augmentations in the feature space so take a representation in the middle of the model and then sample from representations that are similar to it as your new data points you can also synthesize some data if you want you can use Ensemble models and that's where you have models that are dedicated to for example solving problems and understanding what's happening at the tail versus models that are dedicated to "
    },
    {
        "start": 364.16,
        "text": "understanding what's happening in other places and you have a voting between these models you can also have early exit and that's more of a strategy to have more of the model useful for rare classes or for hard cases so this happens when in the middle of the model you decide oh I'm already pretty confident about the class of this particular image or about what I want to do with this particular input and so I'm going to stop processing it and then the later part of the model is where harder inputs will go and then finally you can directly increase the contribution of less frequent data to your loss so if you make a mistake on something in the class that you know is infrequent you want to penalize that mistake more and hopefully the model learns that it should really try hard to predict those classes another strategy is locked alignment so log alignment kind of straddles some of these categories but the the log alignment is based in the theory that you really want to create a base optimal classifier so log line is just for classifiers in the way that it's presented here and the goal is to have the lowest average per class error where the average per class error each of the classes have been equally "
    },
    {
        "start": 425.52,
        "text": "weighted in this average and if you can show mathematically just with a simple integration you can check out these people here that showed it that if you really want to get a base optimal classifier and you already know the conditional probability so you know that the model is going to predict why given this image you can create a p balance just by dividing by the prior probability that you have assigned to this class and if instead of using the conditional probability to assign your prediction you use this P balance to assign your prediction well then you will be getting a base optimal classifier so this is now how it's going to be import like used in a model so usually in classifier models at the end we have something called logins so we have a vector or just a list of integers or numbers that is the same length as the number of classes we predict and afterwards these numbers go through a softmax which is an exponential form function that's going to transform them into other numbers that all Su up to one and hopefully these represent probabilities in some way so what we do "
    },
    {
        "start": 486.919,
        "text": "is if we want to apply this P balance we want to transform these logits into something that is balanced and we can do that just by simply subtracting the log the division becomes a substraction and so this is useful after the model has been trained we already have the unbalanced logits and we want to convert them into something balanced for prediction however we could also have the model learn balance logins itself and that's by rearranging the formula so you want the loss to see the imbalance logits so just have the logits whatever logits the model outputs add log P to it and then the model or the loss will see the the imbalance logits while the model will be developing balance logins so why does this work I guess why why do we why are we able to get this that's because the loss uh cross entropy loss is base consistent and again maybe not because there's a caveat here that I'll get to in a minute but so cross entropy loss is the traditional loss that we use to train "
    },
    {
        "start": 547.68,
        "text": "classifier that we use to train classifier our models it looks like this and it has been shown that an optimal minimizer so a function that minimizes the cross entropy loss so a model that minimizes the cross entropy loss is um nearly the optimal minimizer of the uh loss of like a classifier so you can minimize this and you're actually minimizing the a or you're maximizing the accuracy however in reality we have a restricted set of functions so this all works so this this transformation here only works if you're confident that your logits are actually the kind of inverse of the conditional probabilities but if you're not getting the conditional probabilities then all bets are off you don't really know why subtracting P will help you and because models are a restricted set of functions you never get to this condition of a nearly optimal minimizer of CE and in fact if you look at testing and you have a model that was trained with crossentropy and you now look at it during testing it's most likely going to be overconfident and that's because well "
    },
    {
        "start": 608.56,
        "text": "during training it's going to learn to predict with it's going to learn the train in a while so it's going to try to be very confident in the predictions and the loggs that it's going to make however the model now has constrained itself and it's really only able to predict very high or very low values for the logas so you shift to testing it may not be confident or making mistakes but it's very confident in the mistakes that it makes just because of its structure and so if you consider like this happening then you try log at Al line mauring versus after testing log of alignment dur or sorry during training performs better than during testing and this is also just to say that elements that are important for differentiating R rare classes can still help you understand common classes and that's why it's possible to improve accuracy on less frequent classes without significantly hurting or hurting at all the mower frequence classes all right section two visual explainability so I'll talk about convolutional and Transformer models specifically in Vision because I work in a computer vision lab however these techniques can be applied to other "
    },
    {
        "start": 670.399,
        "text": "models they're just maybe people don't really care to visualize where we're looking at in the image that much or in a text block so what's going on here so imagine we have a convolutional network we start with an image it passes through a series of convolutions at the end it gets some feature Maps so in a convolutional network the model is the image is transformed into a number of different smaller like images or squares or patries you can say these feature maps go get flattened out and they pass through a number of fully connected layers until we get to these Loggins and then soft Macs Etc imagine now we have a class and we know that we like care about for example the class of the cat here we're going to go here uh we can calculate the gradient and back propagate and figure out how important all of the points on these feature maps are for the final class then we can average them so take the just the general average for each of the feature maps of the gradient and use that as a scaling factor to say how important the elements of that feature map are for the "
    },
    {
        "start": 732.12,
        "text": "prediction so we just multiply this in and then we cut off whatever negative values we have in the feature Maps because we really want to care only about the positive prediction or what which classes positively which elements of the matrices positively contribute to predicting the class we don't care about something that decreases the probability of tiger C so another thing to say here is that kind of this seems a little bit a like random right like we back propagated here why did we take global average pooling why didn't we just multiply element wise in the importance of all of these pixels that's because mathematically there's a way that you can show that assuming the constraints that you're only able to multiply one thing in multiplying the global average is the most important thing there's another thing called gr cam Plus+ that's going to make an improvement shortly so graad cam is great but it's not very good at localizing objects why because well we have just this one thing that's being multiplied to the average of the importance being multiplied to the "
    },
    {
        "start": 792.12,
        "text": "entire feature map so there's a small part of so if there's a feature map focused on just the tail for example the part of the object in that feature map is really small so overall the feature map might not contribute a lot to the class but we really still want to get the tail that was part in that part of that feature map and it actually it does contribute to the object so in gr cam yes so if particular feature is small in the map or like particular element of the image is small in the map that map has low weight but the object area is still important so we do want to weight things separately so this is what happens in gr cam Plus+ they say that we're going to wait so the W is the weight that we're going to be assigning to every point on the feature map if we give it the weight the weight is going to be dependent on the particular pixel and you can get what this dependence should be instead of just naively multiplying this derivative by saying that you want the optimal so you want whatever the weight is multiplied by the feature uh by the "
    },
    {
        "start": 854.04,
        "text": "feature map to recover for you your class and so if you solve this equation you can figure out what your AI J is supposed to be and so grad cam was a similar thing except for you wanted you had double UK um happening before or at after the summation happened so this is the difference in explainability between grad camp and grad Camp Plus+ so you can see how smaller features like the neck here and the legs here which may have existed in separate feature maps from the main body of the animal are still visualized okay so next part is visual trans uh explainability of Transformers now unlike grad cam so grad cam had some Theory motivations and they kind of showed that whatever they were visualizing was going to reflect the true class Transformers has a theory backing the Transformer visualization that that I'm going to talk about it has a theory backing however they also have made just some decisions about how they're going to propagate gradiant Etc "
    },
    {
        "start": 915.8,
        "text": "because otherwise it's too complicated so review of Transformer structure if you haven't seen this before and I'll I'm giving this in the context of images but again you could just have words and every word is it's like has some embedding it so what happens in an image is we create little patches in the image and so each of these patches is transformed into a patch embedding and then the patch embedding passes through a series of attention and fully connected layers with or Fe forward networks and feet forward networks are just fully connected MLPs so what happens in the attention Network that's the most important part so you have a part of the image that part of the image has become a vector in the the patch and bedding that Vector is now going to be transformed into three new vectors the Q K and V and together all of the patch and beddings form the qkd matrices then ultimately the Transformer is a matrix product in series first between the Q and the K the queries and the key matrices and then "
    },
    {
        "start": 976.88,
        "text": "it's a matrix product between the result of this soft Max divided by normalization scaling factor and then multiplied by the V the value Matrix and so this is going to give you the output so if you have multiple Transformer heads so sometimes people say that a Transformer has multiple heads so each of these attention Fe forward noww blocks is a layer with within each attention we can repeat this process multiple times so we have multiple different q's and multiple different KS and B's and we then concatenate the results in some way that is multi-ad attention so we've attended to ourselves multiple times so how do we do visual explainability of Transformers so the visual stability of Transformers focuses on the attention map so the product of the q's uh q and K we kind of ignore the values here so if we have an output we can calculate the gradient of just like how we did in grad cam we calculate the gradient with respect to the output for "
    },
    {
        "start": 1038.039,
        "text": "the attention maps and so now that we have the attention Maps we we have the grade and we can say oh maybe this is how important a particular patch is for the output but that's not exactly the case because as we move along the Transformer the tokens or the small like little boxes in the intention map they all start representing things that are far away from the initial positions they were assigned to so before you could confidently say that oh this little box in the attention map that represents this part of the image but now after you've moved some layers down that box can now represent a combination of elements from all different areas in the image because it's just been multiplied Matrix multiplied multiple times but you can unravel this and that's just simply by doing another matrix multiplication of the attention matrices you can get back uh structurally to have every little box in this output represent a particular position on the starting image so but you don't want to just Matrix multiply naively because what if some at some point the gradient is zero "
    },
    {
        "start": 1098.48,
        "text": "by accident you don't want to just zero out your entire result so what you're going to do is you're going to add the identity Matrix here and this is again this this is like a choice that the author has made in this paper so if you add the identity Matrix then even if the gradient at some point is zero well it's okay because the product is still going to just ignore that attention map and continue on as normal there's also this relevance term that gets multiplied into each attention map and that says how important that attention map is for the final output and in fact how important every element of that attention map is for the final output that's a little bit like the scaling that happened in grad Camp so here you can see the results of their visual explainability model for transform so here is just on single class images you can see it highlighting the fish here and then on multiclass images you can see it more clearly highlights different regions of the cat and you have a little bit more of a great here or a little bit more specificity in terms of what is being highlighted so now moving on to the last "
    },
    {
        "start": 1160.72,
        "text": "and largest section of self supervised training so imagine that we have a bunch of unannotated data and we want to create a model that's good at getting representations from data that we can then F tune for a downstream task so how can we do this so I'll talk about two approaches the first approach is we're just going to take all of the data that we have we're for each data point going to cut out a small chunk of it and then we're going to ask the model to predict what was cut out and hopefully in that way it's going to learn to fill in the blanks of the data and learn what structures exist in the data another approach is to say we have a lot of different elements in our data and we want to just have a model be able to understand identity so understand that this data point is itself it's not similar to all of the other data points so we're going to try to spread out the different elements so this is okay so this is the first approach where we're going to cut out some parts of the data and try to have the model fill it in how does it work so this is uh this was "
    },
    {
        "start": 1222.24,
        "text": "built for Transformers and the reason for that is because we're going to be removing like 80% of the data so we're just going to cover up and remove 80% of the patches that we generate for an image we take whatever is left of the patches we're going to pass them through an encoder which is just our big Transformer Network and then we're going to place back all of the empty patches that we kind of ignored in the Transformer and we're going to have a smaller decoder but it's not too small it's not like one layer it does have a few Transformer layers in it and we're going to ask the model to fill in the blanks here and predict the image filled in so the reason that we do this in Transformers and that this doesn't work as well if you try in convolutional networks is because convolutional networks you can't just ignore that you masked out some patches if you put a image that has a bunch of zeros in it in convolution those zeros are actually going to just mess up and like interact with all of the neighbors versus in a Transformer you assign every patch a position embedding and so in that case the Transformer doesn't really care how "
    },
    {
        "start": 1283.039,
        "text": "many patches you give it you can just give it a list of all the patches you have and the other patches you just don't feed into the transform so how did they choose to remove 80% of the image like that seems kind of hard like visually I don't think that I would be able to reconstruct this animal here but it turns out that they just did a lot of experiments and they figured out that 80 was pretty important to have so these are their two experiments and this is basically depending on what they're going to be using the downstream model for so the first is they're just going to then keep only the encoder so the decoder gets thrown away after all of this is happening we only care about getting good features from the image so they're going to keep the encoder and then they're going to fine-tune the encoder maybe add some linear layer whatever fine-tune that encoder for classification and here they see that depending on the masking ratio they used how good fine-tuning how how did fine tuning finally allow us to get the final output and so we got really good accuracy if we pre-train with a very high with a higher masking ratio linear probing is very similar we're again "
    },
    {
        "start": 1343.12,
        "text": "throwing out the decoder leaving just the encoder but instead of allowing the model to then update the en enre model and try to get the correct class we're just going to update the very final layer of the model we're going to add like a single linear layer at the end and only change it the rest of the model is frozen and so this kind of you can see that it also cares more about having a higher masking ratio and it doesn't rescue itself as much as over here because here you have the ability to change the entire model structure so perhaps during the supervised training part it's able to rescue and kind of fill in like learn more things versus here the modeled base is really fixed so what it learned it learned from self-supervised and so here you can see the actual real results of what the Fillin the blanks looks like and it looks pretty good so a little bit blurry but generally really good so the other strategy so the other strategy is contrastive learning so this is where we have different images image X1 image X2 or data X1 data X2 they're "
    },
    {
        "start": 1404.52,
        "text": "going to produce some features Z1 Z2 and we want IM features from one image to be similar to each other and features from different images or different elements inputs to be different from each other so how do we do this there's two methods here that I'll talk about well maybe three so the first one is Sim clear and this is a little bit like the generic based met model method that you would think to do so you start with your input image here you're going to augment it and that's saying that you're going to change it in some way that's improving or that not improving changing it in some way that still keeps the identity of the image still trying to have invariance between whatever happened in the transformation so you want these representations to be invariant of that transformation and so some Transformations that this these authors chose was to prop and resize uh flip the dog do add some noise add some color Etc so we've transformed the image "
    },
    {
        "start": 1464.84,
        "text": "in two different ways now we have two slightly different versions of the image but we ask the model that it predicts them to be the same so we have the model here we're going to get the representation out of the model and then we're going to add a little bit like a small Network just a three layer MLP one hidden layer to because we might not want to use the very final representation that we're trying to make identical we might want to use something a little bit earlier because there may be actually a small difference between this dog and this dog and we don't necessarily want to just use the very final output so we're going to get the represent here but we're going to maximize the agreement a little bit further down in the network between these z i and zjs so how do we maximize agreement so if we want two vectors so we have an image we had like X we had the transformed image XI the transformed image XJ we also had a batch of images so usually when we're training a network we take a lot of images together and we try to optimize "
    },
    {
        "start": 1525.039,
        "text": "network based on how of the average result for all of these images so let's say we have a batch size of n images and each image was augmented and doubled up in this way with two different Transformations well then you can say that you want the similarity between the vector or between the representations of one of the images or the image image in two different transformations to be similar and you want it to be different from the you want the similarity between the image and all of the other possible image or the basically all all of the other elements in the batch even if they're from the same class they're just different images so you want these representations to be different and so you're going to put them in the denominator of your loss so this is the Sim clear loss function and so you have the positive example up top you have the negative examples down here you're going to divide everything by temperature and that's a scaling factor to say how much variability you want to keep in the distribution so if temperature is really high you're going to become more flat and so you're going to see less of a "
    },
    {
        "start": 1586.559,
        "text": "difference between the uh you're going to you're going to emphasize less The Importance of Being different between the numer denominator so once you have this loss function you're going to try to optimize for it and so the authors do that and this is what they find and they find that you need to have a really large batch size to be able to perform well so batch size really only starts from 256 you can't go much lower than that and the more you train the better it is so they train much longer longer than any traditional networks usually do usually you stop at like a several hundred EPO here they went up to a th they still kept seeing improvements kind of pretty close to the end and larger batch sizes were important so you needed a lot of examples in this denominator to get things to work but what if you don't have a large batch size so like 9,000 close to 9,000 images is pretty excessive and nowadays maybe you can do it if you have the GPU but back then this was a couple years ago 2020 2021 that was actually really "
    },
    {
        "start": 1647.48,
        "text": "hard to do so if you have a limitation of your memory you have a limitation on the batch size that you can give and maybe you can give like small batches to different parts of you can separate and do multi-gpu training give pieces of batches of the batch to multiple GPU machines and then try to combine things but also very hard so let's say you have this limitation so what can you do well you can get more negative examples from the previous batches because you're going to assume that even if you have some Randomness your like your image that happened in one batch it didn't happen before because it happened in this batch but you can just naively take the features that you got from previous batches and use them as negative examples because you already looked at the P previous batch and updated the network based on that so whatever features happened are belong belong or were made with a network that is one step behind your network that you're now comparing through so if you are worried about your batch batches coming from different images then you can do what moo did so mochu is "
    },
    {
        "start": 1710.32,
        "text": "momentum contrast method and they proposed that instead of having the images like here all of these images were processed in the same way and with the same model we're going to have the image that we're comparing to be like the the positive example image to be processed by one encoder or one model and then all of the other images that are going to be our negative examples they're going to be processed by something else a different model called momentum encoder it has the structure as the encoder but it updates its weights much slower and so as so basically its weights at every step you're going to get some loss you're going to update the model weights but while you're going to update the encoder weights normally the momentum encoder weights are going to be updated by the new parameters of the encoder so after the encoder was updated times a very small constant plus its old weight so it's really just much slow it's moving along the gradient but it's moving much slower than the encoder model and that means that between batches the momentum encoder hasn't changed that much and so now it's "
    },
    {
        "start": 1771.24,
        "text": "actually okay to keep information from previous batches here and from The New Batch here and there hopefully they're not that much different and so now you use all of these images together as your negative examples and you can uh store in The Next Step you're going to store these new negative examples from the preceding batch and you're going to maybe kick off because you have a you have a limited number of elements that you're storing you're going to kick off some of the older uh some of the older elements that you had some of the older negative examples from the storage que so okay we have time presumably so then I'll mention the slide I was thinking skipping it versus not skipping it so um this is a thing that happens if you separate this is a multiple GPU problem so one of the contributions the authors notice is that if they just tried this method as it was and they ran it they actually found pretty poor performance with traditional batch "
    },
    {
        "start": 1832.08,
        "text": "normalization and that happened because they had multiple GPU machine so they were training the model so what happens in multiple GPU training is you have a copy of the model on each of the gpus you're training it on and then you're going to update you're going to run part of a batch on each GPU and then you're going to update the like sum up the results and then do the gradient for all of these um s get the gradient for all of these models and kind of average them afterwards but you're processing each batch separately on each GPU and when you process each batch separately the batch normalization that happens inside the model is actually batch dependent so on one GPU the batch normalization could be one thing on another GPU the batch normalization could be something else and it turned out that the authors were putting the images both positive examples on the same GPU always and when they did that the batch normalization because it has some trainable parameters bat normaliz learned to just separate out the or to help improve the IM make "
    },
    {
        "start": 1893.88,
        "text": "the image representations of the same image similar and without actually making them truly better and so what they did to solve this the author solved this by just shuffling where they put the positive examples and then also having the batch that they used for the momentum encoder be different from the batch they used from for the traditional encoder on every GPU and that way the batch normalization couldn't necessarily learned to always help align the results of MIM encoder and encoder because batch batches were different between this and this model on a particular GPU and then an alternative solution that later so that Sim clear did and Moco V3 because they would also face a similar problem had they been using that is they just aggregated the Bator parameters across all of the gpus and so this way all of the gpus were having the same B there was little learning to be had so Moco went through a series of updates it went to Moko V2 which just had a similar structure just added some of the "
    },
    {
        "start": 1956.44,
        "text": "same augmentations that Sim clear had and essentially that was that uh Moco V3 is a papered by some different authors and this was less about proposing a v Mo V3 but more about studying how Transformer training behaves and so but let's talk about the modifications they made to MCO so the first thing they did was they modified the loss a little bit so instead of using the loss they had here or similar to sim clear they used an info MCE loss and that's essentially the probability of predicting the match so you have the this part from the term from the numerator also exists in the denominator and that's the only difference for NC loss they also symmetrized the loss and so they had they only used the loss of of augmentation against the second type of so they okay so what they did was they had one "
    },
    {
        "start": 2016.679,
        "text": "image they took the image against one half of the negative examples and then took another transform of the image and looked it against the second half of the negative examples so they split the loss into two parts and then they also unlike Moco completely forgot about the Q so they completely forgot about having a Trying to minimize the amount of memory or the amount of samples we have in a particular batch and that's just because technology had Advanced by this point but they still kept the slowly updating momentum encoder so that gave a few percent performance improvements but the most important thing that these authors noted was that instability when training with a large batch size so if you have a Transformer and you start training it with a very large batch size you can see in this graph that at some point during training its performance just completely tanks and then it kind of recovers and then it tanks again and recovers tanks again and ultimately the final "
    },
    {
        "start": 2076.96,
        "text": "performance is not as good as training with a smaller badge and the question is why did this happen so the authors try to analyze this by looking at the gradient and so how does the gradient change over training time and they noticed that there was a spike a large increase in the gradient that was back propagated against at the moment that like a little bit before all of these descents or um Falls happened and so what they hes siize was that it just when we have a large batch size it can accidentally in the earlier layers there's an origination of this descent or this uh there's a small there's a big change or a relatively big change in some of the earliest layers and as this propagates to the later layers this creates this dip and accuracy and the solution they found and this is they tested this out empirically and it helped is that they simply froze the earlier layers of the model and so this is a little bit like they just decreased the model size and they don't allow some of the initial transformations to ever change and this "
    },
    {
        "start": 2138.079,
        "text": "improved accuracy but what this really did was it forced the model to be like more narrow and constrainted what representations it can represent so it wasn't able to jump to particular to potentially new loss minimizing places uh and so it was also more sensitive to initialization so wow we have time I'm kind of surprised but so this is I can talk briefly a little bit about my research I only have two slides on this so I have two projects that I'm working on so the first project is asking the question of how do we represent data well and so there were these theoretical results that showed that if you have a particular optimization criteria so a new loss then the features that you're going to learn are going to be most optimally with the lowest number of parameters bble to represent your data so if you truncate the features you're going to get the best representation you can get of the data for a particular feature size and so our goal is to adapt this model Training Method to reality "
    },
    {
        "start": 2201.72,
        "text": "and in reality of course there's a challenge of how do we actually achieve and follow this optimization Criterion effectively and reach its true minimum and my second project is medically related and in this case we're trying to understand the patterns of age related Mac generation so right now people know about the disease General associations between for example if you have some lesion starting in one area maybe they'll progress to a different area next and you kind of we have this vague understanding of the disease process but we don't know when the disease will progress to a new area and we also don't really know for most of the disease features how they will evolve over time and if there's any particular populations or patient groups that evolve differently so our goal is to uncover that but that's a very Broad goal so our sub aim is we're Beginning by segmenting the pseudodrusen which are a finding that exists in the eye and you can see all of these little spikes here above this membrane here the or the retinal pigment epithelium and so we're trying to get a density mapping of all "
    },
    {
        "start": 2264.079,
        "text": "of these points and you can see them here on the on Fast scan as well and so if we get this D density map of these points then we're trying to so we have already s close enough to density map at this point but now we're looking at patients and trying to see if there's clusters and different populations of patients that behave in different ways and evolve in different ways and so one of the benefits to our of our approach compared to previous approach is that we are combining the images from cross-sectional and on fast Imaging we also have some better LW functions and model architectures that we're attempting and that's all I have so please ask questions question thank you for your great talk and you introduce two types of self "
    },
    {
        "start": 2324.24,
        "text": "provided learning approaches first one is masking and the other one is um Contra learning so when it comes to Bringing about outputs in speciic research do you have any um suggestion of what to tr for yeah that's that's a good question I think it depends on what you're trying to achieve so um if you're trying to achieve good so let's say your data is not repetitive and you want to have the model really learn to understand like objects as they are so for example if it sees a part of a cat it should infer that this is the rest of the cat and you really care about getting Global understanding in that case I would recommend approach one so I would recommend approach ma because your what the model has learned to do is has learned to look at a small piece and infer what the whole is if you have data that's repetitive and maybe like even like my project here in the this project here this one uses contrasted learning "
    },
    {
        "start": 2386.48,
        "text": "because the goal is less to be able to predict what a new position is because I don't like I don't want to have any bias about where a new Legion will happen depending on if a legion is here want to be biased that alion will happen over here necessarily but I do want the model to distinguish between different images and say oh well this image is not like its neighbor which has lesions in a different pattern so I think it just depends on what you're trying to achieve thank you for talking and I I think uh this is two approaches do you have any idea to combine these two approaches one is the m Mas uh masus provis learning and the other one is a constructive learning so do you have the idea to combine these two strategy I mean I'm sure you could I will say I haven't I haven't seen any papers that granted I'm not this is not necessarily my area this more like I research I haven't seen any Cutting Edge "
    },
    {
        "start": 2448.04,
        "text": "research that combine them but it's so it is a little bit hard because it's it's a challenging both of them are pretty challenging tasks so and they're doing different things so you could ask the representations from this image to also be different from the representations of another image that's going through me like nobody's stopping you from adding a contrastive loss at any point in this process um and in fact people have done like contrastive loss not before before Sim clear people did contrast of loss on patches of images and so they try to contrast and say this patch of the image is closer to this other patch of image than it is to any patch in another image and so that's been done but it was less successful than I think either of these approaches but theoretically perhaps and maybe depending on your application because even like TR like it really like for example Alpha fold and things like that like they develop new Transformer architectures and new approaches because "
    },
    {
        "start": 2508.44,
        "text": "they were dealing with a particular data format and so it may be reasonable in some data format that you would want to do this but yeah other questions another minute anyone has a question online you can type it into the Q&A box hearing or seeing anything "
    }
]