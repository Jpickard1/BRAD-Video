okay we'll get started in just a couple minutes sure management that disease [Music] [Music] thank you very much all right I think we're right at the hour so I think we can go ahead and get started uh this is Marcy I'm actually on Zoom today so you might be hearing a little voice in the sky for me but I think you all for coming to today's tools and Technology seminar series our speaker is also on Zoom today um and remote so if you're in the room and you have questions you'll probably need to speak up to get everyone's attention if you are watching online today you can put your questions in the chat box I'll be monitoring that and can let our speaker know if there are any questions that come in or you can raise your Zoom hand using the reactions at the bottom right of your screen and we can call on you to unmute um so I don't think I have any other announcements for today with that um I'd like to introduce our speaker today uh we have salar fatahi who is assistant professor and Department of industrial and operations engineering um thanks Marcy uh hello everyone uh welcome to my presentation um uh thank you Marcy for the kind introduction and also for the invitation um a little bit of background about me I'm an assistant professor here at the University of Michigan in the industrial and operation engineering department and my research focus is on developing efficient algorithms and computational methods for different um practical data-driven problems uh that uh first of all are scalable meaning that if we Implement them in meaningful scale uh we that we can run it and at the same time they should come with some sort of approval guarantee meaning that if I use them I know for sure that they will converge or they will recover a desirable solution so you can probably guess uh that my research is sitting on the methodology side but at the same time I'm uh craving for some meaningful applications and in fact this combination methodology combined with uh meaningful application led to This research that I'm going to be talking about today so the title of my talk is scalable learning of dynamic graphical models Beyond maximum likelihood estimation now um before we get started I'd like to acknowledge my collaborators this is a joint workbit Professor Andres Gomez from USC and our very own Professor Arden Rao from biostatistics and computational Medicine together with two very strong PhD students uh vishwesh Ravi Kumar and tongshu who used to be a master's student working with me and now he's a PhD student at Northwestern also the research is sponsored by NSF ornr mikti and Midas now um before we get started I just want to give you a brief overview of the structure of this talk first I will briefly talk about the general problem of dynamic Network inference especially in the context of biological processes then I will talk about perhaps the most popular method to solve this problem which is based on maximum likelihood estimation or mle and I will also try to shed some light on its shortcomings and then I will talk about our proposed method and finally we will see some experimental results on gene expression data now as you probably guessed I am by no means an expert in biology or biostatistics so throughout my talk if you have any questions related to the methodology or the algorithm I would be more than happy to answer I could also try to answer your questions on the application side but I would not trust my own answers on that front but we'll see Team all right first uh I'm going to start with some motivating applications um so as we all know many Real World Systems um have some underlying structure that changes over time or space in some of these problems the underlying structure of the system is actually unknown for us and all we have is a limited number of potentially High dimensional and randomized or noisy samples or measurements for example it is well known that different brain regions interact with one another in response to different physical or or mental activities but all we have is the fmri data that is collected from the brain now understanding this underlying brain connectivity network is very important specifically for the early discovery of different brain pathologies such as Alzheimer but this problem is very large scale it is a huge scale problem in fact the full brain image I believe has more than 200 000 voxels which implies billions of potential links uh between these water Waters or or brain Networks and this is also well known that the the underlying brain connectivity Network changes with age and maturity uh for example uh here you can see uh that the level of gray matter in the brain changes with age so instead of learning one static connectivity Network we essentially need to learn a sequence of dynamic connectivity networks and that potentially changes over time to better understand the behavior of the brain with age and another application is in the gene regulatory networks here given the gene expression data the goal is to infer a network that describes the correlation between different between the activity of different uh genes within each cell but similar to the brain connectivity networks Gene regulatory networks are also larger scale and they change over time or space in response to different environment mental or physiological disorders or different diseases like cancer for example here on the left you can see the changes in the gene regulatory Network for a specific cell as the patient develops cancer here the first figure figure a shows the regulatory Network for a normal cell and the last figure figure D shows the regulatory Network for the same cell that has developed breast cancer so you can uh you can see that we can have both of these networks at different times in a single patient or even at the same time but in different cells in fact we'll see an application of this later in this talk so in all of these applications our goal is to go from the collected data or measurements to a sequence of dynamic graphical models that describe this dependency structure in the data now one popular approach to do that is to use the undirected graphical models or Markov random fuels where each variable is defined as a node in the graph and the correlation or dependencies between different nodes are captured by weighted edges in the graphical model so for example if you have three nodes or three variables their model has three nodes in the graph and if there is a non-zero correlation between these variables that are captured by weighted edges in the dynamical setting understanding this dependency structure essentially boils down to estimating a sequence of dynamic Markov random fields for example these Markov random Fields can change spatially over space here you can you can see one example and for example across different cells or over time with age with maturity or in response to different uh exogenous events now uh one popular method to do that is based on the so-called maximum likelihood estimation or MLB where the goal is to essentially find a graphical model based on which the observed data or the measurement is most probable to occur now if you want to also incorporate some prior or site information about the problems such as sparseity smoothness or any other structure such as locality we can add different regularizers to the mle method to promote these types of structures in the estimated model now the regularized version of the mle approach is probably one of the most commonly used inference methods for graphical models and really for for General inference problems so we're going to start with that in fact to better explain this let's consider a very well-known instance of the problem which is the gaussian Markov random Fields now here we assume that the collected data is coming from a multivariate distribution that changes over time and space and now in this context the regularized mle boils down to the to solving the following optimization problem given a bunch of samples that are collected over time and space first we find the sample covariance Matrix uh we showed that with Sigma s and t and then find the fault and then solve the following optimization problem right it is a minimization over a sequence of Matrix variables and the objective has four terms the first term here corresponds to the maximum likelihood estimation of the inverse covariance Matrix when the samples are coming from zero me gaussian distribution now the other three terms are regularizers that are controlled by the regularization coefficients beta 1 beta 2 and better three and they have uh the role of promoting special structures in the estimated inverse covariance matrices for example in individual parameters or individual elements or their temporal or spatial changes now why do we care about the inversible range Matrix well in the gaussian setting um they have a very nice interpretation the zero non-zero element of this inverse covariance Matrix or Precision Matrix at any time or or uh location reveals the support City pattern of the graphical model and the regularization on the inverse covariance Matrix actually enables us to promote different structures in the corresponding graphical model for example we know many graphical models have localized or Sports structures this can be captured with an l0 regularizer in the objective what is l0 l0 basically penalizes the number of non-zero elements in this Theta or the Precision Matrix so if you have a lot of non-zero elements that penalty is going to be large we can also add different temporal or spatial regularizations in the mle formulation for example some graphical models have a sparsely changing structure meaning that only a few edges change over time or space in that case we can also promote sparsity with an l0 regularizer on the temporal or or spatial differences of the parameters some other graphical models such as brain networks for example they change smoothly over time we don't have abrupt changes in the graphical model in that case for example we can use L2 penalty on the differences and there are some other penalty structures that you can see here now let's just consider the special case of sparsely changing gaussian Markov random Jewels uh where each inverse covariance Matrix we assume it is sparse and it also changes sparsely over time we don't have a space component here just for Simplicity in this case the regularized mle corresponds to this highly non-linear and in fact combinatorial optimization problem it is non-linear because of this strange log debt term and it is combinatorial because you're essentially optimizing over zero non-zero pattern so if you're suddenly faced with this cursive dimensionality because a search space will grow exponentially right now although these regularized mle approaches enjoy very strong statistical guarantees it is classically conjectured that they are very hard to solve they're intractable to solve now to get rid of this combinatorial nature the the common approach is to basically replace l0 penalty with its L1 relaxation which is continuous and Comics this is the underlying idea behind several well-known methods like lasso or graphical lasso we can handle the combinatorial nature of l0 so we just relax it to L1 and just hope for the best right uh now uh with this relaxation the the problem becomes convex but the issue is that it will have inferior statistical guarantees especially in the dynamical setting now let's see the performance of this relaxed regularized mle approach on a very simple case study suppose that here we have only four time steps so T is equal to four also each inverse covariance Matrix is 25 by 25 so it's relatively small instance the number of non-zero elements in the inverse covariance Matrix at any given time is is we assume that it's a hundred that's how we design it so they're relatively sparse and uh the number of changes in the inverse covariance Matrix is limited to 10 so it is sparsely changing this is an example of a sparsely changing gaussian Markov random field so um here on the left uh um you what you you see is the mismatch error or the number of mistakes in the estimated parameters using the relaxed regularized ml mle that I just mentioned now you can see that even with an exhaustive surge over the regularization coefficients beta one and better two the best mismatch error we can get a 70 meaning that the estimated Matrix will have 70 mistakes in its sparsity pattern uh the right figure here shows the value of the non-zero elements or the true Precision Matrix and the estimated one at any given time you can see that this this red curve is the true values that we want to estimate uh the the blue curve is the is the value that we estimate using regularized mle you can see that there's a huge bias in our estimation this is because of the shrinking effect of the L1 regularizer okay now let me give you an overview of uh what we have seen so far uh uh as uh we mentioned uh if you want to have statistical efficiency we typically need to solve a combinatorial regularized maximum likelihood estimation for example if you want to impose Sports in our model the best choice would be regularization uh with L1 l0 penalty uh but uh the issue is most of these uh problems are intractable because of their search space the search space grows exponentially so in order to get computational efficiency we relaxed regularization to arrive at a convex or continuous uh regularized mle for example we relax l0 to L1 but we showed that this these methods often do not come with a strong statistical guarantee now our goal is to introduce a method that achieves The Best of Both Worlds it can efficiently handle the combinatorial structure and or the combinatorial nature of the problem without any relaxation and at the same time we want it to come with a strong statistical guarantees now the key idea behind our method is to replace this mle with a more tractable optimization problem now before talking about our key results I just want to say a few words about the tractability of any optimization problem and it's computational efficiency now it is a conventional wisdom in optimization theory that computational methods with exponential dependency in the dimension time memory or data are both theoretically and practically inefficient to use and Implement on the other hand uh there is this class of methods with polynomial scaling that are known to be at least theoretically efficient to use but even these so-called polynomial methods can quickly become intractable with increasing skill of the problem for example regularized mle belongs to this class so within the realm of binomial methods really the only class of algorithms that are truly scalable to massive problems in genomics and brain networks with millions or billions of parameters are those that scale near linearly with respect to the dimension of the problem so the fundamental question is how can we design computational methods for the inference specifically for the inference of dynamic Markov random fields that are almost linear they scale linearly with time memory and data now I'm going to try to summarize our key results in in only one slide this is probably the most important slide of my talk so in case you need to stop listening this would be the ideal place for that the rest of my presentation will be to uh to clarify some of the points I'm going to make next okay so our first key result is that unlike the conventional wisdom uh we can in fact infer Dynamic markup random Fields with exact and combinatorial l0 regularization not only that we can solve this problem in near linear time and complexity and now this is very important because it basically implies that not only can we handle the cursive dimensionality or the combinatorial nature of the problem efficiently but we also can do that even more efficiently than the relaxed regularized mle uh which we already know is statistically inferior okay now the second key key result is that we can find the solution not only for a single sparsity level but for all sparsity levels this means that we can recover the entire solution path for all values of regularization coefficient this is important in many cases uh because uh we don't know what sparsity level we need so we need to do some sort of cross validation for example or Bic to find the correct sparsity level our result says that we can actually do this much more efficiently than just solving the problem over and over again okay because we can recover the solution path the third key result is on the statistical side we show that we can actually learn Dynamic Markov random field consistently meaning with a small statistical error even if you have as few as one sample per timer location now this is something that even mle some of the mle based methods cannot achieve now um how can we achieve this result uh through the following optimization problem it's a constraint optimization our goal is to recover the parameter Theta ST which is the so-called canonical parameter of the graphical model from which the graph structure can be recovered for example for gaussian Markov and field this canonical parameter is the Precision Matrix the objective is purely based on the regularization that comes from the prior or side information such as a smart city or some specific temporal or special spatial structure the constraint here basically bounds the distance between the unknown parameter that we want to recover the unknown canonical parameter and the so-called approximate backward mapping of the Markov random field now I'm not going to talk about the exact definition of approximate backward mapping but roughly speaking it gives us a crude estimate of the parameters of the graphical model that you're trying to estimate okay um turns out that this this uh backward mapping can be efficiently obtained directly from the data for different classes of this distribution such as gaussian Markov random Fields I'm going to talk about that later now let me give you two instances of this problem now if we know our Markov random field changes sparsely over time and there's for Simplicity again there's no spatial component then we can pick l0 penalty for the functions f and g in the objective and if we pick l0 L Infinity Norm as a dense distance measure for the constraint we end up with this problem right similarly if we know that the Markov random field changes smoothly over time and then we can use L2 Norm for the function G now the important question is what do we gain by going from this regularized mle to this constraint optimization problem it seems that we still have the combinatorial nature of the problem because you have zero Norm everywhere next I'm gonna show you that in fact this new optimization framework can outperform regularized mle both in terms of the computation and statistical guarantees okay now before talking about these results or these guarantees well let's consider the same small case study that I showed before just as a recap we generated 25 by 25 inverse covariance matrices that are changing sparsely over time now our goal is to recover these inverse covariance matrices with our proposed method here on the left you can see the mismatcher for our proposed method compared to the regularized mle for different levels of approximation error in the backward mapping you can see that our method in fact leads to zero mismatch error it does not make any mistake for a wide range of approximation levels in the sample covariance Matrix whereas the regularized mle always has a non-zero mismatcher even if there is no error in the available sample covariance now the second figure is more interesting here it shows the non-zero elements of the true and estimated signals the blue curve as I mentioned is the estimated signal using the the regularized mle which we have seen before the estimated signal using our method is shown in green and as you can see it is very close to the true uh solution or the true non-zero values that you're trying to recover now based on this case study I hope I could at least convince you that our proposed method can potentially be a better choice for the inference of dynamic graphical models now to streamline my presentation in the rest of my talk I'm just gonna focus on the sparsely changing gaussian Markov random field so we assume that the Markov random field changes sparsely over time so there's no spatial component for now and the underlying distribution is gaussian I should also mention that our results um can be extended to more General uh setting when we have both changes in both space and time and it can also be extended to more General Distribution Beyond gaussian Markov random views for example discrete Minecraft random Fields but we we're not going to talk about those cases here uh I would be more than happy to discuss them offline and towards the end I'm going to give you some references that have these extensions okay so as I uh is there any question all right uh so um as I mentioned here we consider this uh sparsely changing Markov random Fields with uh gaussian distribution so our canonical parameter is the Precision Matrix and the functions f and g are l0 penalties to promote sparsity into individual parameters and their differences and the distance uh from the approximate backward mapping is measured with respect to the L Infinity Norm okay now as you can see our entire optimization framework relies on the availability of this approximate backward mapping now the question is how can we find this approximate backward mapping in the gaussian setting a recent work actually proposed to use the inverse of a soft thresholded version of the sample covariance Matrix as the backward mapping okay so the idea here is that given the samples first we find the sample covariance Matrix this this is this is going to be a dense and potentially low rank Matrix if the number of measurements is not sufficiently large then the soft threshold the off diagonal entries at some level okay the resultant Matrix is going to be sparse uh but probably full Rank and finally take the inverse of this thresholded Matrix this is going to be our approximate backward mapping now the interesting thing here is that we can actually drive very strong statistical guarantees for our proposed estimation method assuming that we can solve it efficiently using this simple backward mapping okay in particular suppose that the number of available samples at each time which we show it with NT skills logarithmically where DD is the size of the Matrix the Precision matrix it's D by D and also logarithmically with t then if the the other parameters of the problem satisfy certain properties then we can ensure that with high probability we will recovered the correct sporting pattern of the inverse covariance Matrix and their differences and we're going to have small estimation error and the estimation errors decays at the rate of 1 over a square root of N and T okay so roughly speaking this theorem basically implies that the estimated inverse covariance Matrix that we get by solving this optimization problem it's going to have correct the sparsity pattern and a small estimation error provided that the number of samples at any given time scales logarithmically with the dimension of the problem now this implies that even if the dimension of the unknown parameters in this case is the dimension of the Precision Matrix is significantly larger than the number of samples we can still recover the true solution with high probability and with strong statistical guarantees now this is a strong result to some extent but it has an important limitation it requires multiple samples at each time step right the theory says that we need to have at least logarithmic number of samples at any given time but uh in many cases in many applications we may only have one sample per time right uh while the underlying graph continues to change with the incoming data so we may have asked you as one sample per time for for the underlying Markov random field so if we ask this somewhat um ambitious question can we learn the time very Markov random field with as few as uh one sample per time and uh turns out that the answer to this question is yes and the key idea is to use kernel averaging okay the main intuition here is that if the if the underlying Markov random field changes slowly over time then the past observed samples can still probably reveal some information about the the structure of the Markov random field at the present time because they are probably collected from different but very similar distributions okay now I'm going to explain this intuition with some graphs our previous theorem basically said that if if the data arrives in batches okay uh we can we can recover uh the true solution if you have multiple samples at any given time and we can recover the the the the uh Precision Matrix based on these samples on the other hand in different settings the data may arise sequentially or one at a time and each sample May correspond to for example different distributions right here the idea is to consider a weighted average of of these samples where the weights are coming from a kernel that essentially assigns different weights to different samples so if you're at time T for example uh the the samples closer to time T will have a higher weight than the ones that are far away from time t okay now using this approach the the sample covariance Matrix corresponds to this uh weighted average of the samples where the weights are coming from a kernel by just uh I just described that kernel to your H the parameter H is um a parameter that controls the bandwidth of the kernel it is chosen based on the rate of change in the Markov random field for example if uh we know that it is that the Markov random field changes slowly then we can pick a large value for for the span width and if you pick a large value for the span width the weights will be more evenly spread out across time now it turns out that here we can use the same backward mapping uh that we saw before except that here we assume we use this thresholding uh instead of uh the sample covariance Matrix on the weighted average that I mentioned above with this modification we show that even if we have as few as one sample per time we can still correctly recover the sparsity pattern of the inverse covariance matrices and their differences under some conditions okay now uh we're done with the statistical guarantees uh for the proposed inference method but we still haven't explained how we can solve this problem efficiently in practice right before moving on I just want to make sure that there is there's no question foreign so let's move on now let's go back to the the original problem that we introduced uh for the sparsely changing uh Markov random fuels uh now the first and perhaps the most uh important thing to note here in this optimization problem is that in fact this this problem can be fully decomposed over different coordinates of the unknown parameters now our constraint here is the element wise maximum it's it's the L Infinity Norm which is the element by its maximum of the differences which is essentially equivalent to D Squared number of linear inequalities over different elements of the Precision Matrix you can reformulate this maximum into individual linear inequalities now why is it important because it leads to a fully decomposed optimization problem over different coordinates of the Precision Matrix for example suppose that we need to estimate this Precision Matrix uh corresponding to three different times if you try to estimate it using mle for example all these uh unknown elements are going to be coupled together because the mle is not really decomposable our method naturally decomposes into different sub problems each Define on an individual coordinate of the Precision Matrix for example here to estimate the first coordinate in the first row of the Precision Matrix the only need to solve an optimization problem or an optimization sub problem defined over the blue entries uh now more rigorously each of these sub problems uh look like this for every component I and J we need to solve an optimization problem in this form that is coupled only in time it's not coupled over different coordinates it's only coupled in time here the function I is just the indicator function it takes the value 1 if the i j element of theta T is non-zero and it takes zero otherwise and the constraint is just a simple lower and upper bound on the parameter um now to recover the full Precision Matrix we need to solve D Squared number of sub problems in this form now what are the implications of this decomposition well the first implication is that we need to solve significantly smaller sub problems each sub problem is going to be defined only over T variables T is the length of the time the other implication is that the overall complexity becomes linear in the size of the Precision Matrix why is it linear remember the size of the Matrix size of the Precision Matrix is 2 D Squared right d by D Matrix and we have in this case we have D Squared number of variables this is very useful since in many cases the size of the Precision Matrix is very large the other very important implication is on the implementation side based on this decomposition our proposed optimization can be easily parallelized this is very important because most of the regularized mle approaches are not amenable uh to uh to parallelization we're going to see the benefits of this parallelization later in our numerical study now um we still haven't answered one question how can we solve each sub-problem efficiently now each subproblem as you can see here is still combinatorial right they have indicator functions so in principle we may still face with this curse of dimensionality in each of these sub problems but it turns out that we can actually do much better than that now I'm going to try to explain this this algorithm that we came up with through different steps let's start with a special case where this beta coefficient is equal to one this means that the only care about the sparsity in the parameter differences so we do not penalize the non-zero elements in individual parameters in this case it turns out that a simple greedy algorithm actually can give us an optimal solution the idea is the following at time t equal to zero at the initial time we look into the future and find the longest sequence of non-empty overlapping feasible interval models so each of these problems have a lower bound and upper bound they Define a feasible interval we look at the non-empty overlapping feasible interval then we set the variables uh within that range to an arbitrarily value from that non-non empty overlapping interval and we do this same process for the rest of the variables now here's a toy example suppose that these are segments uh that show the upper bound and lower bounds for different feasible intervals at different times x axis at the time y-axis is the value of these feasible intervals using our greedy algorithm first we find the longest non-empty overlapping interval which is this blue box uh in this figure then we set the variable from time 0 to time five to any arbitrary value from this interval after this we inevitably have a change in our variables because we we don't have a non uh uh we don't have an overlapping interval and we incur a cost for that but then we do the same process we just find a non empty overlapping interval and set the variables accordingly now we can show that this very simple and very greedy algorithm can in fact give us the optimal solution for this special case where beta is equal to one okay now the situation is very different in the general case where beta is strictly between zero and one now in this case um if you want to promote sparsity in both the parameters the Precision Matrix and their temporal differences not only do we want to have as few changes as possible but also we want to to set the variables to zero as much as possible now clearly if we uh have a non-z if if zero is not a feasible solution we can just use a greedy algorithm right because we don't have a zero feasible solution as soon as we see a zero uh a feasible zero in these intervals we need to choose between two options first we can just ignore this feasible zero and just stick with our greedy algorithm just to avoid paying a penalty uh for changing the value of the variable but note that here we incur a cost for the non-zero elements the other option is to switch to zero incur the cost of switching but instead avoid paying the penalty for the non-zero elements now if you take the first approach or first option in this case the total cost is going to be 10 times 1 minus better if we take the second approach the cost will be 6 times 1 minus beta plus beta now a simple calculation would show that the optimal solution is to switch to zero um only if the the beta is less than 450. now this intuitively makes sense uh because if beta is small that means that we put more weight according to your objective we put more weight on the non-zero elements so having a non-zero element is more costly than a change in our variables okay now uh this intuition would imply the following possible algorithm right we we use greedy algorithm as long as zero is not feasible as soon as we see a feasible zero we take one of these options if beta is large meaning that we have small penalty for non-zero elements we stick with our greedy algorithm we don't switch to zero because the cost of switching is too high if beta is small then the cost of having non-zero elements is very high so we would want to switch to zero now the question is uh does this algorithm give us the optimal solution perhaps surprisingly the answer is no now there's a contra example for this intuitive algorithm which I'm not going to discuss because of the time instead I'm going to talk about the the optimal algorithm okay for a general case where beta is between zero and one turns out the correct way to solve this optim uh to solve this optimization problem is to cast it as a shortest path problem over a directed acyclic graph or uh or a dag let me explain this algorithm with a simple example suppose that the feasible intervals are in this form first we find the maximal zero sequences in these intervals so for example here we have three zero sequences or zeros that are feasible we map them to three different nodes in the graph so uh each feasible uh zero interval is going to be a node in our graph we also add a source and sync node these are going to be the nodes in our graph next we connect the source to the first node corresponding the first zero sequence and we assign a weight to this Edge then this this red this Edge uh comes from the previous greedy algorithm that I mentioned uh and I'm not going to talk about details of this construction but turns out uh the the resulted crap is going to be a weighted complete uh directed acilogram now the reason why we construct this synthetically generated graph is because it turns out that the optimal cost for the problem that I mentioned for General beta between zero zero and one corresponds to the shortest path between the source and the sink in this new graph and which can be solved very efficiently using a dynamic programming okay now uh finally we can put all these pieces together and we can we can uh solve the original optimization problem that I mentioned and we can recover the Precision Matrix now I'm not going to talk about different steps together but I'm going to mention the last result which is the computational guarantee turns out that the proposed method or all these steps can be done uh in um time and memory complexity that scales with d squared times T which is exactly the number of uh variables in the problem remember e at each time the Precision Matrix is d by D so we have D Squared number of variables and we have two different times so the total number of variables is going to be D squared times T which means that the complexity of solving this problem scales linearly with a number of unknown uh variables or unknown parameters this is exactly what we wanted all right now for the rest of my talk I'm just going to give you some some uh experimental results now in our first case study we consider a randomly generated massive data set here the data is collected from a gaussian distribution with a sparsity changing inverse covariance Matrix now here on the left figure you saw you you you you see the runtime of our algorithm as a function of the total number of variables which is equal to D squared times T you can see that the run time is almost linear with respect to a number of variables which verifies our theorem using our method we could solve instances of the problem with more than 5 million variables in less than one hour on a normal laptop computer the regularized mle in this scale cannot even start running running the algorithm okay and the right figure shows a statistical consistency and we get as we increase the number of variables we get close to 100 percent sparsity accuracy okay okay now the next slide shows the performance of an algorithm after parallelization remember our algorithm is amenable to uh parallelization uh because of this element-wise decomposability using five cores we could reduce the runtime of our algorithm by 40 percent on average but beyond that increasing the number of cores does not really significantly reduce the runtime in fact it may lead to some memory issues which I'm not going to discuss here now our next case study will be on gene expression data I mentioned at the beginning of my talk uh that here our goal is to understand the underlying uh Gene regulatory Network given the gene expression data and uh the gene regulatory Network can be used to identify different regulatory interactions between genes which can probably help us better understand different disease processes like cancer in a macro level rather than individual Gene levels uh here we collect the data from a glioblastoma tissue of a patient here in Michigan medicine the data is collected uh in Michigan medicine and the data is pulled from two adjacent tissues and expression data is obtained from 2500 most variable genes and then through a 3D fancy 3D embedding of the data which I'm not an expert in we could cluster the data in five different parts or regions you can see these clusters here with different colors each cluster will be modeled as an individual Markov random Guild now the important thing that I need to mention here is that um unlike the previous case study here the data does not have a temporal component and instead it is spatially varying so these five different marker of random fields are specially correlated to each other rather than temporal correlation but our method can be extended to this case as well now um next thing I want to mention is the parameter tuning now there are different parameters that we need to find you and obviously based on the available data for example how do we pick the regularization coefficient right in the objective or really other parameters in the in the proposed method now to fine tune the regular regularization coefficient the objective we make them proportional to their spatial and expression distance so if for example two clusters are closed spatially the regularization coefficient between these two clusters will be large because we want to encourage more similarity in their learned networks there are also other parameters that you need to fine-tune for example we need to set the upper bound and lower bound on the constraint of our problem uh and we set these parameters based on pic Criterion we just try different values estimate the network and measure their likelihood and then pick the one that gives us the highest likelihood okay this was a brief overview of the of how we set the parameters now I should also mention that the gene expression data is not gaussian it is known to have a negative binomial distribution now to use this gaussian Markov a random field we need to use this non-paranormal transformation to change the distribution of data to gaussian now next I'm going to talk about very briefly talk about the the results now unfortunately I can't really show the infrared networks in each cluster because there are big networks what I show instead is the 15 strongest edges in each cluster for the infrared Network and the node sizes here are scaled with their degree so larger nodes means that they have a higher degree based on these graphs you can easily see that the different clusters have uh really distinct with regulatory interactions that are really different from uh from each other in fact if you look at the number of edges in different clusters uh you can see that they are drastically different some of the Clusters are super sparse for example cluster 3 has only 446 edges recall that our uh the each cluster has has 2500 nodes another cluster um has close to 14 000 edges this shows that uh uh uh the the the infrared networks are heterogeneous but you also have some level of sparsity that are coming from the spatial regularization uh here you can see the heat map of the similarity between different inferred networks in different clusters uh you can see that some of these clusters have some level of similarities for example here cluster one and three have similarity degree of 0.3 a similarity degree of one basically means that they're exactly identical uh now I uh I also want to mention uh that our results are biologically meaningful here you can see the interactions between the transcription factors in in our infer network if I understand correctly uh these uh transcription factors are proteins that play an important role in the evolution of the cancer uh here are the interactions among the most uh important transcription factors in different clusters based on their infrared networks some of these factors clearly stand out right since they are they have large degree interestingly these are exactly the transcription factors that are that are known to play key role in the glioblastoma cancer which was the subject of our study all right now uh I think I'm running out of time so I'm just going to conclude my talk uh as I mentioned the main goal of this talk was to infer large scale uh Dynamic Markov random Fields with strong statistical and computational identities that was our goal we developed a scalable inference method for dynamic Markov random fields for the inference of dynamic markup random fields are under different site information such as Sports City and other temporal or spatial structures and in practice we showed that they they can we can solve different instances of the problem in fact instances with more than 500 million variables in less than one hour and we also talked about an important application of this problem in gene regulatory networks now here are the references uh this talk was mainly based on the first two papers that I mentioned the part that I didn't talk about was this the spatial component and and other distributions those are based on the the last the the bottom two papers one of them is published the other one is underper preparation now I'm going to stop here and I would be happy to answer any questions thank you sir we'll give you a round of applause thank you um so does anyone have any questions if you're in the room uh you can ask so far there have been no questions uh online but if you're online you've got questions you can put them in the chat box um we can read them out or raise your Zoom hand and we can call to unmute you and again if you guys if there are any questions in the room uh feel free to ask so we'll just give it a couple of minutes just in case anyone online typing foreign or hearing any questions and solar I know that you have another meeting that you need to get to so I don't want to yes keep you uh extra long or anything thanks again thanks uh everyone for attending to talk uh thanks Marcy for the invitation um and yeah thanks for taking the time thanks everyone for coming and hopefully I'll see everyone at next week's presentation bye