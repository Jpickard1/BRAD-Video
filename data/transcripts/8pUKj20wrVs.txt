welcome everyone we can go ahead and get started while the last few people alright so we're gonna go ahead and get started hopefully everyone's enjoyed pizza there's a sign-in sheet that is going around please sign in that of course helps with the pizza so I think most of you guys have been here before so I don't really need to give too much of an introduction to the seminar series itself so I'll just go on with our speaker I'd like to introduce Shenron who is a PhD student in the School of Information and he's going to talk to us about some deep learning models hi everyone glad to be here I'm Shinra and thanks for an introduction I'm still PhD student for a couple more months hopefully everything goes feels like I'm out here around April so today I'm glad to talk about interactive programming tools powered by deep learning models today's talk mania consists of two parts where should I point this I was working one minute ago I'll just use the key ok so the first part is the main thing I want to talk about the day coat mint it's a user interface powered by by modeling body model on the backend it supports user to find code examples and modify code snippets using natural language and the second part is a working progress project image to code allows user to use visual examples like images to find code examples and it also involves new neural networks synthesizing code examples on on the fly oh yeah ok so the first part so this is a paper I presented at user interface and software technology symposium this past October in Japan so the motivation for this project is that we want to help user program more easily so there are two specific problems with programming that we identify to be difficult for users it's not especially novice users first one is that there's often a lot of API is involved in end-user programming and it's really hard to navigate through these API sand you know with a lot of functions and parameters and values if you're not familiar with those and also it's often hard for user to translate something that if they have in mind into the actual API call sequences or actual code snippets so in this project we focus specifically on one type of programming interactive programming but that I mean the kind of programming you do was you know when you interact with ipython notebook or R or MATLAB where you can write a piece of code and edit it and we will see instant are a result or preview so that you can iterate very fast and get insights into data so I'll show you first what we built by using a video demo so two things before showing the demo I want to mention first is that we built this system for one specific library my pot lid it's a Python library commonly used for plotting figures and also in this video I want to assume that the user wants to create a bar chart that looks like this one so it's a toy example but like pay attention that the bars are red and there's a title on the top and so let's see how the user modify an you know a generic example to something like this let me roll the video okay so this is an interface of the system we built it's called common on top left there is a code editor top bottom left is image preview we will see it in a moment and this is a quarry box where the user can enter natural language queries and here's a list of functions and a multi-functional display for you know recommending code modifications so suppose the user wants to create a bar chart from scratch the viewer can simply type in a query create a bar chart so as the user types in a query the system automatically finds what functions are relevant to this query and rearrange these the color represents the likelihood that a specific function will complete this users will fit the users intention the user can click on the highlighted function and what the system will do is to show a list of code example images generated by the code examples found online so these code examples are crawled from Stack Overflow from github and other tutorial pages and the corresponding images are nicely laid out here for a user to preview the user can click on any of the image the corresponding code example is automatically populated into the editor and the user can also see a preview of what is generated by this source code so now suppose the user wants to change let me see the user wants to add a title here the user can type in I think add a title so again as the user types in the query the system automatically really ranks the most likely functions based on this query and then highlights use color to encode the likelihood again the user can click on the title I mean the highlighted function this time it's different because it involves inserting a new line of code and rather than creating something from scratch in this case the system shows a function preview and also this suggested parameters for this function here zero means the first null named argument of the function the user can click on the suggested function a parameter and also click on suggested value so again these values are crawled and counted based on the code examples found online and they are ranked by their frequencies so the user can intuitively see you know what kind of possible values are for each suggested parameter the user can click insert the system will automatically figure out the best position to put that line of code into the existing code snippet and the video preview is also an image preview is also automatically updated now suppose the user wants to change the color of the bars the user can type in change the colors of the bar now what's different this time is that it's not inserting a new line code but it's modifying an existing one so the system will highlight which line in existing code snippet should be modified again this is using her encoding when there are like multiple lines that are likely to be modified then multiple lines will be highlighted in this case there is only one the system can click on the suggested line and it will pop up suggested list of suggested parameters for the user to modify the user can click on the suggested parameter which is color and here the values are for this first specific parameter these are ranked by the frequencies they are used in the code examples found on line the user can click on a suggested value and hit update the code is automatically change in certain and so is the image preview so this way the user completes three tasks creating something from scratch inserting a new line of code and modifying an existing line of code so there are more complex complicated examples I will now show here but I would instead I would dive into into a bit more detail on how this works but any questions before I go on oh well it really depends on the collection of the code examples we have I mean say there are like 50 55 distinct colors used in the existing code example that's what we're gonna show in the universe but it's a if we want to store more engineering effort we can build a color palette for users to choose an RGB is whatever is that I know they're in the library right right so stack more flow we I mean for different things we take two different strategies for github we searched for those repositories that contain matplotlib at the keyword map outlet and then we filtered those whether they actually contain runnable code examples and for Stack Overflow we just looked at the tag of the questions and then it then from there is pretty straightforward to collect a whole bunch of right right so there is some normalization going on in the process I will talk about in in a minute but like the general idea is did so we normalize the different variable names to something more canonical and the system so the identifiers can identify that this module is my partly about pipe lock no matter what I mean regardless of what name people give it yeah we can revisit that issue in a minute okay so yeah so in that video uh just a quick recap I showed three types of programming operations scenario one create something from scratch scenario to insert something new and scenario three modify an existing line of code and we want to build some kind of black box that supports all three scenarios and in existing literature what was explored a lot was really the first scenario only which is you find some you have some natural language query and the system can find your code examples so traditional solutions people can Google or search for things in Stack Overflow and in a software engineering community people have invested a lot of effort into so-called code search or programming by example and in the natural language community people have also looked at natural language based code synthesis but again like I said all of these were focused on the scenario one instead of more nuanced you know operations where you need to not only understand the intent behind a natural language query but also the context of the code example so we think for this particular problem for building this black box deep learning is a good fit for a few reasons first is that is relatively simple to use it doesn't involve a lot of human engineering efforts to cut up the features it also is good for transfer learning if you provide a lot of data you know in one domain it's good it has the capability of you know adapted to a different domain and in this particular case if we have a lot of natural language text about Python in general it may be capable of that the relationship learn from there to specifically related to matplotlib and of course more data and a deeper structure we always lead to better performance and very likely we do have a lot of data for this problem so online and like I said there are Sakura flow data on github and a lot of tutorials text books and you know the massive online course data that involve code examples so how really how does this work it really involves building this black box that supports natural language encode as context code suggestion as output and in order to train this kind of black box it needs three kinds of data a lot of natural language talking about you know the the actions that the user wants to take a lot of code examples and what's more important is the association between natural language and code so in reality here's the data the kind of data we recrawl we from sacra flow get happening in tutorial pages we collected more than 50,000 code examples all related to MATLAB and over ninety meaning words in the text corpus mentioning about reactions about this library so now comes the most exciting part which is to open up this black box and see what's inside and inside this black box we use by the so-called bimodal data model bimodal means it supports two modalities one is source code on Ronnie's natural language and again the tasks for this model is to given the to contradict whatever output that can be used to render in the user interface so there are three specific problems in adapting this model to the actual problem the the first problem is that even any raw piece of code context how do we map it to the model input here it's really a lookup dictionary where there are about 100 different code elements so which I will talk I will talk about this mapping in a minute the second problem is given a natural language query how do we map that to the model input and third problem is the models output is a multinomial distribution how do we leverage first how do we define the elements in this distribution and then how do we map that to the user interface so let's look at these problems one by one the first problem is how to map our a source code into this lookup dictionary so for in so the source code here can have you know variety of structures but then we take the abstractions that each source called correspond so our abstract syntax tree so the abstract syntax tree can be broke further broken down were flattened to us sequences of a set of so called code Engram representation you can understand each Engram as a complete path or partial parts from the root node of the tree to some internal node or some leaf node of the tree so this way we break this tree down to a set of words you can say we're in grams and and these can be can then be mapped to this lookup dictionary any question about this so for example here there is a function call module function parameters province but the representation I created was the function the function and the parameter the function and another parameter and so on so it really we think this is a signature that represents what this code means and it can get more complicated like you know there are like to chain function calls so then you get the tree gets more complex here yes it makes the data space very sparse but that's one thing that you know the embedding model like this one the lookup table is actually good at handling so the space is quite sparse I'm sorry what is zero here yeah it means the sort of the 0th the first non name argument it means that this particular argument is present in the function call yes I may be mentioned it sorry but do you also bring in like the API documentation so you know all the possible things that can be put into what's right yeah we we do bring in API documentation yeah a nice tree it's useful further down in a task - slides down okay so now we are able to map any given piece of source code into a lookup table and now let's look at the natural language part so this one is actually much simpler a lot of existing solutions just simply use backup so cut back of bag of words representation given a natural language sentence we just break it down into a set of words in a sentence and then these are can be very straightforwardly mapped to the dictionary but the there's but there's a problem huh a problem with that there is a problem so in model Trent when we are training a model there is actually no such natural language query for us to leverage right we only see code examples and API documentation so we what we need to do is a step of a step called API documentation lookup where we generate the simulated natural language query from the code examples and in specific from each code elements like module function parameter and value so that we simulate what the user might ask for given this code example and so this is a sort of function as a surrogate of the user intent and during modeling employment we naturally have a user issuing those queries for us so we don't need to worry about that during deployment okay the third problem we want to address is the user interface so this Bible new embedding model has this this part as input and it has these you know internal representation of the code elements as embedding vectors and then they are concatenated as a hidden layer and then this is a softmax layer that generates a multinomial distribution so the problem is given this multinomial distribution over all possible code elements here a code element again can be module function parameter and value there's a lot you can imagine this is a distribution over all the possible element this is one one of the example how do we display it on user interface it is a design issue so we take inspect inspiration from this so-called spot by search interface on Mac you have Mac you typing in the system preferences you're typing a query it shows all the possible solutions at this given moment given this partial query we think this is a brilliant solution for addressing the ambiguity in the users query because the user can not only see the possible solutions for the current partial curry it also shows all the other options all the other options are still there they don't disappear so that the user if they miss type of something or miss something they can directly see all the other alternative options and it really gives the user a lot of flexibility and convenience to find find out what is possible at any given moment and we want to adapt that that idea but there's a problem sorry still getting used to this there are tens of thousands of code possible code elements in here and we cannot afford to display tens of thousands of elements in here it will really clutter the screen space but there is something we can leverage so there is a hierarchical structure in this lookup table remember there are modules functions and parameters they naturally fall in a hierarchy so we can leverage the hierarchy to redesign this spotlight search and we develop something called nested layers spotlight search we're given an expected I mean a district multinomial distribution we first show you know first layer we show the functions and then once the user click on the function it expands to the next layer shows the parameters and expands the next layer to the values and then the user can navigate through these again there's a natural language query boss on the top and then the user can complete that query and as the query is completed that the distribution is changed here and once the user makes change on a on a on a lower layer the change will automatically back probably back propagated to upper layer and then to upper layer and this way it's sort of flows back and integrate the solution back to the code editor so we want to test in reality how well this works and there are really two parts that we want to test first is the how well does the model work that by mallow family embedding model and we adopt information retrieval style analysis to do that and and we also we also want to understand how well does the end-to-end system work we use a lab user study for this purpose so let's first look yeah go ahead how do you training your training sample writing a piece of code then a simulated query from this piece of code right because there is the answer right yes so when we train it's actually displayed in this page so we collected a whole lot a whole bunch of code examples and then we use code example and the API documentation to January simulated queries and then in the training time we feed in the code context which is a bag of code elements and here the similarly a bag of natural language words and then we have some expectation on this side and the expectation is corresponding to what our code elements we selected to generate the API documentation rate the simulated query so it this one links backwards to the process of generating the simulated query and this way we do that propagation to train a model but then same code is also serves as an input to your to your coding for now or it's a different piece of code because there in that case you would be overfitting because if it's well right so that's right that that part I really didn't mention in this in the talk and so it's to avoid overfitting if we expect its code example here then we randomly turn on and off the other components in the code example so that means we simulate you know the user it could the user could want to add this particular line so that we would make the like that line missing in that coding of equal context so it's a random process and it involves multiple iterations to try different combinations of on and off for the API calls what's a great question you had a question okay yeah so let's dive into the first experiment where we want to use sort of IR style benchmarks to study how well the model works really boys boils down to how well the model can rank the expected results among all the possible candidates for code modification so this is how we set up the study we we have you know that we know that API documentation very well but we don't what we don't know is how the user will actually issue what kind of queries in in in reality so we go to Mechanical Turk we present them with two pairs of images where there are slight changes between the image we know what code generates which image and we asked the user to the Mechanical Turk to provide queries like this one so the the description would be how image a can be changed to image B and one example is change the color of the bar and and you know like any Mechanical Turk study the quality control is really difficult we had collected a lot of noisy cases where they say image B is more beautiful and we threw these away of course so overall there are it's a very small scale 306 sixty-one queries collected for five image appears and we threw them to the model so say for example this is an input carry and this is a ground truth because we know which physical element corresponds to that change and if we look at you know where this expected ground truth element is ranked in the model output and here's the result the key thing to notice here is the mean reciprocal rank for the map by model embedding model so this actually means that the expected results on average is ranked on a fourth place so it's the inverse of this number so it's not as bad as it looks 0.2 you see is very low exactly not as bad as it looks and another thing to take away is that when we take away the natural language context which is code only the ranking gets really worse and when we take away the code context which is natural language one way that the ranking is also worse so this means that both type of context information are useful in in the model let's look at the second experiment where we test how well the end-to-end system works we performed a lab user study where we heard 20 test subjects we give them a short tutorial of the system and we provide them with a piece of code and in the image generally generated by this source code and then we give them a so called so this is called starting point image and then we give them give them an ending point image we ask the user to figure out how to change this code snippet to a new Costa generates this ending point image so for example like this one they're always like three change small changes that to be made like you want to make the padding Roger you want to change the color of this title box and you want to rotate this pipe by 90 degrees counterclockwise so a lot of these small challenges to be soft by the user there are two conditions treatment condition is that the user can c-cut man and also google search results and control conditions that the user can only see the Google search result and the Google search results are sort of embedded in our interface right below the suggested codes changes so that the user doesn't have to leave that page to actually open up a different web browser yeah so this is what it looks like here is a system suggested changes and here's the Google search result in condition to user can only see it at the Google search results in their lab experiment so here's a result again we have 20 participants on average in condition one code meant using code man on google average user completes a task using a hundred eight seconds condition - 134 seconds so we can claim that this is shorter but the result is not really statistically significant so that we hypothesize that this is because of the large variety of the coding skills recruited by us from you know school for information master student so then a larger scale experiment or most scrutinizing filtering of the test subjects will potentially alleviate this issue and in the post experiment survey 70% of a user agree with the segment that cut my efficiently helps me solve the assigned tasks and one we also collect their open end feedback and this kind of feedback is very dominating the in the pool feedbacks we obtained the user says I like I like that it provides a comprehensive survey of commonly used parameters to functions it really helped me you know with navigating parameters replace is something that Google doesn't or can't do and is helpful so we think this is very encouraging that the user likes this function design well know like I said there's a wide variety of skill set so there are like two to three users there are really expert level most of them are like use it you know in their class only and what to users haven't used it before future directions so for this particular project we think there's still a lot more that can be done to improve this first of all how do we support a wider range of api's we're not just applauding library of Python and how do we find a representation that can encode the hierarchical structure right now we sort of flatten out the abstract syntax tree to have a sort of bag of code elements but in reality the dependency on the hierarchical structure may contain a lot of information that we haven't leveraged a leveraged yet and also find a banner association between actual language and code and encode API knowledge directly in a neural network model so any question about this project before I jump to the next one okay so yes I'm really what I said before so the next project is programming by visual example it's a work in progress so it's more open-ended at this stage it has a very similar motivations as the first project but it involves different inputs and outputs of the system so again programming is often in madam inevitable when we want to create visualization especially one it gets complex or involves a lot of repetitive parts or or interaction and in this particular project what we are focusing on is that these this particular scenario where the user has a visual example say I I go to I'm say I have a survey result but I I want to create this sort of sort of divergent bar chart to illustrate the Likert scale item distribution scope distribution but I don't know how to code this I do find this example in one say related work that I really like I want to re-implement this it's gonna take some time if I don't know where to start and if there is no direct direct support in the programming library that I favor so how do we solve this can can there be a system that takes this image as input and generate a code example directly for the user and how do we do this so the idea is that we want to combine computer vision to understand the bar existing visualization chart and language modeling which serves as a code synthesizer to directly write the code examples for user this way we think that will generate a visualization prototypes faster and make it easier overall for the user improve their productivity so the roadmap is like this we want to build an antonín pipeline that combines computer vision techniques and language modeling techniques where the input is a chart image and output is source code that generates that chart and of course we want to leverage a state of art in architecture where an encoder is a convolutional network that understands the chart image and decoder is some kind of recurrent neural network they generate source code the difficulty though is that how do we find training data for such kind of tasks because all deep learning models require an extensive amount of data for training and the idea is that we want to find as many code examples that can actually generate images as possible and then we want to generate augment these existing code examples by expanding and varying the parameters in the input space the data space to generate a large set of synthetic chart players for training and and also there is a really relatively little literature on evaluating such kind of automatic code generation let alone based on image so it's really uncharted water here but that's that there are indeed quite a lot of really working each separate component for reverse engineering of charts a lot of work in the HCI community people look at have looked at chart type identification so give me a chart whether they recognize whether it is a bar chart or pie chart that is relatively more straightforward it was solved quite a long time ago chart data instruction and chart redesign and regeneration I'm more difficult it involves the system recognizing what the chart is I mean what type of chart is and then to extract actually say that the actual data values from the chart say this is a bar chart with three bars the system needs to know that this is 1 this is 5 then this is 3 and then and somehow restyle it for the user so there are a lot of work on that domain for neural network domain a lot of active work is going on going with image captioning people have look at the CN plus our infrastructure and on the traditional more traditional approach side our people have to get refuel base or all of these other more you know engineering human engineering heavy based approaches for image based code synthesis there is so this is like both involvement is in this there is a ladies work that that recognize an image which is like a snapshot of an equation from a paper and it can generate logic source code so it involves some kind of image to code pipeline but it's different from our purpose because recognizing an equation and generate the logic source code it involves some linear sort of scanning from the left or right of the equation but there is some global structure like there's a fraction and in chart chart examples it's really different we need to understand the chart type amia understand or what kind of styling and then the graphic elements whether there is a legend there's a title so it's it's more different a problem so here's our model architecture we reused Karthi allows you know TOC architecture is very standard nowadays where there is a C encoder that understands the content of the image and our n encoder decoder that generates the caption one one token at a time I want to show you some preliminary results we have gotten and these are pretty interesting so first about data training data acquisition we acquired code examples from cheats online tutorial Sakura fluent github this is it's basically the same set of code examples we obtained for the previous project and then we filtered these by the president's graphic graphing API function calls and expanded those by varying their parameters so here are some examples so given a one you know sinc simple bar one bar chart I mean given one type of chart which is bar chart there are like a lot of variations that are possible it can be simple bars group bars stacked bars and the background things can be different and then all of these graphic elements can be different too so it's really hard to see clearly these image this source code but the idea is that we vary the styles and the data and the the number of elements that are present in each given chart so that we have a lot of pairs between the images and source code and then we want to first separate the testing of the encoder and the decoder so we first look at whether the existing convolutional neural net where are capable of detecting that and the chart types for this more smaller scale example we look at three big major chart types bar charts line charts and hydrants and then for within each major category we look at three subcategories like previous example for bar trans there are simple bar charts group buttress and stack bar charts and for line charts they're like you know simple line multi-line and so on for patrons there are like this pub explodable pie charts or normal pie charts and then we use the convolution neural network in order to get 768 dimension vector representation of each images and then we you use a support vector machine to see if these features can be sufficient for the classifier to learn a classifier a boundary between these is image image representations so here's an example so there's a confusion matrix between different chart you can see that it does really very well there are relatively little confusion between subcategories for example sync single line charts can be mistaken as simple bar charts but it happens very rarely for within line charts there are some major confusions I am working slightly more confusions going on but we think that overall this is sort of a proof of concept that the image representation can can be reliable at least to some degree so here's a 4/4 chart types are a multi line you we some for some cases we vary the line colors some cases you can vary the marker type types or by their marker is present or not so there are a lot of variations and we haven't really separated out the case where color is the only varying factor yeah but it's a it's a good question so this is this is the same set task where but I visualize the chart vectors charge betting vectors in the 2d space using Disney so what this shows is that the distribution of all the charts in a high dimensional space that are further dimensionally reduced to 2d space each point is a chart and the different colors represent different chart types so as you can see here this is maybe too small but let me read it for you the other green dots are bar charts and all the blue dots are line charts and all right dots are a pie chart so visually you can see that these are separated fairly nicely there are some mixing here possibly explaining the confusion between bar charts and line charts and then you see the separation between within-group so some pie charts are here some pie charts are here and we digged out a date dig deeper and find out that it's really the same that causes this separation so so we remember we vary the things that some things are light and some things are dark and it really a plays a very important role in this case so so these are likely dark themes and these are light things so it's a we need to keep this in mind out further down building the entire system okay so that was chart type of recognition and then we are interested in weather chart features can be recognized by the encoder and in this case we generate the simulated training data and synthetic training data where we vary the knobs where the gridlines can be either present or absent language can be changed line color can be changed and whether the legend is on on this side or this side and so on there are a lot of things going on and we picked 10 chart features as prominent features we want to test against and see we want to see that whether given the learn embedding vectors a support vector machine can differentiate the whether the feature is present or not and to some degree it is satisfactory so the grid lines language and line colors are really performant but for more fine-grained details like whether there's a title here it's it's less sensitive and we right now we can only speculate the reasons I think this is like every image is confining to 256 x 256 pixel space and a lot of the smaller features are gone or like more you know obscure from the models perspective so sorry and then we moved on to see how well the end-to-end pipeline works so this involves given an image looking at how well the system generated source code matches the original ground truth source code and to you so we use a fairly standard benchmark metrics which are commonly used for machine translation and automatic summarization of you know natural language documents and they basically break branches into engrams and then see the Precision's and recalls of these engrams in the target generated source code and then there are some different average things going on here so we compare the so let's first look at this score blue score it involves one gram two gram all the way to foreground of the of the ground truth on the most left-hand side here is the score we obtained by randomizing the chart vector representation so basically we consider we in a pipeline we did some sap sabotage by sabotage by just introducing random noises here and then see what kind of outputs can be generated so this and then we compare that with actually using the source code at the image and this can tell whether the information in the input image is actually being leveraged and used in generating the source code or you know from the other perspective how well the source code is generated conditioned upon the information in image and we see the the difference is very significant and the performance between different these are like different sub chart types are relatively more or less in the close region for this different machine translation metric it confirms the similar trend as this one but for this we really don't understand what's going on here so that's a sort of ongoing inspection normally the score is that the higher the better the performance of the system so that that's not something we're still actively investigating and here we honor understand how well our general domain model let me hide it for me so I understand how our general domain image captioning model will perform on this the idea is that to inspect what is the limitation of these Arland decoders so let me first show you some results which we see so all these are you know obtained by looking at how well a model it performs when the model is trained on ms cocoa images with their general domain data set and there are likely all kinds of humans and dogs and cars and so on but there are no charts so given this bar chart it shows a bathroom is a bunch of urinals on the wall simple choice think that it's pretty funny but it does capture the sort of the pile structures on the great structures on background so it it may indicate that this is a bathroom vo we know in the darkroom it matches the theme structure so toothbrushes inside is interesting on one side it tells us the model does capture or successfully conditions the decoding process on the visual features of the input image but on the other side it also tells us the constraints of the model this kind of model but no matter what information you feed into the model it will generate something that reads like ground grammatical English and on the other side imagine that we train this using source code so no matter what kind of visualization we provide as input it will generate some kind of grammatical Python source code but does it really generate that image it's a it's open question I mean the model will pretend that I know what it is doing but it's actually not knowing it's just imitating what our distribution is in the training corpus so this is the idea of the page that's right and we actually look at the input distribution right and wide is a popular trigram so right in mind this freeze occurs block so whenever the user the model generates a red end it just wants to generate white because it's have seen a lot of red and white as a phrase so this again exposes the same problem that the model is just imitating whatever distribution the Trinity has okay so some up coming work for this particular project is that we want a January training data with more variations to cover a lot more variety of chart types and variations that partially will solve this data sparsity problem like this red and white problem we want the model to be really looking at the signals in image not just dreaming or like recording from memory and also we want to use expand empirically experiment determine which metrics really can in fact I evaluate how good a sense is that Co example is right now machine summary machine summarization machine translation metrics really don't serve well to the need of evaluating programming language and also we want to potentially integrate image to cope with code man so then the workflow will be given a visual example feed it in and the system generated code example and then you can use the natural language to modify it so then it will be expedited workflow for generating any kind of visualization that's it thank you one minute right so the pixels are like live in a 2d space right and you have 250 250 56 the 768 dimensions are the vector 1 D vector representation that represent that image so it's a you can say Syria as a objection or like you know a mapping from a 2d space to a 1d space I mean no actually 768 dimensional space no that's what the embedding part that neural network is doing yeah using PCA if we could give you ready poor performance because it only captures the whatever the dimension has the biggest variance but with neural network a learns a lot more about relations and the similarities between the training data and then you know the underlying structure that exists in that 2d you know pixel space thank you