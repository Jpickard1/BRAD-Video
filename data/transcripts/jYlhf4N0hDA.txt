well I know that people are still coming but I think we should get started so um it's a great pleasure to welcome you bill here and we're really looking forward to a seminar so Jane just gave me a 40 page CV will be also I'm placing quite a challenge here so I'm gonna shorten it there is a very nice summary of Bill's achievements on the is MV website because it's also a good time to congratulate him on on is B is and be innovator a word that he just received was just announced a few weeks ago I guess you haven't received it yet but it's coming so using that very short summary that they posted so Bo Nobel is a professor of Genome Sciences at University of Washington and also professor of computer science as well and he received his PhD degree in well less than 20 years ago and 1998 in short postdoctoral fellowships in fellowship and David how's the lab at UC Santa Cruz and moved to New York three years later and remain there since the time so it was really internationally recognised his work on developing machine learning methods for all sort of high throughput data pretty omics to genomics including start modeling and today we'll hear his recent work thanks Alexei and thank you all for coming I've enjoyed visiting with people here today and it seems like there's a lot of exciting stuff going on it's also nice to be back in the Midwest it wasn't I grew up outside Chicago think of I think of Michigan at the place I always went for vacation nice to be here I'm gonna talk today about a couple different projects as the Lex I mentioned my background is in machine learning and I've made a career out of sort of trying to apply machine learning method to develop new machine learning method for making sense out of biological data so I've got a collection of three different projects I'm actually going to focus on the first and the third and just briefly mention the second one and they all sort of share one aspect which is that there's a latent space that's involved which realistically it's not like I went in thinking oh I've got to find different ways to fill latent spaces or like afterwards I looked back and said oh look he's doing kind of similar in this way so I'm gonna dive in by telling you a little bit about consortium that I've been involved in since its inception in 2003 and probably most of you are familiar with encode stands for the Encyclopedia of DNA elements and it was meant to be a follow-on to the human genome project where the human genome project sort of gave us the book and encode is supposed to help us media right in a sense that encode is trying to find what are the things in the genome that actually do stuff the the functional elements encode is in many ways just a data generation consortium but the idea is not just to generate the data but also to interpret it and the kind of data that being generated essentially measures different kinds of biochemical activity along the genome so you have for example measures of gene transcription most notably RNA seek measurements the measurements of protein binding like transcription factor chip seek you have measurements of DNA local packing density DNA's or attack seek measurements and all of those together are carried out in many different cell lines and and tissues so this is just a picture from another consortium the road map consortium showing you there's a variety of different tissue type that you can imagine doing this kind of experiment in but of course the space is even larger than this because you can have lots of different perturbations you can do CRISPR manipulations of cell lines and so on so a large space of possible kinds of cells and then a large space of possible measurements to perform in each of those cells if you're thinking of this like a computer scientist or an electrical engineer or a statistician it's helpful to say well okay there's all these assays but realistically they are all involved short lead sequencing and you essentially summarized everything as a measurement along the genome that has to do roughly with how many sequences covered each base at the genome so you can represent it all is just one vector it's a long vector because the genome long but this is just a sort of showing a short piece of 50 kilobases well that bars 50 kilobases of signal for three different assays so these are measurements of histone modifications three different kinds of histone modifications in three different cell types where the value at each position shows the level of modification as measured at that position so the point of the first project is that this is a big space of possible data so there's some some types of assays that were performed sort of systematically across many many different cell phones and then there are some popular cell lines that were characterized in great depth with many many different kinds of assays but there's a lot of purple in here also which are all of the assay cell type combinations that have not yet been performed so this slide is actually old this is from the original data set that we performed our initial analyses on so when I say it's big it's true it's big but it's bigger now so there's actually more with more than 10,000 experiments in encode currently but this is a picture of sort of a junk of the data where you can see in yellow the experiments that have been performed and purple the ones that have not yet and about 5% of this matrix is complete or at least of the full matrix is complete the problem is if you are a particular biologist studying a particular kind of cell type and maybe you're interested in a particular transcription factor you go to encode project org and you say alright I'm gonna go get all this data that the tax payers paid millions of dollars for and it's gonna tell me does my transcription factors bind there or is there a histone modification occurring at the position that I'm interested in in the genome and lo and behold they didn't bother to do it now you can email the encode investigators and lobby to have them do the experiment or you can do it yourself but maybe those sell that particular type of cell is hard to get or you don't have the facilities to do it it would be nice if you could try to use a computer to predict what would happen if you did do that experiment so that was the project that Tim Durham tackled so Tim is a graduate student in my lab and he was he worked together with max another graduate student Jeff Halbert who the research scientists in lab and Jeff films who's an electrical engineering and the project was motivated in part by Jeff Albert's participation before he started in my lab in the Netflix prize challenge so Jeff pointed out that recommender systems have been extremely successful in many different settings and in particular they were useful in the Netflix challenge so this was a challenge in which Netflix handed out a database of movies and users and asked people to predict here's a user I'll give you some of their preferences now predict what other kinds of movies they like on a scale of one to five and the idea is that let's say that Tim is a particular user and this is a particularly you'd like to you know matrix decomposition approach you would take the you decompose this matrix into two matrices each with the latent dimension of Cade latent dimension of K values where the row here represents sort of features of the movie and the row the column here represents features of the user then you can say you know for instance this value might represent how much is this movie about cycling and this value might be does Tim like movies about cycling and as you can tell from his sporty gear he does like cycling and so then you can predict that he's probably gonna like this so the idea of relatively straightforward idea in this first project was to try to do this but use instead of a matrix use a tensor so the tensor a version of matrix decomposition was described back in 1970 that's called parrot back but the basic idea is exactly the same as what I just described to you except that instead of movies and users we have cell types say types and genomic position and so what you get are these three different sub matrices or the different matrices that are produced during the decomposition and when you take a sort of generalized scalar product of the corresponding rows or column from each of those matrices and combine them then that gives you a prediction for a particular cell type of particular assay type at a particular position in the genome the optimization this is the original pair of fact paper the optimization it looks like this mathematically it looks a little daunting but it's actually relatively straightforward all you're doing is summing over all the different possible combinations that I just described and saying how close what's the difference between the value I get when I predict at this position and the actual value so here's the actual training set value and here's the value you get when you do this scalar product with some additional factors built into it and you take the mean squared error between those and you put in some terms to do some regularization to prevent overfitting this turned out to be challenging for many reasons the in particular it's a fairly large data set and so we wanted to do it in the cloud so there was a whole large part of the paper about this is about how you get this to work in a cloud computing setting and we did it both on Amazon Cloud and Azure first or mostly for logistical reasons having to do with both funding and getting around bugs but it ended up working quite well now I'm not going to show you all the results there because I want to focus on the next project which goes beyond this and I'll convince you that that one worked even better and that's the sort of the key take-home question so so know it's funny Tony and I were hired at Columbia like basically the same year but I know Tony and we co-advised the student but I don't know anything about what you're talking about yeah I'd like to hear okay oh really okay great and I should I meant to say actually please do interrupt if there's other questions I'd rather stop and get clarifications and so on then then power through and lose oh yeah yeah I see interesting okay so what happened next was that Jacob Schreiber joined my lab and he developed a piece of software all of his software is named after fruits and avocado apparently is a food even though I kind of think of it as a vegetable so it shows up in your salad and so he developed a software called avocado that is sort of the predicted version 2.0 and this was done in collaboration with him and Jeff so the basic concept here was first of all Jacob likes deep learning right so it's coming into it with that sort of plier he's like okay I want to find some way to apply deep learning but we already have this successful infrastructure the key drawback a one key drawback to the way that we did this with the tensor decomposition was you're sort of treating everything independently but more more to the point you're also just doing this relatively straightforward generalized scalar product to combine the latent factors and maybe that should be done in some nonlinear way but turns out there's an existing set of tools called deep tensor factorization that do allow you to do to a nonlinear setting where you essentially take your a say positions type things multiply them all together and then take the resulting vector and put it through a deep neural network and that's called the tensor factorization Jacob made a slight modification of that idea with the idea that we don't necessarily want to assume that we have to multiply the the values together first we want to actually allow the multiplication to be generalized also and so rather than multiplying first and then putting the local ID values through he just concatenated them and this has this the nice side benefit that we no longer have to have the same latent dimensionality or the different kinds of factors so you could have for example more factors for cell type than you have the genomic position and that turned out to be quite helpful so in particular for the genomic position this is where the bulk of our parameters are of this model because even though we're doing this at one point we're doing this a 25 base pair resolution there's still a lot of positions even if we're only looking at the pilot regions of the genome which is the 1% of the genome that encode focused on initially so the idea is could we solve the or address the independence problem that I alluded to earlier while also reducing our parameter space and the idea is well we know that the different kinds of phenomena happen at different scales you're looking for a particular transcription factor maybe that does bind at a 25 base pair resolution or something close to that whereas if you're looking at some nucleosomes sized thing that's a different or even histone modifications happen at the level of nucleosomes a little bit more like 250 base pairs and then you might have gene size or larger elements and so what we did was divide our factors into some that are defined at 25 base pair resolution some at 250 and summit-five Killah base pair resolution and so this reduces the total number of parameters in the model by about a factor of 5 you could ask how do you come up with 25 to 50 and 5 KB and those were essentially just chosen based on our prior knowledge you also ask how'd you come up with 15 35 and 50 that was done essentially through a hyper parameter search so we have to choose the hyper parameters of the model how many layers how many neurons per layer and then how many factors for each of these things we have seven different hyper parameters and we just do a random search over a grid of these hyper parameters and essentially measure the mean squared error so this is just a plot of a histogram of all those mean squared errors for different parameter combinations trained on that one percent of the genome the two bars here show the performance of chrome impute and predicted so mean squared error right so smaller is better and so we chose the model that worked the best on the validation down here so this gives us a performance increase of 4.4 percent decrease in means great error compared to predicted and a 17 point 5 percent decrease compared to the first method and developed in this field by Noah scalice and Jason Ernst called chrome impute this is just an aside to let you know what if you look at the results from that hyper parameter search a slightly different way you can see that the neural network parameters the the topology parameters are by far the most important each of these plots is just marginalizing over one of the of the seven hyper parameters that we were searching for so these are the five different values for a given hyper parameter and these are the histogram of mean squared error values for all runs where that that value was say for example and the main observation is that these are relatively flat whereas these show a marked trend as you get more larger models you're getting improved okay so practically speaking what this means is that you can look at real data like this and then you can try to impute it and you can see that qualitatively all three of these different methods seem to perform well in the sense that they capture the main qualitative features of the data we can tell you that quantitatively that we do have an improvement in in the lowest one which is the avocado imputation there's a bunch of metrics that I'm not going to go through so I'm just gonna skip this slide because otherwise you'll stare at it and try to figure out what they all mean and instead I'll focus on the subsequent part where we tried to zoom in a little bit more to ask what we thought were kind of the interesting questions which is if you think about when are you gonna use an imputation method like this well you really want to be able to do is find Peaks most of the time if you're looking for Satan or histone modifications or even gene expression values you know you'd like to be able to predict where there's large Peaks in the data and most of the time what's interesting to you are Peaks that are sort of unusual right if you look at 27 different cell types and you see a peak at this position every time and then someone predicts in the 28th cell type that there's going to be a peak there you're not gonna be super excited if you like no death right but if it's like half the time peak half the time not and you're pretty good at predicting in the next one whether it is that's a hard problem right so what we decided to do was segregate all of the peaks according to how frequently they occur so here's a picture just showing you know a peak is present in this number of cell types so you have like all the ones that are that are constitutive and those get evaluated separately from all the ones that are super rare and the ones in the middle are sort of the hardest ones to look to identify and so we looked at this three way we looked at the mean squared error separately in each of these different kinds of peaks and then also recall and precision where we had a argument in the paper about how we set the threshold for that and so here's the results what you can see is that here what we've done in is supposed to be over there number of cell types these regions are peak in so you've got some of these things are constitutive some of them are extremely rare Peaks that only happen in new cell types and these ones in the middle are the harder ones where it's sometimes there and sometimes not and if you look at the mean squared error you can see there's an improvement systematically between avocado relative to compute and predicted then we also can do recall and precision and here you can actually include the real experimental data as well because of that threshold and what you can see is that in terms of recall it looks like chrome imputed actually does really well compared to the other two but in terms of precision it does much worse so what this is telling us is that chrome imputed is sort of over predicting Peaks or and alternatively you can think of it as these other two methods are under predicting so you're getting a trade-off here precision and recall between the chrome imputing method and the two methods that we've developed and so of course that's a trade off but as an end user you'd want to know about and sort of help you to interpret the peaks that get predicted yeah and it gets easier and easier to say this is just there in all the time the ones that are easy are the ones that are always there are always not there okay okay so then the other half of this avocado project was to say okay we've done all this learning we now have these latent representations maybe they can be used in a transfer learning setting to help us do other things well so maybe those latent factors provide a useful summary so for example if you just take the latent factors on the cell type axis and you reduce them so they were I honestly don't remember let's say they were 50 dimensional you know what number we came up with let's say they're 50 dimensional we reduce into two dimensions using t-sne and project here in this picture and what you can see is that if you color them according to sort of the canonical region of the body that for instance all of the blood cell types cluster together and so on so that was encouraging it said the latent factors seemed to capture something real about the biology but for a more quantitative approach we decided to try to use the genomic factors these are 110 values for each 25 base pair region of the genome try to solve a number of sort of canonical prediction tasks in genomics and so we used a gradient boosting classifier just because that's a fairly robust and widely used classifier the claim here is not that this is going to produce the state-of-the-art method but mostly to show that these factors are a useful way to represent the genome and we do cross-validation across chromosomes and leave one out fashion and we compare five different kinds of inputs you can either just take the roadmap data directly and put it into the gradient boosting classifier so this is the data set that we used initially or you can take the imputed data so this would be the road map data plus all the imputations so extending it to be a much longer vector where you have all the imputed values as well and those can be imputed by any one of these three methods or you can just take the 110 avocado Laden factor and our hypothesis that those are an information-rich way to represent all the information in those other vectors in a way that's sort of amenable to machine learning so the first experiment we did was predicting gene expression so what we're doing is we're taking RNA seek data from the roadmap project and ten different cell types and we're inputting five histone marks in promoter regions and predicting positions with RNA seek expression greater than point five so this is a fairly standard way to frame this prediction task as a classification task and actually I wrote ten but that's how an older version of this slide there's obviously many more than ten so type so and we've just sorted them according to the performance of the best of actually of the of the baseline so the majority baseline is just if you took the average among all the training set cell types and use that as your prediction for this and for each cell type you can see all of the different colors here and what hopefully you can tell is that there's a lot of green up near the top there I'm sorry a lot of red is what you want the red is up near the top there and it's hard to see in the picture so I put some statistics avocado improves over just using the epigenomic measurements so the raw data in all cell types and by an average at point one four four here this is the average precision so averaging over all the recall axes and it even performed better than the full roadmap compendium this means taking all of the different data not just from your cell type but from every other cell type and putting it into the classifier in 36 out of 47 cell 5 so this seemed like a promising evidence that this was a nice way to represent the genomic axis we also looked at a number of other tasks I didn't include them all we have promoter enhancer interactions predicting replication timing and now Jacobs currently looking at predicting exons but we did this one was frequently interacting regions this is a feature of hi-c data that essentially corresponds to it often corresponds to the edges of particular kinds of topological domains in hi-c data essentially what the way it works is that you look for sort of a cross-shaped region like this where you see particular density within these these bars and that means that the thing at the center of the cloth is a frequently interacting region there's actually a statistical model that tries to account for biases to identify these frequently interacting regions we just downloaded the fire annotations from this paper at all from cell reports and then asked can we plug it into our gradient boosting classifier and make predictions so that's an open question the question was if there's more than two tags coming together it's not clear what the fire is actually correspond to I think that is one model because there's it's frequently and there's so much interaction happening that there may be you know lots of interaction that's right that's right for our purposes all we know is they we have a clear definition of them and so we'd like to be able to say where else might they be along the genome and so this is showing similar kinds of plots and thoughts again it's the mean average precision which is a really weird but I guess the fairly standard term mean and average it's average because it's averaged across all the different recall values and then it's mean because it's mean over a different chromosome let's see no I'll get it over different degenerate positions anyway I tried continue the average precision and the others license or dr. but the point is that we're again comparing the same kinds of things here and the avocado latent factors in nearly every case well in every case the outperform using any the imputed values and today in nearly every case they also outperform using the poll all the real data from the roadmap compendium so this is a much more concise way to represent the genome and it captures it a lot that's in there what you can also see is that if you take the full roadmap compendium it doesn't do as well as that this is data from all the other cell types so you're trying to predict whether there's a fire here by looking at what's going on in in other cell types which is a little bit counterintuitive until you remember that that contains a bunch of other kinds of measurements that weren't necessarily available in the cell type that you're looking at in particular ctcf and some of the other factors that are really informative are often not available in the blue bar but are available in the green bar so what we're currently working on now besides making the data making the model available and getting the paper published is to think of other ways in which this kind of latent representation of the genome can be useful and so if you have ideas for where this might fit in to your own research we'd be happy to hear it the other thing that we're doing is organizing a challenge which was announced last week so this is being organized by myself I'm Chokin dodgy Manolis Kellis and Zhi ping Leng as part of the encode so this is a we've put together sort of a data cube a tensor of data that's got a lot of missingness we've got some prize money for our winners and the challenge has two different phases January 28th May 14th and June 3rd August 14th with the idea that wants to get all the experiments done through great a paper about the result a key aspect of this is that we have truly prospective validation in the sense that data generation labs within encode have agreed to perform additional experiments which have not yet been done right so that when you when we get their predictions we'll be able to test them on test sets that really no one has ever seen before so that's the end of contact one am i doing pretty good alright so I want to briefly tell you about another project with someone who you probably know because he's sitting right there but I didn't want to give you tons of detail because he can tell you all about it but I thought does fit into the same kind of some of the same ideas come out of this so now we're talking about single cell hi-c data this is a project that basically gia did with some help from to June and gokon and we're helpful in in making sense of some of the data formats so having talked to a lot of you I know a lot of you already know all this but just to be sure we're all on the same page single cell hi-c is a sort of a single cell variant of the hi-c assay that measures the three-dimensional confirmation of DNA in the nucleus so the idea is you're gonna isolate a single cell you do this like ligation phil and restriction enzyme digestion and cross-linking and so on and what you get out is a very sparse matrix in which the rows and the columns correspond to positions along the genome and values in the matrix correspond to accounts of how frequently you observe contacts between those positions so in this one each of these boxes corresponds to one chromosome you have lots of intra chromosomal contacts and dark and then the inter chromosomal contacts are sort of on a lighter lighter shade here with fewer contacts one of the big questions is if you look at the RNA seek literature a lot of what people are interested in with single cell analysis is stuff like this this is the monocle software produced by cold trap now in our department and it's trying to reconstruct differentiating the trajectory of development in a population of differentiating cells automatically from the data so this is a projection down to 2d and then followed by this kind of tracing step there are other kinds of projection that you can do just doing things like T as an e it's just showing tissue type clustering in some single cell data as well and so the question is could we do a similar kind of thing a single-cell hi-c data so one of the things that gherkin helped with I mentioned gherkin was one of the people involved here he had been involved in a previous study run through encode in which our lab sort of coordinated for other labs who are developing ways of measuring the reproducibility of hi-c data so they were measurements for how similar is this hi-c data set to this other hi-c data set and so what we did was said well maybe those will work for single-cell as well so what Jia did was took one of those existing methods called hi-c rep and then combined it with a standard dimensionality reduction technique multi-dimensional scaling and took some data where this was published data in which we actually had labels or where the cells were in the cell cycle you can see g1 and cyan earliest mid s and late s and so on and this was in the sort of test set that we initially pulled out early evenly sampled from each of the cell cycle phases and here's from the phone data set and this might not seem so impressive to you except if you hadn't tried a bunch of other stuff the way we did and nothing else gave us much structure and then suddenly I see rep that's pretty convincingly cell cycle even if I didn't color it you could pretty much see that there's a circle going on there right so he accantus this was presented last year at the isnb but the point is this is a very nice way to take single-cell data and project it down into a lower dimensional space a latent space in which the relationships in this lower dimensional space and now clearly reflect at least for this data set the underlying biology of of cell cycle variation okay so now I'm going to change gears again and as promised in the title I'm changing from genomics to proteomics I know there are some people here who do proteomics that's that's nice I spend about half my time thinking about mass protein mass spectrometry and so I want to tell you about some of the work we've been doing on gleans so first just in case some of you don't do proteomics or haven't heard so much about it just to remind you that the key idea here is that you have some complex sample whatever the sample is it's got proteins in it and as you know proteins are complicated looking right they've got all these fancy structures associated with them and biochemically proteins are kind of hard to deal with especially in high throughput so the way that shotgun proteomics works is that you take these proteins and you first cut them up into pieces using something like trypsin and the trypsin gives you or somewhat deterministic cleavages of the proteins and these peptides are other things that are subject to mass spectrometry you put them through a device this is a picture of a device in our department and then out comes a bunch of observations they are spectra which really are just basically vectors again and I'll tell you a little bit more about the spectra in a bit actually I'm not going to tell you that much about them except to say each of these represents one canonically one peptide species and the height of the peaks corresponds to different fragmentation events along the peptide backbone but then you put it through some fancy computer and ideally outcomes and identification for each of the observed spectra you can say this is the peptide and I think was responsible for generating so the project I want to tell you about is gleans Lemes is a recursive acronym gleans is a learned embedding for annotating mass spectra that infant li and it was developed by Damon May a PhD student in my lab and also in collaboration with Jeff film so one motivation for this project is that most protein journals require that your data once you've finished it that you stick it into a public repository so some of the common repositories are things like massive chorus Clyde and so on but unfortunately or the field this repository submission is usually the end of the process so you go along you generate your data you analyze it you write your paper you deposit your data publish ended story and what would be nice is if we could move this up a little bit more genomics is it much more advanced in this respect than proteomics where sometimes you actually make use of public data to make sense of your own data and this happens some in proteomics but we think it could happen more and we're all working on ways to facilitate that so there have been some efforts to do this kind of thing so most of the repositories out there provide an annotated collections of spectra so those allow you to search against the search your data against those annotated spectra and so some examples of those are NIST massive and pride cluster in particular if you look at the pride repository this is one of the more widely used repositories they did a clustering of all pride spectra in 2015 this is one of the few attempts to actually cluster all of the spectra as opposed to just clustering the spectra or which we know what the generating peptide was and the challenge here is that the clustering is pretty expensive and you have to sort of redo it if you want to start over again so we were trying to figure out a way that we could do something akin to clustering but that was a little more flexible and I'm not sure why but it certainly is the case that this was done in 2015 and there's no evidence been done again maybe maybe because it's a pretty expensive thing to do so our concept was maybe we could use the concept of latent embedding to embed all of our spectra into a learned space if we can use a machine learning approach to embed the spectra into a learned space then if you stick a new spectrum into that space you can just see who's close to you and if they have labels associated with them then you can adopt their label so the question is you know would this be helpful well ideally if we do this embedding approach sure it might take a while to train it but once it's trained it'll hopefully be super fast to just stick new spectra into the embedded space and so you don't have to relearn the embedding function all you have to do is click the spectrum in and then look and look and see who's closed so to do this we needed a way to get spectra into a machine learning system it's turned out to be kind of a pain because of the way spectra just the way they act they're there they're not easily summarized abaut as a simple vector they're really bags of Peaks you know that's where each peak is a is a mass to charge ratio and an intensity and the master charge ratios have to be represented quite precisely and there's also a precursor mass which is the mass of the intact peptide divided by the charge and that's also important actually very important so we spent a while and I'm not going to go through much detail thinking about different ways to represent spectra or input to a machine learning system when we came up with this well you have a couple of experimental parameters that are provided as input so the mass accuracy and the fragment mass accuracy of the of the device that generated the spectrum you have peptide information about the intact peptide and then you have fragment information about what the locations are of all the peaks in the spectrum and so these three pieces of information oh and I should mention we also use similarity to a collection of reference spectra where we basically use this the pattern of similarity as another set of features to represent the spectrum and then those three sets of features are each processed in a deep neural network where they they each get processed separately through some layers and then jointly thereafter so many of you probably familiar with these kind of terms from deep neural networks but just in case you're not you have some convolutional layers which are designed to sort of scan across a region of data a data object and find local patterns therein and then you have fully connected layers that take a bunch of information and consider it jointly and so this is a picture we have 500 reference spectrum similarities we have the bin fragment intensities and then we have the precursor feature we handle these these convolutional e the the first two sets convolutional e and then only connected for the precursor features partly because there's no real ordering of the precursor features so a convolution wouldn't really make sense and then we combine all of these through another their concatenated here and then combine down here again there's hyper parameters involved though for example there's a 30 to 32 dimensions is the number of dimensions we chose for the final embedding there's also sizes for these internal layers and so on and in the paper we have some description of how we chose all those hyper parameters yeah so you're talking about whether your pre-training each of these pieces and then what are you talking about how many layers to put in yeah so it's always a question this is a common yes so the question is how do there's a lot more parameters than the ones I just alluded to some of them are implicit like how many layers should I put in and how many nodes in each layer and then also should I do the training piecewise I've trained part of this first and what type of parameters should I use for the learning procedure when when it's hyper parameters of the learning itself I would I wouldn't say we cover them exhaustively we try to do a give a discussion of that but it doesn't do justice to everything Damon did of trying things out ahead of time we the nice thing is that we did all of this development and an independent set of data from what we'll report here so at least we don't have to worry about the the results that we report being over fit but this is a common challenge in the literature is that it's it's typical for deep learning development it contained many many iterations of trying things out and to my frustration as a leader often none of those details are given I think we did a reasonable job but there may be probably more detail we could have given as reviewers when we put up with so much yeah so we are let me just show you the next slide because I think that Atlantis so this is what the actual network looks like you take the previous picture and you stick it in here there's actually two copies of it so this is called a Siamese configuration so there's two copies but they have the same parameters this is network this network in this network had tied parameters all those convolutions everything I showed you in the previous slide sit here they output 232 dimensional vectors which we then compute the Euclidean distance between and we do this kind of contrastive law so mathematically what you're seeing here is this is the label on a pair of a pair of spectra plus one means they were generated by the same peptide minus one means they were generated by different peptides this is the distance in the embedded space that came out of these two two networks and then this is some pre-specified margin and so the concept is that if these two peptides have the same label and they're farther apart than your pre-specified margin then this will try to push them together and if they have different labels and they're closer than that arjun it's going to try to push them apart and it just iteratively does that over and over again this is not something that we develop this is I should have a citation you know a decade ago it's very relatively commonly used kind of architecture okay so the first thing that comes out is an embedding it looks like this if you take into 32 dimensions and you project to two dimensions with t-sne so this is 70,000 embedded spectra were colored by precursor mass-to-charge ratio you can see that there is some structure there if you instead color by charge state you can see it slightly different structure question is is this useful so one thing that you can do is zoom in a little bit more closely so what we did was we randomly chose some of these globs so each of these things is a little glob we randomly chose some of those blobs and then looked to see what was in there or actually no it's sorry we did this the other way around we chose all of the spectra in a randomly chosen charged state and about a1 Dalton precursor mass bin and then we colored them one color and then we randomly chose another charge state and one Dalton Aspen and did another color we didn't do too many colors because you can't see that many colors but we chose for this one 226 spectra in total and there's about what is it six or seven different colors and what you can see is that this seems to be just the main features the charge state and and the masked seem to relatively place these things on the plot so now let's zoom in on this particular one the one that's here if we zoom in on it it actually opens up you can't really see it I was gonna try to get Damon to send me it better there's some dots here which are hard to see and then these ones over here as well so what's going on here well if we color those dots according to what peptide generated them you can see that all of these were generated by the same peptide these are a mixture of different peptides and a mixture but you can see this some of the same colors show up together but then the question is what's going on or what do we do with this so there's a number of different possibilities we the first thing we considered was just trying to do some of that kind of grouping that I described previously like can we go find collections of nearby spectra that all belong together we considered it a couple of different ways to do this and ended up with a relatively simple greedy algorithm so what we do is we find the 1,000 nearest neighbors of each spectrum so it turns out this you can do quite fast we use a library from Facebook called base which is spelled F a is s that allows you to rapidly you can take these 32 element vectors and take tons of them and find your thousand nearest neighbors quite quickly then you sort all the spectra and you sort them first by the neighbor count within some pre-specified distance range tau and then by mean distance to their neighbors and for each spectrum in order if it's not already a spoke you call it a spoke and you and you connect it to all of the hubs around it so here for example this guy got connected to a bunch of peptide spectra that we're near it then you go and grab the next cluster and then the next cluster and so on and you continue doing that and then at the very end sometimes you have to do some combining of adjacent hub-and-spoke communities if the hubs are too close together and that's only because the first step was an approximation you just you got the thousand nearest neighbors and maybe there's more than that within a distance of town that's usually a very small number so then we wanted to ask how well does this kind of community generation process work so we went in to test data or we hadn't used this during training and we asked how well does it do at making pitayas that are sort of pure they have one peptide in them and and not too many not too many communities with multiple peptides in them and so you'd like to be in the upper left corner of this kind of law if we you started by just doing k-means clustering which is seems like a reasonable thing to try right lots lots of people use k-means for lots of things turns out this is really slow there probably are some speed ups that we could make it go faster but it's not going to go as fast as the simple nearest neighbor thing that we implemented but more importantly it makes too many multi peptide communities if you think about it k nearest neighbor or most clustering algorithm really are designed to handle to try to cluster things a much larger scale than what we're interested in we want to find very dense small clusters and so this basically puts you way over here depending on what value of k whereas this hub-and-spoke method that we developed looks much better these are just for different values of tau tau equals point one nine five in that space that we defined using the neural network we chose the tally equals point or nine five because that means that about 1% of the communities that we came up with our multi peptide community so the nice thing is that this is not that expensive embedding a 5.6 million spectra takes about 20 minutes and then the thousand nearest neighbors search takes about seven and a half hours for all of those 5.6 million spectra and then the actual Huggins Bach method takes just a few minutes after that so the question is does this help us so what we would like to do ideally is say okay we've embedded these spectra some of them have labels can we now propagate additional labels on top of the ones that we that we had initially I'm giving away the answer here but we get of the 1.6 million in our data set of 5.6 million that were identified we can get an additional about 8% and I'll show you how that happens so the most obvious thing to do is you say well I look at my communities and if this community contains only peptides with a given only spectra with a given peptide label or unlabeled spectra then I can just assign the label to the additional peptide in that community and that immediately gives us about 40,000 additional spectra the other thing that we can do is find sort of try to characterize what is often called the dark matter of proteomics which is all of the spectra that you can identify successfully when you do a standard database search to try to identify the things that are the problem in general with this dark matter is that a lot of it is non peptide junk right that they're they're noise or other kinds of contaminants and so the idea here is that if we can embed this learned peptide and better and a lot of different spectra all show up near each other and maybe that will help us to focus in on the ones that are likely to be less junkie right more more likely to be real and so what we were able to do is then focus on the the communities that don't have any label and doing what's called a tight target decoy search so because we the because of the way the spectra work we can we can define a much more fine-grained mass-to-charge ratio which reduces the number of candidates you have to search when you when you do the database search and so this allows you to essentially reduce your multiple hypothesis burden and get better statistical power and the result is that by by using that information you get about another 40,000 spectra and then finally you can do an open modification search which means you can essentially allow for lots of different kind of post translational modifications on the peptides on the the remaining unidentified communities and that gives you an additional 40,000 spectra so in the end you get about as I mentioned about oh sorry and then finally that wasn't there there's an additional step of doing an anima done on database search to find an additional 5,000 spectra yeah so that allows us to increase the number of identified spectra by about 8% which is if you were considering doing this to something like all of the data in the massive repository as of a couple months ago there'll be about fifteen point three million additional spectra identified and the other thing that we show in the paper I'm not showing you the evidence here is that the ones that we can't identify they're enriched for low quality spectrum so that suggests that perhaps a lot of those remaining ones that we can't identify really are are problematic in some way so that's where we are currently the future directions for this are numerous so we obviously one thing is we've only demonstrated this sort of proof of principle on a relatively small database so our small small set of spectra so we want to scale up to 100 million 200 million spectra and really see what happens when we do that and to see what what might happen we did a down sampling experiment by sort of randomly tossing out some of our spectra this is not actually retraining the model we're using a fixed embedding but just asking if you start making smaller and smaller sets of spectra by tossing out experiments from our repository what percentage of the of the spectra don't have any labeled neighbour within the distances point out that tile distance before and that seems to be it's not really surprising it goes up as you get more and more spectra right but the point is it's not like it's really flattened off so the Idaho is that it continues to go up and we get much more identifications as we get out you know orders of magnitude more spectra going into the database the other thing that we're curious about is to explore this idea from natural language processing that maybe the latent space encodes some interesting semantics the word to vac is a common popular way to do embedding of words in natural language and this is an example from the word to that paper where they were able to show that if you embed the word King into a latent space you subtract the vector from man and you add the vector for woman that you end up near the vector for Queen right this is a semantic relationship it's captured by would be back similar for lots of superlatives and other participle so you're hoping that we might be able to see similar kinds of things that we look for for example amino acid substitution or a post translational modification or even mass changes and then the longer term idea would be that we could hopefully make this work in conjunction with some of these existing repositories to to better enable community-wide services for understanding and interpreting mass spec data as it's generated so that you could quickly and efficiently upload your data into a repository and get back some feedback about what your experiment resembled what other experiments resemble it what peptides you were not able to identify that could be identified and so on so that the whole story and I think I'm just about out of time but I'm very grateful that you're here to hear my story and I'm happy to answer quick okay well since I have it one question is so you mentioned that the problem with clustering of spectra is that it's quite expensive to cluster them again as the data sets get bigger and you mentioned that you may have an advantage here that you can embed spectra using the same model that you develop using a smaller dataset so at what point you do have to retrain your underlying models as you continue adding embedding spectrum that's a good question so I think the answer will have to be determined empirically I mean what we would do is have some performance measures you know how accurate the model how accurately the model is recovering known peptides my hope would be that once you reach a certain threshold so let's imagine you've got a billion spectra in there probably adding another billion is not going to get you that much more it's at a certain point it's not going to become as important so I want to thank you for demonstrating the multi-omics approach to the seminar we are proud that 12 years ago 1350 14 years ago we started here with a National Center for NIH on integrative bioinformatics analysis demonstrated I want to ask you specifically about the universal spectrum identifier alright Deutsch and colleagues including the yupo protein standards initiative have been working on this I believe it's ready for publication this year are you aware of that do you think it's something to be useful in your work it's a search tool starting with the spectrum instead of the sequence been some recent deep learning and a couple other recent advances in that area so I'll be interested hi so in your avocado model what would cause your model to perform better or worse on Wheldon one cell type versus another is it just the availability of information or is there some other factor but it also has to do it not just how much data you have in that cell type but how much data you have in what kinds of data that is right some some of these types of data are more informative than others but often it's also the case that a cell type access is not really just a linear access right it's more like a dendrogram so if you have if you're looking at a blood cell type then you've got 14 other blood cell types then it's going to be much easier for you because you've got lots of neighbors in the cell type spaces well even if you have relatively small amounts of data so it is challenging to characterize like a priority to say for this prediction I think we're going to do well and for that one we're not because we don't really understand that some of the structure that's me I was wondering if it might be possible to formulate the proteomics task as kind of a regression problem instead of an embedding problem where you have these spectrum peptide pairs and you just try to predict the peptide directly from the spectrum two different questions I thought you were going one way and then you learn another way so you can definitely you can frame it as a classification task or a regression if you have a peptide spectrum pair and you want to know how good is this pairing so the most common way to do it is and this is what Alexa did first was to sort of say here's a peptide in a spectrum this is a true match you're the peptide in a spectrum this is a false match right so that's one way to do it but you I think you were going a different direction yeah so the way you're using the embedding is you're saying I want to assign a peptide to this spectrum so I'm going to look at the assignments of my neighbors in Leighton space but seems like you could also just try to directly predict the peptide from the spectrum given a bunch of true assignments oh I think so yeah that's the those are the kind of thing that we just alluded to so you can for instance the most recent papers trainer a deep neural network to say I'm gonna take as input a spectrum and output a peptide doing that is hard right because you've got this weird secretly your sequence over a twenty dimensional discrete space so there's some bells and whistles to make that work any other questions if not let's so the question was how where a project like encode in 40 nucleon going in the next five years and I would really like to know we don't know for sure I think so 40 nucleon is just in its first phase and it is a what's called a common bond initiative which means it's funded by the jointly by the NIH not associated with a particular Institute and those can only go for two phases so obviously there's no official announcement yet but we're three and a half years in and we're all hoping that there will be an announcement soon in the second phase if there is I would guess that I mean it's a 40 nucleon but it's been very 3d so far there's not that much time happening it's a little bit but I think the next phase will have to be much more time oriented it's also 40 nucleon in particular is focused on imaging and and sort of sequencing based technologies and getting them to work together has been a big focus of the 40 nucleon but it's not done yet it's not even really diamond it started yet we're working hard to make that happen but it's going to run into the next things yeah encode is an entirely different beast right I think encode is really mature by now or in you know and I don't you know the initial phase was essentially technology development the pilot phase the second phase was a scale up to the whole genome the third phase was hey this is working pretty let's pretty well let's just keep going you know it wasn't it was less clear what the mandate was and I think that's also true the fourth place except that it's clear that the data has value right so moving from primaries from cell lines the tissues have been a big push I think moving toward more integrative kinds of analysis is also a big push and I think one of the directions that NIH seem to be going is pushing toward better tools for visualizing understanding and interpreting the data so you know can you make something you know the genome browser was a great thing right at the beginning of the human genome project though we may have moved beyond that and what other kinds of what's the what's the blast of the 2020 you know what's the way that we're gonna interact with the genome in the future in a way that takes into account the fact that we now have 10,000 measurements every single day stare at the Dino that's right so a lot of a lot of the browsers now allow you to see the 3d structure all right if normal question let's think bill again [Music]