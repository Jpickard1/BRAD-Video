[
    {
        "start": 0.719,
        "text": "everyone uh so my name is chain and we're going to talk about our research that have been really going on for the past few years on self-supervised methods to learn representations for holite images right so I'll start with a brief background and then I'm going to talk about a few of different methods that we're introducing and then we'll wrap up quickly with the conclusion right so let's dive into the background um what we're interested in here is biomedical microscopy um biome biomedical microscopy is a very important Imaging method and diagnostic tool for both research in clinical medicine um biomedical microscopy can help us to do a tumor diagnosis molecular classification it can help us predict a patient's outcome right or predict how a patient might react to some treatment right and this is also very important for us in clinical trials enrollment or um "
    },
    {
        "start": 61.399,
        "text": "um for clinical trial enrollment or maybe even formulate a surgical plan for these patients who have a maybe have a tuner right so more recently digital pathology and host Imaging really increased the role of computer vision and machine learning in analyzing this uh microscopy data which is very complex and very large right so there are two type of Imaging methods that we're interested in um the first one is h& staining right is staining has been the the state of the art and standard of care uh for probably more than 100 years or so right this is the this is the kind of images that Pathologists are very familiar with and they're trained with that and they kind of use that to diagnose patients and that's the standard care right now um and the other modality that we're interested in is Sr right so stimulated R histology in Sr is a relatively more novel way to image these specimens using optical Imaging in "
    },
    {
        "start": 123.24,
        "text": "the upside for Sr is that we don't really need to process the tissues right um we can image these specimen fresh so we don't need to do staining we don't need to do fixing and and the upside for this is that we can do this within minutes so we can do this right in the O where we we can have a specimen um coming from the patient and goes directly into the microscope and we can see the image right away and this is and that's why we we like Sr so much because we can get this image very rapidly and we can um have some diagnosis right there um to maybe help the surgeon um to figure out their treatment plan going forward and this is where AI has a very large role in formulating how this in in in looking at how this data is being treated sorry just a quick question so um I can see a lot of theages of HR over H but in terms "
    },
    {
        "start": 184.56,
        "text": "of resolution which better yeah so so the question is um what's uh the resolution wise how does Sr compare to h& um so h& actually has uh so if we're talking about like magnification of the the the specimens um Sr is usually being done at 20x and roughly um and h& is can be done at 20x if you need it uh quickly for diagnosis and it can be done up to 40x um the vast majority of the data that we have are uh when we are actually processing these data is done at the scale of 20x so roughly there is uh in terms of the resolution um they're they're pretty similar um but Sr also have another big advantage in the sense that um the data is very standardized um we don't really need to rely on maybe technicians experience or maybe they overstaining something orain something or there's no variability between "
    },
    {
        "start": 245.04,
        "text": "different institutions Sr is a very standard machine a very standard microscope right so we have the tissue in and doesn't matter which microscope it is the image will always work the same y um so so these are the two uh microscopy modalities that that we're working with um and these images are actually very big so I'll give you an example um these are basically two whight images of h& and Sr so Sr is a bit smaller than the h& because um it's a it's a more focused window right we already have a window region where a lot of this uh this wi space here on the on the h& image really is an important we kind of just throw them out when we're processing them so that's why there's kind of like this size difference um but if we look at this difference in if we look at these sizes in terms of uh megabytes right um a h& Hol image can go "
    },
    {
        "start": 306.639,
        "text": "up to 70 gigabytes right and a Sr image can go up to 600 to maybe a gigabyte right and these images are just so big that it's it's really challenging to process them um with deep learning right and especially if we want a bat size of let's say 1024 right there's not enough GPU there's no GPU on the market that can actually process these images right and just for comparison um if we look at natural images on the image net um they're usually 256 by 256 and they're only a few hundred kilobytes right so this big difference is one of the most important challenges that come with histology or Hol images um so what a lot of uh prior work has been done as to what we do is we essentially patch this Hol image into smaller patches right so that's that's a solution to to to handle this huge size "
    },
    {
        "start": 368.759,
        "text": "right so when we have a whole image we will patch them into smaller sizes right and and these sizes are are maybe 256x 256 or something similar to that um that we can easily digest those sizes with modern techniques and then we will use a patch encoder to patch um to to encode patches right and then from each patch we'll have some kind of patch token to represent these patches and once we have that we will then use a sliding encoder that takes in um the output of our patching coder right we'll go from the patch tokens to some kind of sliding Bing right so we have this two-stage approach um um this this approach is can also be called multiple instance learning or M approach and but this is uh has been established pipeline that that how we have processing these data um another challenge that come with these uh these images is that um all the labels that we have about these data is "
    },
    {
        "start": 431.4,
        "text": "really at the patient or host level right um so if we're lucky maybe we have a pathology report right and in this pathology report someone will a a trained pathologist will describe what they see on the images or maybe they see a few different attributes on the image but they're not really anchored to any point right on the image they'll just see that we see some attributes on this image but they don't necessarily tell you where and the vast majority of the cases that we have are actually we just have a table of the labels that we have right um we know this is a Hydra Leoma um that's great for with um some molecular information that's ID type right um so this very high level information this labels about these data is really coarse and it's really at the patient level right so um and this um make it quite challenging for us to actually train a "
    },
    {
        "start": 492.52,
        "text": "model to process them right so to to summarize the challenges that we have um with these host images we have very large host images right the annotations that we have is is weak labels it's only at the slide or the patient level and These Fine green labels is really really challenging to acquire right we are not going to sit a pathologist in a room and say Here's a million patches and you have to label them right tell us um this is tumor this is blood this is necrosis um that's just not feasible for us to do and even so even if we can do that right um the pathologist can only give us histologic annotations right the pathologist can see um I see some infiltration here um I see normal tissue I see some blood I see some necrosis these very high level descriptions of what we can see on the image uh which is which is good to have but it's really not the most interesting thing that we want to do which is um maybe "
    },
    {
        "start": 553.92,
        "text": "classification in the molecular scale and or maybe how the patient will respond to a treatment or how how long the patient will survive right so the pathologist doesn't necessarily have a a globe that can tell in the future so so um because of these challenges um there is a need for having a self-supervised representation learning methods for histopathology data um so here self-supervised uh means that we we want to learn good representation without actually utilizing these labels um that we have for the patients right we'll use these labels to evaluate um how the model is doing but the goal here is to when we are training these models right we don't want to actually use any of these labels because we know we don't have a lot of labels they're not very good um or they're very sparse so um so in today's talk we're going to talk about um our proposals "
    },
    {
        "start": 615.0,
        "text": "right our vision on how to train a histop pathology a host image representation learning right with a patching coder and the slid encoder in this framework right so and I will start with the patching coder on how we are going to um use SSL to better train these patching coders right and so um so our way the way that we're proposing to train the patching coder is hierarchical discrimination um High disk so I'll give you a little bit of a background how on how existing self-supervised learning work uh for patches right so we have these um patches right we already we have a whole image or have already kind of divided them divided them up into smaller patches so we have this patch which is uh very nicely sized we can fit that onto the GPU um what happens is we actually do different Transformations um to to this same patch right so the idea "
    },
    {
        "start": 675.44,
        "text": "here is that um maybe similar patches should have similar representations right so how do we get this similar uh patches right because we're kind of assuming that this is this patch is all we have right so the the most easy the easiest way um that a lot of computer vision researchers do is to essentially apply different Transformations on the same image and maybe we'll flip it around maybe we'll blur the image maybe we'll add some noise maybe we can change a color change a contrast U make it more bluish make it more reddish uh things like that and and the idea here is that we want to encourage the model to know that these Transformations that we have applied for or on these images are really not important right these images are still similar and therefore they should have similar representations um so once we have these Transformations uh we pass them through a neur network and there there's a lot "
    },
    {
        "start": 735.44,
        "text": "of different research on this how this neuron Network should should work maybe there's some kind of weight sharing between uh between the two neuron Network so it might be the same neuron Network or maybe one neuron network has a few more layers maybe one neuronetwork is not being updated um there's a lot of different research on in this area but the vast majority of them do share this common architecture where there is this what we call a Siamese Network right we have two networks that could be the same to extract features from each of our transformed image and from there um the idea here is that these transformed image should be similar so uh some kind of loss function is being employed to make sure that um these images will have similar representations well um all the other images right so maybe we train our model with a maybe batch size of 52 or 10 24 then all the other images are are not the same image right they're not the same image that we have transform so "
    },
    {
        "start": 795.839,
        "text": "they're what we call negative pairs right and the two images that we transform are the positive pairs so the idea here is to minimize the difference between positive Pairs and maximize the difference between positive and the negative pairs all right does that make sense to everyone okay great um so so this is kind of what people have been doing um for a while and what we what we see here is that there's really a lot of disadvantage of using this technique to to to learn these representations they do a good job for a lot of tasks right but can we how can we push um the performance of this training Paradigm further right so a few different week is of this approach that we have seen um is that it assumes the patching dependence right so um in our mini batch right in our batch of maybe 5 12 images that we're looking at we're assuming that every patch is independent where um that "
    },
    {
        "start": 856.199,
        "text": "may not necessarily be the case right so for example if we sampled multiple images from the same patient right they're definitely not independent right we actually have a pretty good idea that maybe they should also be quite similar to each other um where in a patch uh instance discrimination these will actually be considered negative pairs right because we're we're kind of not treating we're trying to do this without seeing any labels um this approach actually also neglect this hierarchy that we have of our data right because um in our hologic data there's a natural hierarchy that goes from a patient um to slide and then to patches right so so a patient may have a few different slides and every slide may have a few thousand patches right so this hierarchy is is not really taken advantage of right because we know um this hierarchy give us some information on how this data might be organized and finally um this really "
    },
    {
        "start": 917.759,
        "text": "requires strong data augmentations um that is that's really not very informative and it's domain agnostic right we're training these data in the same way that we train um a data on the image net right so um sometimes these augmentations can even corrupt semantically important uh features on the image and maybe have us learn something that's really not important so um to to come back these like weaknesses of the prior work um we we propos highis so this is kind of like the overall of what hiest do and I'm going to um I'm I'm going to focus on just at the bottom first right so what we have here at the bottom of the hierarchy is that um is basically what we have before right we have different patches and um we have the patch and the transformation your positive pair pairs with each other right and in this hierarchy these two "
    },
    {
        "start": 978.8,
        "text": "different views are the positive Pairs and everything else will be considered as negative pairs right so um so that's what we had before and so and this is what we call Patch discrimination right we're discriminating the instances at the patch level um so what we can do is actually um look at where these patches come from right um it maybe these patches come from the same slide right and if we can if we have this information which we almost always have um we can then construct positive pairs um based on different based on their slide membership right so maybe these two patches are from the same host SL image and then they will be considered a positive pair right um they look very different right compared to two different augmentations of the same image but the idea here is that they encode the same underline tumor biology that they actually show us the same "
    },
    {
        "start": 1039.559,
        "text": "thing right so that's what we call slide discrimination we're we're doing instance discrimination again but only at the SL level and finally we can do this um at the patient level as well where maybe we do know um that maybe a patient had multiple specimens and um these specimens um are again the the same views of this the same underlying tumor biology so we can also um include maybe patch here and a patch here as a positive pair if they come from the same patient right so this is this is how we can essentially um utilize this this very large image size to help us find very natural augmentations um just because there different views of the same tumor right instead of having to handcraft them or change the color or flip them um so um and the idea here is that if we use SL discrimination to train a "
    },
    {
        "start": 1101.799,
        "text": "patching coder our patch encoder can actually learn slide level information right um these representations will actually tell us um what is happening at the slide level and like likewise if we do that at the patient level right our patching coder have the information of this patient already right so um so we think this is a very strong way to do these trans to avoid these Transformations and give us better representations for these patches right um so this is a little technical here I'm just going to quickly dive into uh pseudo code for training um our method this is uh um this is quite similar to how most method is being trained um here we what we what we do here is we um we compute the same loss function three times right so we can compute the loss function um based on the patches right and then this is um just the patch discrimination and we can compute the loss at a slight level and that get a slide discrimination and we "
    },
    {
        "start": 1164.039,
        "text": "also compute the loss at the patient level right so we can add these loss functions together um to kind of balance out right the different ways that we want to uh wait maybe past discrimination or maybe we want to wait um side discrimination more to kind of uh have a better custom representation for maybe a different task that we're interested in um so um going a little more into the representation uh going into the into the loss function right so the the loss function so we're employing a contrastive learning objective here um which which we don't have to use contrastive loss but um that's the loss that we kind of studied um when we were developing this method right so in the contractive law we kind of have the postive pair feature similarity in the numerator and the negative pairs feature similarity in the denominator and that's kind of the similarities that we're trying to contrast and we're doing that for all the other images in ourain and "
    },
    {
        "start": 1224.2,
        "text": "to here is a kind of temperature hyper perimeter um so so this is the the standard pretty standard um well studied contrastive learning formula um and but what we do here is we um we we compute this loss over all the positive pairs at a certain level of discrimination right either patch SL patient discrimination and once we once we do that um uh we can do that for all the images in our mini batch right so basically for every image in our mini batch you will find all of the posi the pairs at a certain level of discrimination and we will compute this contrastive loss and um so in this level um this loss can be computed per level of discrimination as well so maybe at training time we'll first find all the positive pairs at the at for patch discrimination which is just the patch itself and then the augmentations right we'll compete the loss with these positive pairs that we "
    },
    {
        "start": 1286.24,
        "text": "identified and then we will do the same loss function but well with a different set of posit pairs that is defined by all the other Patches from the same post SL image and then we can do the loss again with uh with the all the patches on the same patient so this is how we how we formulate our loss function and we have some waiting hyper parameters to um uh hyper parameters to help us weight these loss functions and we do that for all the different levels um so we benchmarked on method on a few different data sets including Sr in h& data so Sr is a seven class classification method where um we're basically um trying to determine the tumor type the histologic tumor type of different of different images and on the tcga side we are doing a more advanced classification on molecular information where we're trying to differentiate idh wild type and white idh mutant type of "
    },
    {
        "start": 1349.24,
        "text": "tumor um and the result is uh uh overwhelmingly amazing for the methods that we're proposing you I I don't want to uh to pay too much attention to other results but I can tell you that um um high this slide and high this patient completely outperform all the baselines that we have tried um by a very wide margin as well um and here hi this patch right so if we only use patch discriminat it's actually very similar to the baselines um that the prior studies that we had and that's also something that we have seen the high this P sin clear are the baselines that that we're comparing against and and and as expected they're very similar to each other we also visualized these representations um using T um to to do to do a dimensionality reduction on on the learn representations and what we can see here is that um both highis in Z "
    },
    {
        "start": 1410.0,
        "text": "do pretty good in terms of learning a good representation but highis is better in the sense that um different tumor types are rather separated and we can also actually see subclusters here within different tumor types right and then these subclusters actually do correspond to different patients right and and this is not something that we are asking them all to do right this is just the objective that we are that we're enforcing when we're training the model and and as we can see this is this is very good in terms of having a by representation and what's more interesting is that this patient cluster is not seen for normal patients right because they're all views of the normal brain which doesn't necessarily have differences between different patients um one of the things I mentioned earlier is that all the prior work requires a strong a mentations um for the different of require these "
    },
    {
        "start": 1472.52,
        "text": "augmentations to form the posit pairs so what happens if we don't have these strong augmentations right what we can see is that um PRI work and also highest patch right remember highest patches also only patch discrimination on these models essentially this cops they can't learn anything because there is no transformation that's being applied to these patches um but as as what we expected high this SL and patient still perform really well even without these domain agnostic Transformations um so in it performs it still performs really well without these Transformations um which is a very good strength of our method because these Transformations are sometimes very compute intensive as well and that can kind of slow down the training of these methods and and we we also know that they don't necessarily correlate to any medical biomedical process so to visualize the different "
    },
    {
        "start": 1534.039,
        "text": "representations um without strong augmentations of the representation of s player is essentially random so to to kind of summarize our contributions in highis um the highis is here to leverage um the inherent patient slide and patient hierarchy for biomedical microscopy representation learning um hi this outperform um all the previous self supervised methods um for cancer diagnosis in molecular mutation prediction in highis um learns very good uh representations for our patches um even without using the strong augmentations that's being required by other the prior works so uh going back to our vision on how to train these two stage networks right we believe that the way to train patching coder is through highis right it it learns very good representations "
    },
    {
        "start": 1595.679,
        "text": "and and is very efficient and doesn't require the strong augmentations that everything that everything else requires um so to tell you more about the slide encoders I'm going to pass it off to Ry yeah before before that I'll take questions yeah um so what exactly is the network that you're using for the forward person so so for these experiments is rest 50 rest 50 that's right and are you using the pre-trained one from imet and then or are you training from Str um for these experiments they train from scratch but uh there's no requirement to train from scratch it will work with image PRT training will work with different model architectures um but for the sake of these experiments that's that's what we did okay and are you still using the S setup or are you dispensing the it's still s setup yeah um but the the way it's shared between the two branches of the network so it's it's one set of Weights yeah it's the same set of Weights it's the same set of Weights yes it's still "
    },
    {
        "start": 1656.279,
        "text": "sign okay so are you freezing any of the layers during like no no okay thank you yeah we have freezing coming up okay how did you res you can think of deeper resolution of the do that yeah so so the way these h& image is being stored is the they kind of store everything at the highest resolution that we have and we can kind of scale the image um to a standard resolution and that resolution is 20x right so um most data that we have is either 20x or 40x right so so for the 40x images we basically take um basically double the few The View and we just uh resize it down "
    },
    {
        "start": 1717.0,
        "text": "okay right uh since for this two S network uh the highest part actually will enable us to Le very good P representation but the ultimate goal of um the safe password the conversation pathology is to having a good slide representation because that is part as most of the pathologies or the actual annotation or the evaluation can be done so here the next stage is after we get this very high quality patch tokens from the patch encoder how do we actually uh have a slide encoder can first of all agre aggregate all those patch token into one slide features and also potentially we having a test agnostic slide encoder which is not specifically TR to any of the uh task or something so here is what we uh proposed we call it slide pre-trained Transformers for abbreviate as spt so uh the uh the advantage of "
    },
    {
        "start": 1777.76,
        "text": "having spt is some of the uh slides encoder you do need supervised post slide learning to this having annotation like the tumor type or the idh status or MGMT some this molecular informations and this is very easily to having a test specific or fitting on the test going to have and here we're proposing to have this slide encoder to narn this uh inherent uh slid post SL structures with the transformation into itself and just without any Reliance on those uh tasks or and these annotations similar as before here we have the host image in with patch name we put them into the patching coder which is already uh train is pretty uh pretty well and it's doing pretty well and also we'll apply these two different postl transformations to generate two views for the same host side images that "
    },
    {
        "start": 1838.32,
        "text": "is where our like either contrasted view or the S uh two different s views formulated and then we'll put them into the same actually we'll put two here but they're shared with uh the our hoside Transformers and they will Al only get one tokens you usually do a class token out of it and then we can apply any like self wise uh learning method like we talked about before we can have like uh s clear we can have uh Bol we can have B crack or even we can use some sh wise method that requires like two different views to do that for example s they integrate both uh Sim clear and also the uh label information for the uh supervisor loss that is part we cluster our we classify our SPD into two way one is uh self survice spt one is supervised spt we call SS spt and "
    },
    {
        "start": 1901.24,
        "text": "supt okay uh so the very important or key ingredient in our spt is how do we generate two different views for the same post side images uh for the host side images there are lots of features and also it's actually quite challenge to do it so we can have a regional hyrogen we we have very mix of the dense tumor we have the normal tissue we have some blood cell and maybe some neotic regions they are very different in the same H side images so we want to handle these challenges and even with the same type of the tumor well we maybe have quite different histological features to handle and also is the common issue for the compation and is even uh even worse situation for the uh H set images is information at redundancy where for this gigascale uh pixel images you usually don't need to have every patch token with everything "
    },
    {
        "start": 1963.12,
        "text": "but how do we kind of handle this information redundancy to reduce them to very dense representation is ultimate goal to formulate very high quality uh slide en coders here we propose three different different ways to handle name uh first is splitting and then is proing and the last one is masking so the splitting is we designed specifically to decreas some mutual information between two different views because uh as we have shown in the like previous highest patch uh uh augmentation if we just apply patch basically two views will have exactly similar uh information in that way your self surise learning objective does not provide very uh beneficial or any uh structure that they can learn so the splitting is we don't allow the same patch belong to the uh two views at the same time in that ways we can reduce this Mutual informations and we can um "
    },
    {
        "start": 2024.559,
        "text": "just keep the some task relevant information intact and the other part is the uh cropping uh specifically we use this to handle to capture the original hyrogen because the uh different regions they usually will contain quite different diagnostic features and we forcing them to have some distance between the two different crops in that ways uh we can uh having a quite distinct but all having a similar histological features the last one is masking the masking is mostly just for reduce the uh redundant visual tokens because uh really like very nearby p P tokens they may just look exactly the same and also to put our uh all the patch token into the deep learning framework really you need a very good P um batching techniques to make sure that all every time the batch size we have is the same "
    },
    {
        "start": 2085.2,
        "text": "so with masking we can enable this efficient bat training here is the general views of using spts on Le two different uh of generating of Le two different views and saying post set images with utilizing uh splitting masking and also de cropping here we have the red is in one View and blue is in the other views and after ining the splitting will not have the same patching most views okay uh the another thought is why we just use the slide level uh Transformations like when we do the patch training we do have the patch argumentations at the patching code uh actually there are some firework they do enabling both patch augmentations and sliding orentation at the same time but uh with hpo with hypothesis like is not necessary to this patch augmentations um because actually uh our "
    },
    {
        "start": 2146.4,
        "text": "SPD framework at least we rely on a patching coder that is already trained in this uh train encoder that is invariant to different patch orations which is usually how the patch encoder is train so uh if we visualize the patch tokens with their different uh augment augmented views on the Disney plot in their in their embeddings you can usually find uh the anchor image of original patches and their augmented views they are very similar to each others and we don't think L will actually allows you extra information about these slide representations and act and actually enabling the orations you may have to also have the uh inference during in this training for the patch tokens however if you just phras name uh and extract all the ineds together you can have a more efficient training because you don't need the forward pass uh for your patching coder you just need uh do one forward pass and "
    },
    {
        "start": 2208.8,
        "text": "save name that's it we also compare our method with firework they also utilizing the pations and but they don't use actually they don't use the uh three uh transformations in spbt and we actually shown that spt uh is outperforming L uh uh baselines using it two stage augmentations okay so uh for the evaluation benchmarks here we have five different benchmarks and we have four different tumor types um there are there are two image modality both Sr and H and on three different class for the SR we do the histological classifications there are seven classes and for the Goma we continue uh benchmarking on this molecular classification of idh BP by mutant for tcj braa is breast cancer we "
    },
    {
        "start": 2270.56,
        "text": "do the subtyping with uh two major uh breast carcinoma subtyping in tcj bre project same aslc is long cancer and RCC the kidney uh the kidney tumor okay uh here's the result for selfis spt the gig SSL in the row above s SPD is the one I mention about like using both patching uh patch level augmentation and also this slide transformation but just the uh masking and it's showing that actually the sspt AL by a large margin and also with be the hipd is another giant uh Transformer hierarchical Transformer uh and is showing like uh the state of result but we can still showing the ssbt having the outperform all those self survice Baseline on top of that since we have another variant of the spt called "
    },
    {
        "start": 2332.24,
        "text": "supervised spt or SBT we also Benchmark uh SBT on all all the State ofth art method um in a supervised fashion for example trans ML dsml and clam all those method and actually we can say for in the self superise method the sssb can be all the superise and for the supervis as uh method we also build our uh uh baselines as well here all different task here we also want to uh uh have ablation study of assessing the impact of different Pion colder here is more like to discover the relationship between the patching coder and sliding coders because usually um we will have like we may have a high dis patching coder it's pretty strong but it's possible you don't have uh a very good patching coder then how our as uh uh SPD "
    },
    {
        "start": 2392.48,
        "text": "can work on it's very like just imag pre train network if you don't want to you don't have the compus to train your own patching cter and also for some of the uh large lab they may be able to generate Foundation model for a patch encoder it's like uh it's a very giant Transformer and put tons of data on it it's like P and uni and by utilizing the spt we can further boost their performance on top of their already extraordinary performance here we classify the patch encoder into three different part for the h h& uh benchmarks we have out of distribution uh which is patch encoder train just on internet we have the near or the in domain uh patch encoder like hipt and we train with Sim clear we train with highis they all the passion goer train on the h& data and also the last two the foundation model p and uni "
    },
    {
        "start": 2454.04,
        "text": "p is um um vision language model they train on the all the pathology image and some uh on Twitter and uh on Twitter they retrieve the taxt image pair and also the PPM some literatures you can find the caption and the figures they describing the this text and image pair and uni they use a foundation uh they have a um they have their own in-house data set which have like U hundred thousand post SL images covering like loss of tumer type and massive massive uh amount of data on this giant encoder and we can say like uh truly the baselines here as a putting method we can say is truly showing the incre roughly showing the increasing performance as you have more and more strong patching coder and ssbd and SBD can uh basically diminish the difference "
    },
    {
        "start": 2517.079,
        "text": "of of all your different patching code and Bo and left them to the same level and if you're using Su spt you can further increase the performance uh this is another uh this is the same oblation study just for the another Benchmark for the SR Benchmark here uh some of the method for example hipt which was trained on H data now becomes out of distribution for the SR data because they are actually quite different and the p and uni is two foundational models actually we show the uh perform we show the there's distribution shift between this two modality and the model just not working so well comparing to uh the h& benchmarks and for the Sim clear and hiis is two method we train them just we train the patch coders on uh in the SR data so they are basically in domain and they just with the baselines they are comparable with uni and s uspt and sspt "
    },
    {
        "start": 2579.04,
        "text": "we can also boost their performance to the similar level like even you have five different patching coders yeah um as a uni claimed uh is a State stateoftheart method just with using the abml is a very easy and a naive way of doing the feature aggregation on the patch tokens uh but with using the supervised SPD we can further improve their uh performance uh high quality patch token uh patch tokens here is the attention maps for all the uh patches we extracted from the host side images and they really shows the without any superation this is a f uh self fully entirely selfis method and we can basically sh the basically close to a unsupervised segmentation task where you can see the clear uh tumor margins from the uh dense "
    },
    {
        "start": 2640.04,
        "text": "tumor and to the blood cell and also here and some nees they also have different attention values to look at uh to look basically the uh the yellow part is with a high attention value they all showing the dense tumor okay so uh SPD is a general framework uh for the h representation learning we show name all different benchmarks and IM it and also SPD use this domain formed Vision language transformation for this post side image view generation which is um which enables this efficient training and also learns high quality features also uh SPD achiev is Pure Performance all the Tas we showing here okay so we have the our uh General uh conclusion for our work here with we proposed this training framework is by General to basically all the whole slide "
    },
    {
        "start": 2703.359,
        "text": "imagees modality whatever imager typ you are having if you're having like giga pixel uh host set images you can just follow this framework to patch uh to patch name into smaller patches this patch you can choose like uh either 20x or 40x you can patch at 512 or 256 depends on U how Grand Nar and or for your Compu your computer and after that we thr name into patch encoder uh the patch encoder can be trained in a self wise way we can use either just the conventional SSL or we can also use high dis to leverage this hierarchical discriminations after we get very high quality patch encoders or we don't we can use a slide encoder to aggregate uh this patch tokens to a single slide feature Fe es and uh here we we propos this High disk and spt to fill in this two uh to fill in uh these "
    },
    {
        "start": 2766.24,
        "text": "two rows here U uh there are lots of Downstream TXS for this framework and also like for example um here we are showing a tumor infiltration detection task uh which is just to detecting the tumor margin of the Goma case here we can see the tumor infiltration from high to low on this uh on this H slide images and we're we're giving a infiltration score and this is a work where recently uh is exception in the nature uh here is a code uh GI have repo for the highis and SP spt you have a look uh way uh finally I want to thank our co-author and collaborators in our missionary neur surgery lab uh at University of Michigan I would also like to extend our uh our gratitude to all our sponsor and any "
    },
    {
        "start": 2835.28,
        "text": "questions any questions about either hi highis or spt we'll be happy to answer what happen if you the and jump the whole Imaging to SL old would you get equivalent results or Worse yeah I think that depends on first do you have a good uh good GPU you can actually fit in like en uh raw image tokens into your slide encoder because here our slideing encoders actually is handling on the embeddings L embedding like 20 48 or 512 however in the imag space like after time them is imagees like it's 256 3x 256 by 256 so usually you still need like at least a uh inet "
    },
    {
        "start": 2896.72,
        "text": "PR train encoder to encode them in iding space because directly train them just in the Imaging space uh is possible but uh I think it's the easier way to to just to separate two roles one is to do a regional level and learning for the patching coder and also AG aggregate name to for to form a better slide representation there are some uh recent methods they actually train them end to end uh after the passion coder training the allow you to uh find T the patch encoder together with your slide encoder through the aggregations all right I was actually I mean you already showed like different applications to different tumors and T so I was I don't know if you wanted to expand on that kind of the further applicability "
    },
    {
        "start": 2956.76,
        "text": "Beyond's um uh actually we here showed more than just foma for the our baselines uh yeah for the all different tasks uh here on the TCG we have G and we also have the breast and the L and kidney so yeah but in the breast cancer didn't always do as well I was just curious that uh I would say breast cancer this data set is pretty challenging here we're actually using uh it's a very highly unbalanced data set the even the state of the art method uh for the supervised part they can achieve like like around 80ish method and we can further improve them into 84 I would say it's really hard to be those baselines and the match we are choosing is mean accuracy is the most uh truthful way to represent the performance on this highly unbalanced data "
    },
    {
        "start": 3019.44,
        "text": "set I think it's like one to 10 in terms the yeah yeah no that's really that thank you um and then my last question is like how feasible this would be for example I know like those the the right three columns are h& but for example for the SR like how feas is it actually to like four you know um for like I guess like lower resource hospitals to have an Sr energy machine um there if you have an idea it's actually cheaper to have Sr machine compared to having a full Lab and have the te technicians have staff to maintain h& lab um so h& is certainly more popular in a lot of places and that's just because that's been the standard for the last 100 years um but if you actually compare the cost of acquiring H h& sorry according Sr device "
    },
    {
        "start": 3083.04,
        "text": "and maintaining a la for um doing h& standing and all that stuff Sr device is is probably cheaper um like you don't need a lot of training to use Sr like I can do it so so so so so so there's that um so I think going forward I think both modalities will be important and quite interesting for how basically the pathology digital pathology Community moves forward right because um I I don't think just doing h& is going to the future of how we do these diagnosis I don't "
    },
    {
        "start": 3143.76,
        "text": "know I was completely this entire time but um it's more like related to uh so I want to ask you seem to be like ay ask so the standard is identify what do you think about the possibility that something pathological before something pathological um so that's that's that's very interesting question but I don't think that's something that we're we're particularly looking at um I think a lot of the ways that we're using these images what happens is that um some "
    },
    {
        "start": 3204.96,
        "text": "patient comes in to to see their phys maybe they had a headache or have something and then they they were probably get M skin then so and then maybe they see a t do something else so kind of at that point is where a lot of these diagnosis tool can come can come and help to to do these diagnosis and where May Physicians can intervene during surgery ofc um but before before that like if you're were talking just about like genic screening or something like that um I think that's something that's possible but you probably wouldn't be have any images to do the screening you probably just do some sequencing data which you can probably do a lot of data money on that as well that's something I'm just a quick question I know um you are always an advocate of um computational uh or like a computational "
    },
    {
        "start": 3266.559,
        "text": "efficiency so my question for you and rley is how how much computational resource you have to use in to run your peline in terms of time or like G I think I can answer your question with our oblations on patch encoder so as I mentioned this is two stage training we have our patch training we also have a slide training potentially we also have a patching stage the patching stage I think it's pretty fast this probably don't worry too much you just patch name and just filter on the like some intensity BL and your filter back CP yeah so that is all CPU is very easy so the uh bot necking part is the patching coder and the slide encoder uh most recently I think most of the patch encoder uh L are the very heavy part to train so for the uh like IM we don't know about IM now because it's already up the shelf but the highis S clear at "
    },
    {
        "start": 3328.359,
        "text": "highis if you want to train in house currently we are using two R40 GPU they have like 48 is GPU memory and we train it for two days yeah roughly and for the ple and uni those are very large model as for the uni as far as I know is they are using 48 800 GPU and TR a week yeah the train is for weeks and then you also need to facturing tuning and things like that right but the good news for us is that imet flip uni if you email me highis um you can get these models for free right essentially so like that's kind of like the heaviest part of training if you want a really really good one it's probably going to take weeks and months to train um so so there's that and then the the the sliding coders are faster um so all of our experiments the "
    },
    {
        "start": 3388.799,
        "text": "signning colod just took one l40 or A40 this one we have um on the uh an arc and we basically took them a few it took it like a few hours to 6 to eight and uh here also this is model size for the data size uh this in domain data set was just specifically training on the data like we are going to evaluate for example we going to evaluate am breast cancer we just train a Pion that just for breast and for this foundational patching enoder they show their extraordinary generalizability on all like different T type you can think of you can find on like online like 30ish Tor type like always like always showing uh generally okay not the best but it's really easier for you to train some data from by yourself like just "
    },
    {
        "start": 3451.799,
        "text": "evalution here with a baseline uh L generalizability on this H is just better if we just stre your own patching coder so uh some of time you can just utilize layer methods and on top of that you can use uh spt to further Bo performance and it has your interest to and this part is light yeah and it's probably just one GPU six to8 hours that's a good place to end so we can than one more time "
    }
]