[Music] [Music] so we're gonna go and get started as a reminder we are starting on the hour now since there's no longer any Michigan time so welcome the tools and technology seminar series I'm there's a sign-in sheet I think it actually already circulated but if you haven't signed in please do so again it helps us with just trying the pizza we want to be able to continue offering that so right into the presentation so today's speaker is Daniel Wong and he is a postdoc in both the Parker lab and the Quan lab in DC a movie directing follow them through some deep ranking about me what are things I want to do to my postdoc listed right tools that were actually useful and used by people and what ironic ironically enough there's a tools and technology seminar but here I am talking about it and just last night and didn't have time practice it so let's just go to it and hopefully it goes well all right so I'm gonna talk about I'm personally get background what is survival analysis so it's a general it's sort of more or less a generalization of regression analysis but was a little bit of a twist let's say yeah I'm clinical trial over here and you have some locations in there not only have you the duration between when they started the study and when they died you also have this def observation variable or satisfy marry variables I'll tell you whether there was observed or not now no now no does not necessarily mean David because of course realistically everybody has to die and actually even make you make a memento mori or Valerie realistic about that it doesn't mean their death was just not observed during the time of the trials they likely died after a trial or well after the trial everybody dies eventually yes means there that was observed during the experiment and the point of saying that one that goes to spinal analysis is to predict the duration I'm sewing event curve such as that now I I don't feel it like biology and medicine because it's called spinal analysis you can imagine there are other events and other fields like engineering where you try to find the time of failure this case it's called reliability analysis and economics is called the racial analysis and it's also idea for the main history now analysis now another interesting fact about survival analysis is that can be seen as a form of semi-supervised learning and semi-supervised learning is a pretty exciting field of a few learning where not only do you have a lot of maybe you have labeled data up you also you may have also a lot of unlabeled data you need to write your algorithm accordingly so and the data is r-r-right the ones that have oh yeah i also mentioned the ones that are have where the deaths or NAS are because you write sensors data and because we do not know their actual death types their sensor on the right side there's also an arm adjusting factors also such thing as left sensor data or you don't know the perfect clients beginning events times but that's not going to be covered here and frankly I have yet to see a very useful application of the sensor data I need to know about survival analysis is a concept called the captain fire has demeanor which attempts to estimate the survival function the survival function as you can see an example here will show you essentially the percentage of all samples that are still alive as a function of time so I was you know everyone is alive so it's gonna be 100% and then of the child everybody's either dropped out so we don't know whether it's good or not or everyone has already died so it would have plus survival probability of zero now this is when you cannot what you can do with these survival functions is you can compare in a statistical fashion to groups they say you have a treatment group and you have a control group and you have a group that you treated with the drug and you want to compare whether you're drugging affected or not you may draw a survival pumpkin have we - aamir or both groups and see if there's any statistical difference between the two girs see your drug and effective using it all great test once you can also pick yellow and fancier let's say you have four different cancer types here you wanna see there's any differences in survival probabilities between important groups and you come out and say oh the green guys died out a lot sooner in the blue guys and it says you know adeno carcinoma and squamous cancer there's probably something would give it up a lot more about cancer than it I did I think it's a you don't really need to know when you're dealing with data you don't actually need to know the underlying biology a lot of time you just need to understand how to apply the mathematical algorithms a little funny so current solutions for survival announced analysis or computational perspective they're actually aren't a lot of things the most popular model for survival analysis is a Cox proportional hazards model it's basically the linear regression or order logistic regression of survival analysis is a simplest when possible it works alone a lot of cases there's also survival random forests there's a support vector machine version and I also mentioned in cops and enemas the customer never know man I've Armand a group I included this last weekend if you would here today you probably be upset if I didn't mention fermented so there is but I was here today so I guess I don't talk about it in terms of software packages there's a you also have very relatively few solution things do you all have their they all have their positive agents their sidekick survival is a Python package that goes to emulate this I can't learn library I will talk more about scikit-learn later because that's pretty important for this talk there's a pretty good famous library a low side of library lifelines for Python probable party to big the big one the one I see that's probably used most often deepest survive analysis is usually more seen as a statistical problem rather a machine learning problem this large survival and it's being a 5-4 arm and you wanted to use survival random forest you actually can't find the right and forest model version and any of these three kind of uses you need a buck needs a climate within its own hard package called random or it's hard to see I think that's hands for random force survival or SVM terrific iPads are these embers but compare this to rake compare survival analysis to regular regression regression you've got so many you have so many options got linear regression neural networks support vector regression random for is out of this gradient boosting XG butane yours neighbors Gaussian process rewritten how many touch me a service here they're probably like hundreds of more methods just for regression alone and don't even get me started on plastic agent so one central question is coffee so today can you extend the flexibility of regression analysis to survival analysis since I giving this talk you already know the answers let me guess right and you can already see it I don't default so now I'm going to talk about the cops before trial hazard model very fast like I said before it's one of the most popular model sports bra analysis is basically it's like it's the simplest model Hospital is the linear regression of or the logistic regression of survival analysis so warning here it's gonna be a little bit of math if you don't like that because you Norway I say through the next few seconds so let X let X idea vector a p-dimensional vector which going to be your features if you're sad fishin yo probably annoyingly polite covariance i always hugging features i would never cut covariance of some subject i the hazard function which is a function that describes the number of events per unit time as follows this form lambda you notice this basically told you that the new order of events beg her for unit time will change as time goes on so at time equals 0 we will probably have zero deaths and as time approaches infinity you also have zero deaths because by that time everybody is dead and there's no one left to die at that point and there are maybe some points in between there where you'll have most deaths like for you at age eighty that's where you expect that's the expected life that's life expectancy for most Americans roughly and that's already expect the most deaths occur and you see that the so kaiser function here is you can separate the variables here between plank and the features vector beta here is basically going to be your coefficients you can kind of see them compared to linear aggression you compare them to the slope the slopes you would see in linear regression you can already tell that via relatively simple model in that case and in the cost model you can define a probability function likelihood function in here where li they know which all you need the probability of an event occurring for subject I necessarily attorney at that time and with this single likelihood function you could define a total partial likelihood function would you even convert to a partial multi cleveland okay and statistics and machine learning all the times like use the law of functions good the log is monotonically increasing function and convergent product system that is very mathematically it has a very nice primitive it's very it's very easy to deal with that's why we use logs and seven not logs and you want to find the beta but you want to find the best coefficients for your model you would optimize beta using Newton's method you some of you have might have seen the newton-raphson method for finding zeros in your scientific computing class this is the optimization version of it here you may notice there's a hessian matrix for me estimators here with where you have to take the inverse of the Hessian matrix and that is actually from a mathematical and statistical point of view this is this is a very elegant algorithm it converges going fast it's it's a very succinct on the board or you could write very easy program the problem is this inverse of matrix here from a constitutional perspective this is a complete nightmare taking on taking the inverse of a matrix is a very slow operation it's also very easy America be unstable you normally would not do this for most normal purposes and I'm going to talk more about how if this is going to be one of the copses month Castellanos weaknesses later now here's an example of the cost follow in action next thing I learned my betas I train a very simple cosmo no I learned my betas and they have a model that has no eight one two three four eight features eight covariance and I have four hypothetical samples here since I have a trained Cox model I could I could I could create a hypothetical fake survival curves of survival functions for each of these fake individuals and you could probably you could probably interpret these curves to me if I had say an infinite number or million copies of subject1 then subject one or sample one will follow this survival function you'll have this many people dying at each time and I had a million copies of subject for then those movie population is mortal although the read survival function here yeah so it's not a very accurate model but it works well for most purposes now how would you evaluate how well your model is actually doing if you predict when a subject or die right you create a creative prediction how will you know your predictions are actually doing you will use a metric it's final analysis called the C index the C index C and next is a generalization of an area under the receiver operating characteristic curve like the one you see on the right here now the area under the offering heuristic curve you know that your it's for classification you know your classification all of those well or the area of Folgers one you know that your model is doing basically random chance if it's equals an area of 25 and you know you should never trust the model at all if it has an area under curve of zero and the C index also has all those two similar principle it's has values between zero and one zero means you never trust it 0.5 means it's completely random and one means is essentially perfect another fun-packed you there's one there's actually a very nice interpretation of the area under the curve it actually describes the probability that a classification model assigns a higher value to a positive sample over any negative sample and that is also how you can sort your jiseon mix you can you can Turpin it as the percentage of concurrent pairs now my very own receiver operating characteristic curve I can't actually draw a nice pretty grasp for you I know how much some of you playing craps autographs and stuff the CNX does not have an equipment graph to interpret now another way to interpret the percentage of the coordinate errors is how well can the model prioritize higher risk patients so let's I have a very large ugly table here to give you some examples the day I have two samples two people here sample a sample B templates at tempo a simple B status are both lengths which means we know exactly when they done a tempo and duration of 15 10 will be hesitation 25 so we know 100% for sure that sample a died on board sample B so my motto could predict a good model to take a higher number a higher probability for sample a versus sample feed you can kind of see this as how who is more likely to die essentially who's more likely to die sooner so is it a quarter-turn care yes now what's an example of this point there let's say I have sample a the status is known but enthalpy to status not known and even though I don't know when status example B died I do know that they will eventually die and that they died after sample a I can conclude I think conclusively I think I can definitely conclude that so my model could predict a higher number for sample agents entropy but my mom imon owes me a mistake and instead they get a smaller number of the sample a versus Apple B so this is a disappointing that's that's bad for the Siemens that would be resourcing see you don't want that now case number three is a little bit more interesting I quote is in the dark case what if simple a as a very low duration by a simple V has a very high duration but example and even though simple B's that is no sample with a tease that is completely unknown well in this case even though sample V has a higher number of example a you cannot you cannot definitively conclude who died less right sample a for you know could have even though they were last reported at target was like these gonna die that tiny close five million for all you know and you would have it's unlikely but it could happen and so it doesn't really matter what numbers what model what numbers the model assigned to these two samples because they can't really decide whether it's an important character not so you just movie they're my ponies a dark spaces in the last case is actually probably one more interesting ones what did we have targeting clients Oh sample a and sample B by the time I was 20 and sample a you know D die for sure - we'll be there ventless officer well you can this is actually still considered a supported pair because you know you don't know when sample be died you know that it died after time equals 20 which is to find that sample a die so you can you so your model of aside instead numbers correctly you can definitively conclude whether is important hair now another interesting fact about the see index is it's invariant to scale and location transformations Pierson Spearman correlation and a you see also share this property so it doesn't matter what the actual values of your predictions are all that matters matters is the relative values so since we're also talking about evaluating models I also talked about Homer pity any insecurity example and if you go on Wikipedia or fitting is defined as the production of an analysis that corresponds to closely or exactly to a particular set of data and many demo therefore fail to additional data or predict future observations reliably and that's a mouthful so I think overfitting is best explained through pictures and means because that's how I understand the world through memes so look at this picture yeah three different models here one of the first ones oh definitely overfitting and you have you can see that you have all these the sinusoidal curves it's oscillating a lot as a classic example of overfitting let's say a polynomial function or polynomial is too high but if you have an optimal model that's just right then the curve will have a few mistakes but overall when you try this new random test dataset that's when error is minimized you can also have underfitting where your model is just too simple and there our error is too large so you need to find that sweet spot you know finally we need to find the Goldilocks number between underfitting and overfitting here's some means that also explain overfitting three but I think this last one it's the best explanation I'll report folder city overfitting is also a very big beginners mistake a lot of times even I make this mistake a lot of times I see ooh my model is doing well but really I'm just fitting into the training data and when you generalize all its new test data you find out your model is not actually or means what you think it does now since I'm also talking we're not talking about overfitting in value models which also talk about cross-validation plus allegations to find ways with video annual various similar model validation techniques or assess a pal of the results of a statistical analysis would generalize to an independent data set one of the most popular versions cross-validation skateboard cross validation on you have a picture of four fold cross validation that perfectly illustrates how it works let's say you have no training let's say I'm no testing it up just training it up what you would do is you split the data into four equal parts or usually equals exclusive parts and each interval of the four iterations you would train on three quarters of the data and leave out one quarter the data for testing and then you repeat that process three more times and then you I mean that would give you a general idea of how well your model generalizes to new test data now when should you why should use cost allocation she's fond foundation we want to flag problems like overfitting excuse cause validation if you want an objective way or somewhat inductive way to compare Marloes and hyper parameters and you want to see which model is the best knowledge Easterday if you want to see which type of parameters to use how big sure your neural network be or how big power one orders your polynomial be and you should also use it if you lack up the competing like an independent objective test set now I'm going to introduce one right that's title of this talk and this is our this it as the same flies its invented by Professor Guan and it's her solution for addressing the survival analysis problem and the relatively simple algorithm then I'm going to give a simple example with me right here what you do is you all of this stuff is a three-step our first you probably add the duration to right sensor samples using capital in my respirator in this case I five sepals year none of them are right censored so skip that first step for now we didn't convert the imputed durations to percentile rates so that's also pretty simple here I have five samples and you convert the ranks going backwards so this shows you who who died first who died person who died class right you go from one to zero source by examples you're going to have percent palette at 100% to 0% those numbers obviously and then you apply a regression model such as random foreign tourists as percent tolerance it's just it's like it's like it's like taking the SATs your scores are converted from a 1,600 points a o to a percentile scale and percentiles are much easier to deal with now let's look at side of a margin complicated example what happens if two of these samples are sensitive rate sensors now I notice this sample here in the sample here is censored now the wrong rate though is a little bit different it makes a little bit less sense but we can you wrap your head around it tiny so now the order goes from the sample one you know that guy divers in sample three bikes suppose second example to value third and before died toward five so die class so you notice now each to value over here is switch order even though their durations which helps would imply that may be subject reagent diet or something - because you don't actually know them something to actually die you assume based on the kaplan-meier estimator you add a little bit of time to subject to because you know you don't know when a night but you know they died a little bit up the you know they died some time after time to seven and and since you're happy my eyes here is a very little data together what ends up happening is you added you just added enough time to assume reminisce this is more of an archetype you added just enough time to assume that's subject to died after so victory and the Guam rates are changed accordingly now the Guam rank algorithm is very flexible you can use any regression model you can use friend course your network is support vector anything you want use anything to your heart's content and the regression model perfectly acute introduces is long range it will have this scene index of 1 that's why we use the glommer guesses and of course the model is pretty simple to understand it's actually already implemented in arbitrage lab without actually very low instruction from us actually and I only just really like things they understand if you don't understand it then they won't use it so sometimes so that's great and it works well in practice and you know things I learnt in practice engineering solutions you love your duct tape in your wd-40 as long as it works and that's what you want really really very great they know who you are and I mentioned dark cases are here those are cases work when you're preparing two individuals you can't fit community food who actually died first supposedly a plaintiff professor got this met the Guarani gather in here implicitly kind of and those those are cases by trying to assume who actually died first goodnight last and normally these are cases are ignored by the partial log likelihood function I showed you earlier which is what the cost model comes to optimize so this is possibly one point agglomerates favor I don't know how true that actually is but it's from the Creator herself in this is assume it's true all right so learning finding one right the clerks on the computer scientists will put everything open source on github if I not be hope the ironic part is it's actually a private key of my friend Vivek here if you point that out to me so you can't actually find it but if you have to do and you have to do want to find it then one day you'll be able to find it here when it goes from private to public now it's completely gone and the reason why I wrote it in pythons because computer scientists are totally in love with Python and you want to be wanna if you want this to appeal to a large computer science machine learning audiences because this is a very this is a very good machine learning problem but survival analysis is competing the hands of a statistician that moment and that's why the art survival library is a lot more popular but I want more machine learning more computer scientists computer computer scientists to enjoy how to do survival analysis that's why I make it like I hope you won't make an art version later I also wanted to adhere to the scikit-learn API and I give a small example on the rice out of here it's please I know you're not only supposed to put code and presentation but hopefully this is easier know if you want to use my algorithm I if you know how do you scikit-learn then you already know how to use the Guang the Guang rank repository they say you just influenced the wandering library using standard I found that very important even if you want to use a regression model 10 so just and of course imagine follow the regular excitement learn API load your model go your data straight a rank turn here fit the rancor to your Y data so since my since that one ring I'll learn all those thus I can learn API it's one of all of the fits and transform standard and then you just fit your regression model to your cope areas or features and using the wand right transform wide labels as your labels and then you can create your predictions here so now I'm gonna talk about why this I had learned I know I come from an iPod I'm gonna explain a little bit more depth here and why it's so like one the 100 quickly there's already wandering all over him because we made a public version so that other people can understand it ahead of time it's not very well maintained but examples on there to get you started it doesn't follow the scikit-learn API and it's not Python and you know how my Python right daddy if you say and so if you want if you want one drink right now you can go on to go on that Osborne just get the R version but my version is better you just can't have it yet okay so what if I can learn why is it so great that you cycler planking learns a popular machine learning library Python widely used in both academic and industrial settings library go on top of Santander library and make successful it's accessible to everybody if you're usable in various contexts now it says everybody I don't know how true that it I don't know how many you here actually use it's like it learning we kind of live in it academic kind of live in a full all right so it's popular and other fields or a but and industry may not necessarily be popular here here at least one at least one or two of uses scikit-learn right but or you should use like you learn I'm an open source which is great and it has so many it's so popular and it has so many useful functions for classification regression clustering dimensionality reduction auto selection pre-processing and it's so standardized in fact a lot of the most popular popular machine learning and related algorithms are on here because they have a very strict criterion on what's allowed considering their library usually as a rule of thumb this is not always followed but an algorithm is only allowed and so I can learn it it's I think at least two or three years old has 200 citations follows the fit transform paradigm and is popular and yes popularity is based on how many a fluency gets on reddit or something like that I know that sounds a little facetious but you be surprised like how computer scientist for at they just they don't really use public journalists like make sure they use archived and ready as their main source of information or blogs so now this quick long-range perspective let's see how well it actually does impact this so I'm going to use 10-fold cross-validation here and I'm sure you're all experts 10-fold cross-validation by now let's focus on the plot here please excuse how the one on the left looks completely different on the right the one on the left came from the 2015 3 años trafficking ALS learning or do get rights disease yeah a mild shock the laterals or tangles so dr. blonde did this challenge several years ago and he just like a long time ago all this data on me and frankly I didn't want to redo the analysis because it's kind of messy going through old data and old code and so I just did a next best thing and copied a lot from her figure and hope that this copy is happy representative of real results what she did was for her winning first place algorithm she'll play she company the guava canoe used the Gaussian process progression as her model this data has six features 7,000 samples and 50% of the data is Right sensor he has a pretty good now five percent better in terms of pin code CV index on top of the cost models not bad pretty good won first place apparently five percent is all you need to win first place right there only tens of thousands of dollars there's lots of movement I will talk more about that later I have a mean for that later alright so since I don't want to reproduce her data because it was a site about hot mess I try to look for other data sets out there so here's another popular it's about what else is it a Veterans Administration lung cancer file this dataset has eight features on the very same samples X 8.6 am percent sensory and I can't really conclude whether one model is better than the other I used one ranking from Mighty One race with three different regression models and importantly and boosting how do these things they're all really kind of like statistically within the range weather so you can't conclude and many one is better so that's kind of it there I'm pretty sad about that but then fortunately this is machine learning there's always another dataset out there so I the preneur dataset the same library I found the 76 gene prognostic signature for no negative breast cancer patients this is a more this is more crazy visit has a two features 198 Campos and 70 24.2% sensor and lo and behold all the guangmei face messes do a lot better than pots in fact I was I mean I can actually hopefully say that on average the difference between the long-range models and the Cox model is even more staggering the differences even weren't staggering that what you saw in the dream college over here so yay we found beef that we justify our existence for writing or writing the algorithm is oosan' the number of futures so is it you know it's you can't actually conclude you can't actually think you can't actually conclude from three samples like windows our model court best small sample size it looks like the more features you have the better but this one app it has the fewest features right so how much there mister or maybe it's how much it sensor all right worse this one is really sorry maybe it may be the more sensor data you have the better go online for more cops but we need a lot more basis in video for your third leg arson you exclude those sensor data do you have about the same components of course Oh what happens if I do this this tempo cost outlay analysis using like training a model using only the non sensor data so I actually I actually tried that with here and it doesn't actually using that code I actually got a get a box like that also within this range too so tell me that I'm using the sensor but there's no difference when you start up please asking like take the one on the right where there is a difference oh no I only used like the 25% that's nonsense really I should actually do that next yeah okay I don't generally machine learning it's a bad idea to get rid of data right you generally wanted to I mean there are some cases where you might want to throw away data like if it's very noisy but generally you don't want to throw away your data but yeah I'll look at that next the cost model couldn't use the uncensored other sensitive what you mean uncensored no the sensor data and of course Moodle use it the cops model caps for dissent the partial log likelihood function that uses its it tries to maximize all the Concordat pairs the problem is it's going to so that means it will also include sensor data point because some sensor data points are included including compares right the problem with a partial log-likelihood supposedly according to dr. Gwon is a the fertilizers lovely hood cannot attack for its are contagious so like when you have two samples that are Spencer you can't conclude who's going to die later but the wrong rate trying to massage the data and tries to conclude who dies later doesn't always work so yeah I said I was going to address with Brian etcetera this is the last slide so final thoughts for someone to apply the model a good long rank algorithm on more data sets because we need to show on as many data sets as possible why our model is the best and why you shouldn't use anyone elses getting but since the gwaan rank algorithm currently follows the scikit-learn API that means I can do all these cool things with active hyper driver tuning I give you ensembl link I could do feature selection and feature the legend is actually one of the more interesting ones let's say you don't care about accuracy but you want you care about say this like this is a semi 16 data said what you care about finding the sixth most informative genes for predicting survival you can already do that with one big long rate algorithm because it almost looks like Hitler nate di and there's actually already other github examples where you see other people doing something similar for different types of datasets another thing I'm interested I'm very excited about is because you can use any regression model you can I you can in theory do deep learning or deep convolutional neural networks with images both big long rank algorithm so you can probably use your imagination you could imagine a scenario where you take a picture of someone and you predict when you're gonna die or something like that or you take a picture of fruit and predict based on the picture of when it's going to rot or spoil right and I say I say this life is my mantra sometimes nice is more art science and sometimes it's more art than science a lot of people don't get that and the go on right now there is not perfect but it does what it's supposed to and it's easy understand works well and practice another argument I'm going to conclude with this some people argue why why shouldn't be watching bother making another model that's just slightly more accurate than other why ad why add to the plethora of other than that already out there and I'm going to use two arguments one is the Burger King argument have it your way so there's a theorem and machine learning called it no free lunch theorem which states that when you average each models performance across all problems all their funds and their overall performance will remain the same will be the same across all problems this basically means that no one model will always outperform any other model on every single data set you need to pick the right tool for the right problem and it doesn't hurt to have more options in fact and this data set over here I actually had a bad big problem with Cox I mentioned very earlier all the way back here that there's a inverse of a matrix here for the cost model that causes huge numerical instability so when I was training my cost model for this data set I actually had / 0 incident girls yeah it couldn't of the determinant of the hessian matrix in that case was almost zero and i was below was below machine epsilon i was causing all sorts of numerical issues I had a church yeah so I had to massage I underlie cues a modified version of cops in this case undecided one link you never run across that pop it pretty much works almost every single time if it doesn't work to tell me and post it on github on the issues water become public and I'll try to fix it yeah so more options is always good and the second argument I will use is from our Lord and Savior and your main focus is speech recognition accuracy goes from 95% to 99% go go from barely using it to using it all the time so I'm going to include that any questions see you next slide the seat see you next one the one yeah yeah this one I think so you know that sample a that first about some be very versatile yeah yeah I've heard a typo there I think we almost solution to generally around is similar to you know it how'd you have the cam crow so mathematically the area under the curve is the you know the AK expected lot right right so I think just so will you have a high sensor data points we just either the area of the crow in that patient and then this is the best for you know ask you mismo it's a dumbass yeah yeah this this person will die I mean I fair you can think of like other solutions like you could do some sort of like I don't know k-means clustering and try to find try to if you have a sensor data point I find there five most similar uncensored buddies and try to conclude there at that time from how similar they are you can pop you can think of all sorts of ways to and cute better I want to ask how many features you can possibly pendulum you handle consciousness or ain't your limit you're completely limited by whatever regression model you use the glamouring album doesn't take that long run it runs an oligo thing so if it if it takes a long time in the run you probably have too much data right the only thing you have to worry about is whether you're a regression model so the vector model doesn't can't handle all your features are all all your examples it's not a problem Andre is it publicly regression model one on punching is learning on you can go at encode data time so are you talking about continuous time features like I don't know like hair length for you at which which would change the call rate algorithm doesn't per day yet there are some versions of the cost model that does the company have n't studied that yet I need a data set that has that before I can bother benchmarking it right but I would like if there's already a regression model that can handle if there's already a regression model they can handle it then it's it's not that it's not that it's not that difficult to combine with long rank yeah very actually some data set we look at some cancer progression stages if you have those data sets I send them by way I'm well I'm gonna write like a high profile paper about this and it's just going to be talk is just going to be applying this algorithm or and over again on different cases sociology examples engineering analysis all sorts of things so we can help you get some of those things that's just come see me alright the more diversity you have another you know please I said when we have 76 dreams how what is the extra each of us ah yeah 82 is clearly a bigger number and a number like H concepts and some of those features there's other features are one hot coated too there's also a treatment t-shirt okay any other questions for Daniel great job [Music]