been here ever since since what 2011 you said 13 2013 okay so it's been a couple years here and he's gonna tell us about carrier screening and Kaymer's sounds very interesting all right well thank you so indeed my talk is going to be fairly disjointed today there are two almost completely now actually completely unrelated topics the first topic is about carrier screening and really the idea here is that you want to determine carrier status they're very specifically chosen alleles this is all about assessing reproductive health risk we're doing this in a clinical sequencing context in a CLIA cap accredited lab and this part of the talk is going to be very applied and it's going to be focusing on very concrete problems and the solutions are going to be somewhat unsatisfying because they're going to be very practical and ad hoc and really the focus here is on product development and making a really nice high functioning assay and so really it's like you know hacking through brush in a jungle you just have to get through you know whatever plant you're going through so you get on down the path you really want to solve things quickly so if you have any solution at all you're happy and you move forward and then I'm going to take a hard turn and we're going to talk about all vs. all camera comparison and so carrier screening is really satisfying work because this is really all about helping people not so much with the next part of my talk this is just going to be a very abstract hardcore algorithm and here the idea is that I'm going to describe a data structure that I've derived by combining ideas from coding theory in particular I've got this well someone else came up with it but I've been using a perfect handing code on DNA fibers and it turns out that if you combine that with a classical string algorithm called a sequence try you can actually get some real efficiency for a specific problem that problem is finding all pairs of cameras that match within a very small Hamming distance the thing about this approach is that it's very specific so it only really works for specific values of K in particular okay it has to be a multiple of five and that's because of the structure of the perfect Hamming code so starting with carrier screening I just want to have a you know work in a corporate environment so I just want to describe my company a little bit we're a company that does clinical sequencing we're focused on women's health were actually fairly young so we were founded in 2011 and we're located just down the street on South State Road just south of the airport we're a clear cap accredited lab so we can do clinical sequencing and we have you know what's what we describe as a menu these are the tests that you can order from us among those are a test for a cystic fibrosis carriers status we have a panel that tests for mutations enriched in the Ashkenazi Jewish population and then we have a test that's not really carrier screening but this is really intended to help psychiatrists with dosing or on specific medications and since this is a bioinformatics talk I just want to say that we have a bioinformatics group we have the six of us including me two of them are from the University of Michigan and mostly what we do is spend time supporting research and development during live product so most of the time when people think about clinical sequencing they're thinking about this sort of diagnostic Odyssey and this is really different than what we're doing so a diagnostic Odyssey is really just a general test of different for differences from the reference in a sample and really what happens here is that a patient will come in but have some specifics that some specific symptoms that indicate bits and genetic testing might be helpful and enable sequence of the patient's genome this might be a whole genome sequence it might be an exome or depending on the position and the indications it might be a panel and then you'll analyze data to find variants there will be a lot of them especially if you're doing an exome or a whole genome you'll find a lot of variants many of them will be very suspicious they'll seem like they might be pathogenic most of them you'll have no idea whether or not they're related to the disease date but occasionally I think about 25 percent of the time at the moment especially if you can do a trio this of the paper the patient and the parents you'll find a variant that's likely to be pathogenic maybe there's going to be some support in the literature and it's going to have high relevance to the symptoms and that would be a success in this context so or this kind of application your bioinformatics pipelines have to have very good power to detect wide class of the variation not only snips but in dull structural variants and all of those sorts of things and the pipelines also have to assist with variant interpretation so most variants you don't know what they do so you have to look in databases you have to come to the literature and you know there ways to do that what we're doing is very different so vastly more directed so we're doing carrier screening and what that means is that we specifically test for the presence of specific alleles although in some cases this might be more like the diagnostic Odyssey so you can do whole gene and cystic fibrosis sequencing but that's not really typically indicated for most carrier screening application the idea is that we're trying to assess reproductive risk for well-established mutations so these are diseases that are typically recessive and diseases with fairly high prevalence and diseases where there are mutations that are very well understood to be associated with the disease and so your bioinformatics pipeline is going to be a little bit different in this case the idea is that rather than being able to find every single thing under the Sun that might be in your patient you just have to be very good at detecting very specific things so as an example one of the canonical cases for carrier screening is cystic fibrosis this is a fairly high prevalence condition it's an autosomal recessive disorder with very well causative mutations if you have cystic fibrosis your median survival is about 37 respiratory failure is the most common cause of death and ACOG which is the American College of obstetrics and gynecology recommendation is that all women should be offered screening for cystic fibrosis and this is a table from the ACOG committee opinion adopted to the opinion but really one thing to note is that first of all there's a fairly significant carrier risk before you've had any testing detection rate is different in different populations there's a very population specific aspect to this and there's still a residual risk after you've done your carrier screening that's because first of all not all mutations are known sometimes there are rare ones not all tests are perfect so sometimes your test result will not reflect your true carrier status but this is a pretty nice example of why you would do carrier screening you know you can sort of reduce the incidence of cystic fibrosis just by giving in people information about their carriers so I just want to also mention one thing about variants so everybody has a lot of variation with respect to some of them and in this context we really care mostly about the health impact of these variants and the field is moving towards this sort of classification system where a variant might be called pathogenic and in that case the variant would be conclusively associated with disease it might be likely pathogenic which is to say there might be evidence from the literature that this variant has associated with the disease but it might not be conclusive uncertain significance this is going to be most of the variance you'd find so there's really not a lot of information like the way that might be in silico tools to tell you one thing but it's very difficult to trust these tools then very might be likely benign so probably not associated with disease but not a lot of evidence to tell you that and then variance might be benign which is to say clearly not associated with disease and there's one very easy way to know if a variant is benign and that's it has very high prevalence in the population so for example if your disease occurs at one in 50,000 and your variant is present in 25 percent of individuals you know it's almost certainly not associated to that specific so the canonical pathogenic variant is Delta F 508 this is in cystic fibrosis this is a three base pair deletion and it leads to the loss of a phenylalanine codon and it disrupts gene function so if you had two copies of this you're very very likely to have cystic fibrosis although I do believe that there are cases where we found on people that are homozygous for this without the disease they're not so this is a an illustration of essentially our screening process so we get a sample we do in DNA extraction and we accept the blood buccal swabs our mouthwash we then put the DNA into a fluid I'm ship and this is a really nice micro fluidics device that lets you do a highly multiplex PCR essentially have PCR pulled over here and and sampled over here and it will mix each one of these samples with each one of these PCR pools this is really where bioinformatics enters into the design process because in order to do an essay in this manifestation you have to design a whole bunch of PCRs and you have to make them work and the multiplex PCR context which can be a little bit painful usually in this situation like the first 80 90 percent of your essays will be fine and getting that last 10 to 20% takes a lot of effort so after the fluid I'm we do a library prep and then we do cooling and then we put it on the sequence that we ran Illumina shop like I guess most most clinical sequencing companies and so we do our carrier screening mostly on a 2500 when the instrument has done we as cassava to Dena multiplex this is what a cassava looks like if you've never seen one and if you've ever actually tried to use cassava you know I like that this image in particular I should say that my hands are not clean I was once a cassava developer but um use bow tie to for our liner and then we use a variant color called pre base this is actually a really nice choice for our application because what three Bay's will do or rather we can run it in a mode where it will enumerate alleles that are different from the reference and one of the real pressing issues in carrier screening is related to incidental findings and variants of unknown significance so the really nice thing about using freebase is that you can enumerate alleles and then only look for alleles that you're specifically testing and so you don't actually have to worry about incidental findings because you never will see them you don't get a B or we don't run this in a mode where you get a B C F out we run it in the mode where we can specifically look for our you carry a delta f5 a later as five or nine hour or one of these pathogenic alleles that we know about this is actually a little bit of an idealization of what the pipeline really looks like because you know between the de-multiplexing step and the aligner step there's actually a fair amount of futzing around you have to do with your reads so for example we could adapter as we also trim primer sequences because in our sequencing experiments we sequence through the primer that doesn't really tell you a lot about the underlying genomic sequence you got to take that out then you align it to you think we use bowtie - it's a pretty next choice for us and then you know you always have to answer the question of what reference do you use that can be important and then if you've ever tried to use tools like the card or gatk or any of you know sort of a lot of these sort of tools that we'll look at a band and tell you about things you know that it can be really fussy about a lot of the characteristics of your reads in the bands you're tagged and whatnot they have to actually do a fair amount of band processing to make it a suitable for these neon schedules we then use freebase to enumerate alleles and we actually run it a couple different times and ultimately one example of why we would do that and then we do variant calling based on the freebase output and for us variant calling is very simple it's just looking at the output of three Bay's and asking is there evidence to support that this patient might carry one of the specific alleles we looking okay so now I'm just going to go through a list of some of the problems or challenges that come up when you're developing one of these assays and when I start with these challenges I'm going to start with a brief slide about the disease and the reason that I'm doing this is to just keep in focus that there's a medical context to this work so a lot of the problems are very specific a lot of the solutions are very practical and ad hoc and so really I want to be clear that the reason that we do this is so that we can address a medical need because if you have some high prevalence diseases in a population you really don't have a good test unless you can conclusively identify carriers so Gaucher's disease is a disease in which fatty substances accumulate in cells and organs and it's autosomal recessive it's caused by a defect in the GBA gene and one of them higher prevalence mutations that causes Gaucher is a type 1 good phase there's a 55 pair deletion in in the gene and this disease has variable severity and onset so the reason I'm telling you about this is because it's it's really challenging and the reason that it's challenging is that there's a pseudo gene just downstream of the GBA gene and that pseudo gene actually carries the 55 piece pair deletion and so if you have a subject with ablution and you have your PCR amplify on that correctly identifies the genetic Legion in the gene it actually will map by your mapper see the pseudo gene because the pseudo gene looks a lot like the gene and so this caused us quite a lot of sorrow and August because this pseudo gene is so similar to GBA pathogenic variant and so here's an example this is a screenshot from a tool called IgG and it's a way of looking at read stats and this is a compressed view so each block represents a different amplicon so in the course of developing a carrier screening penalty typically try a lot of different amplicons with some of these more challenging assays and the thing to notice is that you know you're seeing really homogeneous looking blocks and this isn't a sample that actually carries the the G D Delta P v variant and really what you would hope to see is that there's some indication that a lot of these reads are a little bit disrupted and so you might ask where did those weeds go because you know the amplified is fine well it turns out they align to the pseudo G and this is one of the things that indicates that something funny is going on so here's the pseudo gene these blue lines indicate a difference from the reference and what this means is that the amplicon is actually amplifying the reference deletion but mapping to the pseudo gene and you know this is just sort of like a little challenge that you have to solve in order to correctly characterize this Marian so I'll describe what we did to solve this problem so we looked at the sequence of the pseudo gene and we found one base pair difference actually between the pseudo gene sequence and the actual G D 55 gene and so you know basically the idea is just look at the rates for that single base different than in check for the reference and deletion but the way that we do it is a little bit unusual we're going to do is make a custom chromosome that actually has the sequence of the amount we're looking for one of the other difficulties is that in a multiplex PCR situation some of your applicants will behave very well and some will behave very problematically here's an example of sample from a patient that has the 55 bass pedal lesion and here's one that does not this is actually mapping to the pseudo gene and what you'd really like is that there's some systematic difference in one of these colored blocks is all represent different amp and it turns out that one of the amplicons just for whatever reason ended up working very well so when you have the 55 pace Barrett deletion it looks clearly different than when you down this amplicon is awesome so the approach what we ended up doing was building a custom chromosome for the reads to map to so you know we have the situation where in the healthy state they map one way and in the pathogenic state did map in another way so we just give them a place to land we filter the ban at the end so you leave a file that only has reads from the one amplicon that happened to work and then we focus on this hero nucleotide so what we have observed is that when you have variants exists when you have the deletion on this custom chromosome about 50% of the coverage is going to be from the real Union about 50% it's going to be from the pseudo gene that's going to look like a heterozygous and when the variant is missing then everything comes with the pseudo gene and that's going to look like a home so this is what it looks like when you look at ideally this is the custom chromosome that we just made up here are two samples with the variant you can clearly see this sort of heterozygous looking bang here and here without the variant this is from the pseudo gene and so this is just an example of some of the very practical dirty violent fanatics that you have to make a variant work okay so now I'm going to switch gears and tell you about one more of these sort of practical things this is going to be a story about our informed PDX product so the idea is that there are genes obviously involved in drug metabolism and for some important psychiatric medication there are known variants that make some individuals metabolize these drugs much more quickly and much more slowly than most people and this is actually really important the psychiatrist is trying to adjust your dosage of the medication so the difficulty is it in irrelevant genes we actually have to determine the haplotypes rather than isolated variants so it's not so much that you have one variant in this unit so you have a few and they're you know too far away to be put in the same short read and you have to figure out what is the haplotype status of this subject and you have to do it from isolated individual variant calls and in general you can't but in this case you can so here's the source of our variants we use this list of what are called star allele then a star allele is just a set of variants in one of these drug metabolizing games so this is set to d6 and here's an example to d6 star 1a means that you carry this particular deletion or I'm sorry one thing to notice is in our product we really focused on the star alleles that were relevant for making drug dosage decision and we're really focusing on the major variants for each star allele so you'll notice that some of these are in bold some are just linked to and then some are dark these ones in bold are the ones I think really define the mutation so those are one get to focus on but you also have to account for all the others so what we do is you know we do our normal data analysis where Chloe do the variant calling when we get the list of variants and snips and then the task is to figure out what star alleles are consistent with the snips that we've observed in this subject and then we have to figure out what is the metabolizing status and we do this based on literature and look-up tables so really the challenge is is figuring out what is the star legal status based on these genotypes so for example you might have a subject that carries these alleles in to d6 that might be consistent with these star alleles and depending on which star allele combination you land on you might be an ultra-rapid metabolizer an extensive metabolizers or intermediate or acquire metabolite and one of the interesting things is that there's really a tremendous amount of structure in these yeah please do know that's a really good we um hired someone who knows about this stuff to know about the literature so like this is never us coming up with this knowledge we we go to people who know what they're doing take advantage of yeah yeah so um here's an example of a star little 436 it's that these two variants so it's a PD six thirty years right here but you'll notice that subdue T six star forty one actually has a substantial amount of a black but actually has these two variants but this is actually that the major variant and so you know there's sort of like some sort of nesting or subset relations that 2d6 dark ten here Harry is one of them but has a different one and so actually ascertaining the haplotype can be a little bit tricky when it turns out that there's a fairly simple solution so what I've done is just made a table with all the star alleles per qd6 and the green variants are major and the orange or not major and so what you can do is just enumerate all the possibilities so for example when you have these uh for a subject you'd have these allele statuses and you'd notice that they were consistent with you star allele what we do in practice though is actually just generate all the combinations of all the alleles we look for and again let's keep in mind very simple practical applied solutions these are not elegant methodologies but you know so if you have if you were a star 10 a star 10 a then you would be homozygous for 4180 g2c and you would be homozygous for 100 GeV whereas if you were a star 11 star 14a you'd be home for these two in HEPA d3 and in practice this actually works surprisingly well most of the subjects that we've looked at all neatly into one of these categories they're actually consistent with only one star allele status and not any of the others so here's an example on the top I put the patient genotype red is homozygous and blue his head and on the Rose I'm showing you which statuses are consistent with this combination of genotype and it turns out that in this example there's only one combination that's consistent in that star for our 41 of course this doesn't work all the time so I'm just gonna skip the head but here's an example where you're home for this home for this pet for this and if you look down this table you'll find that there's really no star allele combination it's consistent with these with your genotype so it could be that this subject has a different star allele that we're not looking for it could be you know there are a number of possibilities but in any case this method looks really well it's most of the time it's certainly not although okay this is going to be my last anecdote from the world of carrier screening this is related to a disease called homocysteine area and this is most commonly caused by mutations in the CBS gene and you get a severe buildup of homocysteine in the by any mutations have variable penetrants and this can cause development for the layer intellectual disability myopia and skeletal abnormalities and this is this is a new case and in many ways this is I think my favorite of the three anecdote and going to describe so we had an assay that looked for the variant and we're looking for the snips but we made a very odd observation immediately which is to say that we had 59 positives out of 768 samples in a test set and that's 7% that's vastly too high for the prevalence of the disease and then we got sort of odd allele ratios so we'd either get something that was consistently there but less than 15% or greater than 40% but the weirdest thing about this is that in our amplicon we actually had two reads that overlapped and they both covered the very end but we only saw the variant allele in our one in these cases even though both read one and we to cover the snip that was very weird we'd never seen here's an example so here I'm showing the two leads overlapping this is one amplicon and this is this is the variant right here this is the one that is pathogenic or and you can see here in read one a lot of these reads are carrying it they're also carrying these other variants that are sort of suspicious and this is read too and it looks totally different but these should be from the same molecule so um what do you do in this case well it turns out one of the most effective things you can do is go search the literature and see it people have seen sort of similar things or other things about this particular variant and the story here is sort of pleasing it turns out that in a fairly substantial fraction of a population there's actually an insertion that looks just like what we're testing for and so you know we had one probe over here and we had one probe over here in the normal I'm sorry and in the North this is the wild type probe over here probe over here you know weed one we do they overlap but when there's this novel insertion then read one actually reads into the insertion which actually carries something that looks just like the allele and so it still boggles me that this is as common as it is if this is really the explanation because it seems like this should have a pretty big impact on a medically relevant gene yet nevertheless this actually does seem to be in the population so what we're doing is first of all confirming that this is the right explanation because this is a little bit exotic so happy to sequencing some of these things on in my sake with longer reads or we can actually get through that novel insertion if it actually is there and then the bio informatics solution is you just trim read one so if you don't trust read one and you do trust read to you just trim read one down then your data and album will go away again very ad hoc very unsatisfying but really effective so the conclusions from my carrier screening section or vignette is that it's a little bit different than a lot of other clinical sequencing so really the issue here is that must-have variants are really must have you have to have them in order to have a good test there is no cystic fibrosis carrier screen unless you can get Delta in this world we actually really emphasize established methodologies along with documentation formal validation and quality management so you know this is really more like professional software development been a lot of other bioinformatics stuff it's sort of you know it's very conservative and you really like to be using a method other people have been working on for a long time if at all you can a lot of yeah you actually have a lot of problems that you have to solve in order to make the pipeline work for all the variants you're targeting but the solutions can be ad hoc that's perfectly fine you detect that something that works as long as it's straight or lower you're pretty happy and the solutions that you you adopt will be driven by your assay technology so because you know that you have the two reads and one is causing trouble you just get rid of the one it's got in trouble and you're done and in terms of the bioinformatics efforts that you do when you're developing carrier screens I mean probe design is huge all you have to do a lot of it data analysis tends to be the difficult part because the question is not how do you fix a problem the question is what is actually the problem and so but this read one read two issue you never really would have understood that was going on on the teeth on that paper got a hypothesis and you know can may be addressed and you know we're developing a lot of pipelines all the time and we're it always takes a lot of effort to run research-grade software I mean most of the stuff in next-gen sequencing is written by grad students those stuff yeah this is not a criticism I was a grad student I wrote some trouble for myself but if someone wants to take it and put it into a clinical pipeline you have to really do a lot of testing and quality assurance to make sure that it's going to work and be fit for purpose and then we do a lot of process control which can be challenging with Illumina technology and you know I worked at Illumina but you have to do a lot to make sure that your runs today look a lot like they look six months ago because you want to very stable high-performing solid s and you want to know that the conditions that you established when you validated it or continue to be the conditions when you're actually running okay so that was carrier screening now I'm going to take oh we didn't because we're doing this all in fluid I'm so this is all multi ply PCR not look like running it out on gel health we could have done that yeah yeah absolutely yeah you can also just sequence the library on a my secret like a two by three hundred but yeah you could also just run it on a gel make sure it was longer yeah yeah no please yeah yeah the amplicon was flying boat because the pseudogene looks so much like the one the other one with the tip you pile and you know one of the things that makes this a little more complicated is it these are multiplex PCR so if you have an applicant that's normally one li in the multiplex and then it gets to a different length it'll actually often behave really differently so there's some funny interactions well we I mean we look for it when we can especially for some of the more difficult genes except 2d6 is just a nightmare to design primers for and that's one of the ones that we actually really struggle to you to get in practice what we really focus on is can we validate our assay in samples living to carry the variants and so you know if you hit that target you're done and if you don't hit that target you have to do whatever you can to get that so there's always going to be a little bit enough yeah we haven't looked for it certainly it's an absolutely a concern and when you have a product like this though essentially what you have to do is define its performance characteristics and as long as if it does then you're happy and you have to sort of be careful with looking into things too deeply because then you run into the possibility of incidental findings and finding out things that you don't want to know so we don't necessarily do a lot of investigation of that sort of thing but it is a variable possibility that could cause high jinks in okay so now I'm going to switch gears I'm going to leave all notions of actually helping people behind and I'm going to talk about a very abstract algorithm that I've been working on for a while okay so here's the problem that my algorithm solves find all approximate occurrences of each needle in the haystack so this is an all versus all comparison problem so these are like gonna be 20 murders these are like what I'm calling needles you're looking for needles in a haystack this is the haystack so you want to find all pairs where one sequence is from the needle set and one is from the haystack set and they have it with one mismatch with respect to one another so as conceptually is a lot like read mapping and I'll show you some benchmarking results that sort of compresses to read mapping where the needles are read in the haystack of the genome but you know keep in mind that everything is fixed length yeah and you know when a telegraph but I'm still looking for things to appropriately benchmark this again so if anyone had any ideas I'd be very interested to hear them but I call this algorithm linnaeus because it's you know very heavily oriented towards trees and it efficiently finds all pairs of these these 20 members one from a needle set and one from the haystack settling this match and the way that it works is it builds this index on the needles and it builds the nother index on the haystack these indexes are very fast and efficient to build and then it uses these indexes to guide what subsets of these murders you actually have to compare to one another in order to solve this problem so there's a standard data structure very old tiny very very classic stuff called a try and this is like a tree where each node is associated with its sequence and you know essentially the the deal here is that the the nodes are ordered so they're they're ordered the same in Eddie to branch and so if you know the sequence you can very efficiently compute which leaf it would be associated to and for our purposes we're only going to focus on the leaves they're not going to deal with anything related to these internal node so um you know it's very very straightforward to compute given a sequence which leave it balls into so but as sub I be the base of position I and let t be the lexicographic position of s sub I and let's say that this is just the number of elements so this very simple sum which looks awful here well the PI's have to say that all you have it's like computing it an integer expansion or something that I you know given a lexicographic position of each base you can quickly just compute a number and that's the index of the leaf to which this base would be associated now what I'm going to do is actually switch things out a little bit I'm going to use it very carefully chosen alphabet to construct my try so I'm not going to use DNA sequences I'm gonna use something a little bit different and what I'm gonna use is something called a perfect Hamming code and it's a perfect Hamming code on DNA five mercè so this came from a really nice paper that appeared a few years ago in B and C genomics and these authors were sort of you know working in short read mappers like a lot of people are working on and they want to sort of find faster ways to find seeds and so what they've done is they've presented 64 5 MERS and there are a thousand twenty four or five hours altogether and they have some really surprising properties so every five mer is either one of these code words or it's within Hamming distance one of exactly one of the code words so they sort of partition the space of fibers in a really nice way code word has Hamming distance of one for exactly five five months and I should say that Hamming distance is just the number of mismatches when you line sequences up position by position so there's no notion of insertion or deletion it's just it's just mismatch and these code words are what I'm going to use for the alphabet for the linnaeus tract there's a lot of really interesting stuff in this paper and one of the things that I always kind of liked about it was that it turns out that three times the length of your camera plus one has to be a power of four in order for this to work and so that works for length five like 21 and up but there's not a lot in between so you're sort of restricted in the Kaymer's for which you can derive one of these perfect animations here's a visualization this is just done with grasses but you know essentially it looks kind of like a lot of flowers and this is just sort of a way to sort of visualize what this is doing in the middle is the perfect an encode code word and radiating out are all of the fibers that are associated with this perfect I mean code and if you stared at this image and blew it up you would notice that this sequence for example GA GTT is one mismatch away from this sequence which is the code word GC GTT and it's at least three mismatches away from every single other codeword so what I'm going to do is I'm going to use these code words to build up equivalence classes on longer lengths of sequences and I'm just going to do that by concatenating them so here is a template sequence one perfect counting code word here's another shoji tag and CG AAC here are the five members that they cover or that are associated with them and so think about what happens when you concatenate these two code words each one of these so this Tenma is going to be uniquely close to 256 ten verse because the first five bases is going to be uniquely close to one of the fibers in the second five bases is going to have the same property so you can build up equivalence classes using this perfect anticodon longer sequences as long as you're just concatenated these code words together and a really critical property what's really interesting about this is that the ten rows are going to have a bounded error rate with respect to the template and with that because you know that there's only one mismatch in the first by bases and one in the second five bases all of the ten Murs that are associated with this concatenated pair of code words are going to look a lot like this tender season so this is what the try looks like if you build it on the perfect and encode alphabet so you have five members the branching factor is 64 because there's 64 code words we're only representing the leaves and if you want to represent 20 MERS you have to go out for D and that means you have about 16 million leads so it turns out that it's incredibly fast to compute which leaf is associated with an arbitrary 21 all you have to do is you have a lookup table and five rows and it says here's a 500 what's my perfect Hamming code and you just need to do a couple multiplications and additions and then you get the index of the leaf that you would put that 20 more into it and that's one of the reasons that this approach is fast it's very very fast to build this tri structure so what you need to do is you actually just allocate 16 million lists roughly then for every 20 more in the input sequence you just compute the leaf index and you insert it into that list for the appropriately now this is fast for two reasons first of all it's fast because finding the leaf is fast is just some lookups and some multiplications and additions and there's no sorting we don't care about the order within the leaf you just have to put it into a list so that you you don't have that n log n step so the algorithm I'm just about to describe is an all vs. all search algorithm and it's going to take advantage of a lot of the properties of this tree that come from the path that I have especially chosen alpha and what I'm going to do is build an index on the needles and another on the haystack so let's think about the naive version of this problem so if you're going to you know go through the trouble of making one of these trees and you want to find all pairs of needles that all pairs of Kaymer's from one comparing with here that are similar you know one thing the naive algorithm is that you just iterate for every loop every leaf here then you'd have to compare it to every leaf over here and then for all the sequences in Miss Li you have to compare it to all the sequences in all of these leads so for each leaf in the needle for each leaf in the haystack then for each Kaymer in the needle even for each Paymer in the haystack Li you check them and then you report if they have very close and use them and you know this would certainly take a long time and we can do a lot better but you know you because there's 16 million things here and certain things here you have about 10 to the 14 pairs of leaves to compare and a lot of the Kaymer's with inside them so this is awful so far but you can actually use that bounded error rate property to improve things quite a bit remember that each five more segments of a twenty mark at most one mismatch to the template sequence of the leaf so consider a k-mer X in leaf a and it came or Y and leafy so if the perfect Hamming code code words are different you know that it has to be at least one mismatch in this segment so you don't what this means is that if you're only interested in this camming distance one comparison you don't ever need to look at leaves with more than at least two different code words so you essentially if you're looking for all of the friends of the cameras in this leaf you have to you have to look at all of the leaves that would be computed by taking the same three in one position and just swapping out the other code words for this one because those are all of the places where the friends might look like mismatch and you have to do it for the second and the third and the fourth so this is actually a huge improvement all right so to find the mismatches again all you have to do is look at four blocks at sixty three code words and so that means that you actually for each leaf you only have to look at 253 leads for hamming distance when mismatches to the sequences in that way but you can actually get better by thinking geometrically so this is a metric space and so what that means is that you just have a set and a distance metric and you can commute distances between all of the elements in this set so each code word actually defines what's called a closed ball in this metric space the code word is the center and all of the fibers with Hamming distance one from the points in this closed ball so in the next few slides I'm going to be drawing circles I don't take that too literally but I do mean I can in this topological sense that we're working with closed balls of radius on defined by the codes so there's there's an intuition here that we're going to leverage suppose that we have two closed balls in some metric space the question is can you say anything about the difference that the distances and the points of one ball has compared to the other if you know something about the distances between the center and the answer is absolutely yes your intuition is right if the center's are far apart from one another then the points within the balls must also be far apart from one another so I'm going to use a triangle inequality in a very simple way so the triangle inequality just says that for three points a B and C but the distance between a and C has to be less than or equal to the distance between a and B and B and C and so we're going to use the triangle inequality to exploit this geometrical property that if the center is of two closed balls are far apart then the points within them must also be far apart and if you know about metric spaces and you know about the triangle inequality you know that there's some very subtle and devious applications there are some tricky things you can do to where we're not going to do that I'm actually in the next slide gonna have numbers real numbers concrete numbers that's how simple so um one thing to notice about the code words is that they have to be far apart so you can't have code words that have distance of one or two otherwise you wouldn't have this sort of unambiguous assignment properties so the code words have to have at least distance three and you can show that because the distance between the center's has to be less than or equal to the distance between the center and this point and then the distance between this point with some very very simple algebra you can show that if the Hamming words the code words have distance 3 then the closest that their sequences could be as distance 1 so you'd have to check those but if the words have distance is 4 or 5 then the closest their sequences could be as Hamming distance 2 or 3 and you don't have to check those for these uh off-by-one so we've actually got another improvement so the naive approach is all versus all you could do better by just noting that you have this bounded error rate so that you'd only have to look at a couple hundred but it turns out that if you use the triangle inequality then you only have to look at 30 other code words now I'm sure that there's some really sophisticated way to compute mathematically that a perfect Hamming code five more is hamming distance 3 away from 30 others I don't know how to do that but I can write a for loop and I can it's just a very easy thing to check and it turns out that 30 is the number and so if you are a k-mer in a leaf what this all boils down to is that you only have to check a hundred and twenty one leaves in the other data structure out of sixteen million to find these distance when this matches to the camera sequences you're looking so this is pretty close to the end of my talk I'm just going to present a couple benchmarking results so just a few implementation details i encode 20 more as a 64-bit int and I'm going to do a lot of stuff with bit arithmetic the Hamming distance between encoded in is actually very close to something called a pop count of ax or B under this encoding pop count is a chip level instruction so it can be very very fast I use two bit encoding so there are cases where you could have hamming distance one but a pop comment you you have to do a little bit more work just to make sure that you're not in one of those cases but um you know it turns out that there's some pretty quick ways to do that also using bit arithmetic and if you're into bitter with mythical recommend this book called hackers delight because it's just like a laundry list of crazy cool tricks you can do with bidding coding this is all implemented in C++ and because this was actually work that I did when I was at Illumina it took me a while to get them to agree to let me to release it so I'm going to release it pretty quickly under an Illumina open source license on github I'm still sort of working through the last details of that so if anyone is interested or wants to check it out let me know and I can give you access but it'll be open pretty soon so one of the difficulties is understanding what programs I can use to compare to this because I don't really know of other programs that solve a specific problem so you know putting on my bioinformatician hat one thing that's you know pretty obvious is that you can use a short read mapper to solve this problem and this is actually fairly similar to the sort of thing that bowtie one was actually designed to address back when weeds were very very short so bowtie one is maybe not such a stretch for this particular thing but what you'd do is you'd build an index on the haystack and you'd run the mapper on the needle and you'd run the mapper in such a way that it would get all of the twenty murders the timing distance at most one it's a bow tie one it's actually very good about setting these sort of things you can do it with bowtie too but it's a little bit harder and then there's another map recalled way xerus three that I used to compare at the suggestion of some of my collaborators and you know this one is nice because there's no indexing stuff it's just sort of a scanning method and this is work in progress I'm still working with some of my friends to sort of figure out you know what are tools that are actually closer to this problem so I can get a nicer benchmark but you know in order to benchmark this I just generated some some k-mers and I made sure that that there was varying levels of overlap because one of my concerns was that you know it could be that there's some performance penalty if there are a lot of things that match as opposed to very so here at 10% overlap which means that 10% of the sequences agree our wall clock times for the various methods so I looked at ten thousand needles five hundred thousand needles and ten million needles and in a hundred thousand haystacks a million ten million and you know what you can see is that if you don't have that many needles or that many haystacks then these other tools do really well but actually as you scale up as you have more and more comparisons linnaeus actually starts doing better and better and you know I like to think that this is because of the intrinsic efficiency of the algorithm for this particular thing but it could also just be because the aligner developers haven't spent a lot of time making the index building step fast because in order to use the aligners you have to build the index and you know in most applications you only do that once so who cares if it takes a little bit longer than it needs to as they really spend a lot of time making sure the mapping stuff is really really fast this is not a completely fair comparison but I think you can draw some idea about scaling behavior and the numbers look very very similar there's a high degree of overlap so there's lots and lots of magic okay so this is a the end of my table use my conclusions on the very abstract and not helping people part so this try data structure I think is really interesting it leads to this really efficient all versus all comparison that but it has some funny restrictions although gamers have to be the same length and that link has to be a multiple of that so I think that this idea of partitioning these cameras into equal and disjoint equivalence classes has some really nice properties there are some other applications that I'm still sort of working through but you know what's sort of nice about this data structures that you derived these templates that are associated with a lot of sequence and the templates looking very very similar to all your sequences and then another thing that's nice is there's really no sorting this is an all versus all method intrinsically they don't really care about the order of the needles in each of the leads and it's one of the reasons just an insertion it's really one thing I should point out is that I'm still working on the multi-threaded implementation so when I run all of these the benchmarks I showed were in single threaded mode when I actually do multi threaded mode the no linnaeus is no longer as fast and I think that or it's my explanation at the moment is that these aligner authors you know spend a lot of time making sure that Mel high threading is very good I've spent much less time making sure the mole paper I think is very good so I think that the single threaded mode is a fair comparison but still if you're actually trying to solve the problem you know the aligners are still very good tools if you have a big old computer and you know just as a programming nerd I think that there's a lot that I can do to include the cache locality of the innermost loop I'm sort of looking into that but you know not not all that intensively because this is more of just like a side project so I don't know if the code is going to change all that much what I'm going to put it out there and if people like so I'd like to finish with some acknowledgments so the linnaeus work I did it aluminum and so people that were helpful there were thinning crudely I can grant Ziegler and the new purge entity on the carrier screening stuff I've worked very closely with some very good scientists Coolidge and Adele and Amanda Vasquez and a lot of the biomedics work in the carrier screening stuff a lot of these innovative solutions were done by Hon Wong who actually came here and he was just fantastic and Julie Kim also did a lot of design and she was also actually that's my talk thank you and happy to take questions boy that's a good question um typically the number of bar codes that you have is fairly modest in comparison to the regime's of this algorithm works well so if you have ten thousand bar codes it's not so hard to match them up if you had 100 million bar codes and a highly eric rowan sequencer then this might be more of a realistic application i guess i'm just sort of made this completely off base but i'm wondering like for this linnaeus how that would work and sort of a pooled microbiome type set for you trying to piece out the number of oh to use and you have these counts and these individual cameras that you've been used and it seems like that might be an interesting application for that yeah I think that's right I mean one of the things about this algorithm is you know I essentially I had an idea for a data structure and no biological applications whatsoever so metagenomic seems like it might be a natural that if you sort of want to take a sequence and ask what are the similarities between this and another I mean you could probably also do it again with reading that thing but you know this might work really fairly well if there were some cryptic sequences that you really didn't know 2d6 and the other pharmacogenomic analysis I don't realize these were acquired a whole haplotype to be recognized how do you sit what do you know about the sequences across species we know about the evolution to these haplotypes sort of selective vectors where would we start I mean how much data is there on these low side across distant species I have no idea it's a really good question I think that a lot of these haplotypes are just discovered by sequencing different populations so I think that a lot of them are probably fairly specific to human populations in lineage that said there might be certain functional area that is similar but I'm honestly not sure so if you look at these very low sign in different human populations from the haplotypes in a lineage specific or do you find them in different ethnic populations geographic populations I'm honestly not sure because that we probably need to know for the diagnostic purposes yeah I mean we're really using it to help guide psychiatrists with dosing and so we sort of very we focus very closely on the functional aspect but there is a really interesting population so towards the end of the first part of your talk you mentioned that you have to maintain stability in light of all the variations that Illumina likes to throw at you and a lot of times they change chemistry and all these other variables and that induces artifacts in variant calling it needs to be noticed at the earlier chemistry so can you tell us a little bit about how you kind of assess those systemic variations and the technology and what you do to mitigate the effects of it yeah that's a really good question so one of our advantages is that we are asking very specific questions at the genome and so for example if you go from a PCR library prep to a PCR free library prep all of a sudden you have better coverage in GC agents and your power to detect variants that it was much improved we're doing targeted sequencing and so typically the effect on our amplification and I guess sequencing capacity it is much more modest there so in practice when we get a new lot of reagents where we consider switching to a different chemistry or even when we get a new sequencer and qualify it we do some validation experiments on a set of samples with known variants and we make sure that we can identify those variants and if we correctly identify the variants it's okay if there actually are some small process drift issues because really what we focus on is that is you know the power to detect those variants the things that we're sort of looking for though are drifts in the proportion of samples of specific alleles so you know you want your allele frequency to be really stable over time because the assumptions with population is not changing all that so we look at that we also look sort of a cluster density for example and you want to make sure that just the protocols are fairly stable with respect to cluster density but if you give me okay you certainly might see a jump you know and you know you get a new enzyme so you never know if the enzyme is going to have the same activity or concentration so again you know you get a new lot of that you just run a validation and make sure that things are behaving as expected you also do analyses for Furby to her to new you know the breast cancer subtype which is highly overexpression of certain category of breast cancers no we do carrier screening for assessment of reproductive health risk and so we really try to limit ourselves to high prevalence conditions with very well understood variants that you know essentially out of someone with of diseases so we're very focused in terms of what we're testing but you know I asked is a very interesting problem these are natural amplicons you're talking about PCR and Perconte yeah so around that particular gene her2/neu or b2 chromosome 17 to 12 they're about 23 genes between the center and patients who have over expression at an application that region sometimes just over regulation over expression biases regulation and they can have one or several or many of those adjacent genes also over expressed I don't know if this comes up in any other assays that you're working on but it seems to me that it's a diagnostic clue if there might be golden bat variability to explain why some patients with the over expression of a particular gene have more extreme or less extreme phenotype for a more predictable and less predictable response to specific targeted therapy and we've done that but it seems logical you know they're also interested in splice variants you'd find splice isoforms of this gene and other genes fever don't add up in your amplification of these markers no we wants to focus on DNA this is purely an acute cDNA yeah it's strictly DNA and not only that but we're very focused in terms of what we look for because there's always a risk of incidental findings and so if you look broadly outside of the specific variants you're looking for you might find something with the health impact that you didn't have to report to a patient so we focus very carefully on only looking for the specific variants that are on our test menu just in DNA and you know really the reason is just to avoid any issues regarding incidental findings all right thanks guys [Music]