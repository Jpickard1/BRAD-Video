[
    {
        "start": 2.55,
        "text": "[Music] sure it's a welcome everyone we're gonna get started this is the tools and technology seminar series I see a few new faces which is awesome so I'm for those of you not been here before this is a weekly seminar series at this time and it's a little different than standard seminar series as the title suggests it focuses on tools technologies methodologies sometimes these are you know still in development other times they are you know recently published or recently completed and or maybe you don't have anything to do with the actual creation of the tool but it's just a resource that someone uses and likes and thinks would be of interest to others so there was a sign-in sheet that I just sent around hopefully I can circulate and if everyone can sign in that would be great it just gives us information to help with the pizza and say I'm gonna turn this over to our speaker today we have Kirby sheddin who "
    },
    {
        "start": 64.439,
        "text": "is a professor of both statistics and biostatistics and also director of C scar if you're not familiar see scar it now stands for consulting for stats or statistics computing and analytics research Thanks managed to change the name without changing maximum let's see do not be in front up there okay thanks everyone for coming so it's lunch it's lunchtime and people at least who are here are eating pizza so I hope this can be informal and I know I have more material here than I can possibly cover so feel free to redirect me at some point if you feel like one aspect is more interesting or another aspect is less interesting than just tell me so and I'll be happy to try to adapt so this is this doesn't talk about tools and technology it's actually a reworked version of a more substantive talk on on the application area so instead I decided to take that talk or to meet the needs of this seminar series I decided "
    },
    {
        "start": 125.28,
        "text": "to take that talk and restructure it with a little bit more emphasis on that on the software and the techniques but it's still presented so a mix of a Tech Talk and a and a case study basically what I'm getting at here is an overview of some as experience as I've had over the last year so evaluating and implementing tools for analysis of large-scale and heterogeneous data which is a hot topic all over the place right now and I'm just one person trying to do something a little bit special and unique and also trying to meet the needs of my collaborators the tools and technologies I'll mainly be focusing on today were developed specifically to meet the needs of two collaborative projects I have on my plate right now one is more relevant over here it's an analysis of medical claims data in the context of health services research specifically for cirrhosis although we also have things going on now with heart heart failure in cancer and the other project which I'll just barely allude to at one point is in "
    },
    {
        "start": 185.8,
        "text": "the transportation research looking at data coming off of sensor equipped vehicles we're actually both might have sponsored projects so they have that that in common so the way I want to organize this and like I said I'm happy to move quickly through parts that are either familiar or like less interest to this particular group is to start with a little bit of background about the motivating application in claimant's data analysis and health services research and then move on to the two technical topics that we focused on one is data layout optimization to facilitate person centered analysis of large claims datasets specifically in the context of the cirrhosis study and then the next would be ICS spelling error there but the development of some new tools for streaming data processing to facilitate analysis of big data sets that can't fit into memory and some data management and statistical tools that we've developed to make that feasible and like I said all of this is set in the context of one case study in particular looking at trends in "
    },
    {
        "start": 247.0,
        "text": "cirrhosis prevalence and the cost burden of cirrhosis so this was done as part of a collaboration with Jessica mallinger is a faculty member in gastroenterology and she came in to see scar actually about three years ago and asked if anyone there could help her do some analysis of this type of data and it's taken it was supposed to take three months and it took three but and it led to development of one bunch of tools I'll say a little bit about why that is there's there's some reasons for it when I want to get there we'll get there okay so let me give you a little bit of background about claims data and EHR because that's again the application setting here so claims data in particular it's sort of the poor sibling of the more hot topic which is these days which is EHR analysis so claims data are electronic records that are collected in the process of healthcare delivery that are primarily intended to facilitate financial side of the operation so um they basically record all of the procedures and services that are that are provided to patients and they are then used to "
    },
    {
        "start": 309.639,
        "text": "collect money from that from the payers so the flow the flow of information here is typically that you go to the to see your doctor you go to a provider organization a clinic or a hospital and then the provider organization uses claims to document what was done the claims make their way back to payers which is often health insurers and then the payers supposedly pay and then that's how the providers get the spectacled universe for what they do claims have been around for quite a while for many years people were analyzing claims data we have a couple people at CU scar we spend a lot of time on claims data especially out of the VA and CMS and it was never like really a high-profile topic until the until the health policy and the financial side of the health system became a really important topic in the last ten years or so in this area has gotten a lot more traction it's always been a challenging computational and statistical task because there are lots of sources of measurement error and bias in claims data and they've always been large so it's always been challenging computationally and they've been "
    },
    {
        "start": 371.38,
        "text": "electronic it's basically the beginning the beginning of modern history at least and that makes them especially of interest for quantitative research they they cover large broad populations and so they're especially useful if you want to do research that can project out to a well-defined population like in the case of the cirrhosis project I'll be talking about today the so called ESI or employer-sponsored insurance population which is more than half of the non elderly us populations and people who get their health care paid or through a employer-sponsored insurance plan because the Behcet's are large you can basically use weights to project them up and make statements about this population rather than just statements about a convenient sample which is something that we often aspire to in statistics that don't off always achieve and an interesting thing about these data sets is although from a finance point of view it's primarily of interest that they capture the actual tangible services and procedures that are that are provided we out we actually don't always care about those services and procedures so much but because of "
    },
    {
        "start": 431.71,
        "text": "the way the charging model works is that the provider organizations get reimbursed at a higher rate for treating complex cases so therefore a bunch of contextual information about the patient's is also entered into the claims data so you get information about symptoms comorbidities and other health conditions that the patients have which ultimately influence the reimbursement rate and so we can use that to do things like identify when people first acquire a condition or disease and if it's a chronic disease we can use that to monitor the progress of the person as they usually progress through a sequence of ever-increasing disease stages so that's what I'm referring to there as that happens the contextual information so the contracts out a little bit with electronic health records which like I said is a very hot topic right now - health records unlike claims are primarily intended to facilitate care rather than to facilitate the financial side of the operation they for a long time we're primarily on paper rather than digital although recently they've "
    },
    {
        "start": 492.22,
        "text": "heavily moved in the direction of being digital and unlike unlike claims that focuses as much if not more on the sort of overall condition of a patient rather than generally documenting what billable actions were performed and they often more more heterogeneous in terms of the type of data they could they contain there usually would be a mix of coded data and unstructured text so clinician notes is the canonical part of a health of an EHR that people are struggling with nowadays you know I contrast that to claims data sets which are usually coded or in the case of the data we referred to today almost entirely code consists of coded data so we've got fields that have a defined set of values that they can hold and so we're talking about processing these these them coded values I think on the next slide I have some examples of what I mean by that if that's not clear yeah I do so they well-known type of coded information that you encounter in claims data set for health records data set would be the so called ICD code I'm sure "
    },
    {
        "start": 553.96,
        "text": "everyone's heard of these before icd-9 now pretty much defunct and things have moved on to icd-10 but over the timespan that we're dealing with for the cirrhosis project we have mostly icd-9 data and then some icd-10 data crept in at the end so the ICD system I won't say a lot about it here I'm not I'm not an expert but it's a systematic way to code diseases disorders injuries and other health conditions and I think there's there's on the orders of many thousands maybe up to around 10,000 or so distinct codes although you don't counter and Contra many of them very often and so the idea is you have in any given claim a number of fields that can that the physician or the provider can fill in to document what what conditions and comorbidities and so on that the patient had and these would take the form of I see the ICD type codes those are for like I said symptoms diseases disorders and so on and then in terms of the procedures what was actually done we have other coding schemes like the so called CPT and HCPCS codes for example I've just got the numbers up here that "
    },
    {
        "start": 614.65,
        "text": "are relevant for the for the cirrhosis project which is about livers of course a four seven one three five is the CPT code for a liver transplant and then there's another whole set of issues relating to drugs so there are various ways of referring to encoding drugs I'm not going to talk about drugs today it also I'll gloss over that here but we do work with those codes a lot in other and other projects okay in terms of working with this data some of the challenges we encounter as I mentioned before there's issues with coding bias and errors in a provider can simply enter the wrong code these are people who are have a lot going on the physicians and nurses and so on who are entering the codes are documented providing the raw documentation that will ultimately turn into codes so they may just make a mistake and enter the wrong code but more more importantly there can be various biases because some of these codes lead to higher reimbursement rates if the code leads to a patient being categorized as complex there's a lot of pressure not to not to miss the state or to live per se but to certainly to always enter codes that "
    },
    {
        "start": 676.69,
        "text": "would ever be appropriate if they like lead to a higher reimbursement rate for identifying a complex patient and like I said before procedures are generally well defined in well code but what we're more interested in are that are the health conditions outcomes symptoms and so called contextual information and those are not always so well coded there can be many codes that refer to the same thing we're almost the same thing and we have to deal with that in the analysis I'll say a little bit more about that later and also really important for us as analysts if that's what you identify as the data are not always very easy to get although that's at least at um that's improved a lot in the last year and I'll say why that is in a minute you can't just call up an insurance company and expect them to share their data with you so I'll tell you how that works in a minute but the data alright someone has to pay quite a bit of money for for researchers to get access to this data there are some privacy concerns although they're not necessarily as dominant here as they are in the country and some other contexts because the data is I obtained them as any analyst would obtain them "
    },
    {
        "start": 738.04,
        "text": "have been thoroughly scrubbed so there's some disadvantages and challenges and then on the up side a big advantage is this that essentially every time anyone interacts with the healthcare system it's gonna produce a claim it's almost impossible to have any kind of formal interaction with the healthcare system and have it not lead to a claim so it's very comprehensive in that sense and as long as you can get your hands on the data you can get very very deep and broad portrait of the overall healthcare activity and a lot in large populations ok so moving now a little bit more to the technical side of things the major challenges for people who actually have to get their hands dirty with this type of data and do data processing data management and data analysis a little bit in my somewhat limited experience I've really only been working in this area for about three years the typical workflow is that based on the analysis plan and the research goals of a project you start off by needing to select subsets of people who are wood medium cases so these are people who have the "
    },
    {
        "start": 798.45,
        "text": "condition of interest and that is not always a simple query can often be a fairly complex queries often not just do you have this code or not that defines whether or not say you have cirrhosis we had to use a whole combination of factors to determine who really has the condition and exactly the way we wanted to define it and so but the fact is that the cases for most diseases unless you're talking about something that's extremely prevalent for most diseases you're talking about a small subset of the population so on the one hand when you hear that claims datasets are very large so I'm a nerd I'll tell you that we're dealing with many billions of records in the case of the claims data that I have access to here at UM things will very rapidly reduce in size when you select your cases because the cases like cirrhosis is roughly 0.27 or so percent of the of the non-elderly population so you're talking about point two seven percent of ten billion records rather than something on that order itself however like I've indicated already at "
    },
    {
        "start": 858.88,
        "text": "least in terms of the people I'm working with there's a major emphasis on using this data to project up to large populations and so that means you're using survey style statistics instead of classical sort of convenience that style convenience sample types of statistical methods and that means you can't simply ignore the rest of the data so you actually have to aggregate some characteristics of the non cases to properly adjust versions biases in the convenient sample that we have relative to the population that we're targeting so it's not simply a matter of immediately reducing the data down to a quarter percent of the total we actually have to engage with the entirety of the data at least at some level and in the really detailed analysis of course is restricted to the cases but we're not simply ignoring the doc cases these are also almost inevitably longitudinal style analyses which makes the research much more relevant and interesting scientifically but also makes the analysis a lot more complex so what we almost always end up doing is producing from the raw claims data what what we call time logs so time log would be "
    },
    {
        "start": 919.45,
        "text": "basically like a longitudinal portrait of each subject from the point when you can first identify them as a case to the end of the line whether they died or whether they drop off the insurance rolls or whether they just disappear and that could lead that could contain hundreds if even in some cases thousands of events that would take place along that along that process as they every time they go in to the have enough to have a check-up there be additional codes entered that wouldn't give you more information about where they put their statuses at that point they get drugs filled they go and have tests and the lab test results come back even administrative information about like how their insurance plan coverage policy changes and so on so again even if you're talking about a so-called modest sample size in terms of say hundreds of thousands or up to a million people who have the disease of interest and you're evil easily have tens to hundreds if not thousands of records for each of those people and that's on the data processing side and then the final step of course for those of us who are in the business "
    },
    {
        "start": 979.99,
        "text": "of doing statistical analysis we want to apply ok I advanced it may not be like the most cutting-edge statistical methods out there but they're advanced in the sense of the things you wouldn't normally learn say in an undergraduate statistic class and have on a laundry list of some of the techniques that we commonly use and this type of analysis there i won't i won't even read it but um certainly when you talk about data preparation and computation it's not enough that you just prepare the data to the point where you can do something like a t-test you know the types of statistical methods we have to do it's much much beyond that okay when i say a little bit about the data and this is both information and also advertisement to some degree because it deals with some data that is actually available here at michigan and and i and other people would like to see more people using so in the US as a whole there are too large providers of commercial claims datasets that are used for research analysis and those are the proven and the optima insight data system many of you probably heard about them insight is quite well-known driven is a little bit less well known these are large claims "
    },
    {
        "start": 1041.13,
        "text": "data sets that cover large fractions of the CSI population the employer-sponsored insurance population in addition to this you can also look at claims data from non non ESI population such as we have a lot of people at um to look at VA data claims data from the Veterans Administration health system and from CMS the Center for Medicare and Medicaid Services which would be Medicaid and Medicare but that's not what I have experience with so back on the ESI side we have these large data providers these are companies that sell data and you if you pay them quite a bit of money on the orders of $50,000 or so you can get access to the data for research purposes and ihb is a unit at UM the Institute for Health Policy innovation has several years ago licensed well for many years actually maintained licenses to these two data sets and the licenses are actually actually covered as I'm told the whole U of M so in theory anyone who's qualified and has a legitimate project can and always has been able to obtain these data for their research purposes however "
    },
    {
        "start": 1101.659,
        "text": "up until about six months ago the data were locked away on some med school servers and it was technically very difficult to actually access that he would have to minimum pay pretty high hourly fees for AI HPI analysts to pull data for you because they wouldn't they wouldn't let just take the whole data and pull your son set yourself they had to pull the subsets for you and then they would potentially fork over the subset so you weren't paying for the data but you would still be paying for the analyst time to to pull the subsets that you were interested in working with so the new development from about six months ago was as part of the data science initiative which many of you have I'm sure heard of and Brian atheists very involved with this of course we were able to convince a HPI and they've checked that this was compatible with their licensing terms to put the complete data for both proven and optimum site on turbo which many of you know is the large array of disks underlying flex if you use flex and you you have data that's more or less static data you don't need to constantly updating it may well be on turbo so the data science initiative is paying for "
    },
    {
        "start": 1162.799,
        "text": "the storage cost to have both truven and optimum site on turbo for at least the next couple of years it's a long long term we'll see what happens but for at least a couple next couple of years that's that's what's happening so these data suddenly became a lot more available you don't have to burrow through the med schools firewall or get the ihb on us to pull pull data for you with the right permissions is not just there for the taking but with the right permissions you have to fill out some forms and talk to some people at HDI and get them to approve your project but they're generally pretty pretty pretty accommodating about that then you can actually you could get a correct access to data itself or this hopefully doesn't sound like too much of an advertisement that people at cease Carl's all the data for you and that's where that was the sort of incentive and motivation for us to look at better ways to interact with this data not quite next I'll get there a minute let me just say a little bit about just what market scan is so market scan it's actually easier to understand Optum insight the other of these two datasets that I work with less open insight is "
    },
    {
        "start": 1223.4,
        "text": "simply data dump from this one private insurer called UnitedHealth which is a very large private insurer they essentially take all the data they very rigorously anonymize it there's all kinds of issues there that I won't get into today and make a huge fraction of that information available to researchers and that's basically what Optum insight is driven is something market scan which is a product of this company called truven which was later bought by Thomson Reuters which was later bought by IBM is something very different and more interesting so truven is sort of the middle link between large companies and insurance companies like if GM once wants to provide employer-sponsored insurance to their employees rather than going directly to insurers and negotiating the terms of an agreement they might contract with truven who would then assess GM's needs and look at the marketplace and try to find a good match and then beyond that point shriven would continue to serve as the middle link in the sense that all of the claims would flow from the provider organizations through truven servers to the to the primary insurers and back against and Truman's job is actually to "
    },
    {
        "start": 1283.52,
        "text": "to record and aggregate and analyze those claims to make sure that GM continues to get a good deal for their for their money so that's their business and that's part of that business is sort of a side a small side business they have they can take all that data that they collect which is covers many employers and many insurers unlike UnitedHealth which is it's one insurer and they can again anonymize it and sort of rectify the data put it into a common scheme and then and then sell that and that's basically what market scan is there's a lot more to it than that what I just said but that's basically at a high level 30,000 feet that's what what markets get is so it's interesting to researchers especially those researchers who want to be population projections because in theory at least one would hope that if you aggregate data from many different companies and many different insurance you're getting a more a broader representation maybe a more representative picture of the ESI population than you are through UnitedHealth which is this one one insure so that of course there are strengths and limitations in terms of what each of these two datasets contain markets can for instance doesn't contain data about race and ethnicities that "
    },
    {
        "start": 1343.75,
        "text": "people are interested in you know that that side of the more social economic side of things they go to opt-in because optim does contain more detailed information about race and ethnicity but if you're interested in the claims themselves I think markets can and optim-r very comparable to each other so I think I've said everything there that I need to say Oh in people's a lot of times there's multiple plans people can choose from lawyer they like the people changed insurance that's a very very complicated question there is some clan information if you're interested more in the economic side of things like if you want to look at do people so some people have this is the just the tip of the iceberg on that that I've personally been involved with this if you want to look at in the past not everyone had prescription drug coverage so you may have had coverage for your office visits and procedures and hospitalizations and so on but you actually had to pay for all of your prescription drugs to yourself and other people did it so there's a flag in there saying whether you did or you did it long story short they have to simplify and rectify this stuff probably for just to keep everyone saying and probably for "
    },
    {
        "start": 1403.95,
        "text": "to prevent the identification so they have to approximate and simplify in reality it's not a black and white thing whether you have prescription drug coverage or not there's all kinds of nuances about which prescription drugs may be covered so they simplify and approximate and give you sort of a rectified data set that in some sense covers the all these different plans which in reality are different from each other in all kinds of complicated ways in the issue of people dropping off and rejoining is another complexity they try to when people drop off and rejoin like if you change job they try to link through but they fully acknowledge that they miss a lot of people many people will reenter as a new person when they when they come back the second or third time UnitedHealth an optimum are supposedly better at that because they're they're that they're the the insured themselves they're not missing a third link outside of the system so they can use more information to do the linking so these are issues with claims data these are issues with these commercial claims data sets that have been scrubbed and made available to researchers they're not it's not immediately relevant for my for the "
    },
    {
        "start": 1465.04,
        "text": "technology side of what I'm talking about today so here's a here's a concrete view of what markets can is the market scan is delivered to in the form of a bunch of SAS files that's what that's an answer to just earlier question not as a relational database and specifically what you get is you get about eight or ten different types of files covering eight or ten different types of data so you get information about enrollment information about inpatient visits hospitalizations information about outpatient visits that's the big one that's the one that most people are focused on facility and service charges drugs and I didn't the labs are very complicated I haven't even looked at the lab data but there is lab data and that has that's a big big as well I didn't even clear included here I'll tired at least without the lab data there's something like 14 billion records and all the files combined that's aggregating seven years that we have data for there I think they're thinking about getting 2016 but they don't have it yet that would and the majority of again is either this enrollment information which attempts to get at some of the complexity that Jeff was just asking about so the complexity of "
    },
    {
        "start": 1526.039,
        "text": "when people are covered and exactly what kind of coverage they have which is relevant for some people would often not that relevant for many research questions and then what's almost always relevant for everyone is visits oh oh oh here at the Oh file information which documents every single visit that everyone in the in the data set ever has with a with a provider so this is again flirting bein records three point four terabytes of SAS files and I said a little bit earlier about the license and the opportunity now for um researchers to get access to this data so I'm hesitant never to say that any data is big because there's always someone will come along and say that my data are bigger I'm sure there's people on campus doing I know people doing astronomy in material science research that generated a couple orders of magnitude more than this amount of data but within this within this scope it's it it's decently large data okay so what's this all about in terms of the tools and technology dimension of this well as I mentioned earlier there's actually been a small community of people on campus doing claims data analysis for many years and they kind of flew under the radar and "
    },
    {
        "start": 1587.269,
        "text": "the invariably and always use SAS so the files are delivered to you in SAS format and all the analysts use SAS format even some of the national standards on how to do things are actually explicitly defined through SAS code rather than through abstractions of what what intended to be done so it's starting to look a little bit creaky to be approaching things that way at this point in time there's nothing particularly wrong with sass and I'm not here to to trash us but it would be nice to have options and so the main reason I was interested in getting involved with this rather than trying to pass it off on to someone else who works at the consulting Center was because I wanted to undertake the challenge of performing a large-scale claims analysis without sass and I was willing to do whatever it took to make that happen and well actually long story short that's why I took three years since to have three months because if I had just simply used as I can sort of program it says I probably could have done it in three months I could have borrowed a lot of code that's out there for doing things that people routinely do but I just didn't want to do that I wanted to see if we could have a path forward using modern languages modern computing infrastructure and simply "
    },
    {
        "start": 1649.729,
        "text": "avoid using sass with obviously with a priority or preference for open source where possible so when I first started looking into this and this was a couple of years ago I talked to people who are big in this sort of HPC big data area and you of course hear people telling you that of course there's one and only one way to do this you've got to use spark other people will tell you there's one and only one way to do this you got to use Apollo or Hadoop everyone has their tool of choice and the but they were really thinking was that you would use one of these data science II type of tools like spark or Hadoop to do the data management to construct these time log files like I referred to earlier and then once you've got to distill down to something a little bit more manageable maybe a couple of million rows then you can easily load it into our Python or state or whatever and do your statistical analysis in it that would that would be a very reasonable way to proceed but instead I took I went a different route so what I wanted to do was to see there's a language called NGO that I've been using some other projects some sequence analysis projects and I kind of liked it so I wanted to see if we couldn't come up with an alternative you approach here using go which is a "
    },
    {
        "start": 1710.33,
        "text": "very good language for maximally exploiting the capabilities of a single server so it's not like spar could do but Impala which I was very much about chaining together a bunch of servers and a network and a sort of cloud distributed type of approach go can be used in that way but it's more in my in my way of using it it's more about getting the most out of a single note so nothing here in my opinion should really need a hundred notes to be to be accomplished in a reasonable amount of time so what I wanted to do is to see if we couldn't develop some tools and go to make this work on one note so I don't want this to turn into an advertisement go and I could have used C++ or rust or some other languages that you may or may not have heard of guys just one choice that I happen to go with because I was already sort of familiar with it but let me spend about 30 seconds at most talking about go and then I want to move on from that so go as the language that was developed within Google to solve problems that they had on a number of fronts including managing data centers but also manipulating large data sets so if it "
    },
    {
        "start": 1771.12,
        "text": "can manipulate Google size data sets then maybe it is actually relevant here but no no one else is using it it's relatively new it they first started to talk about it around 2007 but it became a real thing around 2012 you can go to youtube and watch some nice talks by the guy Rob Pike who who was the main person behind it at Google who's also a big figure in the history of computing he was around when C was developed he was Robin UNIX was developed he's the main figure behind unicode so he certainly has some credibility at least in this area the reason that I'm interested in go and this actually connects a little bit with something I was chatting with Jeff about earlier is that goes as suppose alaykum it's a general-purpose language but it's at a lower level than what you typically think of it's general-purpose languages so it's a compiled statically typed language unlike scripting languages like Python and Perl and R and so on which are very dynamic so in that sense it kind of splits the difference between C and Python and potentially obviates the need for the scripting language is to always have like a two-prong strategy in which you have a bunch of C code and "
    },
    {
        "start": 1831.809,
        "text": "then some kind of linkage between the C code and then you have a scripting language up on top so we were just talking about that scythe on in the cases icon or our cpp in the case of our MATLAB has a way of doing this as well so there was a low enough language that you can potentially implement your your intensive type algorithms in go itself but at the same time it's high enough that you can use it also for all of your scripting purposes and so whether you think that's a good thing or not that's a matter of judgment perhaps but I've personally been frustrated with the way scythe on creates sort of mangled C and obfuscates obfuscates the underlying structure of what you're doing so I was sort of interested in the possibility of working around that I actually tried to do this by this poll project and Python to begin with in it you'd just sort of ground to a halt crunching through the three point four terabytes of SAS files okay so let me get on all in specific now in terms of what we have done oh this is just over the last year or so so there's three ways with three tasks we have sort of embarked upon here to make "
    },
    {
        "start": 1892.44,
        "text": "these projects capable of being done entirely in this in this setting one has to do with data layout optimization that I'll focus more on that one has to do with data stream processing and then the last one is about the actual statistical methods themselves and I just want to emphasize it you know everything we've done here I would consider to be a sort of a prototype or like an alpha or pre alpha level of completion in terms of the utility of the code for other projects you know caveat emptor it's it is pretty heavily tested and we've used it in these projects and we've double-checked some of the results against other systems where where it's practical to do so so I'm not lacking confidence about the accuracy of the scientific results we produced it produced using these tools so far but I'm not making any claims about their their ability to solve your problems it's more of an experiment I'm not even I'm honestly not even hundred percent sure if we're going to yeah invest time and supporting these moving forward or if this is going to be learning experience and then we'll take something away from it and move on in a different direction okay so task one like I said data layout "
    },
    {
        "start": 1953.94,
        "text": "optimization so this is about getting the data out of these giant sass files and into some sort of format that you can manipulate more conveniently either in potentially in sass but more to the point outside of sass and this is not go specific what we're going to do here is we want to produce some some data files that are that are sitting on the disk in a documented format and then could be read potentially into Python or R or any other language using standards for you know it's standards for data structures that are in many ways language language agnostic so the way we approach this was heavily inspired by this project in the Python universe called defaults which wasn't very well known but it's somewhat well known basically because does a number of things but what it does that was appealing to me and relevant here is that it defines a disk layout for NATO formatted data which essentially takes a large datasets colonizes them so organizes them by columns and then dumps them into separate files by column and there's more to it than that I'll get to in a minute in such a way that you can "
    },
    {
        "start": 2014.299,
        "text": "then scan through the data very efficiently so specifically what we do here is we have again column wise storage we want to pre sort the data to enable high levels of run length compression and to make it fast to read through the data and to do a so-called full table scan so most of the analytics I need to do on these claims data sets is not a like a querying sort of thing where you would just cherry-pick tiny bits of data you pretty much need to run through the whole data set to accumulate population statistics that show up like saying the denominator of a weighted mean or something like that so I basically need to plan for doing full table scans it's not like one of these things where you'll never need to read the data from top to bottom the other tooth points here will probably be more clear when I say in more specific terms what we did so what this be called since file layout actually looks like in practice and this was adapted to the market scan data because the market scan data has this feature that the data is organized by types of these AOSS IDL and so on files they're not really capable "
    },
    {
        "start": 2075.74,
        "text": "of being merged directly because a row and one file corresponds to something different from a row in another file so we have to actually have seven sort of parallel data sets and then sort of lightly merge them on the fly when we need to so what actually ends up happening here is we do what people commonly call bucketing which is we partition the data set into in our case 100 buckets based on subject ID so you end up in some bucket based on your ID and all of your data will end up in that bucket number so for example if I get if I get bucketed to bucket 47 for my a files than my I information my ol information is an F Walsall is set to bucket 47 that basically reduces the data by a factor of a hundred per bucket and then we can then use twelve or sixteen or however many cores we have on the machine to to crank through those buckets I'm parallels that's what makes it quite fast to work with this data once we get it in this format if I didn't mention this already I don't think I did is that the SAS files in a particularly unfavorable format for "
    },
    {
        "start": 2138.53,
        "text": "data analysis not just the being in the sass 7b data format itself which I could go on and on about but I won't but also because the data are there not really sorted they seem to be ordered by calendar time so basically for 2009 you just have like all the January first encounters all the January second encounters on they're not all sort of that way some of them just seem to be not sorted at all that might be useful if you were doing sort of like facility oriented research like you wanted to look at the pattern of consumption of surgical supplies or like whether people get more bypasses on Thursday or something like that that would be potentially interesting to some people but most of us and those the people I know want to do a patient oriented research so what you need to do is you need to restructure the data so that all the information about one patient is sort of adjacent in the file because you can't be jumping around these giant files picking bits of pieces of data around you want to do that once upfront because then the state of smaller less static they append another year on each year if they choose to buy another year's worth of data but in general it doesn't change no one's gonna go back and edit this data it's it's a fixed "
    },
    {
        "start": 2198.59,
        "text": "when you get it so you put some work in upfront to restructuring the data so that's organized in the way that's most favorable for patient oriented research which is what most people are doing and then you can just keep benefiting from that it'll pay off in dividends as more and more people use the data we hope so so the feature one is the bucketing feature two is the use of native types and feature three is the is the sorting and these things kind of interact with each other in positive ways so that end the colonization right so basically what happens is the same bucket zero file type a you have there's something like 20 columns in that in those SAS files and you create a separate file for every one for every column of that file but only for the people who are in who are assigned to bucket 0 based on their enrollee ID and so these files turn out to be highly compressible because you'll have many wells per person because again these day if you recall is the file that contains information about your your plan type like what you have covered "
    },
    {
        "start": 2258.59,
        "text": "covered and so you end up having many many rows per person because people's plans the details of the plan are constantly changing so you might have hundreds of rows per person and like their date of birth won't change that you have a run of 100 or 200 copies of the same value so that's very simple run length encoding crops that down to one number times the number of times it's repeated and so you get the mini fold compression like in this file you get like hundred fold plus compression because of that very high degree of run length encoding and even an even in fields that are changing like the date changes this is the date in this case when the plan when the plan type changed even the date changes every time but these are still highly compressible because there's still regularity is that a good compressor can can exploit so as we'll see in a minute we are going to compress the data by a factor of six I think it's more like ten actually I say six in the slide but it's really more like ten over the SAS files without losing any informations all the information is still there but it's just represented in a different format that is one-tenth the size and it's not so "
    },
    {
        "start": 2319.369,
        "text": "much about saving disk space this space is relatively cheap but if you reduce the data by a site factor of ten the data volume by a factor of ten and then you need to scan through it which like I said before is something we often have to do then you spend one-tenth the time waiting for disk reads which which is actually a dominant factor and the overall computational time if you're doing something simple like just accumulating summary statistics there's there's actually a bunch of tricks here and it gets even more detailed than I think I want to get into an a unit in a talk but just at one more example like I mentioned before that the in the claims data sets a lot of the data are coded and that and they they saw like icd-9 codes are codes that there's this library or a dictionary that tells you what all the codes mean and you can use a an adaptive coding scheme called u variant which is used in Google's protocol buffers if you've ever heard of protocol buffers there's a lot of the stuff we use it comes out of out of that world and basically what it does is it "
    },
    {
        "start": 2379.76,
        "text": "can use either one two three four however many bytes it needs to represent a code but it but it Junius things so that it uses the one byte representations for the most common codes the two byte representations for the next most common codes and so on and that ends up being a very and so you can you can pre calculate the frequency table for the codes and you can get very high compression by using that type of volley and bar for variable-width integer coding for coded values and because these datasets are heavily heavily consisting of coded values that actually turns out to pay pretty big dividends we have we have we have general purpose software for doing this so if you have a bunch of cells and you want to convert them it's not just for market scan for example we intended from the beginning that this code we write would be applicable to opt in as well as markets can we actually haven't gotten around to running it on Optim yet I don't I just don't have the time to do it right now but that'll happen sometime in the next couple of months so any of any other setting where you have roughly the same structure where you have like large numbers of files and you need to you want to use this scheme like bucket them sort things I think any any kind of "
    },
    {
        "start": 2440.91,
        "text": "health record data not just market scanner Optum what this could be this could be used so we there's lots of tricks in the coding but long story short the time that it takes to do the to create this data layout isn't that important because you only have to do it once but just so you know it takes about 24 hours on one note to crunch through the 13 or the 3.4 terabytes of SAS files and distill it down into this 600 gigabytes of structured information that can then be processed efficiently using other tools that we have written and it's those other tools I'm going to turn to next those are the URLs I think you guys have X they have access to these slides right or you can email me feel free to email me if you want to find these things in you have trouble I'm not know again I'm not necessarily I'm all this is more of a research talk I'm telling you what we did and what we learned from doing it and what we were able to achieve and a little bit about some of the difficulties encountered but if you think these tools might be useful for you I'd be very happy for other people to try them out okay so the "
    },
    {
        "start": 2502.14,
        "text": "second task that we focused on once we've got the data distilled down into that so called layout is to develop some stream processing algorithm so stream processing as a strategy for data processing is nothing new there are a variety of tools out there that treat data like streams for example the one that I was inspired by the most years this Python thing called tools I think there's a version of it called C tools the idea of stream processing basically is that you have something like an abstraction that's capable of reading data from a source and then feeding it to you you know element by element and most of these things have a common underlying strategy is that you have low level primitive transformers that operate on these streams and you change you change these primitive things together to produce very complex behaviors it's a different way of looking at data processing and data management than the more conventional notion that you read the entire data set into memory at once in the form of something like a data frame whether it's a Python pandas dataframe "
    },
    {
        "start": 2562.59,
        "text": "or our data frames the the point is the same your thinking always with this static rectangle of data in memory in this case you're thinking of a dynamic flowing channel of data passing through a bunch of actors right and this even goes back I think another precedent for this is even very early in the history of UNIX is that a lot of the UNIX utilities are designed to be piped together so you can you can have you can do very complicated things even with a relatively limited set of primitives so there's nothing particularly new in the notion of stream oriented processing but I haven't seen it developed at a very high level for forming a bridge between big data sets like the claims data set and the statistical tools which is the third of the three tasks that I'm going to try to squeeze into this talk today the other challenge that we encounter here is that it turns out to be non trivial to implement stream based utilities and go because go I mentioned before it's a statically typed language so you want to avoid and it doesn't support like if you're familiar with C++ "
    },
    {
        "start": 2623.13,
        "text": "templates and things like that it doesn't support that notion of generic coding so you have to do some pretty tricky things to avoid having to re-implement every function 20 times to be able to handle 20 different numeric data types Tonie different data types but there are a lot so we dealt with that and I won't bore you with the gory details this is an example of what our what our D stream package does this is actually from that transportation project just because I had a better a better looking example from that project so don't worry about what the variables mean here but this is just an example of how you would use it we use this to calculate LX Houser scores for for our Health Services Research we use it for defining these time logs in which you when we do a lot of recurrent events analysis of people who have cirrhosis have spells in which comorbidities get worse and then they get better the streaming type tool turned out to be very useful anytime you'd have like recurrent events and you want to do things like calculate the time delays or the time gaps between successive occurrences of a given complication but anyway this isn't a "
    },
    {
        "start": 2683.4,
        "text": "different setting this is in the setting of a transportation research which is another thing I've going on right now and it's not again the point is not that you understand what each row does but the just the high level you start with a source of data in this case it's a CSV file but your source could be lots of other things you define how to get the data out of that source so sort of initialize a stream by connecting it to this data source and then like I said you pass it through a sequence of transplants so this this stream object here gets created and then it gets fed into a transformer and then something new comes out the other side but we just reuse that variable to hold the new thing because we never need to refer to the old thing and again it's embedded inside of this effectively and then again in and out in and out and in and out in and out so by chaining like seven or eight primitive functions together we can create quite complex behavior like most stream based data processing tools "
    },
    {
        "start": 2743.67,
        "text": "it has some properties it's lazy which means the base is like nothing when I let me go back one slide when I first set this thing up nothing actually happens right so until I actually start competing against this data this is just sort of an abstraction it's just sitting there waiting to be waiting to be used and it's only when you start pulling data out the end that that triggers behind the scenes all of these transformations to start churning away and eventually you reach the source and it starts pulling data off the disk and then things kind of unwind in the other way so that turns out to be a good thing because if you actually don't end up using certain data elements in your calculations they'll actually never be read never be processed so it's sort of efficient in that way in that way see here almost out of time so I mentioned a minute ago we use this one one sort of basic task that we use this streaming calculation for its Alex Hobbs our score calculation so Alex has our scores you may know it's a it's a common way to summarize people's health status in one number they have all kinds of "
    },
    {
        "start": 2804.119,
        "text": "limitations as a tool for doing that but they are they are still useful when you want to just include like a control and a regression model for someone's overall health status you often want to include the Elex hazard score their limit Ricci to calculate so roughly speaking how they work is you've got 30 different categories like heart disease cancer psychiatric illness and so on and it's just binary within each of those categories you may or may not have that condition and then you add up the numbers until you're Alex house or score can be anywhere from 0 to 30 although for almost everyone they're between like 2 and 8 so the challenge here is that there's there dozens and if not hundreds of codes that define each of these categories and these codes can appear in different files they can occur remember in the a they can occur in an outpatient visit they can occur in inpatient visits they might show up on the old files they might show up on the AI files and I even show up in these service visits or in the lab data so we have to sort of stream through in parallel all of these different sources of data check for the codes and as soon as we see a code for the first time for a given person we add one to the relic sizer score at a high "
    },
    {
        "start": 2865.019,
        "text": "level that's something that should sort of feel like something that can be accommodated calculated very efficiently in a streaming manner and so I'm using this as an example of a calculation Health Services Research that it's very well adapted to the streaming style calculation you never hold more than say 10 or 20 patients data in memory at once it's extremely memory you know minimal in terms of its memory memory demands and if I didn't say this before we can we can actually stream through this large data set calculate oxides or scores for everyone in the dataset in like less than 10 minutes five to ten minutes depending on that details if there's some options in terms of exactly how you calculate the scores so I think that's that's fairly impressive now if someone told me that with Sparky to probably do it in one minute but then you're using hundred machines so this is not one machine you can do it in ten minutes I'm willing to wait ten minutes and just to kind of close the loop here we have a third the final step of what I normally need to do is then finally you need to take the data once you've "
    },
    {
        "start": 2926.43,
        "text": "restructured it as streams and actually feed it through statistical models so this could be anything from like a survival analysis model to generalize linear model these squares often with lots of ancillary stuff on floating around the edges we almost always have way to be almost always at stratification we usually have some form of matching we often have like some kind of propensity adjustment going on so we need at least if you want to do this all kind of in a unified way we need tools to fit these models and that are capable of supporting all the things that I just mentioned all the ancillary things I just mentioned so we have worked over the last year or so to implement what I consider to be the bare bare minimum of course statistical methodology that's used in health services research all the things I just mentioned and a few others and these are these are actually engineered and optimized to accept data out of these streams so that some there's no conversion there's no data type conversion there's no sort of format conversions that take that need to take place the the least-squares regression or the Cox model or whatever it is you're fitting it just pulls the data right out of the stream and "
    },
    {
        "start": 2986.73,
        "text": "immediately starts cranking away at it and especially for methods that are like one pass methods these these are cool you could you can fit certain types of models on data never like maybe just you know making a single sweep through the data for the methods that require like Cox modeling requires usually like seven or eight passes through the data to converge it's a little bit awkward because you've got to read reaching the data seven or eight times in that case it really only works well for data that can fit in memory once you still down to the amount of data that you need to run to fit a Cox model to other things we can like the Alec size our scores we can calculate out the hole okay so I think I'm out of time right all right so that's just an overview of what we've been doing and happy to take questions or you can use here what you pulled out here to see like is it biased and this way or that way in terms of accuracy "
    },
    {
        "start": 3048.47,
        "text": "I mean are any of it well I think I get it what you're saying so what we know that market scan is not unbiased they don't claim to be unbiased what they claim is that it's enough that you can then use weights to adjust for the biases so the way that works is that we use a survey either MEPs for the ACS ACS is a famous survey that map's is Medical Expenditure panel survey which is a government-run survey which is which is normed against the census and therefore it's supposedly representative of the US population and can be used to subset you can also use it to target subpopulations like the ESI population so what we do when we need to make the population projections as we take markets can we divided up into strata based on demographic variables it's all kinds of debate about the right way to do this you calculate the market scan size for stratum that the population size for stratum based on maps which is considered to be very "
    },
    {
        "start": 3108.79,
        "text": "accurate and that gives you weight that you use to adjust up I'm not sure if that is that sort of responsive to your question either markets can nor optimum side are they're both convenient samples neither one of them is representative just as a unweighted collection of data I didn't mention this it contains around a third of the population so that's a big subset but even a large subset of a population can be biased so we know for example that we undercover in markets can we'd there's more like urban coverage of urban areas than like rural areas there's more coverage of certain parts of the country but they and this is a great research topic I would like to get into this more myself I just haven't had time to really evaluate the extent to which you really are doing an honest population projection from either optimal markets because many people are claiming to do so and whether they actually are or not I think would require a lot of a lot of work but at the minimum we've used these this "
    },
    {
        "start": 3170.83,
        "text": "standard weighting technique to adjust for what could potentially be obvious biases yeah and that's why I'm saying that I think this needs to be looked at more carefully I mean it's we're not claiming that this can project outside of ESI I mean it's it's clearly limited to ESI and I don't know let's see the only thing that we worry about with markets can a little bit is that we know that their client base is large is larger employers so we're getting larger employers rather than mid-sized and smaller we're very small employers typically don't provide this type of coverage to their employees anyway but we know we're missing like the companies with like say 30 to 50 employees because markets can I think starts at companies with 100 and up so if there's something like sociological or demographic or health difference between people who work for medium sized countries and larger companies which there could be then there's something to these biases and the weights may probably help with that but they probably don't completely remove it when you get when you first "
    },
    {
        "start": 3230.93,
        "text": "get the data in there is there like a long laundry list of QC measures that there's not so much that I mean we don't have so if there are errors in the data I'm not sure what we would how we would even know that we don't have I mean I don't think there are there can the data have been cleaned to the point that they like the obvious logical inconsistencies should be gone so I'm not sure that there's any way to do that did you have something in mind about how to check I mean if someone was born in 300 BCE that would be a something but I think those things have been those things have been vetted and this is not like ultra raw data part of the service yeah I mean there could be people who have combination of diseases that like it would almost never happen but in the population as a whole you can't necessarily say that it didn't happen once yeah it's a good know everyone accepts that there's lots of errors in "
    },
    {
        "start": 3291.14,
        "text": "these data and some of those errors can possibly be rectified by like I said like we don't we want to say that someone has a condition just because they have one code for that condition we usually would require them to have like either multiple codes or they would have the code for the condition and then they would get treated in a way that would be compact consistent with them having that condition the more team chi-chi from Harvard was really a student she oh very nice to talk about her work in this area the more sort of hard to diagnose the condition is so she works largely on rheumatoid arthritis the bigger problem this is like someone with the rheumatoid arthritis code was one rheumatoid arthritis code they only have like a 50% chance of actually having it according to like standard diagnostic criteria because doctors will just enter a code like that for someone who says like my wrist hurts or whatever resources you would hope that they're not just entering that's a serious you know conditions it hope they're not just we're entering that code freely but I'm sure that borderline cases where someone has another liver disease and it shows up as cirrhosis and that was the wrong call that's why some codes that doctors called back on because they know that the insurance will pay for it if they lose that I I've even had my doctor "
    },
    {
        "start": 3353.12,
        "text": "Tommy I well I could put the bill decide this but might not get covered and would mainly in sit in the disease and then the major comorbidities and we would hope that that would be something that would be entered into erroneously well we're not looking at we're not looking across the spectrum at many different diseases and this isn't primarily a prediction oriented projects I have just getting started now I have a new project looking at heart failure prediction so then that's very much in that spirit like you want to look at given what you know about a patient right now can you predict with a heart failure within two years that's very much a prediction Atlanta project the salsa stuff just wasn't just wasn't brought to me as that type of a question so that the main goal of the cirrhosis project was more almost more epidemiological in the sense that they wanted to rigorously quantify the "
    },
    {
        "start": 3415.06,
        "text": "burden of cirrhosis which cirrhosis is a disease that is it's complicated it's caught up with all kinds of trends about like you know the rural white populations and opioid abuse and unemployment trends and certain parts of the country it's associated with lifestyle of things like heavy drinking and drug abuse and so on so there's a widespread belief that cirrhosis as a chronic disease is increasing in prevalence but there actually isn't it wasn't a lot of data to document that in the general population it's certainly in the VA and the CMS Medicare populations that's been well established but it's harder to get it's harder to do that kind of epidemiologic work in these populations from many of the reasons that I hopefully came through today and my talk so this is the first as far as we know like really rigorous documentation providing evidence that cirrhosis is increasing and the rate at which is increasing which is not financial unfortunately but it's fast enough that you know there's going to be it's expensive disease to treat there's going to be an increasing burden on the "
    },
    {
        "start": 3475.31,
        "text": "gastroenterologist on that medical practices that have to deal with this disease moving forward because of the increasing trends that would that was it was more of a statistical epidemiological goal here rather than a predictive you have planned to extend the usage of this tools to other kind of thing besides well we're already using so there are three things here there was the the data layout the SAS file crunching and part of it there was the streaming part of it which were already using for transportation data so the answer your question is yes for that and then the third the third task as I referred to it is is broadly just implementing various statistical methods and so there's nothing EHS you know Health Research specific about survival analysis people use that in all kinds of domains but the first thing is probably as limit it would be useful mainly for like administrative data about people not just necessarily health data if you had data on crimes or educational data "
    },
    {
        "start": 3536.87,
        "text": "or something like that but it kind of makes is its works for data that's indexed by person and then by date so if you have many events per person index by date any data set like that the first tool could be used in that setting any last questions thank you "
    }
]