um so as she mentioned I'm in computer science working under Professor Sati nanami um and the last thing I wanted to add is that I'll be graduating soon and on the job market um looking to go into industry so if anybody happens to know people hiring in violent Dramatics I'd be happy to talk after the uh after the talk okay so today presentation is on accurately bench marking based variant calls and today I'd like to talk about two different problems that I've worked on over the last year or so um but before I get into that I'll start off with a bit of background on the problem that I'm working on which is whole genome sequencing evaluation um as you all know sequencing cost is rapidly declining over the past few decades um and the cost per human genome went from over $10 million um for the first effort now it's well under a thousand um and with that uh lowering of cost has come an exponential growth in the number number of genomes that are being sequenced and then some of the downstream applications are of course genomite Association studies uh drug development clinical Diagnostics Diagnostics and benchmarking new methods and Technologies um and for all of these Downstream applications once you have sequenced a genome it's important to be able to compare the genomes of different individuals and so the way that this is typically done because genomes are mostly identical um around 99% similarity across individuals um typically they store everything in a different spaced format so um you have the reference uh Human Genome um along with just storing the specific locations um that there are any mutations such as a g to C here um a deletion of a t an insertion of an A and so forth for um and this difference-based results are typically stored in variant call format where you note the position of the mutation and then what the reference genome said and then what the alternate sequence is so here we can see at position four um see here we go position four a g was mutated to a c um and so forth now what I'm going to be focusing on is specifically benchmarking the results of these variant calls um and the motivation for that is that there's been a ton of new sequencing technologies that have emerged over the last few years and along with those new technologies have come a lot of different uh bioinformatics pipelines each with their own advantages and disadvantages now a simple example of how these Technologies are Benchmark um as I said before you start off um with with your reference and then the sequence that is reported um by your sequencing and you only store the differences so technology one might say that position three a c becomes an a um and then later on a t becomes an a as a mutation whereas a second technology might slightly differ they agree with the T becoming an a at position six but instead it says um later at position n and a becomes a g so something that's important to do is to know is technology one more accurate is technology 2 more accurate and to understand the pros and cons of the two different Technologies um in order to do that typically you have some form of ground truth um and this ground truth is typically created using multiple different sequencing Technologies and sequencing DNA at much higher coverage um so that you're more confident in the set of calls that you have on this data set um so here the ground truth might agree with the T becoming an a um and then agree with the second technology with the a becoming a g at position n and so using this ground truth you can then say whether your variant calls were correct or false positives from your different Technologies and get a good understanding of how accurate they each are now when people do this analysis they typically stratify by the different types of variants because some are more likely than others um and they have different characteristics so Snips or simp single nucleotide polymorphisms are when there's a substitution of one base to another indels are insertions or deletions less than 50 bases in size and structural variants are insertions and deletions that are larger than 50 base pairs and Snips are the single most common mutation um but insertions and deletions account for a large fraction of the total bases that differ now today I'm going to start off by focusing just on small variants um because typically they're evaluated separately from structural variants um because there historically were different sequencing pipelines um in order to evaluate the two so for now we will just look at the the small variants which are sson andells and typically the performance is evaluated in the form of a Precision recall curve so on the x- axis you have recall and on the Y AIS you have Precision so Precision is basically out of all the total calls that you made how many of those were correct whereas recall is out of all the variants that did exist how many did you correctly identify so obviously we want to correctly identify all the variants that do exist and then we want everything that we do call to be correct um so ideally you want your Precision recall curve to go to the top right hand of this graph and here we can see the Precision recall curves um for two different example sequencing Technologies or pipelines um for inel variants now this is all well and good but there is a problem once you start talking about complex variants um so when we categorized in to um Snips and indels and structural variants complex variants or any type of variant that cannot be represented by a single mutation so it's not just a single U mutation of one base or a single insertion it's when there's lots of variants that kind of happen nearby one another and it's really hard to tell what actually happened um so here is an example of something that's not quite a complex variant but it does illustrate the problem that complex variants cause so here if you have your reference sequence um and then two different um query sequences so say you sequence the same individual using two different Technologies um for query one you've represented the variation as a deletion here at this position a deletion of a t and then an insertion of an a a little bit later whereas in query number two you say that this T has mutated to an A and if you note the actual resulting sequences are the exact same was 1 a and then 5 TS and then 3 A's so between query 1 and two your sequences are the exact same so your sequencing technologies have identified the exact same underlying DNA sequence but whether you represent this change as an insertion and a deletion versus uh a mutation will then impact your Downstream benchmarking results so here is an example kind of illustrating this problem um where we have two different pipelines that actually so this is real data for full genome sequencing where we used two different um kind of types of variant representation so in yellow it's the variant representation that favors insertions and deletions and then and purple the variant representation that favors um Snips or single based mutations and even though the reported sequence is exactly the same um because of the variant representation um one pipeline seems to do much better on indel Precision um but worse on sniff recall so that's kind of the the motivation for why variant representation um matters in the form in the uh the area of genome sequencing evaluation so an example kind of instead of at a high level more at a concrete level is if you had a ground truth um that used one representation for a complex variant um that uses substitutions versus a representation that prefers insertions of deletions again um this the actual sequences are the exact same both the reference sequence um and the truth um however it's just the variant representation that has changed and now when you shift towards evaluating two different technologies that have different sets of variant calls the representation that you chose for your ground truth will end up affecting what decisions you make in comparing the Technologies so for example if we were to use the first representation that favors substitutions then technology one will seem to do much better um because it has two substitutions correctly called um and no indels whereas if we instead use the second representation um that prefers indels then we see that technology number two um gets one true positive variant call whereas the First Technology um seems to have not made any correct calls so the variant representation really impacts um your results and the solution to this problem comes in the form of variant normalization so you may have heard um variant normalization um is standard practice um however there's typical variant normalization and then some extensions that we add on to it um so for example if you have a reference in query sequence that differ um you would align them to one another um and represent the variance in some form so this form right here um shows two two bases changing to two other bases then over here um aatc becomes a aatc now the standard process would be to First decompose any substitutions so over here this AG to TC becomes two separate substitutions and the next step is for your insertions and delions to be trimmed um because over here the bases T and C match exactly so we can trim that off the end and we can trim the a off the beginning um to simplify it to only include the difference plus just one preceding base and the last step that's typically done is left shifting so here we have a single a being added um and you can actually add that a anywhere along that sequence of A's and so it's standard practice to insert it at the leftmost position so over here we've shifted the insertion from position seven to position four and this is kind of the typical process of variant normalization and this is sufficient to ensure that for any individual variant it has a unique representation and the problem comes once you start working with complex variants so if you have these multiple um variants that are all nearby one another then suddenly there's multiple different ways to represent this change so here's another example here where the reference and query sequences are the exact same and you're only changing the alignments and therefore the variant representations so instead of using two substitutions and one insertion you could also use one deltion and one insertion to represent the same variation so the problem that we are trying to solve is how do you choose between these two representations and decide which one is better um so what we looked at is something called um alignment based normalization so back here um the two representations result from two different alignments so if you could standardize the parameters for the alignment then you could have standardized representations for these more uh complex variance so alignment is typically done with something known as an apine Gap penalty um which is basically you're trying to uh model the likelihoods of substitutions versus insertions and deletions and so the simplest way to penalize an insertion or deletion would just be directly proportional to the length of that insertion or deletion and that would be a simple linear Gap penalty but what's typically done is an apine Gap penalty where you do have a linear increase um in terms of the length of the insertion or deletion but you also have an additional penalty here for for starting an insertion and deletion um and that's because insertions and deletions are much less common in comparison to substitution mutations um but once you do have an insertion or deletion then having one that's a little bit longer is not that much less likely and so you assign um penalties for two bases matching one another and then for two bases not matching one another which would be a substitution and then you have a penalty for starting a gap um of an insertion or deletion and then a different penalty for extending a gap um for an insertion or deletion and this will help allow you to quantify um the two different representations so here you would have two mismatch penalties for the two different substitutions um and then a penalty for opening and extending um an insertion of one base whereas with this representation you would open two separate insertions and deletions um and then you add the extension penalties to reflect the length of those insertions and deletions and so if you assign values um to substitutions gap opening and GAP extend ition here we choose just any penalty for um for substitution first and then we want um a gap to be less likely than a substitution so it has a higher value um but then it's not that much of a penalty to to increase the length of that then once we have those numerical values we can calculate um whether one representation versus the other um representation makes more sense yeah there is a question and I think I'm allow chenin to speak so CH are you able yeah so for option two uh your Gap your uh dediction and insertion is actually connect to each other so I think that it's just one gap opening plus uh eight extensions when you come as two cap openings yeah so it's here it's being counted as two Gap openings because um it's not two deletions or two insertions that are adjacent to one another it's two separate variants where the first one is a deletion um and then the second one is an insertion and so you have to penalize for opening um the deletion and then for opening the insertion does that make sense okay so my question is why match is so ideally you should give some bonus for for fully match align yeah so um there's prior work that has shown that um you could have a a bonus for matching variants but that's actually equivalent basically you can just scale and shift everything um so that the the match bonus is zero um and so I've just chosen m equals z to start with um just because it simplifies things and then you don't have to worry about that yeah is that what you were going to ask as well okay yeah yeah so for the option two for example for the first delis uh the formula is zero uh o plus 3 e so chain we have three uh three space you know deleted right and it has been reflected in the part of 3 e but uh besides that we have to add the opening my get my point is here since like for the first for the first base we have double punish right because yeah um so for the people over Zoom if you couldn't hear that the question is if the kind of the first Bas is almost double penalized because over here we can see if if you have just one base then you need both an opening and an extension score um and again for that um basically what you could do is set your opening score to O minus E and then um so basically yeah you can just kind of as long as you choose um your opening penalty um to be the right value then you could you could formulate it either way basically yeah okay um so moving on this slide has a lot going on um but basically the whole point point is to show that there is kind of a whole range of values that you could choose um for your parameters gap opening extension and substitution um so here on the x-axis we just have the penalty for starting a gap um relative to the cost of a substitution on the y- AIS we have the cost of extending a gap relative to a substitution um and then kind of As you move through this space you can choose any design point um and as you get closer to the origin your alignment will start preferring insertions and deletions as you move away from the origin you will start to prefer substitutions As you move towards this line you will start preferring to have separate gaps um and in the extreme case if you edit distance alignment where you don't have any gap opening penalty um then you can have as many separate gaps with with no additional penalty um and then along this axis you would prefer to merge your Gap as much as possible um and so based on the application you'll choose a different design point in this space um and so short and long read aligners are typically clustered around here um which we've labeled design Point C assembly based aligners they tend to prefer merging gaps as much as possible so they'll end up um closer to to y equals z um and then structural variant and copy number variant aligners um typically want to allow um insertions and deletions as much as possible um especially in like repetitive regions where there they are more likely than substitutions so those types of aligners will be clustered down here near the origin um and so moving forward I just kind of would like to highlight I have like design points a b c and d kind of across the space um just to kind of show um different representations that that could be used in reporting variance yeah I assume mesen is just kind of depend on o right so that's why yep yeah so everything is plotted um relative to the substitution penalty um so basically once once you normalize everything so that the match penalty is zero and then you divide by the substitution penalty then you've kind of constrain it to like a 2d space um where then you can explore the different parameters yeah okay um and then now I'm going to talk a little bit about what um doing this changed for our evaluations um so the first example is we had a tandem repeat Benchmark um that had an original representation that was um pretty far out there um so if you go back to this slide um I believe it was somewhere up here um and that it very heavily preferred substitutions um so what we did once we normalized it to point C which is much more typical of short and long read aligners we found that the number of snips almost haved um and that there's um significantly more indels now um so these parameters are very important um in the representation especially in complex or repetitive areas of the genome um where there is more likely to be a combination of insertions and deletions um and because you have these long repetitive elements those insertions are deletions Could Happen anywhere along a pretty large range and then it becomes more likely that they interfere with with other Snips and variants so for this data set we found there was a a huge impact um and here's just just an example where the initial representation used 12 Snips and one very large insertion to represent some variation whereas once we normalized it to design Point C in the middle which is more typical we found that this set of changes is actually exactly equivalent um to these two insertions um which is very interesting and not something that you can really notice very easily just by inspection um so some other contributions that we made in this work um were efficient variant clustering um so you only have to deal with the representations of variants that are nearby and could actually impact one another um and then compared to Prior work which does um these evaluations we also allowed inexact variant matches um during the benchmarking and and variant comparison um and then we also changed things so that we used phasing information for the evaluation um because the The Benchmark that we uh compared against was designed for short reads where phasing information was not available um so we extended it um to work with phasing information and then we also had proposed some distance-based evaluation metrics um so I'm not going to cover most of these changes I just did want to briefly Show an example of inexact variant matching um so here's an example example kind of similar to the previous one where this one large variant um plus another substitution was very similar um to this other set of variants in our ground truth um but in this case it's not exactly the same I think there was like one base different um which is why the match is around 98% um instead of perfect so this is an example of where now our tool is able to figure out um that these varant are actually um pretty much equivalent to one another um and appropriately assign credit during benchmarking um and then in order to evaluate we looked at data from Precision fda's truth challenge um so they had 64 different submissions um for the same individual um using a whole host of different sequencing Technologies um and so this was the perfect data set for us to look at um and then just the figure kind of showing the highle results um so we changed the variant representations um to the original representation in black and then the four different design points that I pointed out earlier across the space and then looked at the the qu the F1 quality score which is a balanced um metric combining precision and recall um and ideally for the evaluation you don't want um your um F1 Square to change at all depending on the variant representations um so ideally you would just have a perfect line um and we can show that our work gets a lot closer to this um than than the previous work okay um the next thing that I looked at um which is a paper that should be coming out on bioarchive like either today or tomorrow um was actually um kind of the overall methodology for evaluating small and structural variants so right now people evaluate them both separately so my first work was focused on just small germline variant calling evaluation so just sniffs and indels um and not looking at structural variants um which are much larger and if we look at this example here there is an issue with evaluating them both separately because now that we have intelligent variant comparison we will find cases where a large structural variant is actually equivalent to multiple smaller variants so if we were to separate them into two different uh files beforehand and then do the evaluation we would find that our query had this large structural variant but our truth had no corresponding large structural variant so it would think that's a false positive and then if you look at just the small variance um it would actually also consider this a false positive because there's nothing at the same position and then it would consider all six of these to be false negatives so we need something that can work with both the large and the small variant at once there's another question online from chin again um which design point should we choose in practice should we just use oex combination that minimize the number of variant yeah I basically we we did a lot of thinking about what if we could propose kind of like a universal like recommendation um but honestly at the end of the day it really does depend on your application um so we have designed Point C kind of in the middle of what long and short read aligners usually do um and that's kind of like what we settled on as um reasonable for um kind of like non super repetitive areas of the genome um where just like genes um where there's like a normal amount of variation in the sequence um however like we saw that the structural variant and copy number variant aligners um typically choose very wildly different parameters um and so if you're trying to identify that type of variation then you you probably should choose different parameters um so the most important thing is that you're aware of what effect that parameter Choice has on both your query and Truth data set and that you choose the right one for your application and that across different data sets you're working with you if you're comparing them you should make sure that you're using the same representations across those data sets yeah on that question yeah so presumably there's biological processes that are leading to Snips these indels or variations so even if you end up with the identical reference and uh sort of query one that you know is true and you don't have a way of deciding which just by staring at it there is a biological answer to how that arose yeah through homologous recombination errors or or whatever so just to throw up your hands and say their design spaces open your own interpretation it seems a little bit like you're giving up when there might be an answer that may not be accessible but is yeah that that is a good point that like there was some type of biological thing that happened that ended up with that new sequence right U but the problem is a lot of the time you just have that new sequence and the old sequence yes and you can't really go back and figure out um like what set of variations LED you to that point I guess in some cases it might be possible um where like you have history of other patients that only had like one mutation and then you have yours which has several and then it's most likely that it contains that previous mutation plus something else um so there it would be possible you're right in some cases to using external data kind of try and go back and resolve and and figure out what makes the most sense um but I guess without doing that and just operating on the sequence information that you have then you kind of just need to choose parameters that reflect the relative likelihoods of those events happening and kind of work with that even though you're right it's it's not perfect yeah does that make sense yeah okay um so the second project um is basically on evaluating um both small and structural variants together um so historically they've been evaluated entirely separately um and the main reason for this is that read sequencing was and still is um kind of the dominant old genome sequencing approach um and so they would use short read data to make um small variant calls um and then prior work BCF Val is designed for just small variant comparison and it requires the exact matches um whereas then if you wanted to do uh structural variant calls um more recently um long reads came on the scene and even though they were far less accurate and not really good enough for the small variant calls um because they're so much longer um they will still map correctly um and they were used to call structural variants and then tools such as truear which use kind of puristic um in terms of like percent match the size of the variant uh how near by they're located um would then be used to evaluate whether or not those structural variant calls were correct but more recently um we've seen the emergence of pipelines um that can call both small and structural variance um from one sequencing workflow um and typically it uses long reads as the backbone um that can be polished with short reads or it can be supplemented with like high SE or Optical genome mapping data um but basically now it is possible to call small and structural variants from the same pipeline um but at the same time the current current methodology for the evaluation is still to do small variants and structural variants separately um and even this delineation between small and structural variants um right now the threshold is at 50 base pairs but the reason the 50 base pairs was chosen doesn't really have any biological origin it was really chosen because short reads are generally 100 bases once you start dealing with insertions and deletions longer than 50 bases then the short reads kind of Str to map correctly um and basically the Precision and recall of structural VAR variant calling Falls dramatically once you start looking at bases longer than sorry variance longer than 50 bases um so the solution that we worked on was a joint evaluation of both the small and structural variant at once um and so what we did is we extended BCF disc so instead of just working with small variants it can also do structural and that required a bunch of like performance and efficiency improvements in order to be able to handle those those larger variance um and then I'm just going to show um kind of some of the results that we ended up finding um so this also has a lot going on but basically the top is showing the false negative rate and the bottom showing the measured false positive rate um it's on a log scale so everywhere from 0.1 to to 100% eror R and then we lift at three different uh whole genome data sets uh hii ASM dip call human 100 pav highi ASM um from genome in a bottle um and then for each um VCF for each set of whole genome variant calls we looked at six different subsets so we looked at Snips individually indels individually structural variant individually small variant which is the Snips and the indels what we call large variants which is the indels and the structural um and then all variants so the only three categories that are really typically used are the small variants the structural variants um and then we our pipeline uses all variants so what you want to look at is for the Snips and indels the performance improvement from moving from just small variants to working with all variants um and we can see that for all three data sets um for both false negatives and false positives we can see that there's a reduction from the orange bar to the green bar which is the move from small variants to all variants and then for the structural variance um the move from Red which is just structural variance to when you look at structural insertion deletion and snip um and we can see an even bigger performance there so everything's on a log scale so it's kind of hard to see what's going on um but we found that there was an average around 25% decrease in measured false negatives and false positives for Snips and indels and around 50% decrease um for structural variance and again that's going back to the cases where there's several smaller variants that match up with a large variant um and vice versa so the next thing we did was we compared to Prior work um so BCF disc is our tool that was designed for small and large um VC that's in green um in red is VCF EV Val which was just designed for small variants however they do go up to uh structural variants up to a thousand bases so we were able to get um bars for that however because they require exact matches um you can see that there's a bigger and bigger difference um between um that prior work and R tool as the variant size increases because it's less and less likely that you're going to have a perfectly exact match um and then trari works with indels and structural variants um it doesn't evaluate Snips at all so we have 100% false negative rate um and 0% false positive rate for that so that's kind of the the high level accuracy results um and then something else that we were able to do um is evaluate the f on our three different data sets um and what we noticed is that our results um much more closely matched our expectations um than what whatap does um so oh I guess I forgot um to kind of put in explanatory slides but phasing is basically when you have variants um that occur um on only one of the two copies of your genome um and you don't you want to say that it it happened on the paternal copy or the maternal copy um correctly um and so as you have a bunch of variant calls and you're saying okay this one was on the paternal this was on the paternal this was on the maternal um you want to get all of those correctly and there's two types of errors that happen a flip error is when you accidentally Place one single variant on the wrong Pao type um and then a switch error is where basic basically that variant and all following variants for a while accidentally get placed on the wrong Papo type um and the reason this happens is if you uh have are doing your sequencing and you know there's a few variants over here and a few over here and you don't have any reads that span from one set of variants to the other then you can't link the two together effectively and say whether or not they happened on the same copy um and and so basically we can use this variant comparison tool to then tell whether or not um the variants were on the correct um Hao type or not um and we compare to existing work what's um and we see that so this data set originated um from a Hi-Fi ASM denovo long read assembly um and this third data set originated from the same uh Novo assembly but just using a different method for calling the variance so we would expect very similar uh performance in terms of uh switch and flip errors between the two um we see that whatsap um find 610 switches and we find 494 um but then on the second data set we again find exactly the same number of switch errors um but here the number of errors that What's happen finds has nearly doubled um and we actually find that this is due to the variant representations um which I'll explain on the next slide um and then in contrast this second data set was generated from a Q100 a very high quality um denovo genome genome assembly um and we found that there were only six switch errors which is more similar to what we would expect um than whatsap which still finds over 300 um so there's a bunch of noise originating from what's at um and now I'm going to explain why we found that was the case so this slide right here um if for now you only look at the two variants in red um so at chromosome 1 um at this position there is a deletion of TG and then this format the genotype field is the is GT which is what we care about so this 01 indicates whether that variant occurred on the first copy the second copy or both um and we see that in our query we said that the variant occurred on the second copy whereas in the truth data set we say that it occurred on the first copy um and so in isolation these two are the exact same variant at the exact same position we place it on one copy the truth places it on another copy so that is most like a flip error where we accidentally assigned the variant to the wrong copy and we made a mistake um and that's what the existing tool what's hat will tell you it will report yes this was a mistake however if you take in context all the variants around it um and it is kind of a mess um but BCF disc is able to as its benchmarking decision decide that all of these calls are true positives because once you account for all of the variants together um the sequence between your truth and query for both Hypes matches exactly um so even though if you consider just the individual variant representations and it really looks like there's a flip error here once you take into account all the surrounding variants um it's actually just the variant representations that differ and the two sequences that report are exactly the same so here we can see the the real difference is um here the truth says a TMG are missing um and that basically um it just places the T um later on instead of at at this same position um so it's only the variant representation that's kind of causing all these false positives um and for that reason going back to this slide um you can see that in terms of flip and switch errors uh the existing tool whatap always finds a bunch more um than our tool does um and if you were to look at the variant representation chosen by um this one data set this has the most dissimilar representation which is why it has so many more reported flip and switch errors whereas this data set uses the same representation um and so it has 400 fewer reported switch airs question online fromang who asked does BCF dis do phasing itself or just evaluates against the truth how was the query phasing created in this examp yeah so BCF dis assumes that the input variants are already phased um but importantly it only assumes that local phasing is correct and not Global phasing so I briefly touched on earlier that we have an algorithm to Cluster variants together so basically any variants that are either very close to each other on the genome or they could be separated by a long repetitive region um but because that region is repetitive the representations of those variants could depend on one another um so we group those together into a cluster and we basically we assume that within a cluster all those variants are faced correctly relative to one another um but in between clusters uh we allow arbitrary um phase switches so we'll evaluate the phasing of this cluster and then this cluster and then this cluster um and then do a dynamic programming algorithm um to try and and minimize the number of flips and switches between the Clusters um and so that way it can both discover kind of the the general phasing errors um um while at the same time it doesn't do the phasing itself and requires that you have um at least local phasing information in the uh in the input okay well that's actually it for my talk um the uh the code is on GitHub if you want to check it out um and I'd be happy to answer any questions at this time yeah I do wonder whether your P would be also applicable to those in the genome especially like approximately like 50% of the genomes are likeing all she yeah so um yeah that's part of um actually the the motivation for This was um the fact fact that um previously the benchmarking evaluations were kind of restricted to the non-repetitive regions um and so you kind of ran into this representation issue a lot less um because everything was was represented very similarly um but once you start having stuff in repetitive regions um and in the normal regions um then the the comparison gets a lot Messier um so I would say this tool can still be used for like benchmarking the correctness in those repetitive regions um however I wouldn't necessarily recommend um like normalizing the variant representation in those regions using um the the default parameters in this tool um because um like changes in in the copy number of those like short repeated sequences are are much more expected to happen in those regions but yeah can still be used for evaluation um and benchmarking in in more complex repetitive regions yeah the main limitation that we do have to have right now is um it doesn't really do anything much larger than 10 kilobases um just due to memory requirements at the moment okay other questions right