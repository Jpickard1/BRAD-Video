this worldwide in 2005 he was recruited back to Zurich to the Swiss Federal Institute of Technology ETH Zurich where he is a professor he is chairman of the department or Institute and based in the Institute of molecular systems biology he was instrumental in really launching the modern era of proteomics genomics for all the genes proteomics for all the proteins and developing the tools including specific labeling of certain residues on the proteins and more recent methods that facilitate quantitative analysis of large numbers of proteins and interpretation of the responses of the protein complement to all kinds of systems perturbations in the form of Network biology so quantitative network biology is very his signature we have been involved together in the development on progress of the human proteome project and it's a great pleasure to work with him and to welcome him here to the University of Michigan good evening looking forward to your talk no thanks for coming out do my best to make this interesting for next 50 minutes or so it's a pleasure to be here see number of friends and old colleagues it interacted with and I also see had the pleasure to meet students some of which worked with Alexi and it's risky it was great to see because it was whole stock in our group and actually they had made enormous contributions to the field by putting the whole issue of how do you identify a protein on a somewhat statistically sound basis or Alexis work it was it wasn't so clear what the result that was published actually meant I'll come back to this in a in a minute or so with a short comment so but what we are interested in doing or achieving eventually would be to to determine in how variability genomic potentials or environmental variability so influences on the cellular on a tissue on an animal affect the phenotypes and we think of the proteome actively a molecular phenotype that translates and also the biochemical reactions into the phenotype three observe so this is what we would like to do and now I'm going to introduce a little bit how we approach this and then I make comments about technical advantage to make headway in this direction and then I show some examples there we try to apply the these approaches to answer some questions which are actually quite simple questions but complicated answers it is usually the case so in about I think everyone remembers this is by way of introduction the time where the human genome graphic sequence was announced and this was an best conference the only player here in the back Bill Clinton in the front they announced that scientists had generated a draft of the human genome is written in June 2000 and then they came certain predictions it is actually text on the US government press release that came with this announcement and it said that scientists will now be able to he was the working graph of the human genome who alert patients that they are at risk for certain diseases reliably predict the course of disease beside the diagnose disease ensure the most effective treatment is used in developed immunity extremely lofty and ambitious a predictions of course and the question is how would you read out of a sequence of some 3 billion characters all this extremely pertinent information and we of course know now that this is not so easy that we cannot just take a sequence and read out the anoobas factors and I think what we learned is that the effectively the revolution of the genomics was not necessarily the nest the initial sequence tract but then the enormous decay of cost and the ensuing through code in doing measurements on genomes in there in many systems and many cases many instances many perturbations so this is also well known figure which I took from the NIH and now of course we know that we can sequence most research universities we have facility or we have access to technology which generates re measurement or measurements of large numbers of genomes reliably and not that of genomes but also offer of transcripts and and specific cases of like chip sequencing where we can measure protein DNA interactions fantastic advance over the last 10 years or so so I think the revolution really came apart from of course the milestone of having the genome but the revolution in practical terms to translate this genomics world into a any effective science to answer the questions about post in his press release what came from the past and I could highly producible measurement of potentially thousands of genomes so the massive amount of data generates patterns from which we can learn things even though it now that we are having all this information here over at a genomic site thousands of genomes we have lots of phenotypic information that is coming out from clinical clinical studies for instance all from measuring the lifestyle of person how many times how many steps first walks and what he eats and so on we still have great difficulties from genomics alone to answer some really exemplary questions it seemed very simple but I think would be very difficult for someone to get up and say I can actually with some algorithm make a concrete statement for instance what is the effect of any inherited or somatic mutation on the phenotype so if you had a particular mutation here in a particular genome what's the phenotypic effect in some cases of course through biochemical studies we know that we know the answer this goes back to too early genetics studies been in general I think this is a difficult question still how the tool or move more independent mutations combine the phenotype very difficult question how do the same inherited mutations affect different individuals with different genotypes also very difficult questions clinically for entirely relevant because the same mutations in Indian different individuals have different attachments so this this is I think where we are we have all this information here massive amount of high quality data massive amount of high quality data here the connection is difficult and even relatively straightforward questions we cannot necessarily answer without a lot of work so this is for instance a case bear this hits us as researchers directly influences canta genomic mutations which generate increasingly large datasets of their genomic the cancer genome and the adjacent normal genomic sequence and then plots like these are being generated where with hundreds or thousands of mutations that are discovered in this population of initially few hundred now the populations have got bigger the cohort and then some we see that some distribution this is from a paper that came out in 11th or 20 pioneering canta genomic studies you can see that the summary page some genes which are mutated frequently many genes which are mutated very infrequently but still might contribute and I think to the to the occurrence of this disease and I think one of the difficulties is through all the DS notations in a way that explained their contribution to the phenotype so we think that this is kind of our guiding picture we think that in between the genotypic situation and the phenotype be able to place the proteome and we refer to the proteome in a specific instance or not just as a proteome as a as a artificial or hypothetical ensemble of all the human proteins but we'd be therefore introduced the term prototype which is the acute instance of proteome in the in a particular patient or cell or tissue so we would think that somehow or another through mechanisms we like to learn genotypic variants affect the proteome in terms the proteome in terms of the abundance of the proteins but also in the way they're organized in the cell and we would assume that the prototype irradiate the instance of this proteome how it is present and how it is organized and composed affect the biochemical reactions which eventually determine the phenotype so our guiding kind of guiding light or magnetic north is you would like to understand the prototype be able to measure it reproducibly in large number examples and then do qualitative analysis between here and here and between here and here this is kind of what we attempt to achieve so now the proteome is is very complicated we know of course that the human genome contains roughly 20,000 protein coding genes and before you can start to really do differential analysis of of doing prototype measurement you would like to know what is actually the protein well how it is how it is is it composed which genes are being translated and in what form and and I've just made now a few comments to to this to this heart because we use the map of the proteome a mass spectrometric map for further developing techniques which can do rapidly measurements as it was done in genomics where initially it took a long time to generate the first draft of the proteome but the information in this draft was useful to then develop tools to do rapidly measuring so we'd like to basically follow here so the question is what is the proteome and about two years ago on a lesson created all about a given article two papers came out in nature which which which claimed they both have the draft map of the human proteome draft of the human proteome and they came out back-to-back in nature and they claimed that they had basically exhaustively mapped out the proteome and in and these papers were then subject to relatively big discussions and it's important it's important to to mention that here because I think it also relates actually to the work of Alex in his risk image which whose work prevented or who should have prevented the appearance of these papers in their form as they were published so what is what is what is Howard is data channeling these data were generated by taking a bite right here tip of tissue samples or human human cell samples and do massive sequencing by shot a shotgun methods a discovery method to basically establish an inventory of the proteins in a Cell this is done by breaking down the proteins into peptides and then measuring a peptides in a mass spectrometer and then these proteins or these peptides are assembled in by algorithms into proteins which one third from the data but never directly measured so this is this is data situation that the community community has been generating to map out the human proteome this is courtesy of Eric dojo is in Seattle who maintains a one of these databases which collects the worldwide knowledge of proteins data that have been generated by a mass spectrometer so you can see that the multiple terabytes of data have been generated hundreds of millions of spectra have been searched and this has been growing rapidly over time and and so all this data will collected search and then from this data the number of proteins that were represented by this protein tidav peptides and data is inferred the interesting thing is that this is the growth of peptides this is a kind of a worldwide effort this is the growth of proteins and we can see that the protein level information that that we this is basically the indeed of the ensemble of proteins that mask dramatis worldwide to this day have identified human proteins by objective criteria by tools that were developed by Aleksey it's about 14 and a half knots each one of his papers claimed in the range of 18,000 identified proteins so what we learned here that and this is data here include actually the data sets were published by these two papers and so what we concluded here and there's been a lot of discussion is that the least current methods that we are using in the in the in the laboratories of proteomics researchers the number of proteins that are is coverable the will will roughly cap out at 14 and half thousand proteins if you want to find the other proteins the missing maybe four or five thousand proteins we need to develop some other strategy because they clearly have not come up in test taking another million or so spectra by doing the same thing and then adding it to this massive database will not likely discover a large number of problems so we think that M this is kind of consensus from a lot of reanalysis of this data but currently about fourteen and a half thousand worth of proteins are waste present methods discoverable with thousands of runs of a massive amount investment so this is the situation we have essentially at a map of the human proteome it is not complete but it is very robust and and and efforts led by actually by Gil at Hoopoe he leads an international consortium who should find evidence for the other proteins which are not yet in there but the situation is that we've kept out here and this is a large fraction of the proteome and it's be confidently identified so now what I think and this is our next kind of the goal to work along these lines we think that the real revolution proteomics will now also come as it came in genomics after the draft was established and scientists could start working with this graph to do the measurement on multiple samples we think that this will also now happen in the proteomics if you can us distract here the information of this reliable trap and really start to be measuring proteomes on the many different conditions potentially thousands of them and do it reliably and fast and without without a law of mistakes so basically what I'm saying here at the end of this introduction is we have been focusing if you look at this table here on this graph schematically we look at the data matrix of proteomes and so far we have been mostly been focusing on taking one of the few samples basically generally very very deep inventories and I just described how hardly coke can lift up the basically reached a saturation at the high level but not completeness and I think the challenge now is to take there to take this matrix and extend it over here into the sample number but we can generate data matrices where proteins ideally all of the discoverable proteins are quantified precisely over a large number of samples so for those who work on transcripts and in genomics this is an old hat this has been possible to achieve for quite a long time through methods initially off of operates expression arrays and now called next-generation sequencing for us who work in protein world this is this is very new and has been very challenging to do and I now describe a method which can achieve this to a large extent it cannot achieve it to all the discoverable proteins so it has a limit in terms of which protein it sees or how many but those proteins that it sees and measures it measures highly reproducible so we can generate such data matrices not large number of samples fast with the analyte number of in the few thousand not not 15,000 but put a few thousand depending on the sample between babies three to six thousand products okay now I'd like to show how we do this and under the conceptual difference to the discovery method that was implemented here this is based on an on a different strategy than the discovery of strategy we basically do not say we won't we have a sample and won't discover which proteins are in the sample we have a sample and we want to ask we want to ask history see is the evidence for the presence of a specific protein in that in example yield is a targeting approach we have preconceived notions what we want to measure and what we what we want to measure we basically do a hypothesis testing is this protein present or not if it's person PC signal it's not present you do not see C so there's no ambiguity there but this is this actually declared method of year in that in B well 2013 and M there's several inflamation implementations of that is basic the mass spectrometric implementation of a Western blot where you do the same question you say I have a sample I want to ask is a protein present and how much of it accepted here is a mass spectrometer and the idea is to quantify precisely specific proteins using prior information and the original implementation was a technique called selected reaction monitoring that works very well it's robust and works for maybe up to hundred the proteins per sample and then we developed a different technique which is doing the same thing but massively parallel or thousands of products so this technique are now going to explain this is how the proteome looks to a mask company so we we basically see we take the proteins that are extracted from a sample and we digest them into peptides and no one actually knows well how many peptides are being generated there's estimate from like 10 million to potentially hundred million different peptides that are being generated from a proteome but we don't know really for sure why is it so many because many proteins are highly modified they're they're very variable in splices we don't actually know how a proteome looks like in its composition but what about it's 10 or 100 million it's a very large number compared to what the mass spectrometer can sample but typically this is so the actors here a retention tannins chromatographic dimension where peptides are separated and then injected into the mass spectrometer and this is the mass of these peptides that are being detected by the mass spectrum so within this window that we can specify most of the peptides that gene generated from a proteome will be present and the task is now who in some way or another take all these peptides one after the other or in some other way and determine their sequence and this is done by selecting the pepper and fragmenting it into fragments the peptide alone is not smashed long is not have enough information to infer the sequence of the peptide so how the weighting system classically been done is the math become the picked one after the other of these recursive these peptide ions fragmented it and recorded the fragment inspector that led to discovery of these about 15,000 proteins that are constituting the human proteome map but you cannot it is not fast enough in contrast to next-generation sequencing which is massively parallel the mass spectrometer is not fast enough to go after all these peptides in the real time but always missed some example basically out of a very large pool so the advance that was realized with this technique this was technique is to basically break down this whole range where the peptide solute into in roughly but it doesn't really matter to break it down into tens of thousands of little pixels which which I'll have the dimensions of retention time about hundred millisecond and twenty or so mass units in each one of these pixels there may be multiple of peptide precursors present and then we don't care so we don't try to do one after the other we select everyone who is in a pixel and concurrently fragment so within the duration of about an hour or so about ten tens of thousands of these pixels are isolated or the peptides in these pixels the concurrently fragmented and then recorded in a computer to generate basically a complete fragment ion map of the older peptides were present in this sample but the but but ordered in the way in which pixel they arrived they were they arrived and so this is what we obtain your pain a complete ms/ms fragment I'll map for all the analytes in a single sample injection and this takes now about two hours to to accomplish then the price we pay is that the spec that are being generated fragment line spectra as shown here are complicated and they're composed of the fragment ions of multiple precursors because the everything that in this pixel is isolated and fragmented and recorded concurrently so all the search engines that we were using and used to they always assume that the fragment iron spectrum is derived from one isolated precursor and so they don't work here anymore but I show this here by color mode is this is the green month might be fragments from one from one peptide the red ones from another peptide and they were concurrently pregnant so we we cannot easily be convolute that but we found a trick and that to do this and we use prior information to find basically in these spectra those fragments that that are provide evidence for the presence of a particular peptide we do this by using the retention tinier dimension and we basically extract from this very complicated fragmented map these signals for the fragments that are corresponding to one peptide we need to know what these are we basically first need to generate a a library of spectral assays of arrival expect fragment line spectra for each peptide that we want to measure and this information we use them to go into this complicated map and say this one this one this one besides the color correspond to fragment ions that are correlated on peptide they allude precisely together which is called a precondition that they are from the same peptide and there is a number of other features that are being computed in a in a in a composite function to the in tell us that indeed in this complicated spectrum this evidence for the person of the green peptide maybe the other peptide as well so this is what we do basically it'll say it again use a reference library the precondition the use a number of peptides alone to search we can go into this dataset we generate this peak group which uniquely identify a peptide and we then say if this big group has certain properties and certain probability value compared to noise it say we have quantified and they identify this peptide in the sample this is the approach in taking and I said we need his reference library and Alexey recently developed a tool which build these libraries kind of on the fly it's it's a ingenious way to support this type of data analysis we in our way of doing it we had first generate the library a priori you can induce this and Aleksey basically has a tool now that generates these libraries right right on the fly from the data that are being acquired here and it's sports mode which presents in advance in the field so will be generated some libraries so we have complete library for the VCA for coli for Mycobacterium tuberculosis or complete means that this every protein has minimally one ideally several peptides for which there is spent corresponding fragment iron spectra available and also for more than ten thousand human proteins so aditya at the SA library generation issue is solved either by the availability of such libraries online to people can use also Alexis Alexis tool dia umpire to okay so to complete this complete this technical part we could go readily quickly with within within a day actually working day you can call from biopsy need will need by a few level tissues or from in the range of fifty two hundred thousand cells through a system that reproducibly and quite quickly generate peptide out of these tissues and cells into the mask promoter to generate what a file from each sample and it includes we we need a working day with about 8 hours for example to pile and decayed or this now for about 20 samples a day in an Indonesian and so this is very very our technical well course depends the instrument that's used so this the internet are used it here are good to cheap yes and this one is fairly expensive this initiative was developed and all its you know go from ax instruments and now it also runs in thermo instruments which are more robust and they cost in the range of us probably I would say six seven hundred thousand US dollars and then each run actually is very cheap because you don't need enzymes except a bit of trippingly we don't need like a loom in our type stuff you don't need any kids so I would say it's less than hum talk but that's just it just again yeah yeah ok and then so the that is before we can measure now from such a file about forty thousand peptides and corresponding to maybe five thousand proteins and that is gonna keep up now that different manufacturers try to compete to make the instruments better ok so this is the situation our time but we would add technically we I show this before and now we would like to exploit its prototyped and ask specific questions which I'm now getting to so we basically say we would we do serial measurements on let's say genetically perturbed or otherwise occurred environmentally pure prototype we do this long enough it enough proteins in this sufficient position we can use computational tools to learn some new biology dusty that's the quest for the state starting position now like to get to three vignettes I won't get to this one more time but I would should have deleted that so like ask does the genotype control the quantitative prototype this seems like a simple question but it is actually an important one if you want to say will we have my patient here what is defect on T on to this protein landscape people should know what are the rules is for instance it is how our howard quantitative changes that we have vertically related to genetic changes that's one question the second one we would ask how does the genomic perturbation affect the croc attack on our Tavia wild-type wrote the a wild-type genotype and we now add four instruments on what is the effect on the proteome if this added chromosome is dropping or if we delete a region of the chromosome what is the effect it is simply that the proteins encoded on that chromosome are now no longer present or double but much about this defect and the third question is how this is a she's maintained a constant phenotype in view of genotypic variable so if you have genotypic variable we are all very variable I mean from a phenotypic side but we are all relatively similar you know typically to how is this maintained these are three vignettes I'd like to address now the first one is research question is the genotype control the quantitative prototype and so the experiment we did here the study where we used is technique such as described both to do product called protein quantitative trait analysis in a genetic reference train companion bio gold plain these terms in second but experimental approach was use targeted mass spectrometry of protein component of metabolic system yeast and to relate the abundance of these proteins back to genetic variants in a reference population so how did protein quantitative trait loads I studies aim at collating the protein abundance with genetic variation you can do this if you can measure the quantity of specific proteins precisely and if we know the inner inner panel of of conditions for instance strains what they're Jenelle Jenelle genomic makeup easily can then link the allele a particular lil to the abundance of a protein and and that then it habit is this relation and it is with a resource that was generated by Rachael brem and Leonid Krug lack and these are yeast strengths which are derived from for mating who strains a laboratory strain in the wild had strained and out of this may think they generated hundred brought Chinese strain seconds and each one of these segments is of course a clone I mean it's a strain that is better in cells in there are genetically identical and we know that from each one of these strengths the genome is composed of the up segments of the two parental strains here but a better proportion in different relationships through through through through the combination so this hundred strains are like brothers and sisters we have multiple cells of course they can be grown indefinitely and weakened village genotype is known and it has limited variability because they all derived from is to parents so this is work now I'm showing this is work from all of the complementary of the postdoc in our group and deeply studied so we basically talk see selected the most variable proteins for measuring doing some initial measurements which she didand enrichment or goal in region from metabolic protein he wanted to have some metabolic map and she basically used this pathway tool and the variable proteins to generate a a pathways of proteins that are metabolic that constitute part of the metabolic system and each one of these protein she measured and quantified across each one of these strengths but she ended up with 50 proteins it's a relatively small scale study if now that you've now done it in in three thousand proteins this Voss technique but they don't have the data fully analyzed it I talked about the principle here is read a small scale but the principle is exactly the same we have basically 50 proteins that constitute a coherent system metabolic system a quantified across 96 strengths and this is what we get as the rotate but it is now this matrix I talked about the beginning is our these are the strengths these are the 50 protein that you can see which is normal default proteome that virtually all the pixels here are complete so deserve a few missing values the state are very good for quantity quantitative analysis because we have we have almost a complete data set about 30 proteins validated were validated to beyond the control of at least one gene the dissolve two tails and a number of genomic regions very long so so now what came out this is um now the result of the analysis of this data matrix by trying to now correlate the abundance of these proteins back to the genomic loss and we see we basically star and a network start up here which is very interesting to observe so if the network is composed following the Y the yellow dots here are proteins which are quantified in this study across strength the blue triangles are the genetic loci which control the abundance of these products this comes out from the association study and then the edges here mean that this triangle controlled abundance of the spot it brought if there is the thick connections here this means this protein to physically interact to form a complex that carries out a cup a a particular function so this is a basically function model so today what what we see we see a reasonably even though this is relatively few elements we see already a fairly complicated network emerging busy friend that some proteins are are under the control of several both side we see low side that control mode proteins in their abundance and BC that the members of a complex here which presumably cuddle it form a performed collectively a catalytic function that each one has its own regulatory system in the in the genome is this was surprised to do it to me but I thought if proteins are associated form a complex they might also be under common control like for instance we learn in the operon system in in in Newton prokaryotes and this distinctively does not seem to be the case so this is basically what come out what came out and I want to ask you first does the genotype control the quantitative prototype it certainly does that we can be established is to target mass spectrometry we learned that the parental strains are quiet independent genetic variations is our key the various alleles that affect the levels of proteins from the same module or pathway so it's distinctively different from an operon type model in bacteria we see that selective pressure favors the acquisition of polymorphisms that maintain the stoichiometry of complexes pathways is the theme which I come back over and over again that I think we now learn that is not only the abundance of proteins but also their motive arity how they interact in in three dimensions with each other it turns out to be a theme that come forward and we learned that genetic variability effects metabolic you had it now made an earlier studies in mice basically an analogous breeding breeding cohort has been generated by a international coating called the ext Mouse genetic consortium and we did their similar measurement I just described in these and it means exactly the same result so conceptually nice and and and yeast him to do things in in a similar way control the prototype in similar way and apparently different proteins second question now I wanted to dress is how there's a genomic perturbation effect prototype so the the we're now seeing that there is clearly a linkage between genomics and the quantitation qualitative expression to proteins I would like to see if he pecker that what happens if he drop in basically Hand Grenade what happens at the prototype and the experiment here we it goes back over on a unique opportunity we have to work with which is which is which is cells from twins where one twin is a vial pipe basically normal karyotype and the other has an extra chromosome thank you one so he is yes it's Down syndrome this is extremely rare event and through Giuliana's internal raucous engineer we got access to B cells from this on these two individuals and he saw it basically everything is genetically identical except one has an extra chromosome so the experimental approach is to do proteomic measurement on the cobalt competition turnover of fibroblast from this twin pair where one is sonic and the other one is not so this is basically in a graphical abstracts the day DNA for chromosome 21 is one and a half one is the question is the RNA one and a half one is it just constricted to chromosome 21 transcripts and proteins all same proteins and so we basically have fibroblast from individuals happy to the protein measurements about 4000 proteins were quantified in its content in this experiment so here are now some results we now have the normal state the Islamic state protein versus RNA and we see a component correlation which is what we typically get 14.5 protein in steady state does not this is not very distinguishable but when we look at the total change induced I hope all change of protein the reverse of the ratio of mRNA to mRNA from the two individuals flowed into the two individuals the correlation essentially falls apart so they both maintain a landscape of transcripts the proteins which has certain degree of correlation is quite common in enhance been observed many times when we look at how how do they how do they react at the pro protein a transcript level to the addition of an addition chromosome 21 in this case is virginal college listening is an observation and then we also asked what is since this means that something must happen at the transcription of post transcription level we wanted to see can we distinguish whether this lack of correlation could be explained the princes added protein turnover increased protein turnover increased protein synthesis or combinational so Yun Chang Liu who is the postdoc who works on this it has pulsed silac experiment which basically means he makes all protein light and then at a particular time he does a pulse chase with a heavy amino acid and then he is incorporated over various times and then at various time point of this pulse experiment we heat harvest cells and we can measure the abundance of the old protein delight emergence of the heavy protein that newly synthesized in this period and the ratio of the to total sum which tells us whether there is more degradation than more synthesis so basically that's what we look at every time point he can see thee for every protein that quantifies the the old protein emerges of the new protein and potential decrease of the old protein which gives the dig an indication of degradation so there is formalism that describes this we can basically measure protein turnover on a large number of proteins okay now we BM we post here the data on the transcript level the protein level and the turnover level and so these are three layers of information and segregated into into chromosomes so we see here at the first type we immediately see that dropping in this edition chromosome does not just affect the proteins in transcript from this chromosome that has a broad effect on the overall proteome a transcriptome that is measured we also see that here chromatin 20 language is a small chromosome has a very distinctive upregulation in general of the genes recorded it died and an mRNA level is known on down the recall of the other chromosome field had altered and down regulated here not heal but it up regulated and this is not true for the probe for the protein protein has already a buffered kind of fixed response to leaner to the add addition of his extra chromosome and then we look at the protein trend and the trend at turnover rate we see basically no difference between 21 and his other comical okay so this is just a graphical representation over here of the data basically these are the proteins encoded by chromosome 21 here the transcript all change protein for change and degradation rate for change and we see clearly difference so there's different regulation at the various level okay now we asked so this is just go a notation we see the certain certain groups of proteins are up increasingly and down in time compared to some of these in a minute but now the interesting thing we see is that B then so we're asking the question why are certain proteins behaving differently from others is it if it's not transcriptional it must be something post transcription so d the hypothesis was generated that product that the level of protein at the level of protein complexes buffering is occurring because it can imagine if you have a complex content singing of three or four proteins from other chromosomes and then all the sudden II have an extra dose of a protein from chromosome 21 we would this protein has nowhere to go it will be like with the decorative degrade and so the buff the complex actually acts as a buffer for emerging excess proteins in that it has no place to go you'll be degraded so we tested this lipid data that I just showed so here this graph is the local changed is a miracle normal or called RNA protein and degradation and we have the proteins that are within a complex which is derived from we don't dip into this measurement is derived from quorum or databases which which describe protein complexes and proteins which are not been found to be in a complex which does not necessarily mean that they're not in a complex impairs not been found so the amplitude that we see nightly actually bigger in the reality the definite information you have so we see here that for the chromosome 21 proteins at the transcript level is a slight different not statistically very significant we see is like different that there is the that the transcript in proteins within complex is slightly hide and outside but it's not really significant no for other proteins and all the chromosome the transcript there we'll see nothing and we see a distinctive statistically significant difference for the chromosome 20 known proteins so the ones which are within the complex are not really changing at the ones which are outside of a complex they're changing most more strongly significantly different elicits is ports the hypothesis that be that the complex act at the bottom and before proteins which are not from which were from other chromosomes we don't see this difficult difference and then we see the opposite here this degradation proteins within a complex are are relatively stabilized and the proteins which are outside of the complex are our degree in this indeed clean samples so this supports the notion that at the level of the protein complex these proteins are stabilized in their abundance and this is a poet inscription event and and so this is one way how to sell bottles genomic variants by Prince introduction of additional genes I wit know wanted to learn a bit more deeply we we broke this the regulated protein town in quintiles so these are the strongly up not so strongly opened and slow me down like a lot until he asked how do these proteins hate what are they doing and to our there is the concurrent activities or regulation occurring a lot defined is an interesting binding I probably it's that the deadly kind organelles the city degradation in in Down syndrome so we find the proteins are don't only act coordinately incomplete complexes but also distinctively in terms of um of organelles so maybe and so this year these are the proteins proteins are specifically modeling the pathetic organelles it's all the organelles that we talked about and this is the mRNA protein degradation synthesis rate are calculated out of this data and we see the different organelles proteins in different organelles have difficult patterns so friends dis pattern here lysosome would say the mRNA between the trisomy case and the wild-type is not really changed the protein is the protein level is down degradation level is up so that means a certain amount of protein look present is now more strongly degraded for this liquid call has degraded degradation regulate the other one is is it's here the the transcripts are up increasingly the protein is down in trisomy this this is this is number of proteins from this organelle is not just one protein and then and then this is achieved by a by increasing degradation rate and decrease in the synthesis so complex coordinated expression control of proteins were child which is unique or at least certain patterns in theory of which occurred and article civic participation else so we have degradation regulated decoration buffering not regulated inflation regulated and so on to this we can get these patterns a name and these patterns apply preferably two specific organelles all specific regulatory complexes like print arrival so we we wanted to ask here thus does the addition of a chromosome basically a massive perturbation of the genotype lead to changes in the in the proteome prototype and how is this transmitted into the prototype is it simply a linear translation or from the changes or occur transcriptionally or other other defects at work and weak indistinctively see a lot of other effects distinctively see that the protein abundance and the transcript abundance do not correlate well that there is protein abundance measurement above which changes both way that level of complexes and at the level of horton else because organelles apparently quiet through mechanism if you don't know different patterns of how they is react the proteins in his order is how they react to the addition of these extra chromosome so down syndrome' turn noted here this points down big one important thing we learned again that's exactly what is already in the first example is that though that computation to proteomic buffering effects or is at least in part mediated the protein complex is namely the higher order structure of the of the trophy exactly what we already saw in the east example a quarter mile later go very fast hopefully not too fast to make real understand it like lines the third question and that is how this species maintain a constant spin a pipe in the europe genotype phenotypic variability so the this is of course also an important in view of that a in drought-hit the life every person probably in every cell acquires the number of mutations which are usually not detriment some of course are but we see in Kentish nanak studies that there's a huge amount of variation is acquired and maybe does not really do a whole lot so it's it's somehow buffered and we wanted to ask how is to tell chief Eden so this study design this is work in Drosophila there'll be work with strains which have been caught and then made into lines by Mikhail and covered the published and accessible within measure a phenotype is the being size of a highly highly selected phenomenon of phenotype ending implies because to being cannot quite properly heal of course died on so his ring sizes in these strains even though they're highly variable remarkably variable one at 40 bases snipped of enormous genomic variability they managed to through selection to maintain a phenotype in this from point of view of being size and shape and aerodynamics that the cell actually that the fly can fly it is a remarkable achievement and so we wanted to measure the prototype of this being disc is the precursor of the thing and correlate this prototype it is highly selective so as the phenotype which is highly selected and and not very rare this is the the outline is a discs and they lead to wings and these wings can have various sizes shapes and be measured so this is typical polish is a graduate student she did morphometric analysis of these wings she selected some strains from these flights which halves moldings and some clients that have big wings these are not individual flights of course is our strength so they were identical we need multiple animals the growth is largely detected out his wings his wing discs and elevated protein measurements on these wings bring proteins and we could identify holes the tent out peptides corresponding to about two thousand proteins across the width across the lipid strains and these are the proteins that we see again a matrix adversely no missing locks but we see already that the big Ling molding in orange burst so they didn't they're sufficiently close together that they do not close to in big ring flies in small device this is remarkable given the knowledge genomic variability and the importance of the phenotype they're really not they don't cost it in in an obvious way when we just look at the problems however there is information in the proteins for sure is he say a correlational now basically we look for splitted a signal in noise and we see that the mainly correlate when we correlate proteins abandon system replicate value replicate measurement and this is the blue distribution we have a very very tight correlation that means the data is very high-quality connotative Li and these are data protein measuring from non replicate let's say between pickling and smoldering flies so there clearly this distribution of correlation factors is different from the replicates of amines as biological signal in the data set even though it is not sufficient to lead to very distinctly Pickering whose molding cost the sample are highly related but the dispels this biological signal detected in the data set and now we worked with this biological signal to see how mechanistically lured by chemically can be learn something about how this big ring size small scientist is maintained and what the the not operating mechanisms might be and what the mechanisms are that actually generate a responsible for the big ring and this molding opinion so this is basically a correlation analysis to find those proteins that segregate with so this each dot here is the protein is each protein has : number and we see proteins that are correlated with large ring size up here wrote in modeling in size down there so this is basically just finally those proteins which show in their abundance pattern a correlation with the Bing sighs and then we group these proteins together and we see this is basically done by looking for pairwise correlations of abundance across all these proteins that were measured and have significant differences and we see then that proudly we project and escalations on the string network which is a network of protein interactions that we see a number of the blue is the basic string network these are modules of proteins which had very strong correlation with either Brickley big being or molding phenotype and so he sees the cost of the merger that others are not really affected so the classes that emerge are mitotic cell cycle glycolysis chromosomal protein and mitochondrial respiration those proteasome and ribosome eventually came out that these are basically groups of proteins which are involved in a specific biological chemical function which are coordinately affected or correlated in their expression is a small or a large ring size so what we basically like to say here is we have the world of snips which is extremely rich in these flies and one is 14 nucleotide is named these snips are then translated into quartz in genes into it into RNA proteins in the phenotype we've measured this phenotype and we found the protein which correlate with its phenotype we also did the other way around a plant talked about that we have also a lot of protein cutie else which relates acidic snipped sequence variants who bounds of the protein I don't talk about that talk about its ecology and what we find is that the up at the end is that what determines a large or a small in size is not necessarily the the regulatory proteins which which are we all know like the whole or hip or signaling or insulin signaling their thirdly according to drive the big engines but what really matters is metabolism and we see a shift from the big from the from the being sighs look glucose metabolism it up and at night the respiration is down so this is basically what we also seen cancerous the Warburg effect and so this is one of the big determines that that shifts the cell from other wings from the small ring size to large wingzza we also see one significant change in chromatin this is a factor which affect histone sleep in fact lead to depression iStent it is down it in the east on the down and that generates more loose chromatin and that also relates with the Bing Bing science and the third part is a regulatory supporting life the way basically conclude is there is enormous variability at the level of the genome this is the system this is transcribed and transmitted translated into a protein landscape into a prototype this prototype is maintained extremely stable probably through mechanisms that we encountered before in this come in the chromosome 21 situation that is that be very built it'll here to here it offered and then the date but it is not completely buffered because otherwise the phenotypes would be identical if they are not we see that the pigment that the big determinants that associate with being science are or determining sites are shipped in metabolisms that very very slight shift is like 10% shift and low-end chromatin organization so very very old very very broad processes and that we think now the regulatory process is like insulin signaling its own effects eventually is is small shift is on small shifts it actually catalyze the changes so this is the summary of this art which is dead and then you like initiative paper message we all know that neglecting nature sequencing is it's covering enormous genomic variability fantastic resource to work with the relationship between genotypic and phenotypic variability is largely unknown you cannot really have program yet that says okay this mutation occurred in this backdrop this is what happening the we pursued the idea that the more the modular prototype is kind of translated rosetta stone Lincoln genotype to phenotype because this is very a chemical reactions happen in the last example I showed that that very large biochemical functions like metabolism have a strong the phenotypic determinant and the cell compute changes of perturbation the genome in complex nonlinear ways which we now start on covered by these multi-layered measurements and what we also find in each one of these examples is that the context matters only talk about complexes modules is clearly cell at the proteome level that the protein itself is important but equally important is its insulation to how it is embedded in the proteome as a whole and let me tell you B it's a growth area and the important growth area for proteomics really understand how this complex is formed how they're changing in after some mutation and what they what the functions are untrue consequences are Shelton but I try to show is that there's no technologies allows to precisely measure the prototype not completely but failed extent if we live at least a few thousand products so finally like to acknowledge my colleagues then grant question this post development was the work would be little bit sheer Pedro Navarro and be paid to maybe say X now worked on by Ben Collins in Yun Chang Liu and then this doctor was developed by homelessness george Ottenberg to support his analysis now i also mentioned that alexei has written software that works on this type of data the being fly sized project was code picked up hero Okada Alex apart from our group together with built on against happen or by geneticist and espl project noted by our coffee is helping date announced tomorrow I'm at you claim oh these are on the dress by Ephraim good cologne and recently painted on projected motive in Yan Cheng Liu and collaboration is the annuitant muraki's in Geneva I'd like to thank you for your attention and something like a three yard long monolithic column two separate peptides from eco lying claimed he could see every protein from the e coli genome as somebody actually tried to use some type of new technology separation technology to try to up that number I mean is there limitation because of the overlap that you see between peptides so without seeing the data I will prepare it skeptical that we saw all proteins simply because no one has ever seen all proteins that are encoded in a genome in any case' and the in any situation identity data that I showed this is the accumulation of of virtually tens of thousands of LC ms/ms and people have used various cones of various lengths thickness and so on so it just seems that roughly with the techniques we all use in terms of how we extract coating you are we tied Cheston a Halloween aspect and that's roughly there we get that's not to say that the others do not exist but I think one will need to do something more innovative than just taking a longer column and think that then all the sudden the number goes up and I think this is what kills the project killed is leading and Hoopoe is trying to do it tries to say okay what are the proteins that are missing it should be expressed and then try to find them by for instance I'd Hargitay and but I think whenever when someone claims that they make a gigantic jump and from like 14 to 18 thousand in this business is quite it again jump then it most likely explanation is unless they do something really different the most likely explanation be that it's it's it's it's of people are finding sorry these very low abundance long non-coding RNAs are actually encoding proteins and I wondered if you had a thought about that the second question is a very self-serving John Hardy and I many years ago not only were involved in cloning at the amyloid precursor protein on chromosome 21 but you know people that have trisomy get alzheimers disease and we speculated that it was a lysosomal phenomenon and so I was interested to see you started out there I know they were all different but you had the different organelles across there and I wondered if there wasn't some kind of basic defect in the lysosome in those identical twins so it's really two very different questions right I mean have you thought about looking at the products of long non-coding rnas so they're proteins the we haven't all of them I'm sure someone has influenced it it would be it would be of course a reasonable effort to do that I mean maybe that's something that kill could comment on whether someone in the Hoopoe proteome project is doing but it's a reasonable it's a reasonable thing to do and technically it's a feasible you just have to say we allow certain coding coding frames to be part of this search space then you would either find them or not and I of course have to be careful with how you feel today how you feel to this data but from a technical point if it's nothing degree so the abundance issue is not really a big issue anymore I think we can say that so so it's the some sensible fractionation one can go down to very low level to pocket the issue which we still have trouble is solubility like membrane protein they are largely on to represent it in this and some proteins which only occur in very specific tissues like this is a big discussion about the nasal epithelium like olfactory receptor which of course will be hard to find because you normally do oh yeah so if we don't have any functional evidence which is followed what what the proteins are doing it only we came from the total protein measurements to these organelles and clearly a lot is going on in the lysis of how this functionally translates into items I don't know but one could of course measure them so it really is for comment on a long non-coding RNA another RNA is smaller works and so forth there are a lot of claims for production and it is quite possible that some of these rnase do get translated into protein sequences however those that have been reported especially through articles that he cited at earlier in the lecture have been examined carefully by many other labs now in the datasets provided by those two groups and nearly every single one was a false positive including even a perfect match to porcine trypsin how low that's easily explained and Alexei and I actually am at a workshop with our t32 proteomics informatics graduate students here and many of them are pretty expert identifying these kinds of flaws and datasets it's it's a matching challenge you know even if you have good spectrum you may be incorrect in the protein sequence it could be that the if three for example if the other I solution instead of a leucine that is indistinguishable for sure if you had a methylated arginine you may have an isobaric post-translational modification indistinguishable if you have a single amino acid substitution you may now have a peptide from a very common protein we putted thousands or tens of thousands of times like zero transparent called to be novel or to match to an blank RNA or to a pseudo gene and it's all cost positive as far as we can ascertain let's say we have far more probable explanations for the peptide sequence identified we can have an excellent peptide sequence and if the match to the reference proteome is flawed the reference is incomplete which it truly is for many variants or sometimes the reference podium itself is based upon a rare variant for that gene product and you're measuring a common version so all these examples were found in many many cases okay just checking David and your students got questions I hope oh great talk so actually I have a question about the technique for quantitative proteomics so let's say we have some samples we know the genotype of samples we know some sample carry lung synonymous mutations so we want to qualify some parting expression and use a mass spectrometer so one what appears at best approach to the qualified pose Y type and mutant and proteins simultaneously okay so the one on the ill is mutated is not a lie like you know maybe a substitution well so you really want to get a statement of that you will need to you will need to hit on the peptide that carries the mutation otherwise you would know no information so I would recommend that we would do maybe a few proteins I would set up a targeting I'll say bias oh and which is most besides measurement for a number of peptides some are shared between the two forms tell one's a different and you could then you could actually figure it out quite precisely you would you would have to take into consideration that the peptide intensity signal from the mutated form and the vial that form you cannot just take the signal intent eight it's that much you would have to have an appropriate an appropriate reference peptide in in this case but that's you could do it here so I'm with a heavy label pepperjack it will be quite a straightforward measurement and if you wanted to do this for large numbers of proteins large genomic and I would suggest is both smart method or derivatives thereof which which to say measurement and are not much but in every case in would one you would want to add to be really cool if you just if it just look for forum but the differences all change between samples and then we don't add these references because we're not interested in the absolute mountain but injected that's actually quite robust hope everybody realized this is a dramatic development in proteomics they're targeted proteomics approaches really has been worldwide leader in this development of the proteome wide selected reaction monitoring methods and tools and databases spectral libraries synthetic peptides now available through purchase and now the swath NS which is data independent that contrasts with a data dependent method maybe you say a few words about these critical new methods so Bieber and as for almost time and most most projects in this in this field are it's called shotgun or data dependent method you basically acquire a signal for it put intact peptide masses there's usually many such signals present concurrently and become who makes a decision which one who will select for occupation does it very fast automatically and this several per second but Pathak achievement from the engineers but if there's more peptides in such as can and can be sequenced and many are not see be we made wonders actually when all our piccata she came to our lab and we were interested in this question to what extent by just doing speeding up this data dependent analysis to what extent Lee would likely be able to eventually sequence every peptide in this proteome after that shot at the beginning and so like like what NGS is doing where we can say where we sequence in every run with it or not reads everything many times over and so what would what she did she took some highly purified proteins and and digests it in as we normally would and she then took the best mass spectrometry had a time and and simply sequence as many peptides that we could find and what we found was an interesting what was interesting at all to some extent daunting when you just apply to a sequence of a protein to ruled how trypsin clips he will normally get in the range maybe 5 to 20 peptides that Krypton should generate when it if everything was normal protein is there every side click but she found at least an order of magnitude higher number of or peptides from these highly purified proteins then I look at what these are these are all kinds of complications 15 make mistake it clicked the wrong side the peptide hasn't has oxidized methania me all kinds of little things and then we look what does the what the bonus distribution of this product which all came from this protein and it is so that usually there's like five or so which fairly highly intensity product and then the very very long tail of low intensity product which is probably long hence 100 on thousands of the product the high-intensity product so if your analytical chemist and you want to do one of your proteins it doesn't matter it's just noise but if you have a proteome there you have 10,000 proteins which differ in their abundance by five or six or seven orders of magnitude the proteolytic background from the hive express proteins gigantic signal compared to the main guys from local so then that's going to conclude that by speeding up the machine would be very daunting to get to a point where it can eat through the whole it's all background that's why we thought of using this targeting fortunately it's GI a data independent techniques are now simply imaging the whole the whole space within the dips they can do and it's about five orders of magnitude now and in there the acquisition is basically so that's I think that's a great advantage to measure the things you know you want to characterize in terms of the postulated or demonstrated network and pathways of the biological system instead of measuring over and over again primarily just the most abundant proteins in their peptides he'll go on to discover new proteins in splice went into on his take is targeting techniques do not get you nan cannot find a new protein but they can for these serial measurements they're very powerful they don't they only operate in a space that's known so that they I think they the two branches discovery and this V measurement branch they they both are useful and eventually they will converge very nice talk thank you for all the information I just curious about with this swath MS technique I I think you were referring mainly to label-free quantitation methods and I'm wondering if you could comment on on how reliable that is and and so so you know there's a lot of steps you go through from the from the you know the the solubalization digestion all of those sort of things and then the mass you know differential between sample you know high-end suppression possible how how how how good is the quantitation across the board I you know in this in this label-free approach and and what what can you do as far as quality control to you know to assess you know how reliable your quantitation is is being obviously it works well because you've shown some really great correlations with the genome I'm just curious you know when you get down to looking at individual proteins you know how do you know that the the quantitation is working like you think it should thank you so basically the when you have these large data set is large matrices then you can do what will actually Jordan monograph you can basically correlate abundances between various samples we also do all this repeat site so so we when we start the project which has several hundred or several dozen samples we do a number of biological repeat meaning starting with that you will say from the very beginning from the biological sample let's say five tags and then lead we take from some of these samples be to say five injections so we basically try to establish repeats and the tell us how well was the everything how house wholly producible things downstream from the mass spectrometry injections and downstream from the isolation this this gives the stand a guide a guide line or a benchmark to say if signals in work that are not just various but basically consistent from other samples when you have biologically different samples that this have biological mean that's that would be basically work with repeats we do not usually do although sometimes you do duplicate the very sample that some competitors do this assessment of the variability in one dataset then assume that the other sample in that sample that now for about is it better to in to add isotope label peptides or some form of labeling versus non non labeling I used to be very strongly in the labeling camp now I'm the only indie indie label three can if wrong compares very similar samples because it's also fewer fewer steps they come up in state of the like it's AI lactate or actually labeling data on this work also it just gobbles up the spectrum oh yeah I would say that in it of course depends on the effort that you do and how much fractionation but I would I would think that Sabine actually looked at by people who like Eric when you and you when you look when you look at the transcripts and those of the protein levels oh and do some estimate of abundance that it's really not a problem to get down to low number of proteins if you do lot of depression now we don't want to do a lot of separation usually if you wanted too many samples because it it just becomes attractive but if that I would assume that in the data is fourteen and a half thousand float into the maturity of very low level expressed as long as they are soluble I mean it's something is in chromatin and it's precipitated out of course Yvonne you want I think accessibility in solidarity are much bigger determinant of the blind spot then she rebounds for the floss technique yes so we inject lean check want to microgram of total peptide mass and we know that for instance a milligram of tissue wet tissue consists on a needle biopsy after protein extraction is typically about 50 microgram of a peptide generated so that kind of dead and we can of course not generate one microgram inject the whole thing you do not inject any shooting so but but I think if you have 10 microgram of peptide which would late be too late to some hundred micrograms of tissue then be sure they actually cannot it inject more because that would be so now the problem of course you you can also inject much less but then you simply don't have enough I mean you sacrificed in any cage so in the prototypes are you proceeding to look at interesting post translational modifications splice isoforms probably modified all these pathways or execute some of the features these pathways yes so we're very interested in Norfolk in modifications and this is the V in this in this targeting techniques you have an advantage that you also get detention time information basically when they come up with calm because you're equals you measure repeatedly the same peptide you can reconstruct the chromatographic illusion and one of the with modifications one of the big problem especially specifically is come up with protein phosphorylation is the question at what had rocks residue in that peptide is actually have that attached so to say a peptide is a forceful peptide well resolved and you can say that but where exactly the costing is attached is difficult issue so number of people including also Alexi have written softer thought that attempted to do resolve them and if you have to take three hydroxyl three Syrians which one and you know it from the mass it is one exactly long phosphate but which which one it actually is difficult issue so this tools work of course as well as they can work if the spectrum tell us but if you have two neighboring Syrians and it could be the one or the other and they're simply they may simply in the spectrum even our very close examination may simply not be no fragment that distinguishes the two so what we found and others of course tool is that even though they even though they may look very similar to the spectrum and you run them across the column the column will it reverted calm has remarkable ability to separate the amount so they even even in one one position over of attachment of phosphate will very often I'm not saying every time very open lead to a chromatographic separation so this this of course when you reconstruct has come up with other Peaks that information in his president you can at least say these two or three forms of the peptide that chromatography is separate but have the same mass the same seek backbone sequence and exactly one phosphate we can tell you know let's say two or three forms you may not be able to say exactly the qualities but you may say there's more than one and to resolve them you can simply make a synthetic analog and then it stays clear so I think this is a scan upon hidden advantage to this targeting method because you can factor in come out the graphic time better you could of course also do that in in job gun mode but you don't know a lot not a lot control of control there in the peak that help that is selected Rama forth magnet pace but how it's harder to do there so I'm so glad you mentioned there'll actually many times he sends all here his regards from England unfortunately the scheduler worked for who did not work for Alexi but he's very much in our thoughts here and of course who one of the Rudi's many star protegees we're very proud to have him here buddy thank you so much for such a comprehensive lecture [Applause]