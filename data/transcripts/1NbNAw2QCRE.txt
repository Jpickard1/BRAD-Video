sustaining interest in viral and bacterial genome assembly and analysis and you he was an assistant prophet J craig Venter Institute in Maryland before he went to Hunter College for those of you don't who don't know about Hunter College that's part of the City University of New York which has several campuses around the city it's my estimation to leave campus and Hunter College is a really neat kind of melting pot its classifies as a historically black college and university although I think it's more of a Hispanic weech an urban institution actually he and eleven undergraduates from Hunter College have gone on to win the Nobel Prize it's a really neat place it's a melting pot it's fun I love working here Konstantinos was trained at a place which is a very important place for bioinformatics and that's up at the Virginia Tech in Blacksburg Virginia Tech has had one of the leading bioinformatics analysis centers in the United States largely funded by the Department of Defense and you know it's it's a significant place so he's had very nice experience there complimentary experience at Venter and then more complimentary experience both as a corps director and as an associate professor at Hunter College so I think it's just the kind of person that we're really looking for and we are anxious to hear about your vision for building the next generation so Bob maybe you guys can tell us how many generations there are now the next generation of bioinformatics and with our strengths for next generation of sequencing data analysis so welcome it's great an added thank you Brian well thanks everyone for coming today so here's my titles I think Brian you covered it all very well so it's all the way that's ready when you will just move there in a brand new building and have a lot of collaborations with Welkin also it's it's been great it's not Alex yeah it's been like 20 minutes definitely I can hidden we open it's a good start so revenge of Microsoft I can get it don't run Microsoft I had two talks yeah just hit plain keeping his clothes okay seems like what okay so you know I did my PhD at Virginia Tech coming here in the u.s. first time in 2002 and then since then I kept moving north I spent five years at the correct my name is you that system professor when I was there it was right at the time when the human microbiome project was really picking up at the Institute and I've done a little bit of work with outsource in compute cycles on the Amazon Cloud during my PhD and their computer class that was running out of computer cycles so they hired me to do some some work in that like great experience five years unfortunately was all soft money there so despite having at my last year there two million dollar NSF grants called the I was only covered like fifty percent of my salary so I had to look for something else and then I am looking at academia the opportunity that the City University of New York Hunter College came up so I moved there in 2014 and through great efforts of our president we have this collaboration and a brand new building that you see there with the well Cornell Medical College so we've been there for about one year and a half and by the way actually I always want people to look at it this building at the credibility if you see here those are actually colors with the basis DNA sequencing so you know craig Venter is always a little eccentric that's that's a small eccentricity a big one so okay yeah I am if you sit in the read there and okay stall or his name or something they're very proud okay so let me tell you a little bit about my lab as you can see from the titles were fine home attics lab so I hired both staff and students like I trained some students actually with vision do stuff we do a lot of software development so we have sort of very heavily software focused people there well so for the blind chromatics pipelines that we build we have the Ponca Maddox analyst and also a little bit in the next slide how we build those pipelines you know what one of my patients are usually don't wear lab coats it was from the official opening public school so we have an Illumina NGS sequencer and I'll talk a little bit more about that and we do all sorts of NGS from genomes and transcription of cone snails to cancer cell lines and the focus of my research which will see more is development of novel been chromatic software for large-scale data analysis and I'm very interested in genomic data miner fishing visualizations and even things like getting by informatics to work on mobile labs so of course we publish all our software it's open source some of you might be familiar with depository so some faculty needs Department they had code repositories there and when we publish something where was when our software could be very usable which below I mean that talk but as part of the usability manuals that are published on github we also put like videos we have our own channel on YouTube to explain like how the software works you know features and all those things so the three years have been at Hunter College I've taught like about three new courses for programming computational biology NGS data analysis but Mattox Python and I was discussing just before they stalk with bug eaten I saw that but stop here having a strong band home addicts program we also do a lot of since I run the Beinecke AmeriCorps I will do a lot of specially specialty workshops whether it's going to be like the climatic day analysis or even wet lab work so it's like how to build your libraries with alumina kids and then I have all my students giving talks this little stuff in New York City right from corporate stuff for months many different universes conferences so I have my students going there all the time giving talks especially the programs like science research mentor mentorship program the special program to get high school students that can work in the lab and this is like really top students actually had two students from there one see the research in my lab dementia guard unit for undergraduate who Carnegie Mellon and the other one to Stanford and I also have my students participated women in business plan competitions one that we do was a deep relationship highly selective 10% entry because I'd like to think them sort of out of the box and you know scientifically in which is all papers and okay so I've been lucky enough the three years out there to have almost 1 million dollars in condom my startup from the City University of New York couple of pilot words through the Center for translation and basic research that I was hired to and also in someone associated with well Cornell one of the power that were to the clinical and translational Science Center but most important three months after I arrived they submitted a grant 29 age which was and let me go and step back and say like the City University of New York despite being a good school they didn't have any and I mean inherent yes next generation sequencing in the center people were sending samples out but there wasn't any sequence or anybody 4matic capacitor in school so we put within three months that I arrived as a supplementary to the National Institute help when thematic supplementary for the CD BR and we actually won it four hundred and eighty five thousand dollars so so we're not what I did was I design a build by informatics specific cluster and for those of you who to point whom I can familiar to the clusters here like like it's it's a standard isotonic class all right I said a mid memory no it's with one tiny geeks with prominent six in CPU so we can run like you know genome alignments like that a couple of high memory nodes half a terabyte and terabyte of RAM if you wanna run assembly you would be threaded for those of you again welcome onyx like multi-threaded jobs in alignment in a data storage array all those interconnected with high high speed backplane network it's actually two had knowns for redundancy and now we hope that you know the Center for translational basic research and with the remaining funds what we did is we'll build a natural wedlock sequencing core and aluminum Isaac and then the way we have set it up you know it has nothing compared with its sequencing facilities here at scale but it has all the basic things when you'd like to buy analyzer QB my sake and then the data you could use format they just feed directly in our classes so and some things I'll show you the upcoming slides to we try to make it as it's easy to use and it's transparent so the API submit their samples and then the data appears so an interface on the class and they can immediately go into their analysis and you can see here some examples of project lots of variability from RNA sequencing smaller Rene's cancer alliance and even on snail transcriptomes for that we had for identifying novel peptides that can use this for my so really exciting projects in a lot of collaborators as well both in New York City and elsewhere that we do see both sequencing and bind home at exposes so overall it's been great experience you know a lot of collaborations lots of papers coming out so I've been enjoying it so far so let me copy you talk to you about a little bit about my research and then things have seen like through my experiences and involvement with research building medical mics pipelines for NGS in the last few years so when we think about NGS we think about like big data and big sequence and right like today I was like enough to see the DNA sequencing core here's an example of sequencing for sender something like you know hundreds of thousands of dollars investments lot of data coming out dedicated themes to run both you know the sequencing and then with so many data coming out you know dedicated teams of informaticians to analyze the data but there's been a little change in the paradigm the last few years I mean the companies do that because looking 42 but the way I like to say it is like sequencing is becoming more of a commodity so you have like the sequences that we have they might seek so this one cost like ninety thousand dollars they have a smaller one the mini sequence cost fifty thousand and then you also have those the Oxford nanopore which is like sort of a USB powered sequencer like really a handheld one and then you know the library prep is it's really getting Nam easier and easier so that that's me started there and we didn't have other faculty we didn't have any people to work and of course so we were doing now you know our library prep ourselves right and actually work so if I went in the lab and actually were so that tells it is easy and you know the pricing I guess you can multiplex a lot of especially when you do metagenomics it's decent but it keeps going lower and lower and then you know with those instruments you might not be able to do like all human genomes but I'll have the reference human genome and then you can do resequencing you know transcriptome and stuff so you can and of course a small bacterial or fungal genomes and sixty minutes you know a lot of people I see they NGS with those developments becoming easier and easier and then being used more and more as a standard technique in molecular biology but you know we have the sequence to generate the data and in the way I see is three main problems one is that and of course you all center around by chromatics and one is running by informatics software you need significant expertise right because you know you have all these unique software engineering quirements the second part is that those sequences despite being small they still can generate a few gigabytes of data which you know it's manageable but then if say you wanna run alignments again the reference genome human genome in the multiple gigabytes and then your alignment and run tens of gigabytes and then those 60 gigabytes of memory so you need a lot of computational cost and of course even after you managed to run the but informatics pipelines then you get like a lot of text based data so how do you go about like interpreting those data sets so you know I'll talk about that visualizations and so a little more detail describing so you know when complexity right so when you wanna run it but my when you have five to ten different software components and all those you know you might need to compile it from two source code you might need additional libraries and always work on Linux and command-line then the structure of the pipeline's how do you move data from one to the other and how do you make sure that your pipeline you follow the right steps in the right order the interface to run the command line don't support ability in Bangka much we develop pipelines on the command line on a specific system a specific Linux system so once we build a pipeline how do we transfer it in another system because if we were to to build something and publish it how other researchers and other labs could benefit from our work just to what I said to make it more visual and here's an example so that's the standard RNA seek pipeline with we call it occido pipeline obviously from the names from the Tata Group the credit my name is you options now so basically that's the schematic from the paper they published with the pipeline and it tells you the steps again that's just a schematic on the paper but if you actually go around the pipeline you have to do something like that okay so what about if I told you that there was a way that you could do this pipeline instead of just having a schematic in the paper have this sort of let's call it a workflow canvas and then having the tools as boxes which you can drag and drop from here drag and drop a couple of tools and just connect them and then create the steps of the pipeline and this is an actual working tool which we call a galaxy now I apologize here there's a lot of lines between this tool you have to use your imagination but we tried before it all makes a new project didn't work what can we do so so essentially this is the exact same pipeline that you saw in the schematic in the paper that see the pipeline who are any sick I mean it's under pipeline not anything crazy my my point here is I want to show how we went from that sort of schematic in the paper and actually sort of living breathing pipeline which I don't know what what do you think like could somebody who who doesn't have any experiencing but remarks if we gave them this interface they could actually get the paper and then drag and drop the tools here on this work or converse and connect them with lines together and assemble pipeline which I'd been working or you know after a little tweaking my the work alright instead of running everything on the command line like I showed before and that's what we do with the bioinformatics core at the City University because what I found was like and I'll explain the technical stuff here in a little bit but before that I want to say that what I found in the three years I was there was that we were a small University a small core there was a lot of interest in yes but we wanted to find ways how we can sort of teach a man to fish right so teach our faculty and our poor Stokes how to build by the minds and have the right tool in the interface and then they could pick up themselves a lot of of the work that's that's new I said of everything has having to go through us and and also like as an educational tool and the choice for that was the galaxy web button so essentially what we were doing in the developers in the core was they were right in those XML files so the extensible markup language so essentially what that is is that you describe the command-line tools the inputs aren't all the parameters the data types and then what the galaxy does is like reads this file and then it renders the tool in the workflow canvas and then it's going to render the tool with all the inputs and outputs and also when you try to build a workflow and connect the tools it's going to check what the data types that you specify that it will say create its output and what its input so if they are compatible then it's going to let you connect it or not they're not compatible so it really takes a little whale of a sort of the manual work and by the products and these making I apologize the lines don't show but this is like again an rna-seq pipeline I'm using an example but it's a more complex one because the first one that I show it's kind of it was kind of a simple pipeline you know it just had like two conditions and then two datasets to read condition like forward and reverse this has like eight datasets its input having two technical replicates so that's more of a real rnaseq pipeline that you would run at you need the replicates and then if you would see the lines in here so essentially the first stage of the pipeline is some plastic grooming how we call it or doing some I'll check out the weight second one is the alignment to the reference genome the top hat the third one is the couplings which after you assemble you reads essentially gets the I'm sorry after you align the weights get aligned rates and assembles transcript and then further onus the comedy with differential expression and the captain urge and the cufflinks in the coverage so if you would see here you would see like there's so many lines between those tools and you think like each of those lines it would be like one operation on the command line right so you would wanna run you know you read data set through the groomer secondary data set another groomer third read de algunos reconvicted already which data set in each of those runs would be one command line so one line between here and then it's groomed said you wanna run in through the aligner through the top hat and so on and so forth so imagine like how many command lines you have to do its operation it would generate one output which you can have to get in pity's input to the next tool and things would get like well complicated of course you can write shell scripts and things like that to do that but then again shell scripts are usually more understandable to the person that wrote them and or anybody else and then there's so much if you want to teach mathematics about pipelines and those things there's so much you can do in the command line well at least it's the support here I think it works and just to give you some more examples of pipelines we have this one it's not I mean it's been Clarke analysis so what it does is basically data transformations again like I mentioned when you have been chromatics pipelines Ajith generate text data sets and what we end up unfortunately doing a lot of times is like getting them in Excel and you know doing a little data source column card kind of things which is you know one person does it and then they don't remember what they done we're here we captured the process in a pipeline essentially that gets the rope pipeline out and generates custom reports based on what you know the faculty will collaborate in requested for you know it might be report sorted by a people in the differential expression on you know locations in the chromosome gene names and all those things you know well so this was actually a huge on irony seek project or rod the what they quoted mine discovery rats project funded by do e so they wanted to see like when they expose them in thin piece Mel which genes chains expression and then we also inserted links and also which actually linked to the genome browser the bradleyb and then this was a great project because then they presented the results of the duty and they got the partners so we continue the collaboration then and of course the simplest case with the the galaxy platform you just if you wanna run just simple tool again that's all the command-line parameters of the tool as they are entered in galaxy by iterating the XML that we have defined in the backend and that's what the software developers in my group do a lot and just explain a little more what we have here in one column is the list of all the tools which you can click in one and you get to pull in their face and then you can enter the parameters on it or if you are in the workflow canvas that you saw in the previous slide you can drag and drop those here and create your work clothes that's the galaxy history here so every data operation you do so essentially if you upload the data set or you had a data set hold it already and you run in an operation by operation running and by informatics tool or workloads generates output all those appear here and then the way you have set it out when we run a sequencing experiment that gets as I mentioned before streamed on the cluster and then under the researchers account the sequencing data appears here so we can go by themselves and run tools and run pipelines on them so but what we think we did is that we made it really simple for them to be able to do sort of basic and fundamental maddox to this interface and then that's our cancer again so the galaxy sits on both our head nodes I want show one here so essentially when researchers get the sequence in the data it's here and then they appear in galaxy they can go around workflows on the cluster and then whatever work was done because of queuing system automatically finds will we have like free resources and sub work what out with so that was that was what me down his difference for me running a small button chromatic score and also running my research faculty to save a lot of time while doing the so hopefully so far I convinced you that with the galaxy you know we can take away of all these sort of complexity by informatics pipeline like the Linux some sort of stuff with the workflow Conwell's have the structure of the pipeline in a more organized way which you know we can understand we can use it sort of like a presentation tool or information exchange tool when we do collaborations but no Manik Lee P I it's the interface the interface is friendly so someone without in command line expertise pipelines and galaxies but again we come back to the issue of portability right so I build my pipelines and galaxy works on the head node of my class they're just fine but what about someone who gets a sequence in machine they don't have the access to my class right in another institution and say there are a small institution which you know they cannot they don't have the funds to buy all this infrastructure to have or hire somebody that's so that's the the portability issue so let me introduce a new concept for you which is the concept of the virtual machine so what is a virtual machine one way to say it in one sentence is that it's a fully free virtual machine is a fully featured Linux server in a single binary file so a virtual machine would be that here so way to understand this is that if you think about the cloud right so essentially what is the cloud I'm sure you all have heard like the cloud is something like Amazon for example where you can go like pay you're our and servers right so what Amazon has is their data centers say they have like a huge computing machine here with 50 gigabytes of RAM and you go say like I want to run for three hours on machine which have like 125 goodbye to prom so what they do is that they fire up virtual machines in that virtualization layer I guess they have their proprietary one there's many open source ones too in the next slide it it sort of allocates the resources like half of them 135 in your virtual machine so you get your virtual hardware operating system usually node and then it might have applications or not depending like virtual machine is started or if some pretty good ones and when you when you use this virtual machines whether you you do like more desktop or galaxy the command line stuff and you log into this virtual machine it's not no difference from a regular server like you would sort of for those of you who've done like secure shell or let's say Logan in a remote server and remote being like from here to say the flux that's that's the same thing if you look in the boot so much in an Amazon I wouldn't have any different so this publication that you see here it was the first time chromatic virtual machine that I build we were listening 2010 the paper came out 2012 that had been for markztools and we're by Linux one our great work and great people the exposure so so this one it came our club balance virtual machine on being application layer had more than 100 pre-configured informatics tools and pipelines so the whole idea was that how can we for places which they don't have any previous band for max expertise or access by experts we can make when for max easily accessible so and I was when I was at the correct manner you've had this insertion with a small universe in South Africa a universal import pole and magnetic genomics Network and imagine this this university was so small that beats computer room was basically the only one they had for like library research teaching like anything from business classes like the students going there doing their homework and what we did is that we go to spiritual machines and I was talking about the cloud before but if you notice there was a virtualization layer so it's actually desktop virtualization applications as well one being for example the virtual books good for books like you can download it I go download skype and install it then once you have that you can get our virtual machine which is what we did here and I get on your computer and have all those wonderful maddox tools pre-installed so we went there and then installed the boot so much in turn all the computers so essentially we converted that lab have very easily without going through any minor level to have pentameric functionality we thought some workshops and then of course since it was open-source tools we left it there for them to teach their classes and this is some of the examples in this paper so some of those might be more older tools but those who you who do recognize them but again it's been great lots of collaborations pipelines built in the virtual machine and collaborations so what I want to show you here is sort of like the latest incarnation of this project so what we did is that besides like standard cloud by Linux tools we also installed the galaxies which we use a lot and then as realization suite which I'll show in the next slide and then we moved from virtual books to dr. virtualization it's much lighter much much better performing in um we have a paper which just mean it so so the whole idea is that wrapping all those tools in here and what this paper if somebody's interested can send you a drop but this paper describes is that how we can wrap complex pipelines into the virtual machines and the pipelines they're all build in the the galaxy workflow canvas and actually had students who were coming in without by informatics expertise and they were firing up the virtual machine getting the galaxy interface and then I had them like to rate the literature fine pipelines and then my software engineers were like building the components of pipelines through the XML to expose them in galaxy and then those students without 20 for my expertise they were following the schematics in a paper and assemble in those pipelines galaxies and we're saving that virtual machine and that coupled with the virtualization interface what it does is that when you run the pipeline the output of the pipeline is fed into the visualization which we'll see a little more in a little bit and you know that stuff with the virtual machines ok I can run it on my desk so I can access it but it can get a little technical right so what we have in this paper is sort of a meta script so like you would run say a perl script which does some basic vertical mark analysis from the command line what it does is that it obstructs all the processes of running the virtual machine so you can go around the script and what it's going to do is going to fire up the virtual machine it's going to ask you where your data is so you might have like you know gigabyte so far an ASIC data it's gonna get those inside the virtual machine and drive it through the workflow engine of galaxy running through the pipeline and the output give you the the point the output of the point for Mark's pipeline load individualization very for you to see and we'll see some examples of those so hopefully you know I can base you so far that we can sort of with this approach attack the first problem somebody who don't even have any Penta max expertise in their group they can get the beautiful machine and run our pipelines course the problem the second problem remains and Homeric schools require computational capacity so this one tends to be the busiest one this is what I mentioned before the the Amazon Cloud so again you can go and their data center and run servers which are virtual machines different rooms by the hour they have multiple data centers in Japan there might be I think they have one in South Africa to keep popping up like I mean the course of the virtual machines it varies based on the capacity if you you know request a large one with 250 I think that's probably like higher now while the price might still the same request 250 gigs you couldn't pay like almost $5 per hour if you just get like the smallest one they have with like 4 gigs of RAM you pay like 5 cents and then that's a storage course so all our virtual machines and all things there are pond Amazon so if you know you just have a laptop you can go there fire the virtual machine support data and then do the analysis there and just pay for what you use another option is you know because sometimes with Amazon you know you have to use a credit card and stuff I mean if you know the right people inside and you do like a lot of business with them they can invoice you but nowadays you know especially good example here the Mac Pro I looked at the numbers in the teraflops that you get with the macro essentially the same with the fastest supercomputer that you had on earth in 2003 so in that box and you can get it for $42.99 and it has 64 gigabytes of RAM so you can do a lot of informatics on your desktop or if you are more like a you know Linux person you know you can install one of you can buy one of those lab and those ones we run like RNA seek pipelines one whole pipeline the company completely takes but 20 to 24 hours or something like that so you buy one of those computer to download virtualbox or daughter with again it's very different style like say within South Skype then you pull the boots of machines and you ready to go so we thought we think that you know we can help like small labs that are getting into sequencing they don't have access to big by informatics course you know to analyze their data and again the whole idea was that like I mentioned before I have students or even postdocs who you know they're not really in bioinformatics but they're getting some sequencing and they can go q8 the literature we have the workflow canvas they can assemble the pipelines there and then once you have the pipeline assemble in the virtual machine safe then you can distribute anywhere for example here you have all these amazing computational resources and having met with any of them but I'm sure I would be flexible to unstart some good realization course you can go on Amazon and then if you have your own server in the lab you can run to machine the keys that will remove that all initial effort to install the pipeline's or with the pipelines that we have pre-installed in the virtual machine or if one pipeline is not there that you need you cannot sample it there is it on the Galaxy interface ok so I told you the problem B would be easy completely like fine computation capacity now let's see more difficult one so like I mentioned you get you run your pipeline you get a bunch of data so how do you end up read those right and when we think about let's talk a little bit about visualizations so what do you think about realization but how my christening do you think about something like that right I guess we all seem like that the traditional gmod genome browsers right so what's what's the problem with this one and I'm sort of telling you this story for one visualization application that we developed and published and I think it's pretty numerated the way that things but the way that I was driven was that I looked at this visualization I was like we have this complex software stock right where it's like the nuke servers and then you have databases Apache web server and then top of that the web app each people job or peril see I'm sorry I'm getting too technical here where your application runs essentially what happens is that say you open the the genome browser and then you want to move the little you know five thousand bases right or whatever to see you gene you click move to the right button and then there's a whole like c'mon triggered through that whole stock and then excuse me so it goes in the database fetches the data and then sort of redraws that whole thing which is which is very inefficient and then most of those visualizations are static images there's nothing like touching them there we live in a mobile age so and of course when you have a stack like that thing about how complicated might be to maintain it and then you know you have things like the Google genomics and the Google platform where everything is like very modular you know there you can like everything it was controllable through an application programming interface you know you can fill out the API for short so you can tell the API to go like fetch data from good data and cloud database you can tell it to run a computer in the App Engine and so on and so forth so so what the Google genomics does their goal is to store from the cloud so you can actually store you or $25 per year and it's a big effort to the global genomics alliance we can read all about things but I've seen that we heard like why couldn't like applications invite my to be this way right and I don't like you know I'm a very open source guy so I don't like you know the big corporate but they do amazing things at least give the good example so back to the visualizations so if you read the the New York Times you probably seen a lot of that stuff it's actually JavaScript in a JavaScript library called d3 so we were like okay let's use this language which create those cool visualizations and let's get like very nice some graphics like those and just drive our visualizations from data that are on the cloud like Google genomics for example and yeah that's the component thing is I mentioned so we use d3 with JavaScript library we use html5 so that's a table Gatien and i'll show a little more about the validation so our visualization runs purely the web browser so essentially it's a it's a file that you learn in the web browser and then it comes alive no data stored in the backend database it pulls data live from the Google genomics drop box say for example you have a collaborator that's some sequencing for you puts the runs the pipeline puts the data on Dropbox you can just pull directly from the Dropbox on your web browser that's visual omics Explorer and then visualize them so the reason we went that way like I mentioned before of course first to go away of that complex software stack that's under the surface and the second was like the second I guess was the cloud and the third was like we thought that the browser's becoming the computer and that's true like if you think the Google Chrome laptop right which basically when you get one of those laptops what you get is like you know a full screen browser and I guess in technologies that drive that one is of course this JavaScript and then also the HTML file which is the latest language which actually has multi-threaded computing built into it so you can do like parallel computing the browser in it sense noting it's sensing in reality actually and our goal was to see how we can do something like that I'm been premised with pure in browser computing and this is some of the example data sets of visual Romans Explorer can handle and I will show you also given that it was in JavaScript as if framework all phoned up so you can get JavaScript applications and convert them very easily to mobile application so very easily litigated touch enabled waiting for my visualization and then since I mentioned what you're gonna see I have some videos coming up how I connect to the Google genomics cloud this is some of the data sets available so you have accessible through our browser you have access to all those data sets ok so let me try and play videos so that's the mobile app so basically it just caused all the visualization there and seek also some of those in a little more detail this is one that does follow genetics so you can upload Alex and now it's actually the Google cloud in the interest of Panama a lot of pieces on the right it was the variants in the different chromosomes in my range the number of pins the corner areas you can see what very touching table [Music] when you click one along of the beans it actually gives you the list of all the variants I'm talking about the right one that are in the bean and then you can scroll through them and environment leagues UCSC genome browser and then on the left is our genetic 3 comenta genomic data that's the de la genetic tree realization 16s again because this all runs cloud-based that's why it's in able to run on the phone right because you just pull the data live from the Google cloud but don't do that on your cellular do it on Wi-Fi or here might be a lot of data yeah you can upload like just just regular day is it Dropbox Google Drive and then Google genomics which is their genomics cloud and it just pours live ok ok so this one I want to skip the first part I just so this one is variants the CFR also be deep Al's likely be the chip see you know apologize the contracts it's pretty but so when you first load your data sets it just grouped it default set of beans you can select the chromosome time flex call it a little bit and you can arrange the number of beans when you click that's it so you can insulate that's those videos are the ones we have on the YouTube is part of a link in the publication as part of the tutorials so he select a different chromosome and then on that chromosome can I just a number beans yeah so it's still likely so instead of 10 like more beans seem to increase the number of beans becomes more dense and then you can click on a pin and then it can give you the list of all the variants and it's been and also the pines and it's going to crawl down and each of those is a link to the very end and then it's [Music] then the same you can do with B okay so I'll show you a couple more so [Music] again it's so I'm gonna skip that it just shows it is it so the data says that the RNA seek does is the ones because we're doing the copy the tuxedo pipeline a lot so the visualizes cap the output files so that circle here it shows you it has a default p-value and it shows you how many genes that met that p-value could depend to expression have a deviate Ramadan's the two lines here are the two conditions and you'll see like as he goes to the so we don't have sound as he goes through the chromosome the whole idea to that was to pick up really quickly like genes with help like differential expression to be make conditions obviously goes this guy maybe this one is fpk yeah this one is not while it's linear it's not actual positions on the chromosomes this one is actually lines in the pile let's see yeah so that's something like I mentioned before so again a contrast great but that's in this area if you were able to see here to like like line the data setting is there and that's what is in large here and again the goal was to have interactive visualization of those type of static panchromatic state of files here I can change the p-value you can click on the circle and a specific chromosome expand again do that if you click on that gives you a specific information the comments on the gene yeah so essentially the whole idea is like you can really scan read and pick up you know from the lines visually very fast the differential expression between the conditions and then click waits one of them and get more specific information and then the last one last but not least the Google genomics cloud and again that whether you run on the phone or in the browser it connects live to the Google genomics application programming interface so the reference assembly the data set and then within that 1,000 genomes specific genome coverage and then it's going to pull the data sets from the data sets gonna pull the information with the by so essentially it shows you the the aligned reads so okay so just for more slides to conclude this talk so moving forward I mean I still believe that you know I mean we need to do strong collaborations with biomedical scientists for development multiple informatics solutions I mean close to my research and the stuff I like to do is strategic development of bencomb IT infrastructure or large-scale NGS data analysis but even closer to you know my interest is that how can democratize by informatics so to make them easily accessible for basic research medical practices going applications and what I show you here is not the collection of Palms but sort of what I think the benchmarks field is going and this is actual product you can buy USB powered on sequencer from Oxford nanopore I mean those guys I had to use be sequencing that they felt like why can we do a sequencer which is like powered by light import essentially that's that's a sequencing cheap is the nanopore based cheap essentially the DNA goes to the nanopore and it measures voltage changes to identify the bases they've got scale bits down this is a handheld computer without the next slide we use aluminum mini sake and I mean an apple they do really amazing stuff they have those micro fluidics devices which isn't better which is very easily preparing your library so essentially you throw a DNA sample in here that's programmable also to an application programming library and you'd write some code and do your library preparation and then load that in a sequencer and you ready to go and that's like so that's sort of an alpha testing like so one short Bible that they have and does anybody recognize with this one it's the actually the spot where they put the finger in God account so the reason I put this here is that because if you see all that stuff computing the palm of our hands all secrets in the bubble our hands just doesn't look very far right so yeah so we're not we're not exactly there yet but I think all those technologies made ain't gonna make like a huge chance like the way we do like bio medical check-ups I mean if you go the illumina way the slow wet labs start involved and the sequencer but any luminary have the advantage of data analysis one being the base space why would they try to put on site now so you buy those computers and then the applications for data analysis from the cloud and then the Oxford nanopore they have like good stuff going on with all library prep and sequencers but their software is still at a very early stage and then what we do in my group is that we're looking how to to leverage software and hardware technology like the handheld computers I showed you and sequencers to make been for Macs easily accessible as any other laboratory tool we actually develop in a compute cloud with similar to the base be smart but it can deliver open source by informatics apps to the virtual machines directly in your sequencer and that's an actual screenshot because the mysic but small sequencing has quite a powerful computer in there so you can envision it when this when this is published that somebody would get our apps and be able to like you know just operating from in their labs into the analysis right in the in cement you know like single-use sequencers this is what you saw before the handheld it's called Intel compute stick and the processor that this has is actually the same like the 12 n inch macbook so it's pretty powerful computer can run single chromosomes RNA seek alignment there in a few hours but those are gonna get keep getting stronger so you know I see those things like going together and giving us the next wave of home addicts you also explore in how we can how can use that sort of offline with a big battery here so a compute stake in nanopore sequencing completely off the grid sequencing and running data live at the same time and I think you know while we still need funding from the agencies to develop software and pipelines I think they're gonna start switching and you're gonna see like funding or more applied technical wizard and that's why we'd like to come with that stuff because there's the option because it more and more difficult yet I want grunts where we can start and when that start a small company including SBIR ground which this would make a nice book unlike the post or something like that I'm engineer I so here you have the University of Michigan amazing stuff like a.m. Kickstarter it's you know good works type of work that we like that it can can help like building a company towards SBIR and begin grants like the M truck so if you had an SBIR and you run out of fun and you could use that sort of mediate and of course corporate funders once you have an abstraction so I think just a conclude there will be a time when you will go to a blood checkup and you're gonna get like deep sequencing for example circulating tomorrow or a capsule like tumor cells where you know structural variations or Abdur and gene expression the blood cells and sequencing only become simpler the course will decrease in will go up but the data analysis will build it all neck and I think that's what I want to do keep doing its researcher making by informatics simply accessible as any other laboratory to user into user-friendly software and scalable and with that I'd like to thank you for your attention and you can have like a couple of minutes for questions [Applause] capability you have what kind of research would you like to do with it for yourself so so definitely yeah so definitely I think that developing more efficient pipelines and more efficient and since not only on the computational aspect like to run efficient in one class because the datasets will keep getting bigger and bigger more efficient in a sense to include as much of the data that is out there because it's it's kind of a cycle right so the more sequencing we doing the more projects we have the more we can get those and feed them back to to aid the discovery and get more precise like whether we're talking for discovering variants or discovering gene expression or you know anything for like special temporal stuff like I know here you're very interested in the high C in the chromatin and those things like the pipelines have developed so far it's you know like RNA see exon sequence a para undiscovered chip seek but I'd be very interesting to get into those type of pipelines and I think there's gonna be time we're we're gonna have like multi-omics data I think that's kind of the next step because the sequencer again I think five to ten years you're just gonna get like one used sequencers like the nanopore or something else right but I don't know there might be in some ways we sort of like multi-omics which can measure proteomics and variants at the same time so that's where I think it's a big challenge or for the pipelines yeah you know I think also like as a as a medical school and it's a I was reading through the the pages that's a lot of you know push for commercialization and the open air I think those type of devices that maybe they could find a way of course this regulatory hurdles there you need doctor's office right or even I know the pun images like NIH is that and sort of like getting a sequencer like the sequencing off the grid but much of the grade or developing countries you know all like disease outbreaks this is monitoring and things like that so I think there's a lot of potential there as well poverty reduction yeah like real time so my question is in your many collaborations with paper Sweden how far do you go analyzing the data I'm going to hand it off to your collaborators swimming at the bank for Maddux and genomics Coyle yeah that we have yeah so essentially for example say we do in RNA seek project our pipelines the way they are set up now and even with the virtual machines that latest paper that comes out you can essentially feed them the their own feed house and then they can generate pre-loaded visualizations with the datasets and it depends on the collaboration like I had collaborations where you know even the faculty say it does come to research cancer cell lines they sort of embed the postdoc in my lab and then the poor so this is right sort of on the data analysis and then you know they can work very closely with software people in my group and you know being that cycle where ticket analysis like a bad reputation but I mean my group is about informatics group so we wouldn't be doing work where would they write and say paper in cancer and making statements about what say that thorium doesn't cancel right this is something that comes to collaborations but I would do like you know the heavy lifting then I 9% of ninety nine ninety nine point nine percent of the been commodity analysis I really like your so you know presentation and the idea of doing sort of sequencing in the palm of your hand and democratization of bioinformatics and make it easier and simpler so the question that I have is have you thought about sort of you know data storing the data and sharing the data and these sort of second part of that question is will know that metadata are almost as important as the data themselves so what you thought about you know conceptually at least how to handle these issues yes so so for the metadata I completely agree and the way I approach this is like I try to capture the knowledge say for example for variants inside the pipelines so if we were to build say you know handheld sequencer with a handheld computer that you know you can go off the grid in a developing country and say like you sequence people who tell them if they have variants which will give them cancer but I will capture the knowledge from the database for those barns already in the pipelines so would give you the answer it's when it detects them if there and there of course again it's all like the regulatory staff and how you can make claims and everything how you could be bring these in practice yeah yeah so essentially that's the genomics core which I ran so what we do is that I mean don't think we do like any cell culture or things like that in the new building that we are with welcome so like an open a brief space like all glass slabs so for example when I work with the faculty that once the sequence cancer cell line right so in a lab they grow those lines they extract the RNA whatever that's that's name there and then they bring us basic structure at me and then we just feed that in the loom we used very small of course we like the standard illuminant stuff so we just fit those in the Illumina kids and then we do the library prepping and we put that in a sequencer and then we'll pick it up from there and do all the bind chromatic so that's the 30% of the web and I also have many times that because you know even for people who do wet lab like Coonan their CD that they actually prepared libraries for sequencer a big class for like later Employment Opportunity things like that so we have people from model labs coming in and you know contributing that wet lab time yeah ok ok yeah so so that's kind of also what I showed with with the galaxy right sort of like the best practices and you know that's what I found too that had a great value for me like being like constraining the resources that I have and find in a way where you know I can teach people how to use those tools and of course you know when you have an easy-to-use interface and very accessible interface I can make it much easier and then sort of a removal or the workload we have instead of you know having everyone and saying like ok Randy is on that when we have something like that then we can move the door of the workload that's the one who we're starting to it thank you [Applause]