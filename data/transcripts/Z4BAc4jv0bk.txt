I'm sorry I apologize so I completely forgot so we have plans for dinner today so a Dan burns is taking for dinner Antonia took him last night so if anyone wishes to join them for dinner and just come after the seminar and talk to to dance so we can agree the time and then go for the neck all together and tomorrow after 10:30 Dan opportunities to meet him from 10:30 to 2 1 so this is it from 11:00 to 1:00 so from living one there are proteins to meeting so if you like to meet him tomorrow so just a little talk thank you I have to start out by acknowledging my funding these are the people that have contributed to the work that I've done then these are some of the collaborators and I'm not going to take time to read everything I assume you all speed readers the goals for this talk are to build a case for why pursuit of mechanistic models that never explain biological phenomena require new classes of models I'm going to describe the approach that I've been thinking about for dealings building such models at the end I'll provide some snapshots of our work I'm a big goal is to try to encourage you to explore and contribute to this and all that new frontier this is not an exercise for one person by any stretch in order to really reach the vision that I that I and others have it's going to take a lot of additional work and I in the process I'm gonna present several theoretical ideas for comment just to make sure that you're all not sleeping I found these four quotes in the Cambridge companion to the philosophy of biology that I liked very much I thought I'd share it share them with you I'm sure you've read this first one already this other one biomedical science can be characterized the pursuit of mechanistic models that better explain biological phenomena advances in biomedical science require more explanatory mechanism models that's what I'm interested models that do a better job of explaining the phenomenon that we're interested in and finally science can be defined as the pursuit of better models how many people in a lot of wet lab work when I polled people that do wet lab work and asked them what fraction of the experiments that you initiate actually wind up being people in terms of generating the new knowledge that you anticipated helping you make a decision that you needed to make its publishable in some way I get a range of ratios but one in 10 is kind of the low end of the ratios that's not very good productivity one in 10 one in 10 experiments being being fruitful why is it wanted to I think that even today there is an over reliance on middle models that are always flawed in ways that only become clear after conducting new experiments those experiments could be either wet lab or in the future in silicone experiments I envisioned that the use of new and established modeling and simulation methods near-term next ten years or so should be able to help get that number down from one in ten to maybe one in seven that's a good gain in productivity we need we need that gain by the time your students are halfway through their careers I I believe that we'll be down to one and five and at that time if it goes down to one and five whenever it does then the entire biomedical R&D landscape will it change planes will be completely different than they are today you'll need half as many wet lab scientists ten times as many computational scientists but seriously things will be better because science will be progressing on a more efficient basis it will be standard operating procedure to do many simulated experiments and advance them so I try to tell the graduate students at the path they're on is to become expert in designing and conducting in silico experiments this is very disheartening this shows the amount of money spent by a larger pharmaceutical companies over the past ten plus years eleven years and the productivity and they've experienced that's very discouraging and that trend continues it can't continue obviously something is going to happen either the productivity is going to turn around maybe by the help of computational models maybe not or the model the business model will collapse and if it does it will create a whole new area of entrepreneurial activity in the in silico aspect of that no matter which way it goes is going to be very interesting this is a quote that I found from someone who is lamenting the problems with the pharmaceutical industry and of course both problems are mirrored over right in the academic research sector they spend less money but they're less productive than the in terms of efficiencies in the pharmaceutical industry and one of the reasons is that back in 1970 there were more low-hanging fruit the complexity of the issues that had to be dealt with by the scientists that were making the decisions was just less than it is today but even with this flood of data that we have in the scientific advances that have happened since 1970 the productivity has continued to go down so the productivity in terms of omics data new methods etc is not helping as much as we had anticipated home ectasia have not lived up to expectations so what is the optical not to be more useful I think that the omics data needs an explanatory mechanistic context and when I caught with colleagues that have that data we were looking for how we can merge mechanisms with that information my take-home messages are as follows I believe that discovering and developing new therapeutics with Barza requires acquiring an exporting deep insight into the generative mechanistic networks that are responsible for the phenomenon and that doing so in the face of incomplete data it'll always be incomplete multiple uncertainties they'll always be multiple uncertainties requires building and challenging and our mechanistic models the methods outlined in this talk can I believe facilitate cool and validation of mechanisms help form concrete hypotheses for translation bring into focus concrete details or particular interventions and increase the ease with which accumulated mechanistic knowledge is measured visualized and leveraged I found this little diagram to be very useful in talking with all manner of colleagues these four scales or spectra can be used to describe any particular current research problem and what I do when I'm talking with what lab colleagues is say let's define the problem and then tell me we're on these scales you think you are in terms of information measures can be precise analytically and yet they can be imprecise in terms of the information that they're giving you might be in the situation where a certainty prevails or uncertain people veils the system information can be complete and detailed and by that I mean the fine green explanatory mechanistic knowledge that explains the phenotype that you're interested in it can be complete in detail or it can be sparse invade the observation in phenomena all of them are as expected or occasionally some of them are anomalous surprising puzzling renewed those of you doing wet lamp work where are you I think the answer to that question determines how you should think about models where you are on these various spectra I I think determines how models what models you should use and how those models can best be used and I found it very helpful to approach it from that direction if I'm on the far right I might be doing biophysics problems working on enzyme protein structures classical engineering working with materials if I'm center-left on probably dealing with actual biomedical problems all of the biomedical research people without talk to tend to think that their center left on at least one of the lower three scales looking at the systems information model on the far right the moth the mechanisms the generative knowledge to explain phenomena is known and mechanisms are actually and on the far left their possible and then you have in the middle their plausible or likely my notion is that it's very unlikely that biological mechanisms are ever likely but they're they can be quite plausible on the uncertainty scale certainly can be complete and uncertainty can be pervasive that lab data is interesting it creates some interesting problems for modelers both in terms of uncertainty and in terms of variability if we think about quantitative mappings from wet lab to wet lab and I'm talking about precise quantitative mappings if I'm measuring the metabolic clearance in vitro of a compound in the parasites and it's so many micro moles per minute per cell what are the chances that that precise measurement if I take the same compounds in the same cells and do it three months from now are going to match one to one anybody want to bet on that so if biology experiments don't map one to one from wet lab to wet lab even for the same experiment why do you insist a model map one to one quantitatively to any particular research result it's the question these are two disturbing papers three large pharmaceutical companies over the past 10 years have made it standard practice to actually replicate the experiments that are about to inform important decisions or experiments that aren't going are trying to inform the decisions that have already been made and they found it and these all of the papers came from the best journals less than 25% of the work was reproduced that does that's not a criticism of the scientists it's not a criticism of the methods or anything like that it's just a fact biology can be hard to deal with and this paper talks about the perspective from the venture capital point of view a lot of people bring some very interesting information to venture capitalists and saying I have this and it'll do that and the rule of thumb is don't believe any more than half of it and the hack that you do believe go out and independently have it verified in some way it was actually better than it kind of unquote in terms of small lands independent lands that do work for venture capital firms in terms of replicating will have experiments so this is the reality that we need to be able to work in in order to do model good models why is disabilities I do a lot of work with the parasites omlette my current wetland colleagues do a lot of work with meta sites and they for example engaged simultaneously in dozens if not hundreds of functions that share pathways and components does it make sense that a set of a pata sites and culture are going to be behaving the same way on our one as they are earthly are that the parasites isolated from two different rats from the same strain are necessarily going to be in the same state I don't think so the relic preponderance of those functions coupled with their recent histories need not be the same and two sets of hepatocyte ice similarly from different labs from the same age sex and strain nice for example it's hard to deal with that on modeling perspective consequently the mechanistic cascade that might unfold with an intervention may simply unfold differently in the two sets of parasites the biology - can change a parasite change upon isolation and stabilize differently depending on the isolation method that you use and the culture conditions that you put in them thus it's not very surprising that there are a few one-to-one quantitative correspondence between comparable measures taken from different experiments having the same focus yep how many articles do you read about high throughput experiments that is true even when the protocol focuses on exactly the same phenomena which is the case for high throughput experiments consequently many researchers the wetland researchers place far more trust in the relative phenomenon details than in the absolute quantitative measures it traced no more trust in variable variable comparison rather than value to value correspondence and that reliance is reflected in both published and experimental designs in the vast majority of published mechanistic descriptions so that reality having lived it has kind of influenced a way that I'm trying to come at modeling and simulation because we what I would like to see in models is a situation that mirrors the what's going on in the wetland and yet I still want to use them to make better decisions so if I think about predictions and and line them up with my my graph on the farm right I I might be able to make trustable precise predictions for a new context so if I'm breaking a steel rod under different experimental conditions I can make accurate trust of all predictions about how that and but break but how about a new intervention on cultured cells at the other end of this prediction spectrum the best I can do is just an informed opinion most of the decisions made in pharmaceutical companies are based on the informed opinion of experts even in the face of experimental and computational observations and depending on where we are on this system information scale that Maps up into the predictability scale and tells us something about what we might be able to predict or not predict on the right-hand side of if I'm on the right-hand side of all the scales I can make precise predictions if I'm on the left hand side of one of the three bottom scales I think that the best that what you should focus on is improving your description of the system which means discovery and exploration this exploring mechanism space in discovering incremental more plausible mechanisms we'd like to move from some location on the system information scale incremental in the right [Music] in other words we need models that help us shrink the space of plausible explanatory mechanism down and that automatically says that this that were always going to be dealing with many plausible mechanisms for any particular phenomenon that we deal with so our models need to be able to simultaneously represent many plausible mechanisms instead of just one for any particular experiment a model is going to have a corresponding use case or in silico experimental I use I don't know how many of you want comfortable using the term use case and I just want to define it because I use it quite a bit the news case is the aspects of the referent that the analog system is intended to mimic and I'm switching now from the use of model to the use of analog because the systems that are that I deal with are as you'll see more analogous to the biology than they are models of the biology if we're center-left on any one of the three lower scales I think it's hard to model the biology because we don't know enough to do it but we can mimic the biology it's how and for what purpose the model will be used simulation scenarios the component our analog phenotype the set of targeted attributes that's a use case a lot more models that have use cases that match to the experimental use cases in the wet lab on the right hand side of these four scales continuous mathematics is perfectly appropriate we have a solid deep foundation in in those mathematics and those formalisms are will encapsulated and they work best when we are absolutely on the right-hand side of all four scale if we insist on using those methods when we actually are living center-left we have to make a lot of assumptions a lot of simplifications and the more complicated the model becomes the more complicated that daisy-chain of assumptions becomes and it creates lots of problems so on the left hand side I the only thing that I've been able to see that we can do but now is to focus on the use of object-oriented agent oriented and actor based models and I'll say a little bit about those in in a minute what we can have mixtures of the two types depending on where we are on these various scales so I envision relying on analog models if we're sitting left and I define on analog is anything that is analogous or similar to something else and that exists and operates in isolation even in the absence of a referent it's a system that has aspects and attributes that are similar to those of the reference system in in the Biot biological context it's its biomimetic software that when executed produces phenomena that mimic those of the analog unless what I mean by an analogue when in pursuit of explanatory mechanistic insight I think that the emphasis should be on generation exploration and falsification of multiple mechanistic hypotheses but we're still in a space of lots of plausible explanations for any one phenomenon that we might see and to populate and shrink the space of plausible mechanistic hypotheses I think that focus on should be weekly on validation but more importantly on mechanism falsification false vacation gives us new knowledge validation and verification gives no new knowledge and what we want to do is take that space of plausible mechanisms falsify one of them and shrink it a little bit and the more that we can pressure that space of plausible explanations the more useful knowledge that we'll get what analog capabilities would be needed in order to do something like that I have all this six reusability the analog components can be reconfigured to represent different mechanistic hypotheses and even different wetland systems if I'm going to be exploring mechanisms trying to make the mechanisms more realistic I'm going to be constantly changing in altering my model so I need to have components that can be reused under different conditions the components need to articulate and is it's easy to join disconnect and replace components though Furtick ly and horizontally without having to significantly engineer the analog that's hard to do with a set of equations I would like to have a model or it's easy to do because I know that I'm always going to be constantly in the process of revising and altering and improving my mechanisms my articulation I'm talking about the extent to which the analog consists of distinct interconnected parts or components and the extent of which these components are encapsulated and their internal dynamics are independent of those of other components I envision a lot of biology consisting of encapsulated components and the components within that encapsulation being somewhat independent of each other flexibility it's relatively simple to increase or decrease granularity and that doing so enables cycles of hypothesis testing falsification and refinement local mechanisms simulated phenomena are consequence only of local mechanisms in other words I don't infuse my opinion about what's going to happen in a mechanism into the mechanism I allow the mechanism unfold by a allowing components to interact with each other only locally they don't use information that's not local transparency one can observe and measure simulation details as they unfold and compare them to measured referent lab phenomenon and finally measurable experimental similarity the goal should be to as I see it the goal should be to have an analog that when executed produces phenomenon that's measurably similar for the phenomena over here in the wet lab and I'll talk more about that these six capabilities require on analog components rely primarily on relational grounding and that's a kind of killer and I think that maybe I invented it and I'll just say a little bit about grounding because I think crowning determines an awful lot about how models can and should be used and this is one of the heretical topics cause there's a lot of my modeling and simulation people friends get very upset grounding is the unit's dimensions objects to which variable or constituent with furs or relates absolute grounding all the variables parameters in put out glittering in real world units like seconds of meters in relational grounding the variables and parameters are in input/output in in terms of defined by other components in the model for example if a component output is in a set of formal lumen and long gate bifurcate branch former clef and the receiving component accepts those elements and their relational and grounded in each other I claimed biology uses relational grounding so biology is using relational grounding maybe we should put more effort into using relational grounding in our models grounding in its impact on analog use can be I did come in at on with a following a couple of following observations inductive models typically rely on absolute grounding we had a good conversation earlier about conduct absolute grounding of inductive models absolute grounding can create an entangled daisy chain of often weak assumptions absolute brownie complicates combining models to form larger ones relational grounding enables flexible adaptable analogs but it requires a separate analog to referent and quantitative mapping model I don't think that's a problem I actually view traditional equation based models as a conflation of two models one describing a phenomenon and the other describing the relationship of that nomina in terms of units that's - two models can plate it into one so I have a envision a grounding scale that goes along with these other four scales and they're all on the right-hand side it's I'm perfectly comfortable using relational grounding and if I'm way over on the left-hand side I claim that you should focus on using I think I said set it backward on the left hand side I claim that you should focus on using relational grounding and in the middle you might depending on where you are on the other skills you might use a combination of grounding methods but the locational no scales would inform you as to how you should think about these grounding issues the use of the analogs that I'm describing and their functions are fundamentally different conceptually from the use cases and functions of conventional equation based models I'm interested in increasing phenotype overlap and increasing mechanistic infinitive mcsim Alera T and I'd like to just take a couple of minutes and just talk about some of those ideas the asterisk indicates a phenotype an attribute of the system that I'm interested in we'd like humans animals in vitro systems to have a lot of phenotype overlap but we know they don't so I automatically have a mapping problem between the attributes of interest in these three systems under some really good circumstances we might find a situation like this where there is some attribute overlap some phenotype similarity nevertheless what I would like to do is to be able to build analogues that have their own phenotype and that phenotype can overlap a little bit with the phenotype of whatever the reference system is and then with a little iterative improvement trying to do more in terms of quantitative matching I can actually improve that the the phenotype of the analog ire bill will never completely overlap the phenotype of the wet lab model in the same way that the wet lab model phenotype doesn't overlap the phenotype of another wet lab model the nice thing about analogs of the type that I'm talking about is that if I have two analogues that use more or less the same components and they overlap a little bit with two different biological systems as indicated here I can put on software a wrapper around those two models and explore the space of morphing between one analog and another in other words I can ask the question what do i what what is gained and lost in my analog when I morph it and make it transform into the analog counterpart of the mouse and vice versa when I have to throw out and add in in order to make the mouse analog more like in vitro analog and in exploring that software analog more thing within in silico helps me think more helps me think better about the corresponding process and the biology which I can't actually explore and in time I'd like to be able to do this I'd like to have analogues that have undergone enough validation and falsification and so on and so forth that that more thing method would actually allow me to say something about what's happening in people and have a degree of confidence in that statement it might not it could count as a prediction but it's more like a projection let us say a few words now about how these analogs might increase their mechanistic infinite if ik similarity this is a sketch of what might happen within a mouse when I give it a drug we know that the drug eventually finds us leads to intracellular components there's some type of interaction that interaction propagates to the cell and the tissue level and that causes things to happen in the case of liver at the local level and that propagates up and eventually shows up as system-wide effects and in the mouse that's the system that I'm interested in I want to understand it better so that I can better anticipate the consequences of interventions so I would like to have an analogue where the major components in each of those systems have counterparts in the analogue and I'd be very interested in in improving the mapping between those those different components and that the mapping could be improved qualitatively quantitatively or are in terms of behavior in order to improve that mapping I have to focus a little bit on measurement similarity for example I need to measure things and occurring in the analogue and compare those to measure things in the wetland system and I need to identify a similarity that I'm going to be happy with it's best if I can identify that similarity in advance so before I even build a model I think about a phenomenon that I'm interested in in the wetland system and the phenomenon that I need to generate an analogue and I even say what level of similarity will I'll be happy with if in the wet lab you can only get within a factor of 2 by replicating the experiment and that says something about the level of similarity that you will be happy with during the simulation so I would like to come up with a measure to measure mapping model to tie those two things together and establish a degree of similarity once I've established a degree of similarity I've achieved a degree of validation and I can say that there may be counterparts in the biology to the analog mechanisms and the reason is I'm matching up some phenomena and I'd like to iteratively improve that process if I could do it both at a high level and at a low level simultaneously with the same analog I can prove my confidence in the entire analog and what's more important I'll get to that in a minute what's more important is that if I do there's a high level and a low level I can look at the analogues mechanisms in between and there may be counterparts in the biology but by looking at the analog components in between it might help me identify something that I could actually test in a wet lab experiment and that very focused test no matter what the outcome falsification or not would give me new useful information with the kinds of analogs that I've been talking about a very nice property is that these components can operate at different speeds then that increases the flexibility of mapping between phenomena in the simulation and phenomena in the wet lab my I'm told by my wet lab colleagues that they see evidence in the wet lab of cell types in the same system that seemed to be running at different speeds so we would like to have the ability to have components within analogs run in different speeds so how how would you actually parameter lies an analogue like the one I just described where you get the parameter values from and and how do you get the parameter values in the face of pervasive uncertainty and persistent and tractable experimental variability it's easier than you think I'll give you a very simple example human how they learn tight to epithelial cells are differently isolated from ones that don't get transplanted at UCSF and those are used for studying a variety of things I have no and so on and so forth and we have in vitro cultures that here used to create organoids and those organoids in use for experimentation if you take these type to epithelial cells and you put them in 3d culture at different densities of cells now all form structures but the pace at which they form structures is different and the kinds of structures they form are a little different the cells move around even though their primary type two cells the cells move but they don't always move the same way and also the structures move and it gets kind of complicated and there's a lot of theory out there about chemotaxis and this and that so on and so forth and all we wanted to do is to come up with a simple parsimonious analog that could quantitatively describe the phenomenon that were being observed in the in the wetland so we have three spaces cells luminal contents and media that's all we need in terms of spaces and cells move and in the system so it's not a very complicated system if we discretize it at the level such that a cell occupies a single spot on a hexagonal lattice I can account for the three kinds of spaces and also cell movement and when I do that this diagram on the top the dark spaces are in lumen now white spaces are media and the gold and blue things are cells and in the top panel it shows you a situation where this the center cell is surrounded only by one other kind of object long on the left that's surrounded by other cells and white is surrounded only by lumen and in those situations it's likely that the cells will do different things and these other patterns show other kinds of arrangement so there's a finite space of arrangements that cell can find themselves in the next thing to do is to come up with some likely behaviors of those cells and that space is finite because of the way we discretized it and these are some of the behaviors that they can come up with but I'm not going to take time to go into that right now but we can't come up with a finite set of behaviors and what we can do is we can randomly assign any one behavior to any cell in any particular situation or any given cell cycle so we don't make any decisions you just say they're gonna bathe in one of these ways and when we do that we get a bunch of crazy drama all kinds of weird behaviors so the weirdness tells us that we can shrink the parameterization space by eliminating abiotic behavior we can reduce the number of options that we give themselves in any one cycle until we begin to eliminate any behavior it's not seen within the cell culture and because we have a lot of phenomenon look at size of cyst speed of cells speed of cyst how quickly they form a lumen how quickly they clear out a lumen because we have a lot of phenomenon look at we can parsimoniously and incrementally put more and more homina pressure on those on the space of mechanisms and reduce the parameterization down and we did that and we were able to match quantitatively the data across a large number of attributes and in doing that we found that the behavior options that we gave the cells for any one condition the cells chose only one behavior ninety-five percent of the time so the cells were making had a very good idea of what they wanted to do in any particular situation and we didn't have to make a parameterization decision in order to get to that point so that's the strategy that one can use parameterize a model like this in essence we start with a huge mechanism space and any mechanism in that space has a large parameter space we can shrink both spaces by eliminating abiotic behavior and abiotic parameterizations and then we begin to the process of iterative ly trying to shrink both we take a mechanism select the parameterization ask it if it has achieved our similarity measures if yes we're happy we go back and repeat the process and if no we shrink the parameter space a little bit more and continue that process until we toss out that mechanism is being infeasible under these conditions are as being possible we always follow very strict or so many guidelines we don't we try very very hard not to make the models any more complicated than they need to be in order to explain the phenomenon and whenever we increase the granularity of a model of whatever reason that expands the back of mechanism space and we have to be careful that we don't overdo it when we expand revalidate we always ask can we simplify the now complicated model and make it a little bit less complicated so we and here tightly to this parsimony guideline and I think it's obvious that in order to make progress in doing this sort of thing we need automated methods we did that by hand what I just described okay but everything that we did could have been automated so if someone were working in the software environment and understood that how to do that they could have looked at the records that we kept in going through that iterative procedure and come up with an agent that could have done that entire parameterization process automatically it's an important direction that I see the whole field going and once we've successfully achieved a bunch of validation targets then we pick another attribute and it to the list it's not five attributes we now had six usually adding a new attribute falsifies that mechanism and we have to do something increase the granularity of the mechanism in some way and then repeat the process so that we still achieved the five validation targets that we had before plus the sixth and again you can see that it would benefit from having automated methods that would allow that process to move Bolin I envisioned by the time your students careers are hangover that process will be automated and that automation process will discover mechanisms that don't make sense but withstand an awful lot of falsification it doesn't follow that every mechanism in biology hands is necessarily and logical to us so the automated methods might uncover some very interesting biology following a river protocol facilitates generating generating multiple mechanistic hypotheses and then eliminating those that are least plausible through falsification using not red lab but in silikal experiments so in that example that I gave all of our falsification efforts were done through in psycho experimentation an iterative model refinement protocol is I believe at the heart of this mechanism falsification this discovery process now take a couple of minutes and just give you three examples of the more complicated kinds of models that we've been working on using these same methods I talk about the type to alveolar primary cell forming cysts some of you are probably familiar with NBC kids say NBC Kay sells a couple of my colleagues use in vitro epithelial cells to explore the properties of epithelial cells and have some very nice in vitro methods for growing organoids I use MDC K cells primary breast stem cells so on and so forth in the case of in d ck cells this is a picture of what happens over the course of 10 days in a very biomimetic 3d cell culture the NDC K cells move around sometimes they form clusters at some point between day one and day two they begin to polarize as soon as they polarize they start to form a lumen they continue to divide they reengineer the lumen space and expand and eventually they form a single layer hollow cyst and the size of that cyst kind of left doesn't quite level off but it start growing very slowly because once it gets to a certain size the cells know they form the proper size lumen and so they're more interested in reengineering the lumen contents than they are in forming a bigger looming in the process of doing that they make mistakes so that little arrow indicates that sometimes they try to make an extra lumen where they shouldn't or instead of forming at a nice single layered assist with single cell sometimes the cell bunch up in on one side so they misbehave on occasion and we have quantitative data for this entire process has been repeated more separate experiments time-lapse images lots of data so what operating principles are each of these cells using in each stage in this process we should be able to get a handle on that once we have a set of operating principle once we know what the cells are doing at any one time then we have a plausible hanger or looking at more details but we need to have we need to know what the cell is trying to do in order to have a context for fitting in sub cellular molecular level mechanisms but that was the first task so how and why do these cells do what they do what are the cells operating principles what happens when an operating principle is disrupted by a molecular intervention those are the questions of they're interested in so we completed the process that we described before except now on a higher level of resolution and we were able again to achieve quantitative validation targets across a large number of attributes and remember that a regular hexagon and hexagonal space each of these little dots as a hexagon so this is a very fine-grain hexagonal space each of those dots is a regular hexagon so a regular hexagon in hexagonal space maps to a circle and continuous space so those hexagonal cysts are actually quantitatively matched to maths spheres well we've put a lot of energy into a multi scale model of the liver I don't have time to go into it but again its components with inside components with inside components listed from the organ level all the way down to the components with inside petaa sites and we're interested in adding compounds natural quartz xenobiotic and looking at the consequences an obvious use would be to look at a petal proxy city of compounds another use would be disease progression repair tissue regeneration etc we would like to use the same model for all of those in slick and silicone type experiments the advantage of the approach that I've just described in this context means I can take one normal and silicone liberal model we validated or two different disease conditions and I know what in silico what was required can change the model from normal to disease 1 and normal to disease 2 so I have a hypothesis about what might have actually transpired in in the wet lab it's almost impossible to do experiments on normal systems that are in the process of transitioning to model disease systems so having that in silico capability is useful and we can hypothesize that there might be multiple levels of similarities both in transition and in terms of mapping from in silico to wet lab so this mr. picture illustrates we would like to go one of the more challenging sets of modeling experiments had to do with leukocytes you take leukocytes from mice and you put them in an ecosystem flow system under a microscope they're all they always behave very individualistic but their behaviors are confined to finite sit you can't predict what anyone leukocyte is going to do even halfway through the observation so can we validate can we come up with mechanisms of behaviour of a leukocyte that validate against the behavior of individual leukocytes and then at the population level and then further can we take data from genetically modified leukocytes that have differences in terms of their cell surface properties ligands and receptors use that same leukocyte revalidated against other data and build a model that allows us to link population level behaviors all the way down to what's happening at the cell surface level we in why games or diffusing around and forming clusters or not so we were able to do that with a set up of observations and in this case the cell is that object divided into a large number of a quasi autonomous units and each of those units is represented by a quasi autonomous piece of cell and inside those are other units and then finally inside those or other units and in our right hand side within each small time step those objects actually mapped the individual molecules and they're diffusing around and not forming clusters or forming clusters and that's having an impact on whether the the the cell and here's are not not adheres or whatever so in sorry no equation but the phenomena match very well what this does is it says that we can hypothesize that a lot of the behavior of a leukocyte in terms of interacting with the surface is a result of quasi autonomous units within the cell doing their own thing independent of what the other units around it are actually doing I don't know if that's white or not when we were able to use that hypothesis and come up with some invalidate across a large set of data going back to the take-home messages discovering and developing new therapeutics requires acquiring and exploring deep insight into the complex mechanistic networks that are responsible for the phenomena of interest and doing so in the face of incomplete data multiple uncertainties requires building and challenging better mechanistic models methods that I've outlined can facilitate a rule of validated mechanisms help form concrete hypotheses for translation and bring into focus concrete details like I just showed is it necessary for those molecules to form clusters in order to get the right kind of that agent we can test that type of hypothesis in Silicon you can't test it in vivo you can bring into focus concrete details for the particular innovations and increase the ease with which accumulated mechanistic knowledge is measured visualized and leveraged anyone has any questions I hope I've activated somebody at least well enough so that they'll ask a hard question right yeah I'm just I just wanted you to continue on that last slide I mean we said you want to come to so to speak the next step you know come to some validation and to a new level of understanding whether you could go back to the leucocyte example and say you know quite how far you got whether you carried all steps out of the the six step process you would describe we did we did and in the process we learned a lot about the methods that we were using and one of the lessons that we learned is that we have to and here very tightly to good laboratory practices doing the simulation north and and the reason is that if we take this validated leukocyte and we now increase its granularity in order to validate against this data and now we take it over here and we try to validate it against this data usually we will find out that we need to back up a little bit go back over to this original one and then redo it with the notion of coming back over here we don't keep really good records of what we did it slows us down so we learned a lot of things in the process of doing this some of which I think will help in doing automation it taught us some good lessons about expanding mechanism space that we increase bank granularity etc but this data comes from strictly in vitro where the cells are isolated and go through a chamber in insight where fluorescently tagged cells from one mice are added to another Mouse and a vein is isolated and you observe the fluorescent cells moving through and actually moving through a vein and then taking those same cells and putting them in another Mouse where the endothelial cells have different adhesion molecules than the first set so it went across multiple experimental conditions and told us a lot about the need for relational grounding in order to move the same analog around those various experimental conditions I saw you had your hand up there earlier no involve complex sets of interactions might it be possible to actually measure the interactions over time before doing any modeling no that goes back to my one-to-one mapping quantitative method if we always insist on having more information but will we create the model then the models will always lag behind the wetland and what I think would we need is we need models that are sufficiently good and sufficiently adaptable so that they can one simultaneous with the wetland so my answer to your question is no don't wait till you get more data in order to try to do a model try to figure out a model method that will work for the data that you have yeah please I'm trying to be providing it to generate some more good questions the thought behind my question is that those measurements might be measuring an emergent system property which itself then could then become an object of scientific study because wet lab measure or the in silico measure for example if you have two proteins two hormones that might interact over time for example or brain regions that interact over time or any component such components that can interact over time I think it is possible to actually measure those interactions over time and have any major a measure of an emergent system property that then becomes the object of scientific study before any modeling and then you can model those interactions over time to have a more complete systems perspective I appreciate that and I realize that that's a perspective that's held by a number of people and I encourage that approach going forward however in addition to that approach I think it's also useful to have have modeling methods that don't wait on additional data in order to explore those same phenomena for example if you take multiple sclerosis the mechanism behind multiple sclerosis are really poorly understood does that mean that we can't begin to model MS now I would say there's plenty of data out there about MS in patients that could be the object of modeling and simulation experiments now but it might be pretty crude but they could be used now so I think there room for multiple approaches but I would not encourage anybody to wait to get more data especially with something along the lines of them emergent phenomenon because emergent phenomena depends on the aspect of the system we're looking at and the perspective taken by that by that measure and that's just my opinion if you want to describe the pattern in the data that's one thing I'm not talking about describing patterns and data I'm talking about trying to better understand the actual mechanisms as opposed to describing the pattern of the phenomena I want I want to get at the mechanism and the only way that I can the only way I can shrink the space of plausible models is to have a method to falsify some of the models that populate that space I have to be a repulsive item otherwise that space remains large and it can't be sampled using wet land experiments it's too big there has to be some way to shrink it and I think that's part of the reason that there's been such such a problem with the pharmaceutical industry when I've been when I've consulted with them it's it's very very clear that they're not doing a good job of shrinking plausible mechanism space they're actually adding to it we have to have nothing's to shrink that space otherwise I don't see how we can make the kind of intervention heartless that we want actually so this is a little bit of a requirement on the other things what about so especially if you say you know confronted with initial condition or data variability or you know inaccuracy so can you apply in your scheme of things here some you know physical or engineering principle like stability and you know do you recognize a mechanism for being stable or not stable beforehand and is this you know this might be sort of an opioid criterion you know if it works biologically then obviously it somehow is is immune to certain kinds of perturbations of conditions that's a good question and it comes up when you have experiments that are done in vitro of a relatively long period of times because you wonder if the mechanism your stable or is it changing over over time so that taking off my biology hat and putting on my engineering hat the first thing that I would say is can we come up with a plausible explanation that does not require change and then can that withstand a couple of rounds of falsification effort if it does then we don't need to go down the mechanism change route until we're confronted with some observation that gives us no alternative but this the the stable mechanism is a simpler explanation I'll stick with the the simplest explanation or as long as it will survive that's how one here hopefully it's skeptic I don't have enough knowledge to be skeptical actually two questions you have a liver model out there and it's been well since I've looked at it but the Ingram Shaw I believe and also a group in Germany I've got other bottles yes a lot of little bottles out there and do they subscribe to you or thoughts and methods I interact with the Imran Khan on a monthly basis and I've noted incremental stepping in my direction but in terms of the German group their focus is is somewhat different they're very focused on tissue regeneration and the thought leaders in that group all come from the physics domain and so there's some little hostility about the notion of relational ground so that's fine I think it's good to have an interface there to discuss and as as I tried to point out on those scales I think there has to be a wide variety of different modeling methods available and want to cover those scales and right now we only have a few and we need more modeling methods I mean graduate students the think of entirely new ways of going about modeling some of these systems and and that will contribute in in the future and switch from modeling method depending on what the use case is if the use case is very focused on something that might be mechanical force driven it might need a much tighter linkage and just a follow-up question you have slide my head which had various time scales yes is selling tissue etc have you run into any models that you created which really have such a fine mechanistic action that you can't compute it yes the lymphocytes yes in in in those models then when our simulations are simulations move forward in simulation cycles you and the the logical thing to do is to make a simulation cycle map from a certain number of seconds or minutes and it turns it turned out that for the leucocyte model we ran up against a brick wall until we made that map inflexible and so that early in the early in the operation of the mechanism the mapping from simulation cycle to the time scale was let's say one to one and then at the end it was one to two so by having that flexibility it allowed us to move further faster in that model and that was at the wholesale level in the the ratio of time steps at the cell level to small steps at the more scale is one to ten but in viola in wet lap time it's one to 1,000 so them the coarse grain mechanisms they're mapping the time is one thing and it's different at forcibly if I insisted that it be the same it might force us to make an entirely more complicated system so allowing for different parts to operate at different time scales gives us flexibility to use like a more parsimonious mechanism to explain a larger set of observation thank you oh you mean with in vitro whole animal data yeah we're working we're working on that but now in in the case of compounds like acetaminophen that cause a paradox isn't the pharmacological toxic phenomena Tyndale merged over different time spans and so we're looking into ways to change the mapping between cell simulation cycles and wet lab time in order to deal better with these different emergent properties that come out at different Pines and we're doing that we're doing that right now and so I I think it'll be successful and I can't say that it it will be successful but that's our vision of how to deal with mechanisms that are taking place where every second counts and then we're actually interested well what happens at two weeks then and we would like to be able to to run those simulations and we can do it if the mapping doesn't have to be constant