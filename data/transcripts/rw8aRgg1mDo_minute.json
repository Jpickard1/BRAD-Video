[
    {
        "start": 0.0,
        "text": "today I'm going to be talking about a recently developed method um for integrating transcriptomite Association studies and co-localization Analysis so here's a little bit of a road map of today's talk um so first I'll start with some background just uh you know what is a cool opposition analysis what is a transcriptomite Association study that I'll go into our methodology which we're calling intact and intact GSC I'll show how it works in a simulation study scenario and a real data scenario and then finally I'll introduce the software package for this this methodology okay so getting into some background uh so by now we've had a lot of genome-wide Association studies and these have identified lots of different genetic genetic associations underlying complex traits um but one of the most uh or one one open problem that still stands in statistical genetics is linking these "
    },
    {
        "start": 62.039,
        "text": "genetic associations to uh the genes that they target so we're applying these putative causal genes or PCGS um so traditionally this sort of cath relies on biological knowledge of genes that are that are close to the G boss loci so for example in this Manhattan plot here you can see that the top G was have all been annotated Baseline previous biological knowledge okay so recently a new class of method has begun to emerge that essentially integrates uh different types of omics data in order to identify PCGS so the advantage of this sort of approach is it gives a that paints a better picture of the underlying biological mechanism for complex traits the reclining these mechanism aware PCG implication methods and some examples of these types of methods article quotation analysis and transcript of my Association studies are key losses and one of the most common uh "
    },
    {
        "start": 124.259,
        "text": "multi-omics data platforms that's used in these type of analyzes is is gene expression okay so what is a collaborization analysis uh the goal of this type of analysis is to determine whether genetic variants that are causal for molecular phenotype um overlap with variants that are called for a complex trait so here we have a little cartoon of a simple example um just to show what a crow look like variant might look like so you see it from the from a cartoon sniff X is not only possible for the expression of some Gene Gene a but also for your complex trait uh so we say sniff sniff X represents the code of by the variance um so there are there are challenges that go along with this type of analysis one of them being by the presence of leakage disequilibrium and also if there's multiple causal variants of the locus um so this could be a challenge because uh and identifying Snips like X you might not be mature and the snip is like snip "
    },
    {
        "start": 186.06,
        "text": "X or if basically you have uh individual causal uh eqtl and an individual causal g-wocet that are just correlated with one another all right so here are some more specifics on what goes in and comes out of a co-loqualization analysis so for input you'll often have something like probabilistic qtl annotations from A Fine mapping method as well as posterior probabilities from through gwas and is out but you'll often have a g what's called a gene level co-localization probability and this is calculated as one minus the product over your signal clusters of one minus the RCT where the RCP is the probability that a genome grieving contains contains a single polarized variant okay so what does all that really look like again we have another cartoon another toy example just to illustrate how you calculate that Gene level collocalization probability so again we're returning to Gene a so here you have three different signal clusters R1 "
    },
    {
        "start": 246.42,
        "text": "through R3 each with a corresponding RCP and then when we apply our formula we get 0.995 which is essentially saying there's a high probability of at least one variant for this Gene okay so moving on from co-localization analysis so what what is a t loss um so similarly the goal here is to sort of Link uh our genes to our complex trait uh but this time we're essentially evaluating an association between the expression of a gene and the transcriptome and the complex treatment of Interest so often how these start out is you have a reference panel for example G text if you're looking at expression data that you use to train your expression weights in this cohort and you use those weights to predict expression in a separate g-was cohort and then you predict that or you predict expression and then associate that with your G wasp trade so what you end up with is essentially a transcript and wide set of z-scores "
    },
    {
        "start": 307.68,
        "text": "um and why this uh this sort of analysis is particularly advantageous is because you often don't have data sets that contain both your molecular phenotype data as well as your geost street so this allows you to sort of reap the benefits of the large sample size that you usually have with itchy box foreign so just a little bit of a summary of what we've just gone over um so co-localization analysis were probabilistically quantifying the overlap between called qtls and geosets um the output is your Gene level co-localization probability meanwhile if you're tiwas we're testing association between predicted gene expression and a complex trait and we're getting a z-scores output so recent work uh by Haku at all has shown that when these two types of analysis are applied to the same data they can actually be complementary so it turns out that they actually don't always implicate the same set of genes and what this works showed uh they had a "
    },
    {
        "start": 369.0,
        "text": "they're basically interesting finding was that this can actually Point uh to certain things going on in the underlying biology so for example if you have a gene that has a strong globalization signal about weak tiwa signal that can indicate the presence of horizontal plyotropy meanwhile they're sort of the sort of the opposite of is true that can indicate the presence of an LD hitchhiking effect so down here we have a cartoon to just show the difference between horizontal biotropy and vertical pliotropy so basically we want to filter out those cases that might indicate horizontal plyotrophy because we don't have any causality really going on between the gene the gene expression and the the complex trait um so sort of the punch line here is we want um we want genes to both have a strong co-localization signal and t-wall signal okay so what did I mean by LD hitchhiking effect um so that's sort of basically this figure tries to explain that this is from a different paper so here we have a "
    },
    {
        "start": 429.36,
        "text": "Manhattan plot showing your g walk signals here we have an expression weight Matrix for six different genes genes one through G3 sorry Gene six um and then we have a LV Matrix below um so basically what I want to highlight here is Gene 3 is the only causal Gene but you can see there's no overlap actually between the gene3 and the gene four expression weights um and despite this there is uh there is a correlation between these qtls which induce an associate a t-west Association for gene or even though Gene 4 is not causal um so how do we get rid of these spurious T loss results so previous work has tried to address this question uh the help you at all paper suggest post-hoc processing where the idea here is to directly built throughout your T wasps results using a co-localization probability of 0.5 "
    },
    {
        "start": 489.78,
        "text": "um so this is not a ideal approach because what ends up happening is you have sort of a binary decision is it a PCG is it not a PCG so you sort of end up with a loss of that quantitative information regarding the uncertainty of your classification um and there's also Focus which takes sort of a different approach here they're trying to model correlation among t-wop signals essentially trying to identify which Gene in a region explains the association signal but as we'll talk about a little bit later this sort of approach makes them understand make some strong assumptions about the underlying model okay so done with background uh now our methods um so our research question to just sort of motivate us here is how exactly can we integrate these two types of analysis kiwas and co-localization to implicate PCGS and biological pathways okay so we'll start with a structural equation model to sort of motivate us um "
    },
    {
        "start": 552.42,
        "text": "so the top equation represents uh the gene expression for some sheet of interest um so that's e and then we have it as a function of your genotype Matrix G and your true eqtl effects beta e um and then the equation below it is for your complex trait y so you have that as a function of your gene expression but we also include this extra G beta y term which essentially allows for a horizontal pliotrophic effects okay so we're not the first uh group to essentially try to tackle this problem of implicating tcgs others have done this in the past but their methods often involve additional assumptions so for example if you're at all familiar with the mendelian randomization literature um so usually the goal of an MR analysis is to essentially estimate uh the effect of your your exposure in this case gene expression of why but some of these methods for "
    },
    {
        "start": 613.019,
        "text": "example Mr eager regression uh imposes What's called the inside assumption and then Focus which we already talked about assumes that the plyotrophic effects are constant across variance so these are fairly strong assumptions in our opinion so we want to get around these and the way that we do that is focusing more on inferring whether your expression to why effect is zero rather than trying to estimate it so as sort of a springboard uh jumping off point for our uh our method we're going to first observe that the causal eqtls must be co-localized with the gussets based on our sem okay so it's sort of taking that observation and running with it we're going to try to implicate bcg's incorporating that observation um into a formal Bayesian framework where essentially we are forming a prior based on that co-localization evidence and we're calling our method integration of T loss and co-localization or intact "
    },
    {
        "start": 676.92,
        "text": "okay so getting into some more details about uh what exactly our method does so first we're forming a base factor from our tyz score and this is representing a marginal likelihood of the sem then we're forming that prior based on the co-localization data so this is the actual form of the prior uh where the the pi is an unconstrained EOS prior uh where we're sort of adapting an estimator from the key Value method for this um and then we have ffp co-look where the F function satisfies two properties the first of which being that it's a monotonic function of the colloquization data and the second is that if the collocalization probability Falls below some threshold T then we set it to zero then the rationale behind this is essentially we want to filter out those uh spurious T was associations caused by LV contamination um so at the end of the day we can compute a putative probability of "
    },
    {
        "start": 738.0,
        "text": "putative causality uh just using Bayes rule so one question you might have is you know what exactly do those F functions look like so these are a few examples that are incorporated into our software so on the left you have the step function which is sort of analogous to that that post processing that we talked about where you just filter out any T loss results that have uh co-localization probability of less than a half um right to the right of that you have the linear function which we sort of think is the most intuitive option so this is the default in our software this is just the identity mapping and trick heating a t value of 0.05 and then the two functions to the right X bit and hybrid linear X bit sort of the rationale Behind These is that um can you shrink uh your co-localization probability if it's less than 0.5 but don't set it to zero foreign okay so we just talked about intact in the context of "
    },
    {
        "start": 798.24,
        "text": "a Bayesian framework but we've also interpreted it in the context of demster Shaffer Theory or DS theory for short so if you're not familiar with Ds theory is essentially a framework for decision making that generalizes phase and reasoning um basically our decision here is to decide whether a gene is a PCG or not and we're trying to implicate uh sorry integrate two sources of evidence uh T was and co-localization Analysis and then so what DS3 does is it allows us to quantify the belief for some decision based on each type of evidence as some Mass function we have two Mass functions here each ranging from zero to one so why we're really interested in interpreting intact in this context is for what's called Dempster's role combination which allows us to integrate es independent sources of evidence so what is DS independent mean so two sources of evidence or DS independent if they have independent reliability for reaching the same decision so sort of following our early our "
    },
    {
        "start": 858.899,
        "text": "earlier conversation and the uh and the background about the different set of genes that can be implicated by T was versus co-localization we're going to argue that colloquialization and qls are approximately DS independent we can't say that they're statistically independent because they all can rely on the same uh GEOS and eqtl data sets but if we make this argument we can apply dumpster's role combination resulting in this statement here that the overall Mass the overall degree of belief that a Genus causal uh given the G Watson colloquialization data um is proportional to the product of the individual Mass functions okay so now we're going to sort of more explicitly tie that DS3 formulation to the Bayesian framework so just a little bit of notation here we're going to call it the T was posterior probability piece of T was so this is going to be proportional to our unconstrained prior times the base Factor so we're going to Define our marginal Mass functions as "
    },
    {
        "start": 920.22,
        "text": "essentially being proportional to some monotonic function at or G of your posterior probability for colloquization or tiwas why denser's role combination we can arrive at this conclusion where essentially the punch line here is that if G is the identity mapping then we find that the overall Mass function is actually proportional to the um the posterior probability which essentially means that for a non-model based DS3 framework and our Bayesian framework that was based on our sem the overall rankings of genes will be the same based on these two different approaches okay so sort of uh touching home base here so uh what did we just go over um so our goal was to uh estimate a posterior probability of fugitive causality for each gene in our data set we did this using a Bayesian framework where we basically converted a t y z squared or a base factor and then we "
    },
    {
        "start": 981.42,
        "text": "designed it prior based on our co-localization evidence that was designed to filter out LD hitchhiking signals and then we finally interpreted in the context of vs Theory so by this point you could just think if you're doing the analysis you have essentially a transcriptome-wide set of posterior probabilities so the next question is sort of you know how do we interpret these more in a biological context so if we come back to our original research question you might remember that our goal is not only to implicate PCGS but also to implicate biological Pathways so again we're going to be using that posterior setup posterior probabilities uh basically to find out which pathways are biologically relevant okay so the uh the main way that researchers have typically gone about doing this is what's called the gene set enrichment analysis so this aims to identify groups of genes that are enriched given Gene trait Association evidence so typically this is in the context of a differential expression "
    },
    {
        "start": 1041.66,
        "text": "analysis so usually input you'll have your summary statistics from the de analysis and then you also have a list of Gene set annotations these basically just denote whether whether or not a gene is a member of whatever pathway you're interested in uh you can get these from a few different resources one might be Gene ontology or go and another might be keg um and then for output you'll often have this conversion estimates for whatever genes that you're interested in some examples of these types of methods are bagsy and gsea so basically what we're going to do is take the bagsy framework and sort of adapt it from working with differential expression data to working with our posterior probability output so the idea of bagsy really is to treat this gamma as latent and then use an em algorithm to estimate the enrichment parameters um so just sort of a set up to that we're going to Define some notation so d "
    },
    {
        "start": 1101.9,
        "text": "i is going to be a indicator variable for whether or not a Genus in a gene set of Interest ing prior here to connect the posterior probabilities to those Gene set annotations so the parameter that really interested in here is this log odds ratio output one or essentially Alpha One if alpha 1 is greater than zero then genes in the gene set are more likely to be prioritized okay so the more building up here we're going to form a consistent exchangeable prior using the strong law of large numbers uh and a total expectation law and then we're going to use that to formulate a base factor for each gene is the ratio of the posterior and prior odds um and then finally getting into our em algorithm using those Gene level Bayes factors after initializing our Alpha Vector we're going to first update the conditional expectation of gamma in our e-step and then um our m-step performs a two by two "
    },
    {
        "start": 1163.28,
        "text": "contingency table maximization to update the alpha Vector so we iterate between those two steps until we get convergence um so in practice what we'd find is that uh when the underlying true contingency table is highly unbalanced that being that could be if basically one of your cell counts is very close to zero um basically we have problems with the algorithm converging to in order to account for that we shrink the Alpha One estimate by imposing a standard normal prior and then finally for uncertainty computation we use a numerical differentiation algorithm the compute standard error of Alpha One okay so um just coming back to uh the sort of the very high very high level view of uh both intact and intact GSE you've already seen the sort of upper left portion of of what's going on that that represented the intact procedure "
    },
    {
        "start": 1224.059,
        "text": "and then what we've just gone over was essentially using uh the posterior probabilities from that intact step and uh in addition to um the pathway annotations to perform Gene set enrichment estimation using intact GSE okay so doc methods uh now we'll go on to a simulation study to see how this see how this works um so here's uh some of the details of the simulation study design so we're using genotype data from 500 individuals from the GTX project we're randomly selecting about 1200 genes each with about 1500 common assist Snips uh we're selecting two uh true eqtls for Gene um and we're annotating each gene um as if we're not a member of a gene set with essentially probability 0.4 then we're assigning a causality indicator for each gene uh based on our logistic prior and the gene set annotation "
    },
    {
        "start": 1286.1,
        "text": "um and finally we simulate both gene expression and the complex trait essentially based on our structural equation model that I showed before we're drawing the residual error variances from a standard normal each of the effect sizes are being drawn from a zero five squared or in this case Phi is 0.6 because that's essentially the value that we found most accurately reflects observed signals and noise ratios um and then we're simulating 100 data sets for several different Alpha One values for a total of 500 data sets of total so after doing all this for each data set we apply statistical file mapping so we can do colloquization and T was um so getting into those results uh so essentially what we're doing here is comparing the performance of tboss only methods co-localization methods or postpocket processing and then finally intact um so the way this table is set up is you "
    },
    {
        "start": 1346.159,
        "text": "have the realized false Discovery rate at a Target control level of five percent as the first row and then the second row you'd have power and what I want to draw your attention to is over here for the T was only methods you see that they have the highest power but they have excessive false positives you see the fgr is not is not actually controlled um the co-localization methods here in the middle you can see that they're generally uh relatively uh conservative compared to the other methods and then finally if you look all the way over here on the right intact using the linear prior you can see that this actually has the highest power of all of the properly FTR controlled methods okay so that was intact now we're going to look at how intag GSE performed so the way that this uh this figure is set up is you have the true enrichment parameter Alpha One so there are five different ones of these uh and a hundred different data sets for each one and "
    },
    {
        "start": 1406.64,
        "text": "then on the y-axis you have the estimated enrichment parameter um so the the dot in the middle of each of each uh of each of these is the the mean enrichment estimate and then you have the corresponding 95 confidence interval um so the red is intact GSE and then we're comparing it to a two-stage approach which essentially only uses um pwas data so the first stage of the two-stage approach is to essentially classify a Genus putatively causal or not based on a five percent FDR threshold and then using those binary classifications the second stage essentially uh performs a two by two contingency table maximization you could basically just think of it as a logistic regression of the indicators on the uh the the gene set annotation indicators the dis um so there are a couple things I want to point out here first of all you can see the mean enrichment estimate for intact GSE is less biased generally than the "
    },
    {
        "start": 1468.5,
        "text": "two-stage approach and we think that's because we're incorporating the uncertainty from the intact output whereas the two-stage does not um and also you can see that both approaches are downward biased and a couple of reasons we think for that are first of all that you have classification error you saw the power uh basically none of the methods have perfect power um and also uh the fact that the intact GST algorithm involves the shrinkage step um so that might be another cause okay so that was our simulation results uh so now we'll move into some real data application so uh for the real data application we're going to plot apply intact to four different UK biobank molecular Retreat gwas as well as the multi-tissue gtex eqtl data so we're looking at CRM urate igf-1 and then testosterone levels separately for males and feet females "
    },
    {
        "start": 1529.88,
        "text": "um so why are these traits of particular interest to us so these are actually pretty well understood at the molecular level so the sort of the rationale here is we can use these to Benchmark you know are are these methods implicating the right genes or are they not um okay so the way that this table is set up is each of the entries correspond to a method and uh and a trait and essentially it represents the number of Gene tissue pairs that are implicated by the method and then the number and parentheses by the side to the side of it is uh the total number of unique genes across tissues um so very very a lot of numbers here and basically the only thing I want to draw your attention to for now is the fact that you can sort of see the same pattern as we saw for the simulation um in which if you look at the total number of Gene tissue pairs implicated for the default linear prior that falls between um the fast unlock uh co-localization "
    },
    {
        "start": 1590.96,
        "text": "approach as well as the phosphere's approach okay so next we're going to basically uh compare uh intact to that the method that we sort of touched on in one of the first slides uh the first Manhattan plot that we looked at so if you remember um I sort of talked about that proximity plus knowledge based approach which involves linking G was hits based on previous biological knowledge and genic distance um so uh here what we have are the four different traits in the First Column and the next column we have the number of g-was loci uh based on this paper the next column is the number of PCGS that we're implicating with intact and then to the right of that we have the number of uh core genes that are annotated so these core genes this is terminology from the omnigenic model but basically you can just think of this as uh think of these as genes that are key players "
    },
    {
        "start": 1651.86,
        "text": "in the underlying biology of the trait um so basically that the big takeaway from this table as you can see that since intact is not uh sort of limited by the biological knowledge that sort of predefined list of genes you can see it implicates a lot more than um than the proximity plus knowledge based approach uh okay so now we're going to look at the performance of intact and implicating what we call those those core genes in comparison to the proximity plus knowledge based approach so now we're sort of further breaking down each trait by subpathways so you see the the pathway column gets the name of the name of the said pathway and beside each name you can see that there is uh the total number of core genes in parentheses for that given subpathway so the column to the right of this is the number of proteins implicated by the um proximity plus knowledge based "
    },
    {
        "start": 1711.86,
        "text": "approach to the right of that is the number implicated by intact and then to the right of that is uh the number implicated by uh the overlap between the two different approaches so if you sort of scan this table at least in comparing this column here to this column here you can sort of note that the approximate plus knowledge based approach appears to be implicating more core genes than the intact approach um but there are there are there are still cases where uh intact is implicating more important core genes that the proximity plus knowledge based approach is not so there's a few different reasons that uh we've come up with to try to explain this discrepancy um the first um it's possible that you know for the core genes that we're looking at in a particular pathway remember we're implicating a transcriptome data using intact and the transcriptome might not necessarily most speed or be the most uh relevant uh particular omics platform uh "
    },
    {
        "start": 1774.74,
        "text": "for the for the given core Gene and in addition um the proximity plus knowledge based approach does not uh incorporate an explicit FDR control mechanism um so that that could be another discrepancy that we're seeing here okay so finally we're going to look at how intact GSE performs and implicating these quarantine subpathways and relevant tissues so we're basically getting these relevant issues from previous biological knowledge that's reported in this paper um so the way this table is set up is we have the trait all the way on the left the subpathway again to the right of that and then each entry corresponds to a tissue-specific uh enrichment estimate the Alpha One and the corresponding standard error so the statistically significant estimates are in italics there's nine of them of the 39 tested um tested uh tissue pathway treat "
    },
    {
        "start": 1835.94,
        "text": "combinations um and then the ni entries mean that essentially The observed data is non-informative so you can just think of this as cases where it'd be intact GSE en algorithm did not converge um so there's a few different uh explanations we've come up with with this possible explanations we've come up for this too uh one of them being that again that gene expression could not be the most uh relaxed uh relevant molecular phenotype and also some of the sample sizes for the tissue specific Gtech samples were sort of on the small side for example I think the kidney cortex sample size was about 70 or so all right so uh just a little bit of wrapping up conclusion here so we saw that intact identified genes with higher power than collocalization analysis and properly controlled FDR uh sorry properly controlled the type 1 errors that might have been produced by all the "
    },
    {
        "start": 1896.96,
        "text": "hitchhiking and then for intact GSE we saw that this was relatively accurate compared to that two-stage approach um and that's probably because it leveraged the intact output for the uncertain quantification so there are definitely limitations to this approach some of which we've sort of already seen uh first of all um we're only considering one molecular again type at a time just expression for now um we're actively working on an approach that can incorporate multiple molecular phenotypes um so we've seen in previous work that the available data sets that we have for politicalization analysis have uh actually more power for uncovering co-localization instances so you know this is this is an issue right if you're using your collocalization data as a prior next we can't identify uh treat relevant tissues or cell types we needed basically previous biological knowledge to point us to those and we can't "
    },
    {
        "start": 1957.26,
        "text": "estimate Gene the trade effects using this method it's really just designed for interference rather than estimation okay so if you're interested in this work um this is the the corresponding manuscript and we also have an art package on my GitHub uh working on getting it on to bioconductor um and finally I'll give a little bit of uh I'll talk a little about about a little bit about the the art package um so basically if you go on my GitHub and just look at the readme file you'll see this top bit here with examples so there's basically two functions here that you'll be interested in intact and intact GSE um so I've included some uh some sort of toy data to play with just to show how it works so there's the object simdat which basically includes your genes and then corresponding Pro localization probabilities and t y z scores so if you apply this intact function here this "
    },
    {
        "start": 2018.64,
        "text": "will essentially spit out a vector of the same length as the glcp and Z scores um just with your posterior probabilities um and then there's all of this intact GSE function where it basically if you you plug in your your uh your data frame and your Gene set list which is formatted like this is just a list object with the vectors corresponded to genes that are member genes um basically it will spit out a data frame where you have your Alpha One estimate and standard error for each gene set and you also have basically whether or not it converges do not so I'd like to thank my advisor William Wen for uh help with this this work as well as our co-authors and I'd be happy to take any questions thank you if anyone online has a question again you can either put that in the chat box or raise your zoom in you give a "
    },
    {
        "start": 2079.96,
        "text": "reactions on the bottom right then hang on in the room as a person for freedom is yeah I guess what I'm doing is the probability because I used to be like what's like the grade of the eptl and like versus like the variants that you're testing you know that Rangers of like a very precise variant at that point the range of this did you mean like how close they're they have to be I think generally they're so the signal clusters are generally in cyst to the Gene and I think generally there's threshold for that is like one megabase on either side of the TSS yeah good question you may have mentioned this but uh why did you change the magazine instead of gsdn "
    },
    {
        "start": 2141.339,
        "text": "so the way that bagsy was formulated is it basically made it um sort of the only difference was uh the way in which we actually Incorporated our posterior probabilities versus the output of a typical differential expression analysis and it's sort of naturally set up as an imperial empirical Bayesian framework that just made it easier essentially to to use vaccine than the gsca great question something online yet but you can give it a couple minutes in case anyone's typing and we've got everyone else in the rumors questions feel free to ask you do have a question online so uh great talk and interesting method can you give us a sense of the scale of data available for T loss especially compared to your available gwas data yeah so there there are definitely a lot of "
    },
    {
        "start": 2202.18,
        "text": "tiwas data sets out there um I believe methods like you know S Credit scan and and the tiwas method itself are designed to work with um with gloss summary statistics so it's getting easier and easier to just do a t loss using uh using gwas um yeah I don't know if that if that helps uh and there are there are a lot of there are a lot of I believe uh you know a lot of authors post their their um their summary statistics for tiwas um in particular so I think that there is a you know throughout it's definitely not to the point that the the available GEOS data sets are but there's definitely a growing a growing set of data out there to do these types of analysis on Dana said yes okay "
    },
    {
        "start": 2263.4,
        "text": "again maybe just give it another minute or two in case anyone online is typing well not seeing or hearing any other questions so let's thank Jeff one more time thank everyone for coming and hopefully see you next week foreign "
    }
]