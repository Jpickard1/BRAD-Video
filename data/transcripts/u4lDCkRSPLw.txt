thank you I'm not sure this is working if I can't hear it is okay it's connected right so thank you for that kind introduction and and all those memories going down memory lane I'm going to talk about a topic which is very close to many things that we do in bioinformatics and in informatics in general social informatics and medical informatics informatics involving any type of search or mining or pattern comes under this category which we're calling large-scale correlation line so you're mining for correlations every time you do a Google search you put in a text or you maybe go to their image search facility you're trying to find correlations between the query and the database and anytime you blast or go to gather or any of the the ontological search sites that we use in order to find out more about the biomarkers that they come up and most importantly what I'm going to be talking about here in terms of the examples is when you're doing very high throughput biomarker collection where you're collecting samples on perhaps a human genome or some some other very high dimensional quantity and you're searching for patterns that might exist in the interactions between those variables not in the average levels under treatment a and treatment B you know are the average levels different but rather the interactions between the variables which may be completely buried under noise in the mean levels so that's that's what this is all about and so what I'm going to be doing since I got this very very I think cogent introduction from Brian on mathematics there will be some mathematics here and I hope when you leave this room you'll be convinced that mathematics is key to being able to describe just how many data points you need how many examples you need what kind of correlations can be discovered in a data set if you are limited in the number of samples you have and know how many variables are in the data base which might be you know an RNA see chip example and so that's going to come up there's gonna be some mathematics it's not going to be heavy emphasis but it's gonna be there and and I hope you'll you'll appreciate how useful it is by the end of the top so I'm going to give some motivation then go through principles but basically the mathematical principles underlying correlation mining that leads to tools tools that can be used to describe whether you have sufficient amount of data to make a statistically and biologically significant finding on correlation and then we're going to talk about a couple of applications which one being network analysis of network discovery is quick dude extraction of correlations from very large sets of iron markers with few samples of starved type of framework and then another application which involves prediction where instead of trying to correlate biomarkers together from the same class you're trying to correlate ones that have biomarkers to another set and we'll see that the theory that I developed here really nails this problem okay so first of knowledge modes of some of my collaborators first and foremost balaraju right nam he was collaborated on the math and that I'll be presenting here and I'm including he's a current student PhD student of mine in EECS rob brown and young shen long were early by informatics students of mine rob brown applied here got in I believed in the bioinformatics program that decided to go to UCLA the third yeah he could be rather out there yeah especially today and then Geoff Ginsburg Haley's asking Chris Wood's at Duke medicine to provide the data and then many sponsors all right so let's let's go to motivation so why are we interested in correlation what can correlation yield in terms of insights that just looking at average difference between two treatments can't reveal well I have I have two pictures here this comes out of a data collection that that we designed with our Duke collaborators and I'll be talking more about the specifics of this data collection I'm not going to describe what they say other than to say you have blue and green dots on a plane a plane as it measures the expression in terms of abundance of mRNA from an app and metrics human a-133 a chip for two pairs of genes the colors on these dots which denote different samples are two treatments they're actually not treatments their effects two different effects from inoculation with a virus the blue are those people that got sick and the green are the people that did not get sick that were able to control the virus and exhibited no symptoms and solid our women hollow-core correspond to men and then the size of these dots is the hours elapsed since they got inoculated the virus so what I'm trying to show here is that if you look at the means which are in these two diamonds here of the blue and green treatment sets they're very close together right but look at this these are the regression lines the correlation lines we have effectively assigned Linda positive correlation over treatment to between these two genes they're both Co expressing in the same direction sometimes positive sometimes negative in some people it might be they're both positive and some others they might be both negative in that particular treatment class and the other treatment it's a negative correlation so this is this chorus corresponds to a very significant contract which is not revealed on the difference between the difference of means is on the order of you know 0.1 bold change point 1.3 culture so correlation is the key here in terms of these two genes now this is another example where we also get a similar type of conclusion where correlation is actually serving as a proxy or picking out changes in treatment due to aberrant at average change we have two clusters here these are males either females and this is a y chromosome gene that separates them so you see that separation through again this type of correlation difference all right so this is this I hope motivates why we're interesting correlation will also of course it's a student in the mean but they're for the purpose of this talk I'm not going to be talking much without the mood focusing on these correlations that we're going to try to extract all right so what is correlation mining correlation mining is taking data which is very high dimension like mRNA expression or maybe various symptoms of symptoms under different condition in a respiratory viral a challenge study different viruses or lady activation Maps if you're doing a functional MRI study of particular types of connectome differences that occur under to get three different treatments or maybe you're looking at tissue engineering we are trying to based on say the electron back scatter diffraction data come up with a way to combine two different types of michaelis microscope measurements of your particular sample and correlate them in order to figure out what the geometry of the say the fine grains in this calcium synthetic bone sample formation so you take this data you sample it with some budget is it gonna come in and in the future and the example spark example have a budget you can't purchase an arbitrary number of samples they are very expensive so there's a budget that determines what you sample where you set on how you sample and there's a value of particular form of information for the particular type of influence to try to do and then you expect features you do your correlation mining to find correlations between features that could be different associated with different genes or the symptoms at locations in a connectome a study and then you form visualizations or instances you know 1817 pathways where the each one of these corresponds to a gene known in this network is a gene the edges correspond to the amount of correlation that exists between those genes so they're very close together it means that highly correlated and then you can do clustering or correlation to find patterns in the connectome that are spatially organized that lead to cognitive pathway discovery or discovery of difference between ADHD and control or what happened right so that's the the key to making this work is error control we have a enormous amount of data we can work with but we all know that data alone is not sufficient to make inferences you can have too much data like there's there's a there's the principle that the more data you have the better you are is false unless you have a good model for how that data dimension can be fit into some kind of lower dimensional description of the the mechanism that generated your data having more data is actually a curse not a blessing because you effectively start to find spurious correlations everywhere and that's where error control comes in and that's where the mathematics is going to come in and finding ways to control that error from the outset without knowing what the true correlation is only knowing what the number of samples are how many variables you have that is biomarkers you have in the genes let's say and the desired level of correlation that you're looking for the lower the correlation you're looking for the more samples you're going to need right if you have a correlation equal to one you'll be you can pick that out with two or three samples but at lower lower correlation you need you so I'm going to call P the number of variables so biomarkers say 23 thousands that comes off of CDF custom definition file for ethyl metrics chip for example or a new of new generation compiler for RNA seek and in this 23,000 vertices these genes that come out of this aggregation we have up to you know 10 to the 5th edges that's a lot of edges and we might only have maybe 270 samples that's the actual number of samples that we ran in this challenge study I was talking about in a viral correlation correlation between viral inoculates in the two categories that I described so is 270 samples enough to be able to tease out all of these correlations and the answer is no if you restrict the correlations to be greater than say point eight point nine maybe you can ascribe a lot of confidence to those edges right that are really really short because those correspond to very high correlation but the longer edges they're very very noisy so this is basically the problem of big data right that there's a large number of unknown hogs a number of hobbies being the number of gains of being a gene that has lots of correlated Co Express ORS looking at a teen expression edges or a number of sub graphs which is exponential in the number of biomarkers is a very small number of samples to make inferences and as a crucial need to command a concert so let's just go back and recall what correlation is I don't want to insult your intelligence but it's good just to remember yeah let's just let's get get this out sample correlation is basically a measure of collinearity right you have two genes let's say x and y in blue and green the genes express over different samples according to this called a waveform a function over time let's say you're taking these samples at the time there's perfect correlation between these two genes they're not exactly the same but with a translation that is subtraction of the sample mean you can make them the same and then the correlation coefficient will be equal to one if their opposite side all you have to do is multiply one of these with a with a negative one and you're back to this situation and that's a negative correlation okay so we're looking for sample correlations that somehow represent the population correlation so there's a difference being sample correlation which exists in your samples and the population correlation that would exist if you had an arbitrarily large number of samples you can never pay for that study right but there is some correlation there and the question is how accurate can we be in estimating that relation from the sample correlation so here's an example which I hope will do I Drive home the fallacies that can occur and that have occurred in lots of publications in in scientific fields that we know about where one has a list of teens and let's just pull two of those out and let's say this is the time index at which these samples were acquired you have these blue and green jeans again and you're looking for correlation and you say okay obviously they're not perfectly correlated either with minus 1 or plus 1 coefficients but are they may be correlated with correlation coefficient greater than 0.5 that might be by logically significant right it might be that you just don't have this scene may have the two genes may have a linear correlation but there's some auxiliary variable you're not controlling for right if you could control for it maybe it'll be perfect correlation so if you do the sample correlation over this 50 samples for these two genes you gunned up with a number that's almost equal to zero so you would say no their correlate they're not correlated its level of 0.5 because the sample correlation is is almost zero however if you want to be smart about it and say well actually this is collected over time and I'm doing time studies I've given some treatment at time zero and the the organism that I'm pulling these samples from is somehow evolving over time well maybe I should divide this up into Windows and you could ask are there windows where I get point five or greater correlation and you can easily find windows through which that's the case so here's one with an it you have negative correlation coefficient of point 0.5 here's one you have a positive correlation coefficient so the answer to the question of are these correlated might be well yeah I have a if I have a window of time where things are changing but but you know they don't come into phase and a lot of phase 180 degree I'm seeing correlations well these are completely serious because those were generated but from an uncorrelated iid source in MATLAB okay so you can always find correlations if you look for them yes exactly that's the multiple testing problem so not only not only have you reduced the number of samples and now you've taken multiple windows so your your the p value associated with declaring significance that some are you know at point five correlation level is going to have to be corrected right it's that's easy to do with two they're looking for one correlation with two genes it's easy to do there we know how to do bonferroni Corrections and both discovery rate analysis when we go up to more sequences we have a double whammy because now we have multiple testing not only over these time windows but over multiple correlations so with twenty correlations you have you know on the order of two hundred twenty twenty sequences you got on the order of 200 correlations right 22 choose two and so now we can ask are there patterns of correlation here and again at putting our threshold level point 5 we find twelve cost correlations and as you might have guessed this is completely random I've just generated these from a MATLAB write generate me find these correlations and find these patterns and you it's very easy to start impugning some mechanistic model but he's spurious correlations that we unfortunately then we see you know graphs like this where you have a network of correlations that are completely spurious but they have survived this point five level test this is a real problem this reporting of correlations has led to retractions published papers in journals like new Journal of Medicine the other other journals that that have unfortunately not paid attention the reviewers didn't pay attention to this problem of false positives so this is a very interesting article by young and car we were director and an encoder Ector respectively of the National Institute of statistical science in raleigh-durham and so they did they basically got the data from these various papers and tried to basically replicate conclusions and you can see that's that these not none of them replicated 1912 studies replicated back three of them replicated in the negative direction so it was the opposite conclusion that was actually the case when they when they replicated this experiment so these are from the perspective of the credibility of the scientific enterprise really big deals right if you can't trust papers that appear in the New England Journal of Medicine or the general American Medical Association know that that's a big deal yeah and so what's the problem well the problem is that there are always false discoveries and there's a phase transition that describes where these false discoveries become a problem so when's this show this is a histogram right so what you see here is I've just drawn at random from a hundred Gaussian random variables I've taken 101 samples and now I've computed all the correlations between them sample correlations I just paused the histogram of those correlations so you can see that there's these hundred variables there's actually no correlations so everything is spurious every one of these levels corresponds to a spurious correlation of say 50 correlations found at a level close to 1/3 and this point where we get a dramatic increase in the number of false positives occurs at some level correlation which depends upon how many samples you that you only have 10 samples basically any correlation that you observe is going to be spurious because the based on vision occurs very close to one so what it says is that we need to know something about where this base transition occurs in order to be able to go in with our eyes open to right sample sizing a study where you have a large number of variables these might be metabolites or or some set of rt-pcr transcripts that you're testing and the theory that we developed specifies exactly where these phase transitions occur under very weak assumptions basically the assumptions are that everything isn't fully connected you know these networks that I was showing before they're there they're only on the order of P edges not on the order of P squared right which would be a totally connected network everything was correlated everything else if you make that assumption then you can get very accurate estimates of where these phase transitions occur and now you can use these to say well you only have ten samples you'd better be not not be looking for correlations that are less than twenty point eight nine right because they're just going to be spurious if you have 25 samples it completely confident you find a correlation that's you know say 0.8 or 0.9 because it's well above this is critical threshold so that's that's the the principle point that I'm going to be making throughout this talk is the existence of these correlation thresholds and the importance of being able to specify them to be able to predict before you do your experiment or once you've done the experiment whether you have enough sample all right so the principles behind designing reliable correlation lining are basically to determine which among several regimes sampling Lee genes you're in this is not be your mathematical limit right there's a mathematical statement and it's the number of samples P is the number of variable biomarkers you have so the standard central limit theorem the law of large numbers is the classical statistics that we learn says that you've got to have in increasing faster than P right you got to many more samples than you have variables in order to be able to compute P values to be able to use the central limit theorem to perform hypothesis testing with you know reliable uncertainty quantification um there's another regime which is called we call a sample critical regime which is when an MP both increase at kind of the same rate so that constant C so they both increase in the same rate and for this it turns out that well you can't say anything precise about the correlations but you can actually say something about whether the correlations exist you can't quantify the correlations the value of the correlation but you can ask are there edges of a particular level if this condition is satisfied this is the regime if you like where you can you can reconstruct the network but you can't actually quantify the how strong the edges are and then there's this other regime which is that we call the sample starve grazing where the number of samples is fixed and P goes to infinity this is the what's called the law of small numbers so this is a much less familiar concept to people that have taken statistics classes even advanced statistics classes it's a new theory that's closely related to ran a matrix theory goes back 20 or 30 years to some work by by Stein Palestine it's a central limit theorem which is not to the Gaussian limit it's to a different type of one and so you can let n be fixed and P go up to infinity I argue my premise here is that is the big data paradigm but that is where we need new statistical techniques in order to quantify uncertainty is when we have an enormous number of variables but we only have a few samples that are available to make decisions and the ant equals one patient that's right that's precisely right so important to the side whatever procedure you're looking for for the regime that you're in it's just a table that gives some terminology large sample at classical asymptotics that do do all these these well-known names to people and statistics going back to Ronald Fisher but small data it's it's a great application domain there's a mixed asymptotics have been recently developed in the in the 80s and 90s by some well-known living statisticians and then there's this new regime which which we are inventing and proposing is the real big data regime that people have to be concerned about which we call the purely high dimensional easy it's where the number of samples is fixed and the P the biomarker dimension gets out of the curly lon all right so why is correlation important I've sort of given you a a one example for looking at correlation or two treatments right in it when you're trying to find areas of biomarkers that are interacting but it's much there's much broader context of course network modeling another is talking about learning or simulating descriptive models for the interaction the COPE expression let's say between genes there's prediction so for casting a response variable predicting whether somebody is going to recover from you know a pathogen response or whether someone's going to get sick relative to say a measurement they're there on an inducible pathway before they show symptoms of classification and anomaly detection localizing unusual activity all of these involve correlation to some and these are just equations that some of you who have worked in in the bioinformatics statistical field will recognize the linear minimum mean square error predictor for doing prediction of Y from X involves the inverse of the correlation of the independent variables called the classification if you're doing say quadratic discriminant alysus and you're looking to discriminate between two different types of a whole expression that works one that has correlation Sigma zero the other one signal one then it's the inverse is the difference in the inverse correlations that matter so this is the optimal way of testing that and then anomaly detection eleven's test can tell you whether if you have some nominal Col expression that's in evidence in say some some organism is there a deviation at some particular time from that nominal due to pathogenic insult or other environmental stress related component and that also depends upon the correlation so it's just is to emphasize that correlation just is not this kind of visualization tool that and glad it comes in just about everywhere you have a problem of extracting some testable hypothesis from your data and then there's okay so this shows some of the the categories of correlation estimation graphical model selection which is basically looking at where are the edges like problem I mentioned before where quantifying that the value of the correlation is not as important actually saying whether there is correlation or not above some level independence testing correlations cleaning all these are different versions of the same problem involving yes estimating the correlation or the inverse of the correlation as I showed you for the prediction and Omni detection and classification all involve estimating the actually inverse of the correlation or literally the partial correlation the normalized inverse so you get a coefficient between minus 1 and plus 1 that characterizes the elements of the inverse correlation that's called the partial correlation and so this table shows the various regimes I was talking about in terms of how many samples you have rather relative to the number of variables and what you can do in those regions so if you have a number of set let's go to the standard regime where you're just trying to perform estimation right you know that there's a sparse number of correlations so there's only on the order of P correlations in your model not P squared then you're in this regime and what this says is that n is got to basically increase as P log P right so n is increasing as P log P increases so to get more more variables you're going to have to have correspondingly superlinear more samples right in order to be able to do the parameter estimation in order to minimize this norm squared error between this case inverse covariance matrices help you back off and you say well what if I don't have the money to put to buy P log P samples what if I can only buy P well then you can only do support detection you can only estimate the the graph you can't actually put a number on the the edges that connect nodes in that graph you can back off even further and say well I don't even have I mean P might be on the order of 23,000 and you know purchasing 22,000 RNA seek chips is pretty pretty large undertaking so maybe you only have log P order log piece apples in that case you can't even localize where those edges are but you can certainly say whether there are edges or not right so if you take some sub Network some subset of the biomarkers if you only have on the order of log P samples you can reliably determine whether you have edges and heat is the number of edges in that sub Network being greater than zero and if you back off even further if you have fewer than log P apples you can still you can still do something you can discover those you can discover that there are edges for correlations that are very close to what right here you can discover it is the correlations that are close to Lone Star which is this critical phase transition threshold here this critical phase position compressional goons do one so all these regimes are important this regime is the most punishing this is how do you estimate the performance of your estimate right so let's say you have an estimator of correlation now you want to build a confidence interval around that that estimator well now you know order to build the confidence interval reliably if you don't know the underlying distribution you know we're not of sidon Gaussian distribution or anything here then you need on the order of exponential in P samples in order to make a good estimate of the performance yeah well that's that's a good question so P yeah the question is what if you're not sure what the value of P is so that can arise before you do the experiment right so you you have so you have a choice between a saying you know a relatively modest number of biomarkers there's a very very large number of biomarkers and there you have to basically choose right what you want to what what kind of a budget do you have or biomarkers versus your budget this happens now in a in a setting where you're doing data analytics you know people this p is that's the sensor that's the microarray that's the ayah the the RNA seek a number of a pros right so you generally know P after you've done the experiment right but in designing your experiment which is what you're pointing out is is very very important and in fact the example I'm going to present at the end gets it a part of that yeah another question yes yes exactly and so that's that's the basic picture here ok so let's say that you're collecting samples and this is an argument for adaptive sampling adaptive data collection you don't have a million dollars to collect 23,000 samples right what you can do is as you start collecting samples you're you enter you first enter this screening lesion right where you can detect whether or not there are edges right among your variables with and so once you once you enter this regime then you know that there's this experiment has legs right there's there's there are differences let's say between the co-regulation network under treatment a and a CO le regulation and network under condition B but you don't know what the you don't know how to localize those differences so then you take more data that's with the same treatment until you have an update at you enter this other a game where you can do support recovery that is Network asked estimation so you can find out that there are edges and you can actually localize where those edges are and now it if you want to build confidence intervals on the values of those edges then the first thing that happens is you enter the estimation regime where now you can focus your collection just on those edges that you found right all the other the other variables you can throw away so you don't from 23,000 variables at the schooling stage support discovery maybe that falls down to you know maybe 150 variables now you can spend a lot you can spend a lot more time all right you can spend here your limited resources and a lot more samples of fewer variables and enter this estimation regime and then in the future if you want to put confidence intervals you get into the into the performance estimation relation that's the progressive correlation mining paradigm that comes directly out of this type of point of view that with different amounts of data you can predict knowing what the value of P is and knowing how significant the correlations are that you want to discover design the experiment so that you're collecting enough sample yeah no no oh no definitely not yeah the only assumption let me just be clear the only assumption here is that the network of correlations is fuss right yeah so that you know everything's not connected everything else yeah right if you have far so right so if you have prior information that tells you that you know because there's been study ABC that have established that there's no evidence that there are links between this pathway and that pathway well then maybe you don't look at this pathway that helps you to reduce the number of variables but you know you don't always believe what you read so you may want to you know do just some initial exploratory design where you use the full genome let's say and verify by what you grid yeah so this allows you to go back and forth between between these these regime so I'm not going to spend much time here on running out of time that I wanted to get the main ideas across on the mathematics but there's hub screening is just a version of edge screening where you're looking for exhibiting no hub screening is a version of screening for vertices that have edges but their degree the degree of those edges has to be greater than or equal to some quiets exactly you might be looking for nodes that that have degree you know greater than 10 this would be one of them here so you can screen for nodes for biomarkers that are connected at some level correlation to more than 10 other violent right that's that's hummus cleaning and house cleaning of order one is simply looking for connectivity right being a vertex with at least one edge is the same as being a vertex at commit that's connected so one can do this this one can formulate the problem of hub screening through a theorem I'm not going to dwell on this theorem at all I'm just going to point out that using this theorem you can theoretically define where these phase transition thresholds occur of various numbers of samples and in this particular case we have 500 biomarkers 500 genes if you like we get a mathematical equation for the phase transition point where are things things start to go to pot and you can look at false discovery probabilities so that this theorem gives you what looks like almost a Poisson limit of the number of golf discoveries where the rate in that Poisson limit is precisely determined by this phase transition and then you can see that it is critical threshold it depends on a hub degree that you get mathematically you can look at this as a design matrix if you like these curves tell you for different numbers of variables so let's start with 10000 biomarkers that were these curves here we're looking for hundred of the degree three or more is just looking at connectivity and the number of observations you need versus the critical threshold so if you want for example to discover correlations that are greater than 0.7 in magnitude then if you want to discover those correlations in terms of this connectivity right not hub degrees of greater than greater than than two or three then you're going to need about 50 samples 50 60 samples in order to hit that if you have 10000 biomarkers if you go to 10 billion biomarkers you only have to basically double the number one sample in order to retain the same correlation sensitive this is the blessing of big data right it's that you can accommodate enormous increases in the number of biomarkers with only a small increase in number of samples because of this logarithmic dependency between sample versus number P log P over N regime so samples by you a lot if you're in this sample star gravy now if you're in the sample rich regime out here you know they don't buy you a whole lot because we're getting pretty flat right so this is just showing some of its natural results since he's the respiratory virus challenge study I was mentioned before this appeared in several papers most recently in a Science Translational Medicine paper with his ass so inoculation is given at this point zero there pre-challenge Staffing red and green our subjects that started to become systematically ill at some point during the talent study this is time here so we've divided the post hoc the subjects that great symptomatic and those symptomatic advocacy groups the other at accolade of the opportunity 50% study and so these are gene chips that we collected this is in the previous generation technology because we did this experiment in 2008 this just shows you the data we did it for West Coast official virus 83 and 2hn on on flu also the common cold various patient's reckless clean and London University a total of over 2,000 samples with I'm not showing here these are the duration of the studies and you know over 120 subjects Susan it's pretty massive undertaking a challenge study that would do a wealth of data that I'm going to kill you just snippets of the results the other the papers you can see more so this is for the particular h3n2 study in fact the first thing we did d2 and d3 were to a tree and two studies the repetition 17 subjects were inoculated sampled over seven days leading to this heat map of each one of these is a is a gene chip with number of samples a partition this and to this the clinically determined asymptomatic and systematic categories post on set three onset of symptom and we had you used the brain or a Michigan brain or a custom definite you know you built a quite a while ago and lots of people are using yeah yeah yeah yeah and that we had symptoms that were scored to zero one two three on a ten symptom scale including things like you know what he knows fever and coughing and so now you look at the samples of these these categories that I showed you before we have clean ovulation sample post inoculation samples the post inoculation samples are symptomatic or asymptomatic classes we know not a number of snappings we know the number of biomarkers so we can compute these phase transitions so this in this regime the clean ovulation samples of the couldn't the critical threshold is point seven so anything you find it'll it's less than 0.7 in sample correlation is completely unreliable and if you want a ten to the minus six family-wise error rate threshold then you have to you have to raise that 0.92 yeah and so I'm in only gonna focus on on the pre-inoculated samples just as a to give you a taste of what we can do so this is now screening the partial correlation this is the inverse correlation as opposed to the correlation itself when we compute the inverse correlation through a pseudo inverse so what's called a more in rose generalized inverse because the inverse doesn't exist you only have you know new samples right you only have effectively thirty four samples but you know twelve thousand biomarkers see there's no inverse to this matrix but the pseudo inverse can be computed and the theory we developed applies to the pseudo inverse also under these assumptions that I was mentioning sparse correlation and what you can see is very interesting this is for the pre inoculation samples so these people are coming off the street there's no there's no treatment yet coming off the street this is measuring what is their interactome right and so what you've seen here are some paths metrics genes that are basically controlling housekeeping in in the Apple metrics beam chip and then we had some some nonspecific ribosomal gene protein as different factors here and now here he's very interesting these are part of the interferon an inducible transcription factor pathway that basically controls the new response system so total ike receptors not like receptors are in this pathway and this is just saying that their pathways are working right then the sentinels are there these these these individuals have they have working immune systems and so this this result if you look at just the correlation you find a mess connections and not modules that that one could not make any sense of but it's through the inverse correlation that you observe this very nice picture it really reveals the feeling and then you can also draw pictures like this which show as a function of the correlation that you found that i here I've set the correlation threshold to 0.92 the false positive rate of 10 to the minus 5 and so this would be for connectivity the degree of the node than the degree greater than one so this would be connected to at least two other genes the other genes and so forth and at this threshold we found biomarkers that were connected at 0.92 a greater to at most 14 other genes co-expressed at 14 other gene and then you can look at the various themes that fall along these categories and so to pick them off in this visualization of this 12,000 node network with many connections and I'm going to finish now with this particular design example where instead of looking for a CO expression between jeans you're looking at the correlation or the association between gene expression on these 12,000 genes and some other variables the mother biomarker which here is going to be simplest so we're going to try to effectively find those genes not just that are correlated together but that are highly predictive of the symptom there's a symptom in the future right so the concept here this is DARPA funded this study the concept was to basically cure before that was that was the bye word right now cure before symptoms occur yeah yeah right typical DARPA speak and it but but the idea you know certainly is very interesting that you can try to predict ahead weather if sensing will occur here's the symptom maps as a function of time for all the examples organized and symptomatic you can see the a lot of the ace of genetics with complaining of symptoms too so this is a difficult problem right yeah yeah the asymptomatic were so Aiko somatically so hyper people I must say also in here I'm actually this data don't we also have viral titration data from shedding there are a lot of hypoid mary's in this group so so yeah there's so it's a difficult problem we're relating their self-reported symptoms to molecular expression so don't expect to see you know numbers that will it'll throw you off you see the numbers are not going to be as impressive as one might expect if you were looking at correlating something that was more tangibly related to co-expression but my point here is to show that one can do significantly better using this progressive sampling strategy that was motivated by these so this shows why it makes sense to do progressive sampling let's say that you're doing experiments that are very costly he paid about two hundred fifty thousand dollars for each challenge study number of probes per chip in terms of pricing if you look at Agilent customer raises back you a couple years ago actually now oh they're very clean yeah so a couple years ago so it's basically linear right so what this means is you can pay a lot for a lot of phones and maybe a few samples and then you can back off and get a lot more samples with people chromis once you've discovered where the edges are that's that's what this yeah that's what this picture is made and so this just shows what we're trying to do try to predict these symptoms with the samples molecular level gene expression and the predicted correlations cleaning when you knock down the number of genes that you're going to assay as a function of the correlations that were observed with relatively fewer samples making lots of biomarkers weren't able to quantify that you're able to detect and so there's this is related to a lot of other approaches compressive sensing sparse regression and so forth you're not going to dwell on there's a theorem that comes with this that tells you how you should sample this theorem in particular is very important you have a total budget of T T dollars this is the number of samples you should allocate to get optimal detection performance this is the optimal prediction performance at the end of the day after you do this two-stage process screen a lot of with a large numbers of biomarkers a few samples then use the screen samples which are much fewer and purchase a lot more samples of it in the future and so n is the number of samples that you should should buy that have the full bio market set and it's only log of the total number what's happening so this this regime tells you that again you have a blessing of dimensionality that you only need a logarithmically a lakota allocate logarithmically in the number of total samples to the expensive one the rest can be cheap so you can take more samples but overall you've sampled fewer biomarkers on the average and this just shows the advantages so this is our Tuesday procedure is a function of n the number of samples will you drop from you know twelve thousand or ten thousand here to a fraction of that and as compared to the state-of-the-art methods which are very very far away from your will operate in points in terms of average prediction root mean square error we're also doing quite a bit better the computation is much much less than it is yeah it is doing these other types of methods which involved lasso involves optima optimization yeah linear programming and that the new proposed it's basically thresholding with the cylinders yeah we applied this to the symptom prediction problem and again these numbers aren't great in the absolute sense 0.69 5/3 in terms of needs prediction error it means that standard deviation is you know there are integer values right the zero one two three four of different symptoms levels so what we're saying is that you know you can predict within maybe two this type of standard deviation and but the important thing is that when you use this new method you reduce that by about 15 20 percent so you reduce the error by 15 to 20 percent simply because we're able to separate smartly and not sample in batches and devote our resources to sample sampling biomarkers that are not predicted we find out which ones are predicted first and then we sample and then these are just some of the some of the genes that came out which is important this is the conclusion so one requires lots of care in this big data setting where you have many people samples then biomarkers and as much less than P inadequate to apply the standard central limit theorem type of measures bootstrapping is very unreliable you know no matter how much bootstrapping you do if you have insufficient sample size you're always going to find spurious correlations and associations bootstrapping is completely data-driven so the data is not sufficient to to check itself you need to have a mathematical theory to tell you this what are those critical phase transitions as a function of the number of biomolecules so we have this new big data setting purely high-dimensional with universal peace there's a threshold and and I haven't discovered I haven't talked about these other topics of structured covariance these are forms acquire information that one can incorporate toklas isn't is a interesting one because hopeless means that you are that's correlated over time but you're stationary over things aren't the correlation isn't changing over time right so Copelan structures can be very useful when you're doing longitudinal serial in synaptic studies correlation from time to time and also correlations between a biomarkers and then one can also extend this to nonlinear correlations using a method that can be developed called measure transform correlation mining and spectral correlation mining with stationary time series again related directly to this copious dr. so that's basically the story I wanted to tell you today and I thank you for listening [Applause] yeah yeah yeah I've got to so the the purpose of this study was not so much to put together a tool as it was to demonstrate the advantage of doing this kind of aggressive sample fighting right but we are indeed concerned about how to apply this in a kind of setting where you're trying to do could actually accept it and so what we've done I haven't showed you the data here is that what I showed you in this example was separate networks for each symptom I that was averaged over all symptoms what we've done is we have also developed a network that best describes all symptoms right so that network is penalized or scored if you like on the basis of how well it discriminates not between the levels of a given symptom but the levels of several symptoms the the problem here is that with this panel of symptoms are 10 symptoms there the standard panel clinical panel that is used to compute Jackson scores for respiratory viruses but some of these viruses don't for example called a cause febrile illness right so fever is very difficult to predict right in in those situations because any report of fever being feeling feverish is probably going to be highly unreliable because it's self-reported so one has to specify which symptoms are the most relevant or the particular clinical objective that you have in mind and that that's what that's that's up to the clinicians to finish got a very good question exactly yeah yeah they're very descriptive and they're people didn't like that our hypochondriac and there's they're people that you know they see a needle and they they start to get symptoms right and here they're they're not seeing a needle what they're seeing is a is a cotton swab with h3n2 virus on the tip of it [Music] yeah great question so if you if indeed if you use correlations and I didn't I didn't emphasize this enough you use the standard correlations you get into this prop that's masking you know what a is correlated to b and b is correlated c then they're all quantity right and on the other hand if you look at the inverse correlation partial correlation then a correlated to be correlated to see the partial correlation between B between C given DNA only depends on B so so effectively that's why I didn't show you the other network which was a complete mess this network is the partial correlation network which eliminates some of those problems those masking problems not all but it does eliminate some of them and then what what one can do is as you suggest you can design the threshold so it's low enough so that you discover a little more than what you really would be comfortable with but you still have dropped down the total number of biomarkers you have to deal with by maybe a factor 10 and then you can you can focus into those and do a subsequent analysis yeah yeah yeah yeah yeah so it's you're talking about now having a time series of yeah maybe treatments well yeah exactly not the other yeah and that those are all yeah great open open problems let me put it that way and we're following up on the Lord we have we have a great data set that's longitudinal and serials that's Apple but we don't really have enough samples to look at the kinds of questions you're talking about so we still have to aggregate you know over large chunks of time so yeah he distinction wasn't in much smaller probably much smaller than P yeah we usually describe this as overfitting exactly it's a normal highlighted in the report from the National counter scientists on predictive omics based test called evolution of translational Comex yeah what I wanted to ask you is how much closer to P doesn't have to be to make some of these analyses feasible yeah and that that's that really gets to this this fatal I'm showing yeah let me go back to it just to put yeah and this really this really tells you what you need yeah this is this is the story I could have given the whole talk just for this one slide so if you look at these ratings forget about all these other just look at this one no this tells you you know if you're into your task is merely to estimate correlations and you don't want to over fit then you need to have basically and more than P you know it's got to be more than P and maybe that P is something as a p of a set of biomarkers that you've discovered using screening actually detection to come a much larger set of biologically but still in has got to be you know on the order of P log P in order to be able to do that reliably on the other end if you don't have that number of samples you have to find figure out which one of these regimes I am I in right am I here if I'm here or I only have log P of samples well I can't do network discovery the port detection finding the edges I can't certainly do estimate all the correlations correlation coefficients I certainly can't do you know ascription of uncertainty to those correlation coefficient but what I can do here is I can detect if there changes like between treatment a and treatment B in terms of the sample correlation networks that they describe so by using this type of table you can put yourself into the into the right task minutes we usually don't think about that in this in this setting I mean we usually think well I want to test these hypotheses and these hypotheses involved estimating the correlations with this level of confidence and now I do my sample sizing and I find out I can't afford it right what this tells you is take it take a step back see what you can't afford and then this table will tell you what you can do and then you can adapt right yeah yeah for the transitions threshold yeah what do you mean exactly what's the meaning of the priests religious threshold and how do you find it there are questioners but it depends on the underlying distribution yeah very good question so the phase transition threshold is the threshold of correlation coefficient that is the necessary and sufficient threshold for you to be able to reliably say that you're on the right side of this this bomb right but what do you mean exactly ly moly the only corresponding to a p-value yeah so yes so the the p-value I didn't show you in this theorem here but there's associated with this phase transition threshold which roughly speaking corresponds to a p-value pretty close to 10.95 or something like that if you specify a p-value you're going to get another threshold which are you threats by a threshold on your family lies error rate in your gran get a p-value threshold right and that'll be larger than this that's why in that example I was showing before were we looking at the baseline samples the phase transition threshold was at 0.7 right but if you wanted a p-value that was less than 10 to the minus fifth well then the threshold at 0.92 okay so the p-value is not shown explicitly in this picture but it comes right out of the field and now write your next question is what what in what sense does it depend upon the underlying assumptions so this there's a short answer is as long as you're a sample your samples come from a distribution that is elliptically conflict so it doesn't have multiple bumps it does have multiple bunts you're gonna have to cluster and focus in on one of the subclasses that has a politically contoured distribution as long as that ellipse that gives you the constant contours of your distribution isn't too flat meaning that there are collinearity right in your data meaning that there are correlations that are almost equal to one then you're going to get away with with this and so what we're saying is you can put down sparsity coefficient you can state that you have elliptically contour distributions and at that point these will be reliable yeah thank you [Applause] Thanks