[
    {
        "start": 15.04,
        "text": "if i could click the script all right so so the thing is is that you don't need any type of data in order to do artificial intelligence right the only time that you need data is if you don't understand the way the world works if you knew how the world work you could create intelligent systems that would be representative of what we already know and could give predictions and prognostications of what it is that we're supposed to do next you use data to try to give yourself some guidance now the thing is is that ai and this is not meant to be readable at the moment clearly it's not right ai has a long history you know ai itself is at least 80 years old at least from a computational sense now the thing is is that the notion of artificial intelligence and medicine if you could see this you can probably see it better online goes back to around the early 1970s maybe a little earlier and and it starts all with this paper that that "
    },
    {
        "start": 76.72,
        "text": "short life had published about um expert systems in the use of the diagnosis of infections within the blood okay now this was this was a paper that ted developed to try to figure out like what would it take to get a computational system to assist a physician in making diagnoses either at point of care or a point that is going to assist in the treatment of the patient in a relatively quick amount of time this is like a great vision of what artificial intelligence could be right and so this is think about this is 50 years ago at this time it's like well what happened right so it took until about 2019 for the federal government for the nih to put together a committee i guess it was 2018 to put together a committee that was going to look at specifically what were our needs with respect to ai and the sponsorship of biomedical research "
    },
    {
        "start": 138.08,
        "text": "now this is not to say that artificial intelligence completely languished for about 50 years it's just that the entire concept as like a field and something that is really going to drive biomedicine it took a little time to germinate now what was notable about this was that they published their report a couple years ago in this report there were a bunch of recommendations and these recommendations covered everything from make investments in infrastructure to come up with the ethical principles for what it would take to put ai out into practice both for research as well as for a clinical application so i'm prefacing this with ai because you know when i was initially asked to come in i was told you know it'd be great if you could talk about data science and like the whole point of this is like the future of data science but what you have to recognize is that data science itself is part of the bigger picture of ai and yet at some point it's not right "
    },
    {
        "start": 200.48,
        "text": "so what you have to understand about data science is that it's this really interesting amalgamation of a bunch of different fields and and so i'm standing here and a lot of you look at this from the perspective of bioinformatics okay but let's look at this and that it begins with knowledge you have to have some type of knowledge not necessarily data okay you also have to have some mathematics and statistics at the table and if you bring the two of those things together what you're looking at is biostats and then you bring the computational sciences in and when you bring the computational sciences in you have the opportunity to define bioinformatics or biomedical informatics which is where i live or you have the ability to find machine learning which was at the center of computer science and mathematics now that is squarely where ai ends up living okay but data science sits at the intersection of all three of these right it's where the knowledge really comes to drive what it is that you're trying to learn otherwise all you have is just foundational methodologies but we know that those foundational methodologies can fail spectacularly if you don't "
    },
    {
        "start": 262.0,
        "text": "understand the domain in which they're being applied in okay so data science itself again like ai it's not new so back in the 1960s john 2k in the future of data analysis really started coming up with the principles for what data science would be and if you're not familiar with 2k you really should take a look at like the history of statistics in this one i'll i'll talk more about this in a bit but in this paper he talks about a couple things it's like i really love the way that people wrote in the 1960s and scientific papers because it's completely different than the way we write today right so one thing he said was for a long time i thought i was a statistician interested in inferences from the particular to the general he said but but as i've watched mathematical statistics evolve i've had cause to wonder and doubt i want to write papers like this all right i have come to feel that my central interest is in data analysis which is intrinsically an empirical science and and then he goes on and he says how "
    },
    {
        "start": 322.0,
        "text": "vital and how important is the rise of the stored program electronic computer okay and what he what he went on to say was that the computer was going to be what drove everything we did because 2k believed in information visualization he said if you can't get intuition directly from the stats itself you may be able to get info intuition from looking at the data okay and that's where he saw this interplay between what it is for data to just be data versus this whole notion of human computer interaction and the ability to play with it so so let's fast forward to where we are now like we're at the point where we're generating data at an obscene rate this leads to a number of new challenges and this has been in the big data communities what they refer to as the big vs okay so so what we're seeing is an increasing amount of volume of the data we're going from terabytes to petabytes we're easily starting to hit exabytes in certain areas especially if "
    },
    {
        "start": 382.639,
        "text": "you're in imaging and the velocity at which this data is occurring is speeding up right it's coming in at ever faster rate and we're always being asked to study the data as quickly as possible right people want the data released from new studies almost instantaneously and they want to be able to do a new study overnight the variety and the heterogeneity of the data is also starting to change right it's not just all about one type of data there's multimodal data and the data has ambiguities and there's inconsistencies associated with it that we need to have a way of wrangling and resolving as we're doing this and then the veracity of the data comes into question all right now what i mean by veracity is do you trust the data there are errors that exist in the data people if you're collecting the data in terms of like electronic medical records systems you have to wonder is the patient telling you information that is trustworthy and that's not always the case and then finally i i i advocate for for a fifth v which is which is what i call value and that you need to be able to "
    },
    {
        "start": 443.919,
        "text": "demonstrate that the data that you've collected and that you're using has a benefit to the population that you're trying to apply it to and if it doesn't have value then you may be collecting the wrong data and you may be looking in the wrong space so so when you're talking about data science and when you're talking about machine learning more generally you know there's this there's this process that we really need to think through because if you don't represent the process you may end up working in an inefficient manner so you collect your data and then you're going to subject it to some type of an analytic process and and a lot of what we do today in machine learning is you can really think of it as a black box i don't think anybody's really going into a deep net is going to hand manipulate any of the edges that connect any of the nodes does anybody do it no you're not alone right so you use this tool right that somebody's built or you may have refined it yourself and then what comes out of that tool is some type of a model right you look at this model and you say you know does this gene interact with "
    },
    {
        "start": 504.72,
        "text": "this gene and does it have this influence on the outcome for a patient or for a research participant and you say does this make sense or are the confidence intervals something that like i'm not completely comfortable with and and if it's if it's not you go back and you retrain right you tune your system you tune your model until you're comfortable with it and if you're a graduate student you keep doing this until you make your investigator happy right at the same time you might say i don't have enough data and that's where you go to augment the system and you start looking around saying do you have data do you have data or can i go about generating more data right this is an entire process all of these are steps that if you address them appropriately you can rapidly reach speed up the investigational environment now where i'm coming from so i live in a med school i live in in the medical center itself and one of the places where we generate data is the electronic health record system or the emr depending on how you want to look at it "
    },
    {
        "start": 564.959,
        "text": "now i i really have to thank bill stead and dan macy's for this perspective and that this is an environment that is an opportunity to serve as a living laboratory now we don't want to do it you know instantaneous experiments on patients in an uncontrolled manner but as data scientists we should have the opportunity to look at this as an environment where we can issue a query pull information out of that environment study the data and then push knowledge back in in a manner that's going to facilitate the care of the patients but again this is a process but i'm arguing that this is an environment where you need to let the scientists be the scientists so i'm going to give you a couple of ways in which i think data science has an opportunity to drive what we do as scientific investigators in the biomedical domain and give you some illustrations of what we've been able to accomplish over the last couple decades and why we think this is the right way forward or an opportunity okay so principle one "
    },
    {
        "start": 626.24,
        "text": "is that you need to figure out how to make data more accessible and and so one of the ways that we did this at vanderbilt was that we took all the electronic medical records data which which sits in what i refer to as the operational environment that that's the hospital and we said one of the things that slows us down is when we ask for access to data to people who are managing these systems and making sure that the patients are receiving care when we ask for that data the response that we get is we'll get to that in a couple days it's like that that's not fast enough so what we did was we we created a pipeline and and it took a while to get the administration on board for doing this but in addition to that just siphoning off all of the data that was being generated in the system we we figured out that we needed to standardize it make it more easy for people to work with and so we we partitioned all the data into different types and then we had a whole set of standardization processes associated with these with with the notes with the clinical orders with "
    },
    {
        "start": 687.279,
        "text": "messages that took place the labs and so this is just all sitting in a research environment that's outside of the control of the operational setting so right now we've created this environment that you know it's around 2.5 million medical records and it just sits on on a computational environment that we make accessible for research okay so so that's step one now step two so once you make that data accessible you need to make analysis fast and and so one of the ways in which we did this is that we we licensed initially this this little super computer in a box all right so this is a native twin fin and and at first i didn't want it and then and then i'm working with people in the men's center and i i saw an illustration of this thing which this is a this is a warehousing appliance that the way that it was designed and we got to work with the group on this a bit is that it's a bunch of blade servers that are all rated together and then we put fpga chips on top of them and the fpgas "
    },
    {
        "start": 749.44,
        "text": "allow you to dynamically compress and uncompress the data such that you can do queries at scale efficiently what this allowed us to do was we didn't have to index medical records anymore to query them we took all of the data and we dropped it into 64 kb blobs to the point where if you were going to query the data you would get an answer back across a search over all of the free text on 2.5 million records in about a half a second i was demonstrating this to one of my my summer undergrads a couple days before i got out here and they were just like oh this is amazing one of the reasons why we did this was that when we made the data accessible to researchers and we said okay issue your queries see what you can pull out of the data you would issue the query we had all the data loaded on the oracle databases such that it was load balanced across like 50 databases would run people would wait for about five minutes to get an answer they go off they get a cup of coffee and they look at the answer go that's not exactly what i want and then they'd issue another query and they do all this again okay when we made "
    },
    {
        "start": 812.0,
        "text": "the data accessible this was in 2007 we had about 10 people who were power users of the system and they were interested in doing this and they were willing to do this over time when we moved it over to the super computing type of environment we ended up ramping up the number of people who were interested in it because you could do interactive query response you were able to address your queries in a refined manner you could do this over time with almost instantaneous results you made the data playable and so we moved from about 10 users to about a thousand users over the course of a three year three month period the other thing that we did was that we took the time to like build research pipelines and and if you're in genomics or in your proteomics i think you're familiar with the principle of just figuring out how do you create a process of taking the data moving it through the analytic infrastructure and then building it out into like some type of a visualization so that you don't have to rewrite your code over and over "
    },
    {
        "start": 873.12,
        "text": "and over again um so the paramo system was a was a project that that i worked on with some friends over at ibm to try to figure out how do we speed up queries over electronic medical record systems and how do we build machine learning models on top of this one of the things that we did along the way was that and you're not meant to be able to read this this is just a dependency graph what we found was that a lot of the statistics that we were interested in computing they they drew upon the same foundational raw counts and intermediate statistics that were going to be useful over and over again and so we just said let's compute that once so that we can only to we only have to have this process you know we can optimize this process and so we ended up speeding up the creation of a machine learning uh model from going from like about 10 to 20 minutes and we brought it down on average to about 20 seconds okay so so this is all about making things faster making things more accessible then the "
    },
    {
        "start": 933.199,
        "text": "question becomes cost because what i just explained to you was we spent millions of dollars in investment in infrastructure that worked at vanderbilt but in reality what a lot of people are doing is they're rushing out to the cloud and they're going out to google cloud platform or they're going out to aws and they're saying i don't have to have my own servers the public cloud is already out there a lot of the tool boxes that i'm interested in working with sit out there so why not move out into the cloud okay so here's the day of an introduction to the day if somebody works in the cloud so you got your data you subject it to your analytic infrastructure sitting on the cloud you run it once you spend you know a decent amount of money and the first time you run it what happens you get junk right the first time you run this i guarantee the answer that you're looking for is not going to be there this is almost always the case when somebody runs a machine learning framework all right so what happens next seeing your favorite wants to shout it "
    },
    {
        "start": 993.6,
        "text": "no you pay more money and you run it again right you tweak some things you get a little bit more data and you run it again right and then what happens out comes something that looks a little less junky right it's like okay i'm moving in the right direction okay so so i i say this out of like experience with some of my grad students and post-docs who have burned through thousands to tens of thousands of dollars trying to build their machine learning frameworks out in this domain right so next step i can try yeah there we go okay so so you throw lots and lots and lots of money at this and eventually you get to the point where you do something so amazingly wonderful you get a nobel prize right i just in this but but you know this is the principle that what ends up happening is that right now when you move everything out to the cloud for biomedical research infrastructure it is not cost effective and and this is just like a poor allocation of funds this is still an open problem though i "
    },
    {
        "start": 1053.84,
        "text": "don't have a solution to this what i'm actually advocating for and this is why i say there's a promise associated with biomedical data science is that we need to figure out what's the right type of a hybrid strategy and so this is what we started working on in our group where you do some things local you do a lot of model tinkering local on your own servers and then when you're ready to run it at scale you move it out into the cloud but how exactly that gets managed that's not really solved yet okay so the next thing that you need to work through or that an illustration of what we've worked through is this notion of regulatory frictions and and i get this question a lot it's like okay well that's really amazing that vanderbilt could build up that infrastructure but how do you speed up the time to access the data they're going through the irb and so what we did was that we took the irb out of the equation and we ended up we we generated a completely de-identified i say completely de-identified de-identified as a is a legal term but we "
    },
    {
        "start": 1115.679,
        "text": "de-identified all the medical records system and therefore whenever somebody wanted to gain access to this we said we've already gone through the entire process of getting a non-human subject's designation and therefore you don't have to make an application and wait months for access anymore now this was done we created what was called the synthetic derivative on top of our research derivative um and then and then we extended the research the synthetic derivative to include biospecimens and and i could spend a very long time explaining like the entire process for biospecimen collection but this was done in the context of clinical care this was leftover blood specimens that we had consent for using in coordination with the medical records for research purposes and so we we just in this situation it's a consistent hashing process so that we've got a consistent way of updating these records over time and this is an environment that we've we've been generating for the last 15 years um what's notable about this is remember i said there's there's a process you "
    },
    {
        "start": 1176.559,
        "text": "can't see exactly what it says up at the top but this process though we started the creation of this resource back in 2007 it went back several years before that and we really wanted to make sure that we were engaged with our community so that we weren't using data in a manner that was considered to be inconsistent with their expectations and so there were a whole series of focus groups and patient surveys and we ended up running newspaper stories um out in like the tennessean which was our major newspaper just so that people were aware of what was going on we said this is going to happen we're going to start collecting your specimens and combining with your medical records for research purposes and so we went through all these steps to get there and then when we went and we got approval we told everybody we're going live and we didn't go live we did a six-month test run where we said here's a phone number for somebody that you can call if you have a concern with what we're doing we got a couple phone calls that phone number went directly to my department "
    },
    {
        "start": 1237.52,
        "text": "chair at the time and and the major argument was not from most of our patients the argument was actually from faculty within the university who disagreed with doing research in general it was a really long story on that um cool so we started and and once we made that data accessible where we had the data and we're humming we created a two-tier access model the first one was for initial hypothesis generation because not everybody wants to see the data all the time so we created an interface it looks a lot like i2b2 if you're familiar with it and and people can issue what you see is what you get queries and then we give them back aggregate results there was no oversight associated with this whatsoever um the second step was once you have an intuition of what you want to do next is that you you come to us and you you bring you you enter into a use agreement and and this is where we say we're going to give you individual level records but we also don't want you sharing all this information with the rest of the world there's not complete "
    },
    {
        "start": 1299.36,
        "text": "accessibility at this point so that's the point at which we give out the individual level records and people are able to start doing research with it we did not set up the infrastructure at that time that allowed people to run any type of the analytics on our own servers so at that point there was still this separation and said you still need to have a computing environment on hand to do this analysis and that was because everybody wanted more flexibility um a couple things that we did along the way that i think are notable uh one was that we we created a natural language processing uh environment for doing de-identification of the clinical notes uh initially we we worked with a spin-off from upmc which was called did and we ended up wrapping their technology with some pre-processing and post-processing strategies and and i'm saying here that we did some tests where we showed that we tuned the systems that was hypersensitive so that we could get recall on patient identifiers that was like almost at 100 but you could never completely guarantee "
    },
    {
        "start": 1360.64,
        "text": "it um that was based on a lot of rules it was a lot a huge dictionary and regular expression based system and i was really concerned that we were spending way too much money on this and so we we then looked at what could we do from a machine learning perspective and we started playing around with pick your favorite machine learning methodology um we ended up really focusing on conditional random fields to make the scalable and reusable at other institutions um and so the the methods that we developed ended up getting adopted into a couple of different technologies out there one was called hyde from lee shang's group in emery and then another one is is the one that has kind of become the standard which is what's called mist that that john aberdeen developed with us um and and then more recently i've been doing a lot of work at the at the mayo clinic and with a new company that's called inference and we developed an ensemble-based learner approach that's based on transformers like bert and and so this is a technology that that you know has become way more scalable way more reusable "
    },
    {
        "start": 1423.279,
        "text": "but the way that initially was designed was that it had this this redaction problem and and so redaction's good you want to get rid of the identifiers but you run into this residual problem like i showed you before the recall was 0.999 but that means that every once in a while you might have a name pass through and so we were like well that's that's not really a good thing um and so that that forces you into a situation in which you go well there's there's an unknown level of risk associated with this like it might say like the senator's wife or it might say like you know the name of a child and so we said look this is why the use agreements are in place at this point but we said could we could we push this further and so we were like okay one one of the things that you could do and seem to make sense was that for all the residual information what if you took all the redacted data and then you replaced it with fake information that looked like the real data and so this turns into what we call like a text-based steganography solution or it's hiding in plain sight "
    },
    {
        "start": 1484.48,
        "text": "and we're like okay well looks good but let's bring this in front of humans and let's ask them if they can tell the difference so a lot of numbers here short version of the story is that the humans get really confused okay they're just not good at telling the difference all i'm saying here is that what that meant was that that effectively raised the de-identification performance to a level that we felt extremely comfortable in sharing this information i'm now working on the institution to try to allow some of this data to go out into the public domain um but it runs into what's called a parrot problem if you're not careful and this is where i want to show you like this is one of the ways in which machine learning goes point so so the parent problem goes as follows let me just make sure we are on time here power problem that goes as follows i use machine learning to learn a pattern to detect where all of the identifiers are using the real data okay annotate the data find the things that you've "
    },
    {
        "start": 1546.0,
        "text": "annotated now there's going to be some things that are residual so let's throw all the fake data into where the real data was all right let's throw that data across the pond let somebody else look at it now here's the challenge if that person assumes that i've done really well in doing the de-identification then what they can say is i'm going to annotate all of the identifiers that i see in your data as an identity as fake information even though some of it is not real even though some of it is not fake and i'm gonna go train my own model using the fake information now at that point that parroting or that mimicking of what we've done is going to allow them to learn the model that i used to find the identifiers and so when they detect fake information and pull it out the only thing that's remaining was the real stuff that i failed to get the first time around so this is a dangerous precedent to set so it looked really good but the parent problem was real "
    },
    {
        "start": 1608.0,
        "text": "um and so we said okay so why does this happen this happens because machine learning migrates towards finding patterns that are representative of the system at a large so we said okay let's train the way change the way we do training so we we we introduced a new strategy for machine learning which said let's do training once and then let's hold out all the things that we were good at getting the first time around and then let's retrain the system with a focus on all the things that were considered to be difficult the first time around now let's make that a second model all right so do the initial training let's look at all of our false positives and retrain again and then let's pull all those out and let's focus on the things that were really hard again and then let's do that one more time or like let's just keep doing this until we can't learn a better model anymore and so this is this is what you call an iterative scalable learning framework um this was a this was a strategy that we "
    },
    {
        "start": 1669.44,
        "text": "showed that when you do this with with medical records data and we did this with a public data set from the itb2 program we showed that it required about four steps in order to get to a point where you had a much more high fidelity system such that when somebody tried to parrot us they could do no better than random we felt really good about this we felt really smart um so we said okay cool so now we know that if we're going to do privacy if we're going to do something at a level that we feel comfortable with that we're not really going to be attacked down what could we do to show the value of this data because it's one thing to create the resource but until i get to go back to the ceo of my medical center and say uh you know there's a reason why you're investing million dollars in us or millions of dollars in this you know like let me let me give you some idea of what the return on investment looks like um so let me just i'm going to walk you through what a couple of people in in my center are doing with the data so "
    },
    {
        "start": 1730.08,
        "text": "so so tom lasco oh nothing shows up here wow you can't see it um okay so this is a visual so so tom here you go um what you would see are are like lines you in this situation you'd see your pattern uric acid patterns over time um what tom did was that he developed basically the first deep learning approach using gaussian processes to show that you could come up with different types of trends and that you could see that there were different phenotypes of patients that existed in the system some who were more responsive than others to various types of care um so so they're learning new types of phenotypes on the state of clinical phenotypes um and and they're allowing for more richer representations so you get better correlation with outcomes more recently what they've been working on is that they can show that by looking at the lab data you can get predictions of breast cancer at an order of around three to four years earlier than when "
    },
    {
        "start": 1791.2,
        "text": "the diagnosis shows up in the medical record that doesn't mean that we didn't know that they had cancer it just means that the diagnosis wasn't there um with the natural language data jejunin's group is trying to understand what the semantics are and what the sentiment is that is expressed by patients when having a discussion with their providers so it's like why would you care about the sentiment at that point so one of the things that he was really interested in was you put somebody on tamoxifen after subjecting them to a double mastectomy and you say look you need to stay on this for five years in order for this to really have some type of of an effect and preferably for ten years but but when you listen to what the patients say you learn all of these things about why they're having trouble staying on the regimen some of it is financial difficulties some of it is an artifact of side effects and what what jejune ended up learning when working with a with a cognitive psychologist was "
    },
    {
        "start": 1853.039,
        "text": "that you can you can learn the different uh personality types of the patients and you can see when individuals are going to be trending towards staying on the the the uh the regiment versus when they're not um if you is anybody familiar with like the five big factors of personality yeah so if if you use those it turned out that there was like one huge driving factor to determine if patients were going to stay on their regimen and that was narcissism not kidding you so if you if you look at the way people talk they were like they were there were people who had fear of failure and they they did not want to present themselves to their family or to their friends as people who were quitters and they were just very concerned with their own self-preservation and we were just like oh that's really interesting now we don't want to make all breast cancer patients narcissistic not necessarily right but we do want to use that fact to say that we want to try "
    },
    {
        "start": 1914.799,
        "text": "to figure out how do we create systems that orient providers to recognize when patients aren't necessarily exhibiting symptoms of of staying on on track okay so what's interesting about this was that we were only able to do this once we de-identified all the patient portal communications because the electronic medical record system was mainly about documentation done by physicians and nurses and it didn't take the patient's voice into account so this is this is a whole project that led to a national cancer institute study or an r1 that started a couple years ago to try to figure out how to expand this to other types of treatments um another thing that you can do that that might not be expected right i know that i'm talking about bioinformatics department but let me give you some intuition to what you can do with healthcare administration um so so yo chen's work uh who i'm proud of because one of my former postdocs showed that that if you look at the way the organization interacts with the medical record system and thus is interacting with each other "
    },
    {
        "start": 1975.12,
        "text": "then you can see organizational structures and you can use that organize structural org structure to give some intuition into whether or not a patient is going to fare well in their management and and so what he showed through various types of social network analysis strategies uh was that you could find bottlenecks with respect to post-trauma management and he was able to show that for some types of patients if they hit like the right type of management structure they'd be getting out of the paper out of the hospital about a day or two earlier which which is substantial and that's that's really impressive in terms of like the amount of time that you save the amount of money that you save and it reduces to a lower amount of morbidity for the patients okay so that's just an illustration of what you can do with medical records data both obvious and non-obvious so now you want to get bigger and you want to get better so these are demonstrations of what happens at vanderbilt but you you really want to have safety in numbers and so what do i mean by that "
    },
    {
        "start": 2035.519,
        "text": "so we we have this entire infrastructure that's talked about this a process for learning and so now i want to look out and i want to come up to michigan and i want to say do you see what i see could you do the same studies locally that i've now done right and so i wanted to have a situation in which you're able to run exactly what we've done you know i would love to be able to create doctorized environments where i just send this to you and say give me your answer right the whole point of this is to generate robustness in the patterns that we learn so how do you get there so the the way that which we started this was with with the establishment of the electronic medical records and genomics network so this was an nhgri sponsored program that started back in 2007 um and and it really was this consortium of everybody that kind of looked like us and and what we found man this just looks horrible is that you you you would generate a phenotype like so for instance if you wanted to study patients with diabetes or type 2 "
    },
    {
        "start": 2095.76,
        "text": "diabetes in the ehr you would develop it locally and then you take that phenotype strategy and then you throw it across the fence and you say hey use the same thing with your environment tell me what your recall and precision is with doing this and then we learn what worked and what didn't work it was this diagnosis code defined correctly for you do you use a different diagnosis code um we had like illustrations for instance where we'd find that at vanderbilt we would use a a diagnosis of like irritable bowel syndrome whereas up at northwestern they were saying crohn's disease and so we just had to have some type of a harmonization there but this was an entire process and we thought that we could automate it extremely difficult to automate right out of the gate so what we ended up doing was we created a public resource that everybody could contribute to and this became phkb or the phenotype knowledge base and so fee kb was everybody could throw up what their definition of a phenotype was so "
    },
    {
        "start": 2156.8,
        "text": "that other people could see what was being done at other institutions and then they could provide comments and feedback on what worked for them and what didn't this is this is kind of like the baby steps towards trying to get coordination um more recently this doesn't look that great more recently we got to the point where remember i told you that the nih was starting to get interested in ai a couple about about a year ago they began uh they issued this rfa for this new program called bridge to ai where they want new data sets that are going to be generated across different sites and they're going to do this uh in a manner that's going to make the data publicly accessible and reusable so so this is a program that's supposed to start in about a month and i would i would encourage you to keep track of what's going on here uh there's some things i can't say publicly yet okay so so this is the notion of getting more data and getting more people involved but then we also want new data "
    },
    {
        "start": 2217.119,
        "text": "types all right so i encourage you to get to broader data so we've talked about dna we've talked about some things with socioeconomic status and medical records and there's all types of clinical trials data but what we're not seeing so much is all the other stuff that people want to use to try to understand what's the etiology of patient disease and management so like how do you get into like retail data how do you integrate social media data fitness records get lifestyle information and then how do you do this with respect to families how do you do this with respect to like a parent and then what's going on with their children so this is where the all of us research program comes into play is anybody familiar with all of us some of you okay so all of us was uh something that francis collins started you know about six years ago seven years ago it was signed into like we got a special designation from the white house to do this like it was a special allocation it did not come out of the nih's budget um basically before obama left office he "
    },
    {
        "start": 2278.16,
        "text": "signed off on an allocation of around 1.2 billion dollars it's now grown to about two billion dollars to say we want to build a million person cohort where it includes medical records genomic data as well as fitness information and and patient reported outcomes so we were interested in this because this sounded a lot like what we were already doing at vanderbilt in the emerge network and and so what you can't see it here but what we we built it uh so vanderbilt ended up becoming the data and research coordinating center michigan was was part of that so the the center is actually distributed across vanderbilt michigan northwestern uh the broad institute and university of texas health sciences center um the browser let me see if i can can i make this smaller it just goes like this okay so if you're not familiar with it i "
    },
    {
        "start": 2338.16,
        "text": "strongly encourage you to go to our data browser so this is going to show you all the data that we have in an aggregated fashion for all the participants so at the present moment in time we're at about 400 000 participants as as we're growing and so you can look from the ehr domain what you can't see here is that as of last month we just released the first batch of uh whole genome sequencing on everybody it's 98 000 whole genome sequences uh as well as the survey questionnaires that we've put together um this is just an illustration of of the types of things that that you'll find in there and so what you see up here even though you can't see it very well is that this says 160 000 and this says pain okay uh this is an example of this is traumatic injury at right around a hundred thousand um now what we what we ended up doing was we created a researcher workbench and this was where we partnered with verily and the broad institute to take "
    },
    {
        "start": 2399.92,
        "text": "example uh to reuse the terra bio system and and what the way that we architected this and the way it's currently running is that it's all sitting out on google cloud platform and then on top of gcp we put all the data into a bigquery environment and then on top of bigquery you have jupyter notebooks that sits on top of it and so when you come into the system you can create your own virtualized jupyter notebooks environment and then start interacting with the data and so it's available now we're now up to about a thousand users and it's it's growing on a daily basis um as i was alluding to this this is this is a tiered level of access so we have public data the aggregate summary statistics and then we created sandbox environments for everybody to work in um the first is what we revealed two years ago which is what's called the registered tier where you get access as long as you're a trusted organization and you enter into a data use agreement with us specifically and and we de-identified all that data to a low risk of identification "
    },
    {
        "start": 2462.24,
        "text": "and then the controlled tier is what we just released two months ago and that's that's where you get all the genomes um i'm gonna run out of time i could spend a long time walking you through how de-identification works in this environment but i'm not i'm not going to do that what i do want to highlight is is kind of like the future of data science and and some of the things that need to be addressed and and specifically i wanted to to highlight this notion of of data skew um and i refer to this as a you know you get skew in you get skew out some people refer this as bias but it's not bias it's it's data skew you know so an example of like so when we do de-identification of that data to put it out the door there are certain types of demographics that we need to hide in order to make sure that the data is not readily identifiable particularly for small populations and so what happens is like so you don't have many transgender individuals you hide the data you don't have many pacific islanders you hide the data you don't have many people from rural settings so you hide the data so in "
    },
    {
        "start": 2523.52,
        "text": "other words this notion of de-identification it's only useful when you have data of a sufficient size so what that means is that you have a privacy utility trade-off and let me put this up and so so what ends up happening is that this is what it looks like for a majority population which in the u.s has typically been you know caucasians and however when you shift over to minority populations you you end up with a much faster trade-off and then with your super minority populations your extremely small populations it's an even faster trade-off now this assumes everybody has an initial level of utility that's equivalent but in reality this is this is uh it's not quite like that it's more like this right this is the problem with equity so when you say that there is a fixed privacy requirement that everybody needs to adhere to according to hipaa or according to the common rule that that what ends up happening is that everybody ends up with a different level of "
    },
    {
        "start": 2584.96,
        "text": "protect a different level of utility at the end of the day okay so so this brings up the question of fairness right and a lot of people are now starting to look into fair algorithms and algorithmic bias and so so what we need to recognize and figure out how to solve is that when you amend minority data more than the majority data you end up in a situation in which you get to learn more about the majority which means you will make better policies for the majority population and the rich so to speak get richer so you have to ask this question of you know should you apply an excessive amount of majority records should we should we say that should be should you amend more majority records in order to achieve balance or should we create policies that allow for a better trade-off between utility and privacy so that we figure out that we say it's not just privacy that's driving whether or not data can go out the door but societal value so there's a push for diversity in this "
    },
    {
        "start": 2645.68,
        "text": "respect and that's the future all right so i i think that that everybody recognizes this this history that if you look at genetics research over the last 50 years there's been like a huge white bias and so if you if you look at the studies that have been done what you see up here is this is the number of individuals involved in studies this is from a patient paper that we published with all of us about a year ago um that the majority of individuals that have been involved in these studies have been caucasian and then studies have not always been about caucasians but you still have over 50 percent focused on caucasian populations uh and and the big drawback to this is that when you go to build risk predictors or when you go to build algorithms based on these populations they don't generalize well to some of the minority populations this is something that we've seen over and over again so so one of the things that we've done within all of us and and i'm here to encourage everybody to do this with all the other cohort studies that they're working on is that we've made it "
    },
    {
        "start": 2707.76,
        "text": "our our goal to ensure that that we have more than 50 representation from groups that are been underrepresented in biomedical research over over the last century now that doesn't mean that we get to a point where the balance has completely tipped in that direction because we're still in an active recruiting stage but but we're working to create a more diverse resource as a result um and and so then this is also where the nih i think has recognized that there's an opportunity and so the about five years ago we we had the big data to knowledge push towards enhancing diversity in biomedical data science and this was their first batch of r25s and then and then more recently this this call just ended uh there was an r25 push to get uh more diversity in biomedical informatics and data science careers and so there is an active investment that the nih is starting to make but these are only r25s for those that are familiar these are these are "
    },
    {
        "start": 2768.16,
        "text": "relatively short-term training environments it's not the same as making major investments in huge training programs so so we've taken it upon ourselves to invest some of the money that that we've received at all of us to create these types of training opportunities uh and so we we have created a minority student research symposium and we started partnering with with historically black colleges and universities as well as minority serving institutions to try to show what you can do with the resource that we have as well as highlight what we don't have the ability to do um i'll i'll leave you with this part just just couple things um so one the nih did institute a new program last summer that i'm i'm one of the pis for which is called aim ahead which is the artificial intelligence and machine learning consortium to advance health equity and researcher diversity big mouthful right which is why it's called ahead and the whole goal of this is to create these relationships with hbcus and msis to really get them "
    },
    {
        "start": 2829.04,
        "text": "involved more in the research process so that we have more researchers and not just data but researchers who are trained from underrepresented groups um if you're interested in learning more about this you can go to aimahead.net and so we are going to be doing a summer symposium uh on health equity and ai that uh is going to be done at the end of this month and so the details are up on that website um i think you know i i can talk as i said to somebody before this presentation started i can talk until the cows come home uh i have a whole nother section prepared but i think i'm going to pause there because i'd rather give you guys some time and so i'll stop and i'll say i'll say thank you for for inviting me to come to talk with you it's been two years in the making when brian asked me to come out and give this talk it was uh it was a couple months before the pandemic hit and then and then it was i remember it was april of 2020 and he goes we're gonna we're gonna try and get this you know to work out in may and i was like "
    },
    {
        "start": 2889.28,
        "text": "brian i don't think this is gonna happen you know so so it took two years but but i sincerely appreciate you allowing me to come and speak with you all thank you and i know it took a little extra time but if you have questions like i guess we should we should take them now all right i just want to ask you a question about incentivization about incentivization um okay exactly um so our incentivization both from the researchers perspective and the participants perspective for example i've seen that a lot of the minority populations might think that what's the point of enrolling when it's not going to really help me so "
    },
    {
        "start": 2949.44,
        "text": "how do you incentivize that branch second is for the researchers a lot of this is uh [Music] data cleaning a lot of the stuff that nobody wants to really do how do you incentivize that for the people who have ideas but not the incentive structure to do it okay so let me let me take those one at a time i'll take the easier one first um this is crazy the easier one is the the participant encouragement um so there's there's two components to this one is that you really have to listen and you have to ask what they want out of the program this was before all of us started we we did a road show of studios not promotional but we we went around and worked with various not-for-profit organizations as well as patient representatives "
    },
    {
        "start": 3012.8,
        "text": "and ask them you know like what type of research do you want to have done what type of data do you want to have collected at the same time you you and so that allowed us to prioritize certain types of information over other types of information um one of the the things that we learned was that was that it turned out privacy was not a big driving factor for a lot of a lot of participants they they really just said are you going to use the data so that either i or people who are like me get benefit and if you do that we're on board with it now that's not the only thing that you're supposed to do but but this notion of having empathy and and recognizing that it's not just data that it's people and that you can somehow get back to them that's critical so we did not initially have plans on doing any return on results to participants and and after we ran the road show we said this is going to become a priority and this was tricky because you're going "
    },
    {
        "start": 3074.88,
        "text": "to generate whole genome sequencing data on people and you're going to give it back to them and you're gonna and so we said okay well if we're gonna do that then we really need to have some type of genetic counselors at the table thus we entered into a partnership with color genomics um and so so we now have genetic counselors that are right now handling they're not getting requests from 100 000 individuals but they are definitely getting a decent number of requests for what does this mean the answer isn't always certain so this is still an exploration and process so that's that's one thing that you have to recognize and this is one of the things i try to do within my own program and tell my own students that that at the end of the day you know you can't just say that data is data especially if you're pulling it from humans and so i was telling some people about this last night that a lot of the computer scientists and the data scientists that are in the informatics program i i send them out to shadow out in the hospitals and and they have to do this otherwise it's like so for instance yo chen when he was a postdoc and he was starting to get involved in like like looking at "
    },
    {
        "start": 3135.839,
        "text": "trauma data i was like you need to go spend some time in trauma uh and and he came back to me after about a week and he said i hope i never see a gunshot wound again you know so it's like but he had a much better understanding of how the physicians interacted with the patients and how they were doing documentation um the second question which is about wrangling and about how do you do data curation i don't have a good answer for this it's not sexy it's not fun nobody likes to do data curation the only thing that i can recommend at the moment is that is that you do it once and you only do it once this was this was a big problem for us i'll give you another example of where we were able to address this when we were making our de-identified medical records data accessible for research we created unique ids for every record for every research study what we learned was that the investigators were getting their postdocs and students to do data "
    },
    {
        "start": 3197.28,
        "text": "cleaning and we were not asking them to redeposit that back into our environment and so they would say okay well i'm going to do a study on like t2d and now i'm gonna go do a study on hypertension and they're gonna issue their query and they're gonna see records on the same same patient population and they're gonna go i just annotated and curated you know a hundred thousand of these records why can't i not link these two environments and so that that led to a change in policy where we said okay you know what unique ids it's static it's for the same investigator but it's also for everybody within the institution and that was just like a really simple tweak that allowed us to do redis uh redepositing of all of the annotation that was done and so it's a process that kind of drives things moving forward but the process itself is not fun it never will be fun right data cleaning is just something that like with all of us it took us almost two years to we we have an entire quality and cleaning core "
    },
    {
        "start": 3258.64,
        "text": "that robert carroll and and chiang hwa wang but that put colombia run and i don't want to be them you know but but they do a great job of it that uh looks at automated means of uh pipeline development and so on uh can the data curation task be accommodated by such a pipeline that's a seated question um you we talked a lot so what what what brian's alluding to is so so dan atkins uh chaired this national academies committee that we just published the report on for automated workflows in the future of biomedical science research or scientific research and one of the things that we do talk about in that report is the need to have better data curation strategies and have that embedded in in the system but we also in that report if you get a chance "
    },
    {
        "start": 3320.16,
        "text": "to look at it we talk a lot about human in the loop with with ai driven workflows and that you're always going to need the human to be there but you want to make it easier for the human to interact with the system so you don't want to have curation like for instance you don't want curation to be done outside of the environment and then have errors been you know occur when you bring the data back in you want to have the entire curation process as one part of the pipeline we haven't solved how to do this yet that's that's kind of next generation automated workflow that's what we talk about all right so i'll go ahead and ask the announce question so um vienna suaren asked a question of um thanks for a great presentation brad on the research infrastructure "
    },
    {
        "start": 3381.119,
        "text": "de-identification say different use cases may need different ver different versions of data when building an infrastructural resource like the did data set how do you design ahead or build variations as you were went along [Music] oh uh okay great question so does everybody understand that question short version of the question i'm going to i'm going to rephrase what vienna said is that so let's say that you need to have different versions of the data to solve different problems let's say that you need one data set that's highly specific in geographic detail and you need another data set that's highly specific in like like date of birth for instance you're studying neonates um can you have two different versions of the same underlying data set the answer is yes but there's both a computational solution to determine whether or not the two data sets could be used to triangulate against the underlying data um and then at the same time you "
    },
    {
        "start": 3441.76,
        "text": "the way that we've done is we don't we don't test for triangulation but you could which the census bureau does when they share lots of data uh we we ended up just putting it onto policy and procedure in that this is just a matter of making sure that there's their separation of those data sets when we've created different versions of the underlying population that's that's the short version of that answer um you let me read it all right really interesting talk thank you uh no they said thank you lots to chew on and related to your points about students going into the hospital but with current future public access to the resources you're you've generated what do you find to be the most exciting and most concerning eg unintentional negative impact kinds of applications from these data that's easy okay so i have problems sleeping i have problems sleeping because i've "
    },
    {
        "start": 3502.0,
        "text": "been running privacy and security for all of us for about two years now and um for anybody who's ever managed very sensitive information resource what i'm worried about is somebody who was like me as a graduate student i know that sounds weird but i was the type of guy who wanted to just poke holes in the system and show how you re-identify people uh and and i can tell you that the only the data is very it's not easy to re-identify people we never say it's impossible though what i worry about is that there's going to be somebody who enters into a use agreement with us and then just completely ignores the use agreement and does one of two things either they snarf all of our data and they throw it up onto a publicly accessible environment that we no longer have any control over or they actually do a re-identification and say to to hell with your youth agreement people are being put in risk and and i need to demonstrate this to the world "
    },
    {
        "start": 3563.119,
        "text": "um i like that keeps me up at night i i worry about our program showing up on the front page of the new york times uh in a very negative way um so that that's that's one of the greatest concerns um the the other the other question about you know what is what is the most exciting the most exciting in my opinion is that it what's going to be happening with bridge to ai over the next couple years is that you're going to see an investment in the ni by the nih to create six very large data sets that are going to ensure that we have diversity and representation at the table in a manner that we haven't seen this for these types of data sets historically so so that that's like a really great opportunity it's going to evolve over the next couple of years so so in response to arvin's question and your response what do you do with sort "
    },
    {
        "start": 3624.16,
        "text": "of repetitive tasks that people don't love to do there was a quite compelling pbs newshour segment a few weeks ago about hiring intentional hiring of large groups of autistic individuals who were quite competent and quite reliable supposedly to do these kinds of tests any experience along those lines i do not however i will give you another illustration of some of the work i've done with daniel fabry so um we had a grant from from the office of director a couple years ago to create crowdsourcing systems um specifically for annotation purposes for machine learning and um so that we we've this we've developed a system published a couple papers on it that we've used at vanderbilt and a couple other sites where we've we've hijacked the med "
    },
    {
        "start": 3686.319,
        "text": "students and the physicians and the nurses into helping us do annotation of records uh and so we've embedded into student classes where you know we we're giving uh we'll give an assignment for annotation for like t2d as an example um and so we created the infrastructure to like bring forward to them de-identified records and we just want we'll ask them like does this record have x or y or z or where does it occur in in the record um now the the thing about this is that as with any ad annotation you need to get reliability and so what we showed is that you you really need to get a decent number of people annotating in order to get certainty in the annotations um so we didn't go to autistic individuals but we did go to people who had slightly better than random knowledge about medical phenomena it's worked so far for about 10 different projects "
    },
    {
        "start": 3747.92,
        "text": "but it's still very much in its early stages i think it has legs but we do need to figure out how to scale it up because there's only so many times that you can go back to the well to get people to invest their time um i would say that there have been illustrations of how to do this at scale with the concept of what's called games with a purpose uh and so if anybody's interested in this you you should look at the work of luis von on uh has anybody ever seen captchas or or recaptchas you know i'm sure you've seen it uh so this so luis invented captchas when he was at yahoo but but the you know the whole notion of a captcha where recap recapture was really great because like it was partnered with the million book program where in order to get access to something you forced somebody to type out what letters they saw on the screen so it's like all these old books very difficult to read and so you get an answer the answer isn't always correct but you get enough people to give you the answer on average is "
    },
    {
        "start": 3809.44,
        "text": "going to be correct so crowdsourcing may be a way of scaling this up don't know exactly how it's going to be done yet i don't know how long how much time we still have um so i'm one of your participants in the study i'm in it and and i actually uh i'm one i'm like you early so i'm going to try to figure out if i'm going to find myself based on rare variants that i have access to from other sources about myself and see if i'm in the thing i got blood drawn about two months ago so i think i am in the batch that you are there okay um so i do think uh this re-identification is an issue but i do also think there isn't very much evidence i think people worry about it too much i "
    },
    {
        "start": 3870.319,
        "text": "think even though you are worried about it i am with george church that if i put my snip profile on the internet not that much bad things will happen and i i mean i did buy long-term care insurance before i did any genetics um the other question is a more scientific one and that is about 90 or so of human variation is in africa and most variation is in europe and asia and the ld profile in africa is lower so if meaning shorter so even if we had as many africans as we had europeans in all our big jiva studies we would not be able to predict as much in those populations how are you going to go about fixing that part of discrepancy which is kind of one of the underlying biology "
    },
    {
        "start": 3934.559,
        "text": "i'm not going to answer your second question i don't have an answer to it i think it's a great opportunity for people to provide feedback and write papers on how to address this so i'm not going to answer that question um is that okay um so so so you're let me let me jump on the first question if you don't mind um because that's a big one for those that know me like i spent a lot of my time doing research and privacy and identifiability work um so your your perspective on privacy is one person's perspective on privacy and unfortunately i so i first of all i agree with you wholeheartedly i think that the problem of identifiability has been overblown however i don't think that this is based on my experience i don't believe that the re-identification problem is one of a personal privacy issue i actually think that the reason why we could have problems is that if "
    },
    {
        "start": 3996.4,
        "text": "there are promises that are made by the organizations that are sharing the data if it is shown that those promises cannot be adhered to then it ends up providing less faith in the research infrastructure itself and and i worry about what the implications of that where somebody goes the nih gave us this promise and the nih broke that promise will not matter who was re-identified will not matter how they were re-identified what will end up happening is that there will be a perpetuation of stories in the media about how horrible of a research sponsor the nih is or vanderbilt is as being the steward of that data so if an individual is interested in providing their information into an environment where there's less control like pgp like what george has set up great that is one opportunity for doing so but if there's a federal agency that's taking responsibility for that sharing of that information there's a different situation in hand so it's not just all about personal preference "
    },
    {
        "start": 4058.88,
        "text": "this is where things are going nih funded to do their disease research in the rdc and then other networks we have formally evaluated that and it gets back to margaret's point it's what your study participants tell you you ask them they gave you an answer so that privacy is not their main concern participation is and particularly for minority population and now a largely wide audience tells non-wide audience oh we know the truth about privacy and therefore we will exclude you from participating in studies sufficiently i think the key point is and we are doing that in the niddk kpmp purposefully that it's not us it's the participants who are making these policies and those participants derive are featured as such to participants "
    },
    {
        "start": 4120.719,
        "text": "and we have press releases prepared if we show up on the new york times where our participants will respond to these communications and so be very careful that we don't perpetuate the same pattern as we have done before if we say we are listening to study participants we have to even if they tell something to us where we feel we would not make that statement no no it's a great point and so and so as we're definitely not in disagreement um it is really a matter of tailoring the data disclosure policies to the expectations of the participants the there's only one thing i will caveat this with which is a bit of research from the economics community in the behavioral economics community um so one of the things is that most many people uh are very poor reasoners about risk in the future um and that and that if you offer them a "
    },
    {
        "start": 4181.359,
        "text": "service right now in exchange for information or exchange for an acknowledgement that they will accept risks in the future most people go i'll accept that service i'll become a part of your your project or i will i will you know click through the end user licensing agreement on your website and just give you all the information that you want about me um and then when there's actually something that happens in the future people have a very vitriolic reaction and and they go oh well if only i had like known what exactly those risks were so nervous we explained that to you at the beginning and so that puts you in a very strange situation because you're then saying you took the we took your perspective and we opened up the system sometimes it's better initially to have a bit more protection and then slowly open it up over time once we have a better understanding of how the system's going to play out i think this is a learning process i "
    },
    {
        "start": 4242.0,
        "text": "think that we're in the midst of changing our traditional views of what research infrastructure and what cohort studies look like but it's going to take time again we're in agreement "
    }
]