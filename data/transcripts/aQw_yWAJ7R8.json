[
    {
        "text": "I think we'll go ahead and get started",
        "start": 0.0,
        "duration": 102.659
    },
    {
        "text": "just the introduction my last you can",
        "start": 101.22,
        "duration": 5.009
    },
    {
        "text": "sort of get settled in welcome everyone",
        "start": 102.659,
        "duration": 5.131
    },
    {
        "text": "tools the technology seminar series I",
        "start": 106.229,
        "duration": 3.061
    },
    {
        "text": "think you've pretty much been here",
        "start": 107.79,
        "duration": 3.899
    },
    {
        "text": "before there's a sign in sheet going",
        "start": 109.29,
        "duration": 4.89
    },
    {
        "text": "around please sign it as a reminder it's",
        "start": 111.689,
        "duration": 4.351
    },
    {
        "text": "just to help us with the pizza so that",
        "start": 114.18,
        "duration": 5.07
    },
    {
        "text": "we can prove that we had people here and",
        "start": 116.04,
        "duration": 5.1
    },
    {
        "text": "therefore continue to get pizza for",
        "start": 119.25,
        "duration": 6.03
    },
    {
        "text": "future years or semesters I guess anyway",
        "start": 121.14,
        "duration": 6.54
    },
    {
        "text": "he's to introduce speaker today Alex",
        "start": 125.28,
        "duration": 4.8
    },
    {
        "text": "Clinton who's a graduate student and DC",
        "start": 127.68,
        "duration": 5.91
    },
    {
        "text": "MMB and it's going to talk to us about a",
        "start": 130.08,
        "duration": 5.92
    },
    {
        "text": "neural networks for biomechanical image",
        "start": 133.59,
        "duration": 7.509
    },
    {
        "text": "analysis thanks Marcy thanks for having",
        "start": 136.0,
        "duration": 9.049
    },
    {
        "text": "me and yeah so I'm first year our",
        "start": 141.099,
        "duration": 6.301
    },
    {
        "text": "fourth-year PhD candidate in the",
        "start": 145.049,
        "duration": 5.711
    },
    {
        "text": "department and I was already told that",
        "start": 147.4,
        "duration": 5.88
    },
    {
        "text": "my team is matching my origin that",
        "start": 150.76,
        "duration": 4.08
    },
    {
        "text": "everyone is big and red because I was",
        "start": 153.28,
        "duration": 4.89
    },
    {
        "text": "born in USSR I guess that's a success",
        "start": 154.84,
        "duration": 5.88
    },
    {
        "text": "already and what I'm going to talk about",
        "start": 158.17,
        "duration": 5.37
    },
    {
        "text": "today I'm going to talk about I'm gonna",
        "start": 160.72,
        "duration": 5.129
    },
    {
        "text": "give an introduction in the neural",
        "start": 163.54,
        "duration": 3.96
    },
    {
        "text": "networks at artificial neural networks",
        "start": 165.849,
        "duration": 4.14
    },
    {
        "text": "deep version of them and the kasam",
        "start": 167.5,
        "duration": 4.909
    },
    {
        "text": "convolutional architectures and then",
        "start": 169.989,
        "duration": 4.771
    },
    {
        "text": "briefly mention some of the exciting",
        "start": 172.409,
        "duration": 4.39
    },
    {
        "text": "applications in biomedical image",
        "start": 174.76,
        "duration": 4.409
    },
    {
        "text": "analysis and then in the end if I have",
        "start": 176.799,
        "duration": 4.77
    },
    {
        "text": "time left I will also talk about some of",
        "start": 179.169,
        "duration": 5.07
    },
    {
        "text": "my work on a sparse commercial neural",
        "start": 181.569,
        "duration": 4.26
    },
    {
        "text": "networks for nuclear shape",
        "start": 184.239,
        "duration": 3.9
    },
    {
        "text": "classification so before we start it in",
        "start": 185.829,
        "duration": 3.63
    },
    {
        "text": "the traduction I would like to ask you",
        "start": 188.139,
        "duration": 4.5
    },
    {
        "text": "guys how many of you do not know how",
        "start": 189.459,
        "duration": 5.911
    },
    {
        "text": "artificial neural network works like",
        "start": 192.639,
        "duration": 6.93
    },
    {
        "text": "show all right fair share I guess so",
        "start": 195.37,
        "duration": 6.359
    },
    {
        "text": "yeah I actually did this is the biggest",
        "start": 199.569,
        "duration": 3.931
    },
    {
        "text": "part the introduction so I actually try",
        "start": 201.729,
        "duration": 5.01
    },
    {
        "text": "to be as clear as possible on these",
        "start": 203.5,
        "duration": 4.949
    },
    {
        "text": "little steps introducing the",
        "start": 206.739,
        "duration": 3.481
    },
    {
        "text": "architecture so I probably spend the",
        "start": 208.449,
        "duration": 5.43
    },
    {
        "text": "most time most time here so you have",
        "start": 210.22,
        "duration": 7.47
    },
    {
        "text": "this the intro so basically these these",
        "start": 213.879,
        "duration": 5.76
    },
    {
        "text": "models then there are machine learning",
        "start": 217.69,
        "duration": 4.169
    },
    {
        "text": "models there are family of biologically",
        "start": 219.639,
        "duration": 5.301
    },
    {
        "text": "inspired algorithms invented around 50s",
        "start": 221.859,
        "duration": 6.151
    },
    {
        "text": "later they have been outperformed by",
        "start": 224.94,
        "duration": 4.96
    },
    {
        "text": "ACMs and random forests that became more",
        "start": 228.01,
        "duration": 4.74
    },
    {
        "text": "popular more like to go tools for",
        "start": 229.9,
        "duration": 5.549
    },
    {
        "text": "analyzing data for classification and",
        "start": 232.75,
        "duration": 8.729
    },
    {
        "text": "regression but later into thousands alex",
        "start": 235.449,
        "duration": 8.51
    },
    {
        "text": "debt and some other new networks",
        "start": 241.479,
        "duration": 5.13
    },
    {
        "text": "architectures started that so-called",
        "start": 243.959,
        "duration": 5.291
    },
    {
        "text": "deep learning Renaissance and now they",
        "start": 246.609,
        "duration": 4.71
    },
    {
        "text": "are super popular and the working where",
        "start": 249.25,
        "duration": 3.959
    },
    {
        "text": "you will and then usually there are a",
        "start": 251.319,
        "duration": 3.48
    },
    {
        "text": "couple of reasons that people talk about",
        "start": 253.209,
        "duration": 3.361
    },
    {
        "text": "when they say why they're working now",
        "start": 254.799,
        "duration": 4.981
    },
    {
        "text": "versus 50s and you know previous century",
        "start": 256.57,
        "duration": 5.819
    },
    {
        "text": "because we now we have lots and lots of",
        "start": 259.78,
        "duration": 5.04
    },
    {
        "text": "labeled data that is available for you",
        "start": 262.389,
        "duration": 4.331
    },
    {
        "text": "know for",
        "start": 264.82,
        "duration": 4.41
    },
    {
        "text": "for people for researchers and we have",
        "start": 266.72,
        "duration": 5.01
    },
    {
        "text": "much more computational power including",
        "start": 269.23,
        "duration": 5.59
    },
    {
        "text": "some specialized higher hardware and",
        "start": 271.73,
        "duration": 4.44
    },
    {
        "text": "this specifically I will talk a little",
        "start": 274.82,
        "duration": 5.09
    },
    {
        "text": "bit later about the progress in in GPUs",
        "start": 276.17,
        "duration": 8.19
    },
    {
        "text": "so and then when I mean that they're",
        "start": 279.91,
        "duration": 8.28
    },
    {
        "text": "working now that means that many",
        "start": 284.36,
        "duration": 6.06
    },
    {
        "text": "computer vision applications many",
        "start": 288.19,
        "duration": 5.29
    },
    {
        "text": "machine learning applications these",
        "start": 290.42,
        "duration": 6.42
    },
    {
        "text": "neural networks outperformed all these",
        "start": 293.48,
        "duration": 5.46
    },
    {
        "text": "models including you know previous",
        "start": 296.84,
        "duration": 3.96
    },
    {
        "text": "versions SVM's random forests and",
        "start": 298.94,
        "duration": 3.75
    },
    {
        "text": "established new state-of-the-art for",
        "start": 300.8,
        "duration": 4.05
    },
    {
        "text": "example this renaissance started it alex",
        "start": 302.69,
        "duration": 4.08
    },
    {
        "text": "net when one of the biggest computer",
        "start": 304.85,
        "duration": 5.87
    },
    {
        "text": "vision competitions on image recognition",
        "start": 306.77,
        "duration": 5.57
    },
    {
        "text": "2012",
        "start": 310.72,
        "duration": 4.39
    },
    {
        "text": "this application this application of",
        "start": 312.34,
        "duration": 5.02
    },
    {
        "text": "this neural network dropped the previous",
        "start": 315.11,
        "duration": 5.43
    },
    {
        "text": "result error by 10% and it was the",
        "start": 317.36,
        "duration": 5.52
    },
    {
        "text": "biggest drop you know they were",
        "start": 320.54,
        "duration": 5.49
    },
    {
        "text": "gradually reducing the error year after",
        "start": 322.88,
        "duration": 6.0
    },
    {
        "text": "year but in 2012 using this net they",
        "start": 326.03,
        "duration": 4.98
    },
    {
        "text": "were able to achieve 10% increase in",
        "start": 328.88,
        "duration": 5.67
    },
    {
        "text": "accuracy so that why that's why this",
        "start": 331.01,
        "duration": 5.61
    },
    {
        "text": "brought a lot of attention to these kind",
        "start": 334.55,
        "duration": 5.85
    },
    {
        "text": "of models and they are a very coarse",
        "start": 336.62,
        "duration": 7.68
    },
    {
        "text": "model of neurons in the brain so this is",
        "start": 340.4,
        "duration": 7.89
    },
    {
        "text": "what was sort of developed in early 50s",
        "start": 344.3,
        "duration": 8.04
    },
    {
        "text": "60s based on the how people would try to",
        "start": 348.29,
        "duration": 8.28
    },
    {
        "text": "imagine very simplistic realization of",
        "start": 352.34,
        "duration": 7.8
    },
    {
        "text": "the human brain neuron and so this is",
        "start": 356.57,
        "duration": 6.54
    },
    {
        "text": "still you know valid to the day so and",
        "start": 360.14,
        "duration": 5.19
    },
    {
        "text": "then this is the central part I think of",
        "start": 363.11,
        "duration": 5.1
    },
    {
        "text": "the any neural network functionality",
        "start": 365.33,
        "duration": 3.99
    },
    {
        "text": "this is how it works",
        "start": 368.21,
        "duration": 3.42
    },
    {
        "text": "and it's fairly simple actually because",
        "start": 369.32,
        "duration": 5.04
    },
    {
        "text": "you know you have the axon coming in",
        "start": 371.63,
        "duration": 5.25
    },
    {
        "text": "from some other neurons and it comes to",
        "start": 374.36,
        "duration": 5.4
    },
    {
        "text": "to this neuron and then it brings some",
        "start": 376.88,
        "duration": 5.82
    },
    {
        "text": "kind of a signal signal has a value then",
        "start": 379.76,
        "duration": 5.52
    },
    {
        "text": "in a synapse there is another value",
        "start": 382.7,
        "duration": 5.34
    },
    {
        "text": "called a synaptic strength it basically",
        "start": 385.28,
        "duration": 5.61
    },
    {
        "text": "regulates how much of this signal comes",
        "start": 388.04,
        "duration": 4.94
    },
    {
        "text": "into the neuron into the cell body and",
        "start": 390.89,
        "duration": 5.64
    },
    {
        "text": "then this neuron has multiple you know",
        "start": 392.98,
        "duration": 6.189
    },
    {
        "text": "multiple inputs from other neurons that",
        "start": 396.53,
        "duration": 3.42
    },
    {
        "text": "are connected to",
        "start": 399.169,
        "duration": 3.031
    },
    {
        "text": "and they're all burning the signal and",
        "start": 399.95,
        "duration": 5.07
    },
    {
        "text": "then this synaptic thanks is just a",
        "start": 402.2,
        "duration": 5.22
    },
    {
        "text": "multiplicative coefficient that gets",
        "start": 405.02,
        "duration": 4.68
    },
    {
        "text": "multiplied by the input so you get some",
        "start": 407.42,
        "duration": 4.86
    },
    {
        "text": "portion of the input coming in into the",
        "start": 409.7,
        "duration": 4.95
    },
    {
        "text": "system body so this is I think fairly",
        "start": 412.28,
        "duration": 4.2
    },
    {
        "text": "simple concept when you have when you",
        "start": 414.65,
        "duration": 4.38
    },
    {
        "text": "have many inputs and this many inputs",
        "start": 416.48,
        "duration": 5.25
    },
    {
        "text": "get multiplied by some coefficient those",
        "start": 419.03,
        "duration": 4.56
    },
    {
        "text": "coefficients their property of the",
        "start": 421.73,
        "duration": 5.07
    },
    {
        "text": "neuron and then in the model not in the",
        "start": 423.59,
        "duration": 5.1
    },
    {
        "text": "human brain but in the model we consider",
        "start": 426.8,
        "duration": 3.9
    },
    {
        "text": "them parameters so they are learnable we",
        "start": 428.69,
        "duration": 4.08
    },
    {
        "text": "didn't know them when we start learning",
        "start": 430.7,
        "duration": 5.31
    },
    {
        "text": "from the data and then so body also very",
        "start": 432.77,
        "duration": 5.01
    },
    {
        "text": "simple things happen we take this",
        "start": 436.01,
        "duration": 4.98
    },
    {
        "text": "coefficients multiplied by inputs we sum",
        "start": 437.78,
        "duration": 5.01
    },
    {
        "text": "all of them across all the inputs that",
        "start": 440.99,
        "duration": 5.91
    },
    {
        "text": "this neuron has and then we add bias and",
        "start": 442.79,
        "duration": 6.27
    },
    {
        "text": "bias basically is just another value",
        "start": 446.9,
        "duration": 5.04
    },
    {
        "text": "that is corresponding to each cell",
        "start": 449.06,
        "duration": 5.43
    },
    {
        "text": "parting and that it means it's basically",
        "start": 451.94,
        "duration": 5.94
    },
    {
        "text": "in a ability of each neuron to fire so",
        "start": 454.49,
        "duration": 7.17
    },
    {
        "text": "if the bias is large then even given the",
        "start": 457.88,
        "duration": 7.41
    },
    {
        "text": "small input that probably is this neuron",
        "start": 461.66,
        "duration": 6.03
    },
    {
        "text": "is gonna probably fire these biases or",
        "start": 465.29,
        "duration": 5.16
    },
    {
        "text": "even negative then it's probably not",
        "start": 467.69,
        "duration": 4.65
    },
    {
        "text": "gonna fire even if it's received a",
        "start": 470.45,
        "duration": 4.47
    },
    {
        "text": "strong signal from other from other",
        "start": 472.34,
        "duration": 6.77
    },
    {
        "text": "neurons and in the end there is a",
        "start": 474.92,
        "duration": 5.85
    },
    {
        "text": "activation function which basically",
        "start": 479.11,
        "duration": 5.59
    },
    {
        "text": "means this function that the function",
        "start": 480.77,
        "duration": 6.47
    },
    {
        "text": "determines if neuron is going to fire",
        "start": 484.7,
        "duration": 5.64
    },
    {
        "text": "given the input and given the bias given",
        "start": 487.24,
        "duration": 3.85
    },
    {
        "text": "this summation",
        "start": 490.34,
        "duration": 3.42
    },
    {
        "text": "so basically output of the neuron is",
        "start": 491.09,
        "duration": 5.33
    },
    {
        "text": "going to be this function applied to",
        "start": 493.76,
        "duration": 6.03
    },
    {
        "text": "separation all the inputs multiplied by",
        "start": 496.42,
        "duration": 8.26
    },
    {
        "text": "coefficients and the bias so basically",
        "start": 499.79,
        "duration": 8.31
    },
    {
        "text": "if this sum is above certain threshold",
        "start": 504.68,
        "duration": 6.57
    },
    {
        "text": "if you consider for example this output",
        "start": 508.1,
        "duration": 5.31
    },
    {
        "text": "be in binary for simplicity just for a",
        "start": 511.25,
        "duration": 4.53
    },
    {
        "text": "second let's assume this is binary then",
        "start": 513.41,
        "duration": 4.95
    },
    {
        "text": "this function is basically a threshold",
        "start": 515.78,
        "duration": 5.699
    },
    {
        "text": "that decides if this big enough to fire",
        "start": 518.36,
        "duration": 5.04
    },
    {
        "text": "or not and if this sum is big enough",
        "start": 521.479,
        "duration": 3.481
    },
    {
        "text": "than neuron fires if the sound was not",
        "start": 523.4,
        "duration": 3.72
    },
    {
        "text": "beginning of the new another fire so",
        "start": 524.96,
        "duration": 6.08
    },
    {
        "text": "this is all fairly simple",
        "start": 527.12,
        "duration": 3.92
    },
    {
        "text": "okay so let's consider a binary neuron",
        "start": 532.36,
        "duration": 7.3
    },
    {
        "text": "and then for historical reason was",
        "start": 537.079,
        "duration": 5.971
    },
    {
        "text": "called perceptron in 60s so consider",
        "start": 539.66,
        "duration": 5.19
    },
    {
        "text": "this perceptron consists of just one",
        "start": 543.05,
        "duration": 4.89
    },
    {
        "text": "neuron it has two binary inputs so x1 x2",
        "start": 544.85,
        "duration": 5.58
    },
    {
        "text": "only can take zeros and ones the values",
        "start": 547.94,
        "duration": 5.22
    },
    {
        "text": "can be 0 and 1 we know the weights let's",
        "start": 550.43,
        "duration": 4.86
    },
    {
        "text": "assume that we already you know learn",
        "start": 553.16,
        "duration": 4.38
    },
    {
        "text": "them or we know a priori these weights",
        "start": 555.29,
        "duration": 6.69
    },
    {
        "text": "are both - to the biases 3 and it's a",
        "start": 557.54,
        "duration": 6.419
    },
    {
        "text": "step activation function that very",
        "start": 561.98,
        "duration": 3.39
    },
    {
        "text": "simple step activation function",
        "start": 563.959,
        "duration": 4.351
    },
    {
        "text": "everything below zero ends up being zero",
        "start": 565.37,
        "duration": 5.43
    },
    {
        "text": "everything above the zero ends up being",
        "start": 568.31,
        "duration": 5.1
    },
    {
        "text": "one this is the function f that we're",
        "start": 570.8,
        "duration": 4.5
    },
    {
        "text": "going to apply to the summation of this",
        "start": 573.41,
        "duration": 5.39
    },
    {
        "text": "you know these inputs and the bias and",
        "start": 575.3,
        "duration": 6.0
    },
    {
        "text": "so let's write different inputs for this",
        "start": 578.8,
        "duration": 6.37
    },
    {
        "text": "neuron consider this binary input 0 + 0",
        "start": 581.3,
        "duration": 6.899
    },
    {
        "text": "X 1 and X 2 so we buy basic calculation",
        "start": 585.17,
        "duration": 6.719
    },
    {
        "text": "you multiply 0 by -2 0 by -2 at 3 you",
        "start": 588.199,
        "duration": 9.0
    },
    {
        "text": "get 3 and then 3 you apply F 2 to 3 that",
        "start": 591.889,
        "duration": 8.88
    },
    {
        "text": "you ended up getting here you get 1 so",
        "start": 597.199,
        "duration": 6.211
    },
    {
        "text": "when you have 0 + 0 this neuron outputs",
        "start": 600.769,
        "duration": 7.351
    },
    {
        "text": "1 so basically it fires if you take 0 &",
        "start": 603.41,
        "duration": 9.09
    },
    {
        "text": "1 0 by negative 2 is 0 1 by negative 2",
        "start": 608.12,
        "duration": 8.329
    },
    {
        "text": "is negative 2 plus by as 3 is 1 and F of",
        "start": 612.5,
        "duration": 7.529
    },
    {
        "text": "1 is 1 so it's 1 again basically neuron",
        "start": 616.449,
        "duration": 6.64
    },
    {
        "text": "fires again the other way around is the",
        "start": 620.029,
        "duration": 4.5
    },
    {
        "text": "same because the weights are symmetric",
        "start": 623.089,
        "duration": 3.87
    },
    {
        "text": "both negative 2 so 1 0 is going to be",
        "start": 624.529,
        "duration": 5.18
    },
    {
        "text": "the same outputs one neuron fires again",
        "start": 626.959,
        "duration": 6.18
    },
    {
        "text": "if you consider 1 + 1 it's going to be",
        "start": 629.709,
        "duration": 7.18
    },
    {
        "text": "negative 2 plus negative 2 and plus 3 so",
        "start": 633.139,
        "duration": 6.991
    },
    {
        "text": "that's going to be negative 1 and then",
        "start": 636.889,
        "duration": 5.401
    },
    {
        "text": "the our activation function of this",
        "start": 640.13,
        "duration": 5.34
    },
    {
        "text": "negative 1 is going to be 0 so only when",
        "start": 642.29,
        "duration": 7.229
    },
    {
        "text": "it's 1 1 this movie is not firing it",
        "start": 645.47,
        "duration": 6.599
    },
    {
        "text": "always fires except when both inputs are",
        "start": 649.519,
        "duration": 6.781
    },
    {
        "text": "1 and that corresponds to response to",
        "start": 652.069,
        "duration": 6.87
    },
    {
        "text": "simple negative end operator basically",
        "start": 656.3,
        "duration": 5.729
    },
    {
        "text": "you only get zeros when there when there",
        "start": 658.939,
        "duration": 5.34
    },
    {
        "text": "is only get once when there is",
        "start": 662.029,
        "duration": 5.43
    },
    {
        "text": "all of them is zero but one both ones",
        "start": 664.279,
        "duration": 5.271
    },
    {
        "text": "they're zero and this is the opposite of",
        "start": 667.459,
        "duration": 5.19
    },
    {
        "text": "logic and logical operation such a",
        "start": 669.55,
        "duration": 6.729
    },
    {
        "text": "negative end so basically this shows how",
        "start": 672.649,
        "duration": 6.06
    },
    {
        "text": "using this very simple binary perceptron",
        "start": 676.279,
        "duration": 8.16
    },
    {
        "text": "you can model logical gates and negative",
        "start": 678.709,
        "duration": 9.48
    },
    {
        "text": "end or you know exclusive or any of",
        "start": 684.439,
        "duration": 6.15
    },
    {
        "text": "these operations can be melted by just",
        "start": 688.189,
        "duration": 7.95
    },
    {
        "text": "one neuron and when you start",
        "start": 690.589,
        "duration": 9.11
    },
    {
        "text": "considering the many layers you can do",
        "start": 696.139,
        "duration": 8.87
    },
    {
        "text": "what complex operations so basically",
        "start": 699.699,
        "duration": 9.971
    },
    {
        "text": "what we said work is lifting we build",
        "start": 705.009,
        "duration": 7.75
    },
    {
        "text": "these weights we have data that are",
        "start": 709.67,
        "duration": 5.25
    },
    {
        "text": "inputs and we have the light correct",
        "start": 712.759,
        "duration": 4.95
    },
    {
        "text": "labels or output of regression and we",
        "start": 714.92,
        "duration": 4.68
    },
    {
        "text": "don't know this weight so we want to",
        "start": 717.709,
        "duration": 4.261
    },
    {
        "text": "approximate this weight the problem is",
        "start": 719.6,
        "duration": 5.399
    },
    {
        "text": "perceptron models that given the binary",
        "start": 721.97,
        "duration": 8.25
    },
    {
        "text": "way binary inputs and binary outcome we",
        "start": 724.999,
        "duration": 8.491
    },
    {
        "text": "don't know have much of a small change",
        "start": 730.22,
        "duration": 5.729
    },
    {
        "text": "in the weight will lead to the small",
        "start": 733.49,
        "duration": 4.349
    },
    {
        "text": "change in the input because input is",
        "start": 735.949,
        "duration": 5.7
    },
    {
        "text": "always gonna stay either 0 of 1 and then",
        "start": 737.839,
        "duration": 6.06
    },
    {
        "text": "even if you change your weights a little",
        "start": 741.649,
        "duration": 5.19
    },
    {
        "text": "bit gradually by learning here if output",
        "start": 743.899,
        "duration": 4.831
    },
    {
        "text": "is always gonna be it doesn't matter if",
        "start": 746.839,
        "duration": 3.99
    },
    {
        "text": "you move from one or two or three it's",
        "start": 748.73,
        "duration": 4.02
    },
    {
        "text": "always going to be just 1 so it doesn't",
        "start": 750.829,
        "duration": 3.961
    },
    {
        "text": "provide you feedback how changing the",
        "start": 752.75,
        "duration": 3.899
    },
    {
        "text": "weights reflected and they change of the",
        "start": 754.79,
        "duration": 6.57
    },
    {
        "text": "outcome so this is necessary for",
        "start": 756.649,
        "duration": 6.93
    },
    {
        "text": "learning to know how change in this",
        "start": 761.36,
        "duration": 4.289
    },
    {
        "text": "changes this weight how does it change",
        "start": 763.579,
        "duration": 3.99
    },
    {
        "text": "the output so we can have a feedback",
        "start": 765.649,
        "duration": 4.771
    },
    {
        "text": "from the model so basically we cannot",
        "start": 767.569,
        "duration": 5.101
    },
    {
        "text": "learn from data in perceptrons unless",
        "start": 770.42,
        "duration": 6.06
    },
    {
        "text": "it's a completely binary data because",
        "start": 772.67,
        "duration": 7.8
    },
    {
        "text": "output is binary so yeah I mentioned",
        "start": 776.48,
        "duration": 5.359
    },
    {
        "text": "that and the solution would be",
        "start": 780.47,
        "duration": 3.599
    },
    {
        "text": "approximated as binary step function",
        "start": 781.839,
        "duration": 5.97
    },
    {
        "text": "with some continuous function to replace",
        "start": 784.069,
        "duration": 6.841
    },
    {
        "text": "this function with something continues",
        "start": 787.809,
        "duration": 4.661
    },
    {
        "text": "that gonna provide you not just binary",
        "start": 790.91,
        "duration": 3.899
    },
    {
        "text": "response but would actually show you how",
        "start": 792.47,
        "duration": 5.25
    },
    {
        "text": "much if you change your weights how much",
        "start": 794.809,
        "duration": 8.401
    },
    {
        "text": "our output is changing and some common",
        "start": 797.72,
        "duration": 7.29
    },
    {
        "text": "activation functions that are providing",
        "start": 803.21,
        "duration": 3.35
    },
    {
        "text": "the alternative to step function are",
        "start": 805.01,
        "duration": 6.51
    },
    {
        "text": "sigmoid or logistic logistic function",
        "start": 806.56,
        "duration": 7.24
    },
    {
        "text": "basically squashes real number to the",
        "start": 811.52,
        "duration": 4.95
    },
    {
        "text": "range between zero and one so basically",
        "start": 813.8,
        "duration": 6.27
    },
    {
        "text": "you have from zero and one and this sort",
        "start": 816.47,
        "duration": 5.55
    },
    {
        "text": "of like a smooth version of step",
        "start": 820.07,
        "duration": 4.26
    },
    {
        "text": "function but the good thing about the",
        "start": 822.02,
        "duration": 5.28
    },
    {
        "text": "sigmoid is that you get real value",
        "start": 824.33,
        "duration": 5.25
    },
    {
        "text": "output is not just zero and one you get",
        "start": 827.3,
        "duration": 6.87
    },
    {
        "text": "you know basically continued continuous",
        "start": 829.58,
        "duration": 6.99
    },
    {
        "text": "values so if you change you change your",
        "start": 834.17,
        "duration": 4.62
    },
    {
        "text": "weights a little bit and weights are not",
        "start": 836.57,
        "duration": 4.8
    },
    {
        "text": "binary you can actually get mad binary",
        "start": 838.79,
        "duration": 4.35
    },
    {
        "text": "response how much your output is",
        "start": 841.37,
        "duration": 4.38
    },
    {
        "text": "changing in the sigmoid function couple",
        "start": 843.14,
        "duration": 6.95
    },
    {
        "text": "of versions of similar functions are",
        "start": 845.75,
        "duration": 8.01
    },
    {
        "text": "hyperbolic tanh squashes real numbers",
        "start": 850.09,
        "duration": 6.22
    },
    {
        "text": "range between negative 1 and 1 actually",
        "start": 853.76,
        "duration": 3.99
    },
    {
        "text": "zero sanphet which is good for",
        "start": 856.31,
        "duration": 4.38
    },
    {
        "text": "optimization purposes and this thing is",
        "start": 857.75,
        "duration": 6.09
    },
    {
        "text": "also very similar but it's actually",
        "start": 860.69,
        "duration": 5.31
    },
    {
        "text": "scaled version of Sigma also very",
        "start": 863.84,
        "duration": 4.68
    },
    {
        "text": "similar to step function also smooth",
        "start": 866.0,
        "duration": 6.75
    },
    {
        "text": "outputs the real values and more",
        "start": 868.52,
        "duration": 6.93
    },
    {
        "text": "recently introduced and more commonly",
        "start": 872.75,
        "duration": 6.36
    },
    {
        "text": "used these days is rectified linear unit",
        "start": 875.45,
        "duration": 5.88
    },
    {
        "text": "it's also sort of an approximation of",
        "start": 879.11,
        "duration": 5.82
    },
    {
        "text": "the step function except it's it's the",
        "start": 881.33,
        "duration": 6.68
    },
    {
        "text": "same zero below zero except it's the",
        "start": 884.93,
        "duration": 5.85
    },
    {
        "text": "linear with the slope one when it's",
        "start": 888.01,
        "duration": 5.47
    },
    {
        "text": "about zero and calculates like as",
        "start": 890.78,
        "duration": 5.04
    },
    {
        "text": "maximum between 0 and X and what it",
        "start": 893.48,
        "duration": 5.73
    },
    {
        "text": "gives its zeros everything that below 0",
        "start": 895.82,
        "duration": 5.34
    },
    {
        "text": "when the neuron does not fire but it",
        "start": 899.21,
        "duration": 4.56
    },
    {
        "text": "still gives you the continuous response",
        "start": 901.16,
        "duration": 4.89
    },
    {
        "text": "when you have the positive positive",
        "start": 903.77,
        "duration": 5.21
    },
    {
        "text": "summation under the under the function",
        "start": 906.05,
        "duration": 6.12
    },
    {
        "text": "so these functions are commonly used in",
        "start": 908.98,
        "duration": 5.83
    },
    {
        "text": "practice people use really a lot but",
        "start": 912.17,
        "duration": 4.169
    },
    {
        "text": "we're going to consider some examples",
        "start": 914.81,
        "duration": 2.94
    },
    {
        "text": "using the sigmoid because this is",
        "start": 916.339,
        "duration": 3.091
    },
    {
        "text": "classic function using logistic",
        "start": 917.75,
        "duration": 3.06
    },
    {
        "text": "regression that many of you are",
        "start": 919.43,
        "duration": 5.64
    },
    {
        "text": "hopefully familiar with so consider this",
        "start": 920.81,
        "duration": 7.29
    },
    {
        "text": "single neurons still one neuron sigmoid",
        "start": 925.07,
        "duration": 4.83
    },
    {
        "text": "binary classifiers so we're going to",
        "start": 928.1,
        "duration": 3.27
    },
    {
        "text": "consider binary",
        "start": 929.9,
        "duration": 3.78
    },
    {
        "text": "output but you're gonna model it is the",
        "start": 931.37,
        "duration": 6.72
    },
    {
        "text": "real real values and actually actually",
        "start": 933.68,
        "duration": 7.68
    },
    {
        "text": "this model is exactly modeling binary",
        "start": 938.09,
        "duration": 5.34
    },
    {
        "text": "logistic regression it's exactly the",
        "start": 941.36,
        "duration": 3.84
    },
    {
        "text": "same what your typical logistic",
        "start": 943.43,
        "duration": 4.38
    },
    {
        "text": "regression classification does you have",
        "start": 945.2,
        "duration": 5.49
    },
    {
        "text": "the inputs or your features it have",
        "start": 947.81,
        "duration": 4.8
    },
    {
        "text": "weights or your parameters you want to",
        "start": 950.69,
        "duration": 3.66
    },
    {
        "text": "weight each feature with the parameter",
        "start": 952.61,
        "duration": 4.74
    },
    {
        "text": "you have total bias and then we have the",
        "start": 954.35,
        "duration": 4.44
    },
    {
        "text": "same function but instead of step",
        "start": 957.35,
        "duration": 4.53
    },
    {
        "text": "function now we have the sigmoid and the",
        "start": 958.79,
        "duration": 7.11
    },
    {
        "text": "sigmoid function is going to be because",
        "start": 961.88,
        "duration": 6.21
    },
    {
        "text": "how it's calculated 1 over 1 plus",
        "start": 965.9,
        "duration": 5.13
    },
    {
        "text": "exponent to the negative x we're just",
        "start": 968.09,
        "duration": 7.02
    },
    {
        "text": "gonna put our this part put it in the",
        "start": 971.03,
        "duration": 6.75
    },
    {
        "text": "function and then it gets this so output",
        "start": 975.11,
        "duration": 6.11
    },
    {
        "text": "of our model is going to be like that",
        "start": 977.78,
        "duration": 6.0
    },
    {
        "text": "it's a ballistic interpretation is very",
        "start": 981.22,
        "duration": 5.05
    },
    {
        "text": "similar to when you do a classification",
        "start": 983.78,
        "duration": 6.0
    },
    {
        "text": "with logistic logistic model basically",
        "start": 986.27,
        "duration": 8.34
    },
    {
        "text": "this gives you a probability of class",
        "start": 989.78,
        "duration": 7.62
    },
    {
        "text": "being present or or or this particular",
        "start": 994.61,
        "duration": 6.45
    },
    {
        "text": "label being of class 1 given your inputs",
        "start": 997.4,
        "duration": 6.86
    },
    {
        "text": "and given your weights basically it's",
        "start": 1001.06,
        "duration": 6.36
    },
    {
        "text": "because logistic regression outputs the",
        "start": 1004.26,
        "duration": 5.91
    },
    {
        "text": "values between 0 & 1",
        "start": 1007.42,
        "duration": 6.18
    },
    {
        "text": "it's a probability for class 1 and if",
        "start": 1010.17,
        "duration": 6.76
    },
    {
        "text": "you take Phi 1 minus that probability",
        "start": 1013.6,
        "duration": 5.82
    },
    {
        "text": "you get the probability for the second",
        "start": 1016.93,
        "duration": 6.33
    },
    {
        "text": "class so it's a simple binary estimation",
        "start": 1019.42,
        "duration": 5.88
    },
    {
        "text": "of probability for two classes you get",
        "start": 1023.26,
        "duration": 3.9
    },
    {
        "text": "one value and the second value is going",
        "start": 1025.3,
        "duration": 3.9
    },
    {
        "text": "to be one minus one and the Sigma it is",
        "start": 1027.16,
        "duration": 3.63
    },
    {
        "text": "very nice for the purpose because it's",
        "start": 1029.2,
        "duration": 3.33
    },
    {
        "text": "exactly between zero and one so when you",
        "start": 1030.79,
        "duration": 3.3
    },
    {
        "text": "get the value it's always you know",
        "start": 1032.53,
        "duration": 3.87
    },
    {
        "text": "normalized you don't have to scale it",
        "start": 1034.09,
        "duration": 6.06
    },
    {
        "text": "and usually as a as in logistic",
        "start": 1036.4,
        "duration": 5.76
    },
    {
        "text": "regression with this classification for",
        "start": 1040.15,
        "duration": 4.74
    },
    {
        "text": "this parameters W are estimated",
        "start": 1042.16,
        "duration": 5.18
    },
    {
        "text": "iteratively using some kind of",
        "start": 1044.89,
        "duration": 4.26
    },
    {
        "text": "minimization of cost function for",
        "start": 1047.34,
        "duration": 4.09
    },
    {
        "text": "example negative log likelihood which is",
        "start": 1049.15,
        "duration": 4.89
    },
    {
        "text": "maximum maximum likelihood estimation",
        "start": 1051.43,
        "duration": 4.62
    },
    {
        "text": "typical method for logistic regression",
        "start": 1054.04,
        "duration": 4.29
    },
    {
        "text": "with some gradient based methods and",
        "start": 1056.05,
        "duration": 5.97
    },
    {
        "text": "Newton methods or you know already in",
        "start": 1058.33,
        "duration": 5.94
    },
    {
        "text": "descent or something like that so using",
        "start": 1062.02,
        "duration": 2.85
    },
    {
        "text": "those math",
        "start": 1064.27,
        "duration": 2.82
    },
    {
        "text": "you can have the data you can have the",
        "start": 1064.87,
        "duration": 3.96
    },
    {
        "text": "labels and you can estimate the",
        "start": 1067.09,
        "duration": 3.6
    },
    {
        "text": "parameters in the same way as you would",
        "start": 1068.83,
        "duration": 3.78
    },
    {
        "text": "do it in a logistic regression or",
        "start": 1070.69,
        "duration": 6.78
    },
    {
        "text": "logistic classification and then when",
        "start": 1072.61,
        "duration": 8.06
    },
    {
        "text": "you want to move from binary logistic",
        "start": 1077.47,
        "duration": 5.85
    },
    {
        "text": "classification to multinomial or",
        "start": 1080.67,
        "duration": 5.44
    },
    {
        "text": "multi-class logistic classification you",
        "start": 1083.32,
        "duration": 6.6
    },
    {
        "text": "just basically have your parameters ever",
        "start": 1086.11,
        "duration": 7.29
    },
    {
        "text": "going to be a matrix and then your X is",
        "start": 1089.92,
        "duration": 5.82
    },
    {
        "text": "going to be the vector of inputs but it",
        "start": 1093.4,
        "duration": 5.4
    },
    {
        "text": "would be also a vector by the number of",
        "start": 1095.74,
        "duration": 5.4
    },
    {
        "text": "classes and it's all the same you just",
        "start": 1098.8,
        "duration": 5.34
    },
    {
        "text": "you just have multiple inputs instead of",
        "start": 1101.14,
        "duration": 4.95
    },
    {
        "text": "one for example instead of two classes",
        "start": 1104.14,
        "duration": 3.48
    },
    {
        "text": "you're going to estimate three classes",
        "start": 1106.09,
        "duration": 3.69
    },
    {
        "text": "so your model is going to output cable",
        "start": 1107.62,
        "duration": 3.689
    },
    {
        "text": "and vector of probabilities so",
        "start": 1109.78,
        "duration": 2.94
    },
    {
        "text": "probability for the first class",
        "start": 1111.309,
        "duration": 2.491
    },
    {
        "text": "probability for the second class",
        "start": 1112.72,
        "duration": 3.72
    },
    {
        "text": "probability for the third class and as",
        "start": 1113.8,
        "duration": 6.0
    },
    {
        "text": "in multinomial logistic classification",
        "start": 1116.44,
        "duration": 5.15
    },
    {
        "text": "of logistic regression your normal",
        "start": 1119.8,
        "duration": 3.96
    },
    {
        "text": "probabilities are going to be normalized",
        "start": 1121.59,
        "duration": 4.93
    },
    {
        "text": "to one so when you sum up all the three",
        "start": 1123.76,
        "duration": 4.02
    },
    {
        "text": "probabilities for all three classes",
        "start": 1126.52,
        "duration": 4.11
    },
    {
        "text": "they're going to add up to one and this",
        "start": 1127.78,
        "duration": 6.39
    },
    {
        "text": "is also a neural network consists on one",
        "start": 1130.63,
        "duration": 5.88
    },
    {
        "text": "neuron exactly corresponding to binary",
        "start": 1134.17,
        "duration": 5.58
    },
    {
        "text": "which is generalization of by a logistic",
        "start": 1136.51,
        "duration": 5.52
    },
    {
        "text": "qualification to multinomial so when you",
        "start": 1139.75,
        "duration": 4.4
    },
    {
        "text": "when you doing this again you take your",
        "start": 1142.03,
        "duration": 4.68
    },
    {
        "text": "you take your W switch our K by D",
        "start": 1144.15,
        "duration": 5.32
    },
    {
        "text": "multiplied by X which is d by 1 so when",
        "start": 1146.71,
        "duration": 4.95
    },
    {
        "text": "you multiply K by D by D by one you get",
        "start": 1149.47,
        "duration": 5.61
    },
    {
        "text": "K by one idea biases by this is K by one",
        "start": 1151.66,
        "duration": 6.3
    },
    {
        "text": "you get the cable and output put it in",
        "start": 1155.08,
        "duration": 4.8
    },
    {
        "text": "the function and this function is going",
        "start": 1157.96,
        "duration": 3.78
    },
    {
        "text": "to output cable and vector of",
        "start": 1159.88,
        "duration": 4.35
    },
    {
        "text": "probabilities so that's when you take",
        "start": 1161.74,
        "duration": 3.63
    },
    {
        "text": "those probabilities you do not know",
        "start": 1164.23,
        "duration": 3.0
    },
    {
        "text": "which is problem means with which",
        "start": 1165.37,
        "duration": 5.34
    },
    {
        "text": "probability each class is possible for",
        "start": 1167.23,
        "duration": 6.77
    },
    {
        "text": "the given input and for given parameters",
        "start": 1170.71,
        "duration": 7.65
    },
    {
        "text": "ok so I'm just gonna show you a little",
        "start": 1174.0,
        "duration": 7.09
    },
    {
        "text": "bit of the demo or one you know let's",
        "start": 1178.36,
        "duration": 12.08
    },
    {
        "text": "hope this works so this is Google",
        "start": 1181.09,
        "duration": 10.65
    },
    {
        "text": "tensorflow",
        "start": 1190.44,
        "duration": 4.18
    },
    {
        "text": "online demo or playground as they call",
        "start": 1191.74,
        "duration": 5.27
    },
    {
        "text": "it where you can build your own network",
        "start": 1194.62,
        "duration": 4.08
    },
    {
        "text": "online",
        "start": 1197.01,
        "duration": 3.67
    },
    {
        "text": "on their website and then you can run it",
        "start": 1198.7,
        "duration": 4.53
    },
    {
        "text": "on different input data so let's just",
        "start": 1200.68,
        "duration": 4.71
    },
    {
        "text": "create this simple one you're on your",
        "start": 1203.23,
        "duration": 4.89
    },
    {
        "text": "own network and there are two two inputs",
        "start": 1205.39,
        "duration": 8.64
    },
    {
        "text": "X 1 and X 2 and X 1 is basically the",
        "start": 1208.12,
        "duration": 8.88
    },
    {
        "text": "values on the x-axis this is the data so",
        "start": 1214.03,
        "duration": 4.53
    },
    {
        "text": "you have one group of points another",
        "start": 1217.0,
        "duration": 3.66
    },
    {
        "text": "group of points and they're both in you",
        "start": 1218.56,
        "duration": 4.83
    },
    {
        "text": "know class 1 and class 2 and X 1 is just",
        "start": 1220.66,
        "duration": 6.0
    },
    {
        "text": "x-axis so from minus 6 to 6 and X 2 is",
        "start": 1223.39,
        "duration": 6.12
    },
    {
        "text": "just Y axis also from negative 6 to 6 so",
        "start": 1226.66,
        "duration": 4.59
    },
    {
        "text": "this point for example is going to have",
        "start": 1229.51,
        "duration": 5.61
    },
    {
        "text": "X around 3 and Y around 3 and 1/2 so",
        "start": 1231.25,
        "duration": 7.95
    },
    {
        "text": "what we're gonna do is we gonna run it",
        "start": 1235.12,
        "duration": 9.15
    },
    {
        "text": "and see how it it's the data so fairly",
        "start": 1239.2,
        "duration": 9.3
    },
    {
        "text": "simple under 60 iterations already one",
        "start": 1244.27,
        "duration": 6.78
    },
    {
        "text": "neuron is able to separate two classes",
        "start": 1248.5,
        "duration": 6.18
    },
    {
        "text": "so you can see the X one is you know X",
        "start": 1251.05,
        "duration": 4.77
    },
    {
        "text": "1/2",
        "start": 1254.68,
        "duration": 4.02
    },
    {
        "text": "what is it negative 1 on on a left and a",
        "start": 1255.82,
        "duration": 5.76
    },
    {
        "text": "positive on the right and X 1 X 2 is",
        "start": 1258.7,
        "duration": 5.13
    },
    {
        "text": "negative at the bottom positive at the",
        "start": 1261.58,
        "duration": 4.47
    },
    {
        "text": "top and one neuron you can see the",
        "start": 1263.83,
        "duration": 5.16
    },
    {
        "text": "visualization of the these weights that",
        "start": 1266.05,
        "duration": 7.11
    },
    {
        "text": "are multiplied and added the bias this",
        "start": 1268.99,
        "duration": 6.27
    },
    {
        "text": "visualization of one neuron is exactly",
        "start": 1273.16,
        "duration": 4.53
    },
    {
        "text": "the linear classification so as you're",
        "start": 1275.26,
        "duration": 5.46
    },
    {
        "text": "on logistic regression this is exactly",
        "start": 1277.69,
        "duration": 5.31
    },
    {
        "text": "the same process and then when when you",
        "start": 1280.72,
        "duration": 4.71
    },
    {
        "text": "train it the as I said iterative",
        "start": 1283.0,
        "duration": 6.45
    },
    {
        "text": "training repeats multiple that multiple",
        "start": 1285.43,
        "duration": 8.97
    },
    {
        "text": "the maximum likelihood estimation so you",
        "start": 1289.45,
        "duration": 7.68
    },
    {
        "text": "can play with it and try it on a",
        "start": 1294.4,
        "duration": 4.38
    },
    {
        "text": "different data sets for example if you",
        "start": 1297.13,
        "duration": 5.4
    },
    {
        "text": "have these classes that are you know one",
        "start": 1298.78,
        "duration": 5.4
    },
    {
        "text": "class hardened by another class and try",
        "start": 1302.53,
        "duration": 5.43
    },
    {
        "text": "to fit the model here it does minimize",
        "start": 1304.18,
        "duration": 7.14
    },
    {
        "text": "the the loss function for some time",
        "start": 1307.96,
        "duration": 6.48
    },
    {
        "text": "because it tried to grab as many points",
        "start": 1311.32,
        "duration": 5.25
    },
    {
        "text": "of one class into into one section",
        "start": 1314.44,
        "duration": 4.62
    },
    {
        "text": "without other ones but the problem here",
        "start": 1316.57,
        "duration": 4.77
    },
    {
        "text": "it is not able to separate these classes",
        "start": 1319.06,
        "duration": 4.89
    },
    {
        "text": "linearly so that's the problem if you",
        "start": 1321.34,
        "duration": 4.5
    },
    {
        "text": "take the logistic regression if you take",
        "start": 1323.95,
        "duration": 3.99
    },
    {
        "text": "just simple inputs you know without",
        "start": 1325.84,
        "duration": 5.01
    },
    {
        "text": "without any dependencies between them it",
        "start": 1327.94,
        "duration": 3.86
    },
    {
        "text": "is not going to be",
        "start": 1330.85,
        "duration": 4.97
    },
    {
        "text": "to fit the model on this kind of data if",
        "start": 1331.8,
        "duration": 10.2
    },
    {
        "text": "you this supposed to be squares I think",
        "start": 1335.82,
        "duration": 8.85
    },
    {
        "text": "if you add the feature squared and try",
        "start": 1342.0,
        "duration": 4.86
    },
    {
        "text": "to fit this is able to fit it because",
        "start": 1344.67,
        "duration": 4.02
    },
    {
        "text": "you provide the information not only",
        "start": 1346.86,
        "duration": 3.66
    },
    {
        "text": "about the features themselves but you",
        "start": 1348.69,
        "duration": 5.22
    },
    {
        "text": "also provide the information about their",
        "start": 1350.52,
        "duration": 6.32
    },
    {
        "text": "square so it's able to build like the",
        "start": 1353.91,
        "duration": 5.31
    },
    {
        "text": "equation of the circle using those",
        "start": 1356.84,
        "duration": 8.74
    },
    {
        "text": "features and if you provide the x1 x2 to",
        "start": 1359.22,
        "duration": 9.66
    },
    {
        "text": "that feature multiplied features then",
        "start": 1365.58,
        "duration": 5.79
    },
    {
        "text": "let's also be able to build quite",
        "start": 1368.88,
        "duration": 8.27
    },
    {
        "text": "complex nonlinear signatures however",
        "start": 1371.37,
        "duration": 9.24
    },
    {
        "text": "however it's not going to be able to fit",
        "start": 1377.15,
        "duration": 6.1
    },
    {
        "text": "this data and the other way around it",
        "start": 1380.61,
        "duration": 7.16
    },
    {
        "text": "will able to fit this data like this but",
        "start": 1383.25,
        "duration": 8.34
    },
    {
        "text": "it won't be if you give just squared",
        "start": 1387.77,
        "duration": 6.19
    },
    {
        "text": "features you won't be able to fit this",
        "start": 1391.59,
        "duration": 5.57
    },
    {
        "text": "data because it only can do this",
        "start": 1393.96,
        "duration": 6.51
    },
    {
        "text": "circular shapes not possible to do this",
        "start": 1397.16,
        "duration": 7.45
    },
    {
        "text": "this distribution so basically what it",
        "start": 1400.47,
        "duration": 7.59
    },
    {
        "text": "says you have to you have to play a lot",
        "start": 1404.61,
        "duration": 5.16
    },
    {
        "text": "with the features and then as many of",
        "start": 1408.06,
        "duration": 3.18
    },
    {
        "text": "you know in a logistic regression that's",
        "start": 1409.77,
        "duration": 3.0
    },
    {
        "text": "what you do you choose choose what",
        "start": 1411.24,
        "duration": 4.11
    },
    {
        "text": "features to use and you have to choose",
        "start": 1412.77,
        "duration": 5.43
    },
    {
        "text": "the power of the feature you can't have",
        "start": 1415.35,
        "duration": 4.44
    },
    {
        "text": "to consider different interactions",
        "start": 1418.2,
        "duration": 4.53
    },
    {
        "text": "between features and manually filter",
        "start": 1419.79,
        "duration": 6.06
    },
    {
        "text": "from all this endless possibility of",
        "start": 1422.73,
        "duration": 5.58
    },
    {
        "text": "feature interactions or features what",
        "start": 1425.85,
        "duration": 4.08
    },
    {
        "text": "features to use for the specific data",
        "start": 1428.31,
        "duration": 3.21
    },
    {
        "text": "and then you know when you fit your",
        "start": 1429.93,
        "duration": 3.27
    },
    {
        "text": "model on data you actually don't know",
        "start": 1431.52,
        "duration": 3.57
    },
    {
        "text": "the underlying distribution so it's",
        "start": 1433.2,
        "duration": 4.59
    },
    {
        "text": "actually really hard to do and the",
        "start": 1435.09,
        "duration": 5.16
    },
    {
        "text": "solution for that could be if you just",
        "start": 1437.79,
        "duration": 9.12
    },
    {
        "text": "start using two layer networks so the",
        "start": 1440.25,
        "duration": 8.7
    },
    {
        "text": "trick here what it allows you to do it",
        "start": 1446.91,
        "duration": 5.04
    },
    {
        "text": "allows you to have multiple neurons in",
        "start": 1448.95,
        "duration": 6.27
    },
    {
        "text": "the layer organized in a layer such that",
        "start": 1451.95,
        "duration": 6.48
    },
    {
        "text": "each neuron gets all the inputs so every",
        "start": 1455.22,
        "duration": 5.28
    },
    {
        "text": "every neuron gets on the inputs but",
        "start": 1458.43,
        "duration": 3.84
    },
    {
        "text": "neurons don't talk to each other in the",
        "start": 1460.5,
        "duration": 3.83
    },
    {
        "text": "layer so everything every",
        "start": 1462.27,
        "duration": 5.269
    },
    {
        "text": "learn something like one neuron was",
        "start": 1464.33,
        "duration": 5.67
    },
    {
        "text": "learning in previous experiment each",
        "start": 1467.539,
        "duration": 5.281
    },
    {
        "text": "killer left sounding but then you can",
        "start": 1470.0,
        "duration": 5.25
    },
    {
        "text": "consider interaction between between",
        "start": 1472.82,
        "duration": 6.209
    },
    {
        "text": "these neurons in even higher level so we",
        "start": 1475.25,
        "duration": 5.61
    },
    {
        "text": "basically take what the first year and",
        "start": 1479.029,
        "duration": 3.421
    },
    {
        "text": "learn what the second neural learned and",
        "start": 1480.86,
        "duration": 3.659
    },
    {
        "text": "then you consider the combination of",
        "start": 1482.45,
        "duration": 3.87
    },
    {
        "text": "them in this neuron and this neuron",
        "start": 1484.519,
        "duration": 4.951
    },
    {
        "text": "looks at all of these learned neurons in",
        "start": 1486.32,
        "duration": 4.92
    },
    {
        "text": "a previous layer called hidden layer and",
        "start": 1489.47,
        "duration": 4.26
    },
    {
        "text": "then it can combine the decision",
        "start": 1491.24,
        "duration": 4.59
    },
    {
        "text": "boundaries that these neurons produced",
        "start": 1493.73,
        "duration": 4.789
    },
    {
        "text": "and produce some more complicated or",
        "start": 1495.83,
        "duration": 5.459
    },
    {
        "text": "complicated decision so basically in",
        "start": 1498.519,
        "duration": 4.63
    },
    {
        "text": "lady in any multi-layer neural network",
        "start": 1501.289,
        "duration": 3.781
    },
    {
        "text": "there is a full paralyzed connection of",
        "start": 1503.149,
        "duration": 4.41
    },
    {
        "text": "all units in the json class so input",
        "start": 1505.07,
        "duration": 4.349
    },
    {
        "text": "layer - hidden layer hidden layer -",
        "start": 1507.559,
        "duration": 5.701
    },
    {
        "text": "output layer and why it's important to",
        "start": 1509.419,
        "duration": 8.25
    },
    {
        "text": "to make it why in Whiting deep is that",
        "start": 1513.26,
        "duration": 7.169
    },
    {
        "text": "it's mathematically proven and there's a",
        "start": 1517.669,
        "duration": 7.071
    },
    {
        "text": "famous paper on the neural networks that",
        "start": 1520.429,
        "duration": 7.321
    },
    {
        "text": "no such neural network with one hidden",
        "start": 1524.74,
        "duration": 5.02
    },
    {
        "text": "layer one input layer and output layer",
        "start": 1527.75,
        "duration": 5.1
    },
    {
        "text": "can approximate any continuous function",
        "start": 1529.76,
        "duration": 6.539
    },
    {
        "text": "so as we considered before before you",
        "start": 1532.85,
        "duration": 5.01
    },
    {
        "text": "can approximate very easily with one",
        "start": 1536.299,
        "duration": 4.201
    },
    {
        "text": "neuron some logic gates like L or and",
        "start": 1537.86,
        "duration": 5.189
    },
    {
        "text": "negative end but actually this kind of",
        "start": 1540.5,
        "duration": 5.549
    },
    {
        "text": "architecture given the you have enough",
        "start": 1543.049,
        "duration": 4.74
    },
    {
        "text": "neurons in the hidden layer it can",
        "start": 1546.049,
        "duration": 4.59
    },
    {
        "text": "approximate any continuous function and",
        "start": 1547.789,
        "duration": 5.151
    },
    {
        "text": "it's actual rated to 30 ins Hilbert",
        "start": 1550.639,
        "duration": 5.16
    },
    {
        "text": "separates problem that was proved by our",
        "start": 1552.94,
        "duration": 5.53
    },
    {
        "text": "load russian mathematician in the end of",
        "start": 1555.799,
        "duration": 6.48
    },
    {
        "text": "last century so basically the width is",
        "start": 1558.47,
        "duration": 5.88
    },
    {
        "text": "important to increase the representative",
        "start": 1562.279,
        "duration": 4.89
    },
    {
        "text": "power to let the network be able to",
        "start": 1564.35,
        "duration": 5.579
    },
    {
        "text": "represent some complex continuous",
        "start": 1567.169,
        "duration": 4.5
    },
    {
        "text": "functions because well in the end what",
        "start": 1569.929,
        "duration": 3.391
    },
    {
        "text": "we trying to do we trying to estimate",
        "start": 1571.669,
        "duration": 4.321
    },
    {
        "text": "those parameters that will tell you",
        "start": 1573.32,
        "duration": 5.52
    },
    {
        "text": "what's the function of the data given",
        "start": 1575.99,
        "duration": 6.27
    },
    {
        "text": "the inputs and given the parameters at",
        "start": 1578.84,
        "duration": 7.319
    },
    {
        "text": "the same time just having width is not",
        "start": 1582.26,
        "duration": 6.2
    },
    {
        "text": "enough because it it's not guaranteed",
        "start": 1586.159,
        "duration": 4.531
    },
    {
        "text": "the training of the rhythm we'll be able",
        "start": 1588.46,
        "duration": 4.3
    },
    {
        "text": "to find that specific function even if",
        "start": 1590.69,
        "duration": 5.459
    },
    {
        "text": "you have endless hidden layer and you",
        "start": 1592.76,
        "duration": 4.74
    },
    {
        "text": "know unlimited number of neurons and",
        "start": 1596.149,
        "duration": 2.161
    },
    {
        "text": "hidden layer",
        "start": 1597.5,
        "duration": 2.55
    },
    {
        "text": "it's not guaranteed to converge to the",
        "start": 1598.31,
        "duration": 3.87
    },
    {
        "text": "correct function and then this layer",
        "start": 1600.05,
        "duration": 4.53
    },
    {
        "text": "when it's to because maybe unfeasibly",
        "start": 1602.18,
        "duration": 4.62
    },
    {
        "text": "large the computation become very",
        "start": 1604.58,
        "duration": 5.49
    },
    {
        "text": "expensive and then it might over feed",
        "start": 1606.8,
        "duration": 5.67
    },
    {
        "text": "the data that you have so it also very",
        "start": 1610.07,
        "duration": 4.86
    },
    {
        "text": "hard to properly generalize without",
        "start": 1612.47,
        "duration": 5.06
    },
    {
        "text": "overfeeding",
        "start": 1614.93,
        "duration": 2.6
    },
    {
        "text": "well this empiric ID lines there is no",
        "start": 1620.68,
        "duration": 8.92
    },
    {
        "text": "there is no different algorithm how to",
        "start": 1624.67,
        "duration": 7.12
    },
    {
        "text": "have to choose the architecture in terms",
        "start": 1629.6,
        "duration": 4.41
    },
    {
        "text": "of depth and it it's mostly empirical",
        "start": 1631.79,
        "duration": 4.38
    },
    {
        "text": "there are some theoretical insights that",
        "start": 1634.01,
        "duration": 5.76
    },
    {
        "text": "are started looking and how if you add",
        "start": 1636.17,
        "duration": 5.97
    },
    {
        "text": "one more layer but you use the bits that",
        "start": 1639.77,
        "duration": 4.5
    },
    {
        "text": "would help for specific process of",
        "start": 1642.14,
        "duration": 4.38
    },
    {
        "text": "function but there is no you know global",
        "start": 1644.27,
        "duration": 4.56
    },
    {
        "text": "theory behind that and it's mostly done",
        "start": 1646.52,
        "duration": 4.56
    },
    {
        "text": "empirically over time over the course of",
        "start": 1648.83,
        "duration": 5.04
    },
    {
        "text": "years the many interesting paper",
        "start": 1651.08,
        "duration": 5.61
    },
    {
        "text": "recently was a paper called learning to",
        "start": 1653.87,
        "duration": 4.89
    },
    {
        "text": "learn by gradient gradient descent by",
        "start": 1656.69,
        "duration": 4.02
    },
    {
        "text": "gradient descent so basically they took",
        "start": 1658.76,
        "duration": 4.32
    },
    {
        "text": "a neural network and it trained the",
        "start": 1660.71,
        "duration": 5.61
    },
    {
        "text": "neural network to estimate the sizes or",
        "start": 1663.08,
        "duration": 4.95
    },
    {
        "text": "architectures of other neural networks",
        "start": 1666.32,
        "duration": 4.11
    },
    {
        "text": "to make them the best for specific",
        "start": 1668.03,
        "duration": 5.58
    },
    {
        "text": "datasets so there are people are trying",
        "start": 1670.43,
        "duration": 5.01
    },
    {
        "text": "many different things how to have to",
        "start": 1673.61,
        "duration": 13.59
    },
    {
        "text": "figure out what to use yeah so basically",
        "start": 1675.44,
        "duration": 14.88
    },
    {
        "text": "this these are outputs for classes as it",
        "start": 1687.2,
        "duration": 6.06
    },
    {
        "text": "was for them for this guy yeah this case",
        "start": 1690.32,
        "duration": 6.06
    },
    {
        "text": "so this is output layers basically the",
        "start": 1693.26,
        "duration": 14.13
    },
    {
        "text": "probabilities per class for binary you",
        "start": 1696.38,
        "duration": 13.92
    },
    {
        "text": "need one right because you you get one",
        "start": 1707.39,
        "duration": 6.14
    },
    {
        "text": "value and it's going to be probability",
        "start": 1710.3,
        "duration": 6.72
    },
    {
        "text": "let's say 60% then the second class is",
        "start": 1713.53,
        "duration": 5.55
    },
    {
        "text": "going to be one minus that probability",
        "start": 1717.02,
        "duration": 3.18
    },
    {
        "text": "40%",
        "start": 1719.08,
        "duration": 6.19
    },
    {
        "text": "so with those tangent functions or the",
        "start": 1720.2,
        "duration": 7.46
    },
    {
        "text": "others you don't need like two to the N",
        "start": 1725.27,
        "duration": 6.519
    },
    {
        "text": "output neurons like to calculate",
        "start": 1727.66,
        "duration": 6.859
    },
    {
        "text": "even we what do to them so if they're",
        "start": 1731.789,
        "duration": 6.181
    },
    {
        "text": "binary and you have like 16 classes you",
        "start": 1734.519,
        "duration": 7.311
    },
    {
        "text": "have to have to the form right out of",
        "start": 1737.97,
        "duration": 7.5
    },
    {
        "text": "all the combinations to get 16 yeah I",
        "start": 1741.83,
        "duration": 5.559
    },
    {
        "text": "mean you can have as many other boots as",
        "start": 1745.47,
        "duration": 3.779
    },
    {
        "text": "you need basically per class you can",
        "start": 1747.389,
        "duration": 4.17
    },
    {
        "text": "have thousand outputs neurons for it for",
        "start": 1749.249,
        "duration": 3.451
    },
    {
        "text": "each class and they're going to be",
        "start": 1751.559,
        "duration": 3.511
    },
    {
        "text": "because how you formulate the logistic",
        "start": 1752.7,
        "duration": 3.599
    },
    {
        "text": "regression they're going to be",
        "start": 1755.07,
        "duration": 2.789
    },
    {
        "text": "normalized so they're going to be from 0",
        "start": 1756.299,
        "duration": 3.93
    },
    {
        "text": "to 100 single one of them every single",
        "start": 1757.859,
        "duration": 6.601
    },
    {
        "text": "one of them so that's very common when",
        "start": 1760.229,
        "duration": 6.361
    },
    {
        "text": "people have thousand classes of images",
        "start": 1764.46,
        "duration": 5.459
    },
    {
        "text": "something and they basically you do you",
        "start": 1766.59,
        "duration": 6.029
    },
    {
        "text": "classify your output the probability for",
        "start": 1769.919,
        "duration": 5.1
    },
    {
        "text": "particular image being of one thousand",
        "start": 1772.619,
        "duration": 4.68
    },
    {
        "text": "classes so you get a vector of thousand",
        "start": 1775.019,
        "duration": 4.801
    },
    {
        "text": "probabilities here and that's the",
        "start": 1777.299,
        "duration": 6.12
    },
    {
        "text": "highest probability is going to be the",
        "start": 1779.82,
        "duration": 8.209
    },
    {
        "text": "most probable class for that image yeah",
        "start": 1783.419,
        "duration": 7.291
    },
    {
        "text": "so here basically as Jeff said it's hard",
        "start": 1788.029,
        "duration": 4.27
    },
    {
        "text": "to choose the network and there are some",
        "start": 1790.71,
        "duration": 3.51
    },
    {
        "text": "caveats you know there if you increase",
        "start": 1792.299,
        "duration": 4.291
    },
    {
        "text": "using that increase the capacity of the",
        "start": 1794.22,
        "duration": 3.929
    },
    {
        "text": "network increase the representation of",
        "start": 1796.59,
        "duration": 3.659
    },
    {
        "text": "power so you can see here that for",
        "start": 1798.149,
        "duration": 4.86
    },
    {
        "text": "example tricky then you're on 6000",
        "start": 1800.249,
        "duration": 6.331
    },
    {
        "text": "neurons you it fits better but at the",
        "start": 1803.009,
        "duration": 5.52
    },
    {
        "text": "same time you start over fitting the",
        "start": 1806.58,
        "duration": 5.159
    },
    {
        "text": "data so you lose your generalization and",
        "start": 1808.529,
        "duration": 6.0
    },
    {
        "text": "start pinning specific data set to avoid",
        "start": 1811.739,
        "duration": 4.5
    },
    {
        "text": "that you can use regularization like",
        "start": 1814.529,
        "duration": 5.07
    },
    {
        "text": "simple l1 l2 will do also there are",
        "start": 1816.239,
        "duration": 5.461
    },
    {
        "text": "specific things called dropout when you",
        "start": 1819.599,
        "duration": 4.05
    },
    {
        "text": "switch off some neurons from time to",
        "start": 1821.7,
        "duration": 4.439
    },
    {
        "text": "time for example this is l2 organization",
        "start": 1823.649,
        "duration": 4.89
    },
    {
        "text": "with different regulation terms you can",
        "start": 1826.139,
        "duration": 4.5
    },
    {
        "text": "see how smooth is a little bit the",
        "start": 1828.539,
        "duration": 3.87
    },
    {
        "text": "decision boundary depending on how much",
        "start": 1830.639,
        "duration": 4.5
    },
    {
        "text": "regularization you give so this is",
        "start": 1832.409,
        "duration": 5.301
    },
    {
        "text": "usually done by some kind of",
        "start": 1835.139,
        "duration": 5.27
    },
    {
        "text": "cross-validation on a training set you",
        "start": 1837.71,
        "duration": 5.86
    },
    {
        "text": "estimating these hyper parameters and",
        "start": 1840.409,
        "duration": 4.96
    },
    {
        "text": "trying to find the best combination for",
        "start": 1843.57,
        "duration": 6.9
    },
    {
        "text": "specific tasks for specific situation so",
        "start": 1845.369,
        "duration": 8.67
    },
    {
        "text": "and when you learn the weights in this",
        "start": 1850.47,
        "duration": 5.639
    },
    {
        "text": "model is very similar to as we discussed",
        "start": 1854.039,
        "duration": 4.291
    },
    {
        "text": "before in logistic logistic model",
        "start": 1856.109,
        "duration": 5.4
    },
    {
        "text": "logistic regression weights are randomly",
        "start": 1858.33,
        "duration": 5.25
    },
    {
        "text": "initialized there are small random",
        "start": 1861.509,
        "duration": 4.831
    },
    {
        "text": "numbers to avoid the situation when each",
        "start": 1863.58,
        "duration": 4.409
    },
    {
        "text": "neuron learns the same so they",
        "start": 1866.34,
        "duration": 3.539
    },
    {
        "text": "initialize a little bit different just",
        "start": 1867.989,
        "duration": 3.841
    },
    {
        "text": "sample from some uniform distribution",
        "start": 1869.879,
        "duration": 5.13
    },
    {
        "text": "and something called forward pass they",
        "start": 1871.83,
        "duration": 5.699
    },
    {
        "text": "performed and former pass forward pass",
        "start": 1875.009,
        "duration": 4.681
    },
    {
        "text": "is what we already did you take the",
        "start": 1877.529,
        "duration": 5.041
    },
    {
        "text": "inputs you pass the values to the hidden",
        "start": 1879.69,
        "duration": 4.979
    },
    {
        "text": "layer you multiply by you know the",
        "start": 1882.57,
        "duration": 4.349
    },
    {
        "text": "coefficients you add the bias computer a",
        "start": 1884.669,
        "duration": 4.801
    },
    {
        "text": "function here and then you output the",
        "start": 1886.919,
        "duration": 5.911
    },
    {
        "text": "probabilities after that gradient of the",
        "start": 1889.47,
        "duration": 5.01
    },
    {
        "text": "loss function that you have like a",
        "start": 1892.83,
        "duration": 4.289
    },
    {
        "text": "sigmoid logistic function loss function",
        "start": 1894.48,
        "duration": 6.269
    },
    {
        "text": "for example that is basically calculates",
        "start": 1897.119,
        "duration": 8.16
    },
    {
        "text": "how your outputs are close to the true",
        "start": 1900.749,
        "duration": 6.6
    },
    {
        "text": "values so basically estimate the error",
        "start": 1905.279,
        "duration": 5.041
    },
    {
        "text": "of your model compute you can compute",
        "start": 1907.349,
        "duration": 4.68
    },
    {
        "text": "the gradient of this loss function and",
        "start": 1910.32,
        "duration": 4.919
    },
    {
        "text": "then you can what they call to back",
        "start": 1912.029,
        "duration": 4.98
    },
    {
        "text": "propagate the gradient because you want",
        "start": 1915.239,
        "duration": 3.77
    },
    {
        "text": "to know the gradient of loss function",
        "start": 1917.009,
        "duration": 5.34
    },
    {
        "text": "given to respect to all the parameters",
        "start": 1919.009,
        "duration": 5.02
    },
    {
        "text": "but you not only have parameters here",
        "start": 1922.349,
        "duration": 3.991
    },
    {
        "text": "you also have the parameters here so you",
        "start": 1924.029,
        "duration": 3.931
    },
    {
        "text": "have to but propagate the gradient",
        "start": 1926.34,
        "duration": 3.929
    },
    {
        "text": "backwards to estimate all the gradients",
        "start": 1927.96,
        "duration": 5.309
    },
    {
        "text": "and after that you can use your favorite",
        "start": 1930.269,
        "duration": 4.26
    },
    {
        "text": "optimization techniques such as",
        "start": 1933.269,
        "duration": 3.77
    },
    {
        "text": "stochastic gradient descent to minimize",
        "start": 1934.529,
        "duration": 5.671
    },
    {
        "text": "the to maximize the likelihood and",
        "start": 1937.039,
        "duration": 5.62
    },
    {
        "text": "minimize the negative log likelihood and",
        "start": 1940.2,
        "duration": 4.049
    },
    {
        "text": "there's back propagation I'm not going",
        "start": 1942.659,
        "duration": 3.33
    },
    {
        "text": "to talk about it how it works how you",
        "start": 1944.249,
        "duration": 3.36
    },
    {
        "text": "estimate the gradient over the whole",
        "start": 1945.989,
        "duration": 4.17
    },
    {
        "text": "model but if you want you can google it",
        "start": 1947.609,
        "duration": 4.471
    },
    {
        "text": "separately it's a beautiful algorithm",
        "start": 1950.159,
        "duration": 3.33
    },
    {
        "text": "it's one of the best algorithms that I",
        "start": 1952.08,
        "duration": 4.65
    },
    {
        "text": "seen in my life it very simple works by",
        "start": 1953.489,
        "duration": 6.78
    },
    {
        "text": "the chain rule and it was not even",
        "start": 1956.73,
        "duration": 6.539
    },
    {
        "text": "proved to the current state until I",
        "start": 1960.269,
        "duration": 5.22
    },
    {
        "text": "think 80s or 90s only in Ages or 90s",
        "start": 1963.269,
        "duration": 3.9
    },
    {
        "text": "they actually came up is also a",
        "start": 1965.489,
        "duration": 4.701
    },
    {
        "text": "relatively fresh fresh thing but it",
        "start": 1967.169,
        "duration": 6.87
    },
    {
        "text": "makes it easier to train many many many",
        "start": 1970.19,
        "duration": 6.789
    },
    {
        "text": "error models by propagating the",
        "start": 1974.039,
        "duration": 8.151
    },
    {
        "text": "gradients and then let's",
        "start": 1976.979,
        "duration": 9.74
    },
    {
        "text": "[Music]",
        "start": 1982.19,
        "duration": 4.529
    },
    {
        "text": "anything needs to jump out of the local",
        "start": 1987.87,
        "duration": 7.12
    },
    {
        "text": "minimum yeah so that's a good question",
        "start": 1991.39,
        "duration": 8.91
    },
    {
        "text": "that's a moment which I'm not gonna go",
        "start": 1994.99,
        "duration": 7.41
    },
    {
        "text": "for because I'm out of time already but",
        "start": 2000.3,
        "duration": 6.12
    },
    {
        "text": "the short answer is that empirically it",
        "start": 2002.4,
        "duration": 5.58
    },
    {
        "text": "was showing that when you have multiple",
        "start": 2006.42,
        "duration": 4.32
    },
    {
        "text": "layers and many neurons in the layer the",
        "start": 2007.98,
        "duration": 4.71
    },
    {
        "text": "data manifold that you that you are",
        "start": 2010.74,
        "duration": 4.53
    },
    {
        "text": "trying to optimize on the manifold that",
        "start": 2012.69,
        "duration": 5.01
    },
    {
        "text": "you has the local minimus and Maximus",
        "start": 2015.27,
        "duration": 5.97
    },
    {
        "text": "it's relatively smooth so if you're",
        "start": 2017.7,
        "duration": 5.79
    },
    {
        "text": "using good optimization techniques such",
        "start": 2021.24,
        "duration": 4.95
    },
    {
        "text": "as as the SGD vis momentum or some of",
        "start": 2023.49,
        "duration": 4.95
    },
    {
        "text": "the modern ones like a de Delta at the",
        "start": 2026.19,
        "duration": 5.4
    },
    {
        "text": "atom there have they're able it's so",
        "start": 2028.44,
        "duration": 5.16
    },
    {
        "text": "smooth that are able to get even even",
        "start": 2031.59,
        "duration": 4.35
    },
    {
        "text": "get a local minimum they are able to get",
        "start": 2033.6,
        "duration": 6.06
    },
    {
        "text": "out and then it the only way these guys",
        "start": 2035.94,
        "duration": 5.28
    },
    {
        "text": "are converging because you're able to",
        "start": 2039.66,
        "duration": 3.84
    },
    {
        "text": "find a global minimum right so they're",
        "start": 2041.22,
        "duration": 4.98
    },
    {
        "text": "the surface of the manifold is so smooth",
        "start": 2043.5,
        "duration": 6.06
    },
    {
        "text": "that generally it works but again you",
        "start": 2046.2,
        "duration": 5.55
    },
    {
        "text": "have to guess your architecture right if",
        "start": 2049.56,
        "duration": 4.5
    },
    {
        "text": "your architecture is wrong if you have",
        "start": 2051.75,
        "duration": 4.56
    },
    {
        "text": "wrong number of layers run number of",
        "start": 2054.06,
        "duration": 8.25
    },
    {
        "text": "neurons might not converge yeah discrete",
        "start": 2056.31,
        "duration": 11.01
    },
    {
        "text": "demo so let's have now two neurons on a",
        "start": 2062.31,
        "duration": 7.95
    },
    {
        "text": "second layer and then I guess we're not",
        "start": 2067.32,
        "duration": 6.15
    },
    {
        "text": "gonna do the simple classification let's",
        "start": 2070.26,
        "duration": 17.85
    },
    {
        "text": "start with this one it's all well it can",
        "start": 2073.47,
        "duration": 17.61
    },
    {
        "text": "it affects it affects the order that you",
        "start": 2088.11,
        "duration": 7.1
    },
    {
        "text": "get information about about the manifold",
        "start": 2091.08,
        "duration": 10.29
    },
    {
        "text": "yeah yeah yeah that it might especially",
        "start": 2095.21,
        "duration": 7.69
    },
    {
        "text": "because if you use a stochastic gradient",
        "start": 2101.37,
        "duration": 4.47
    },
    {
        "text": "descent which takes leading patches it",
        "start": 2102.9,
        "duration": 5.19
    },
    {
        "text": "doesn't look older all the things at the",
        "start": 2105.84,
        "duration": 5.76
    },
    {
        "text": "same time then yeah then it's you know",
        "start": 2108.09,
        "duration": 5.19
    },
    {
        "text": "it's randomized so every time you",
        "start": 2111.6,
        "duration": 3.11
    },
    {
        "text": "restart even here if you",
        "start": 2113.28,
        "duration": 3.14
    },
    {
        "text": "every time you restart you don't",
        "start": 2114.71,
        "duration": 3.39
    },
    {
        "text": "converge to exactly the same solution",
        "start": 2116.42,
        "duration": 4.86
    },
    {
        "text": "unless it's a very simple problem so",
        "start": 2118.1,
        "duration": 5.04
    },
    {
        "text": "yeah basically having two neurons you",
        "start": 2121.28,
        "duration": 4.32
    },
    {
        "text": "see how they learn this boundary in this",
        "start": 2123.14,
        "duration": 3.99
    },
    {
        "text": "boundary so instead of boundary now we",
        "start": 2125.6,
        "duration": 3.72
    },
    {
        "text": "have two so it's actually able to pick",
        "start": 2127.13,
        "duration": 4.32
    },
    {
        "text": "up you know most of the points still",
        "start": 2129.32,
        "duration": 3.84
    },
    {
        "text": "having problems here in the middle with",
        "start": 2131.45,
        "duration": 3.12
    },
    {
        "text": "some blue ones and then still having",
        "start": 2133.16,
        "duration": 4.37
    },
    {
        "text": "some orange ones on the side so",
        "start": 2134.57,
        "duration": 5.22
    },
    {
        "text": "apparently the capacity of this New York",
        "start": 2137.53,
        "duration": 4.87
    },
    {
        "text": "is still not enough to learn this kind",
        "start": 2139.79,
        "duration": 4.89
    },
    {
        "text": "of data set so what I can start doing is",
        "start": 2142.4,
        "duration": 4.98
    },
    {
        "text": "it can start tweaking the tweak in the",
        "start": 2144.68,
        "duration": 4.98
    },
    {
        "text": "network and see how many neurons you",
        "start": 2147.38,
        "duration": 5.64
    },
    {
        "text": "actually need to learn enough there is",
        "start": 2149.66,
        "duration": 6.87
    },
    {
        "text": "to represent this situation well now if",
        "start": 2153.02,
        "duration": 6.09
    },
    {
        "text": "you have three you know they have this",
        "start": 2156.53,
        "duration": 6.24
    },
    {
        "text": "part of this or this so it has like one",
        "start": 2159.11,
        "duration": 7.98
    },
    {
        "text": "two and three decision boundaries so one",
        "start": 2162.77,
        "duration": 7.22
    },
    {
        "text": "two three still close but not enough but",
        "start": 2167.09,
        "duration": 5.43
    },
    {
        "text": "the simple yes would be if you have four",
        "start": 2169.99,
        "duration": 6.4
    },
    {
        "text": "of them across and then from bottom up",
        "start": 2172.52,
        "duration": 10.41
    },
    {
        "text": "that would be easier so yeah basically",
        "start": 2176.39,
        "duration": 9.6
    },
    {
        "text": "it converges relatively well trying to",
        "start": 2182.93,
        "duration": 5.37
    },
    {
        "text": "with the blue dots you can actually add",
        "start": 2185.99,
        "duration": 3.81
    },
    {
        "text": "the regularization here I'm going to do",
        "start": 2188.3,
        "duration": 4.76
    },
    {
        "text": "this right now but that would improve",
        "start": 2189.8,
        "duration": 7.46
    },
    {
        "text": "avoiding this you know this situation",
        "start": 2193.06,
        "duration": 8.62
    },
    {
        "text": "add more will fit even better so five",
        "start": 2197.26,
        "duration": 7.47
    },
    {
        "text": "neurons fits perfectly maybe even out",
        "start": 2201.68,
        "duration": 8.07
    },
    {
        "text": "overfitting instead of doing that you",
        "start": 2204.73,
        "duration": 9.04
    },
    {
        "text": "could try add one more layer and see",
        "start": 2209.75,
        "duration": 8.46
    },
    {
        "text": "what that does so adding another layer",
        "start": 2213.77,
        "duration": 6.96
    },
    {
        "text": "also helps because this neuron in the",
        "start": 2218.21,
        "duration": 6.33
    },
    {
        "text": "second layer is combining two of the two",
        "start": 2220.73,
        "duration": 5.25
    },
    {
        "text": "of the decision boundaries and this",
        "start": 2224.54,
        "duration": 4.71
    },
    {
        "text": "combination helps to segment out this",
        "start": 2225.98,
        "duration": 5.25
    },
    {
        "text": "part and this neuron combines the other",
        "start": 2229.25,
        "duration": 3.81
    },
    {
        "text": "way around so it combines two segments",
        "start": 2231.23,
        "duration": 4.56
    },
    {
        "text": "out this so very easy to see that more",
        "start": 2233.06,
        "duration": 4.65
    },
    {
        "text": "neurons you have especially in a second",
        "start": 2235.79,
        "duration": 6.29
    },
    {
        "text": "layer it's able to build very complex",
        "start": 2237.71,
        "duration": 4.37
    },
    {
        "text": "where very complex combinations of",
        "start": 2242.14,
        "duration": 5.26
    },
    {
        "text": "decision boundaries so again linear",
        "start": 2245.63,
        "duration": 2.88
    },
    {
        "text": "decision boundaries",
        "start": 2247.4,
        "duration": 3.24
    },
    {
        "text": "one two three and the second level is",
        "start": 2248.51,
        "duration": 7.65
    },
    {
        "text": "learning combination of Oh base two of",
        "start": 2250.64,
        "duration": 8.64
    },
    {
        "text": "these two and this learns combination of",
        "start": 2256.16,
        "duration": 5.82
    },
    {
        "text": "all three actually but is it as a chole",
        "start": 2259.28,
        "duration": 5.79
    },
    {
        "text": "so yeah also that's a pretty good job",
        "start": 2261.98,
        "duration": 5.61
    },
    {
        "text": "however the capacitive network is not",
        "start": 2265.07,
        "duration": 4.23
    },
    {
        "text": "good enough to learn this kind of data",
        "start": 2267.59,
        "duration": 3.57
    },
    {
        "text": "because especially because it's it makes",
        "start": 2269.3,
        "duration": 4.08
    },
    {
        "text": "a full circle and then has a little bit",
        "start": 2271.16,
        "duration": 5.73
    },
    {
        "text": "in the middle so just combining lis many",
        "start": 2273.38,
        "duration": 6.9
    },
    {
        "text": "linear boundaries is not able to figure",
        "start": 2276.89,
        "duration": 6.63
    },
    {
        "text": "out and actually come lab converging so",
        "start": 2280.28,
        "duration": 5.22
    },
    {
        "text": "that's one of the one of the situation",
        "start": 2283.52,
        "duration": 4.35
    },
    {
        "text": "when it cannot find proper",
        "start": 2285.5,
        "duration": 3.93
    },
    {
        "text": "representation cannot hit the local of",
        "start": 2287.87,
        "duration": 4.83
    },
    {
        "text": "the global minimum so what you can do we",
        "start": 2289.43,
        "duration": 7.08
    },
    {
        "text": "can either build a very wide network as",
        "start": 2292.7,
        "duration": 7.38
    },
    {
        "text": "we discussed before and then it will try",
        "start": 2296.51,
        "duration": 5.22
    },
    {
        "text": "to converge and I will try all the",
        "start": 2300.08,
        "duration": 3.06
    },
    {
        "text": "different things and you can add",
        "start": 2301.73,
        "duration": 3.33
    },
    {
        "text": "regulation to sort of smooth this",
        "start": 2303.14,
        "duration": 5.16
    },
    {
        "text": "situation when tries to come up you know",
        "start": 2305.06,
        "duration": 5.87
    },
    {
        "text": "some some solution for for this problem",
        "start": 2308.3,
        "duration": 6.09
    },
    {
        "text": "at the same time it sometimes it's",
        "start": 2310.93,
        "duration": 7.54
    },
    {
        "text": "easier to add layers if you reduce this",
        "start": 2314.39,
        "duration": 9.21
    },
    {
        "text": "but you second layer might be surely",
        "start": 2318.47,
        "duration": 7.59
    },
    {
        "text": "let's do a little function when you have",
        "start": 2323.6,
        "duration": 5.22
    },
    {
        "text": "a big of networks sigmoid and tan edge",
        "start": 2326.06,
        "duration": 5.1
    },
    {
        "text": "become not very stable so really is more",
        "start": 2328.82,
        "duration": 5.55
    },
    {
        "text": "popular in for practical purposes and",
        "start": 2331.16,
        "duration": 7.89
    },
    {
        "text": "for converges probe purposes and then",
        "start": 2334.37,
        "duration": 6.93
    },
    {
        "text": "you would have to wait a little longer",
        "start": 2339.05,
        "duration": 5.01
    },
    {
        "text": "but yeah so I think this is enough so",
        "start": 2341.3,
        "duration": 4.53
    },
    {
        "text": "you can see already you know it does",
        "start": 2344.06,
        "duration": 3.24
    },
    {
        "text": "fair in a good job there is a little bit",
        "start": 2345.83,
        "duration": 3.96
    },
    {
        "text": "of the still not fitting here or you",
        "start": 2347.3,
        "duration": 5.43
    },
    {
        "text": "know not true there is a this data in",
        "start": 2349.79,
        "duration": 7.89
    },
    {
        "text": "that area yeah so it's still still not",
        "start": 2352.73,
        "duration": 7.68
    },
    {
        "text": "zero but pretty close so that's a pretty",
        "start": 2357.68,
        "duration": 5.79
    },
    {
        "text": "decent job on the on the spiral data so",
        "start": 2360.41,
        "duration": 5.22
    },
    {
        "text": "that house here shows you have the",
        "start": 2363.47,
        "duration": 3.78
    },
    {
        "text": "capacity of network either one layer",
        "start": 2365.63,
        "duration": 3.6
    },
    {
        "text": "very big one layer or multiple layers",
        "start": 2367.25,
        "duration": 7.65
    },
    {
        "text": "kit like see if you have one layer with",
        "start": 2369.23,
        "duration": 8.13
    },
    {
        "text": "four neurons but we have two layers with",
        "start": 2374.9,
        "duration": 5.52
    },
    {
        "text": "two needle each do the same purpose no",
        "start": 2377.36,
        "duration": 4.6
    },
    {
        "text": "not exactly so",
        "start": 2380.42,
        "duration": 3.84
    },
    {
        "text": "sometimes because you have to have",
        "start": 2381.96,
        "duration": 7.139
    },
    {
        "text": "enough in a simple decision boundaries",
        "start": 2384.26,
        "duration": 7.15
    },
    {
        "text": "to start making the combinations if you",
        "start": 2389.099,
        "duration": 4.801
    },
    {
        "text": "make the first layer very small it's not",
        "start": 2391.41,
        "duration": 3.9
    },
    {
        "text": "going to be enough for a second layer to",
        "start": 2393.9,
        "duration": 4.5
    },
    {
        "text": "build these configurations so second",
        "start": 2395.31,
        "duration": 5.16
    },
    {
        "text": "layer is building like a combination of",
        "start": 2398.4,
        "duration": 4.26
    },
    {
        "text": "first layer decision linear decision",
        "start": 2400.47,
        "duration": 4.29
    },
    {
        "text": "boundaries so if you don't provide",
        "start": 2402.66,
        "duration": 4.32
    },
    {
        "text": "enough then second layer might not be",
        "start": 2404.76,
        "duration": 4.71
    },
    {
        "text": "able to build enough or if second layer",
        "start": 2406.98,
        "duration": 4.889
    },
    {
        "text": "is too small maybe those combinations",
        "start": 2409.47,
        "duration": 5.22
    },
    {
        "text": "are won't be enough so you need to like",
        "start": 2411.869,
        "duration": 4.441
    },
    {
        "text": "you need to build three combinations and",
        "start": 2414.69,
        "duration": 3.09
    },
    {
        "text": "you only have two neurons and a second",
        "start": 2416.31,
        "duration": 9.18
    },
    {
        "text": "layer so it won't be able to do that so",
        "start": 2417.78,
        "duration": 9.45
    },
    {
        "text": "that's why people started to do the",
        "start": 2425.49,
        "duration": 5.28
    },
    {
        "text": "deeper networks because as you go you're",
        "start": 2427.23,
        "duration": 4.8
    },
    {
        "text": "you hit the layer if you doing",
        "start": 2430.77,
        "duration": 3.93
    },
    {
        "text": "everything on one layer the computation",
        "start": 2432.03,
        "duration": 5.94
    },
    {
        "text": "grows much more expensive so it's very",
        "start": 2434.7,
        "duration": 5.1
    },
    {
        "text": "expensive to compute one big key to the",
        "start": 2437.97,
        "duration": 3.75
    },
    {
        "text": "layer versus a couple of couple of",
        "start": 2439.8,
        "duration": 3.45
    },
    {
        "text": "layers with this smaller number of",
        "start": 2441.72,
        "duration": 5.1
    },
    {
        "text": "neurons and then it was hard before the",
        "start": 2443.25,
        "duration": 5.339
    },
    {
        "text": "perfect mid propagation algorithm but",
        "start": 2446.82,
        "duration": 2.97
    },
    {
        "text": "it's much better now",
        "start": 2448.589,
        "duration": 3.27
    },
    {
        "text": "because of that efficient algorithm how",
        "start": 2449.79,
        "duration": 6.6
    },
    {
        "text": "to evaluate these deep networks so and",
        "start": 2451.859,
        "duration": 6.0
    },
    {
        "text": "then I'm going to talk a little a little",
        "start": 2456.39,
        "duration": 2.67
    },
    {
        "text": "bit about the convolutional neural",
        "start": 2457.859,
        "duration": 3.031
    },
    {
        "text": "network and then basically it's a fully",
        "start": 2459.06,
        "duration": 3.779
    },
    {
        "text": "connect so this fully connected",
        "start": 2460.89,
        "duration": 3.719
    },
    {
        "text": "multi-layer networks don't scale for",
        "start": 2462.839,
        "duration": 4.02
    },
    {
        "text": "images because images they're data that",
        "start": 2464.609,
        "duration": 4.921
    },
    {
        "text": "has two T's 2d structure so for example",
        "start": 2466.859,
        "duration": 5.671
    },
    {
        "text": "for one 256 256 by three colors RGB",
        "start": 2469.53,
        "duration": 5.37
    },
    {
        "text": "image one fully connected neuron that we",
        "start": 2472.53,
        "duration": 4.26
    },
    {
        "text": "discussed before he would connect to",
        "start": 2474.9,
        "duration": 4.469
    },
    {
        "text": "every single pixel in there and that",
        "start": 2476.79,
        "duration": 5.28
    },
    {
        "text": "would have almost 200,000 parameters so",
        "start": 2479.369,
        "duration": 4.951
    },
    {
        "text": "you take number of neurons and number of",
        "start": 2482.07,
        "duration": 4.08
    },
    {
        "text": "layers and the simple idea is a",
        "start": 2484.32,
        "duration": 3.51
    },
    {
        "text": "restricted connection between neurons so",
        "start": 2486.15,
        "duration": 4.86
    },
    {
        "text": "not not every neurons connected to other",
        "start": 2487.83,
        "duration": 6.779
    },
    {
        "text": "neuron or input such that if each hidden",
        "start": 2491.01,
        "duration": 6.15
    },
    {
        "text": "node is can unit only connected to small",
        "start": 2494.609,
        "duration": 5.071
    },
    {
        "text": "subset of the inputs also you want to",
        "start": 2497.16,
        "duration": 5.459
    },
    {
        "text": "use the the patterns in the image that",
        "start": 2499.68,
        "duration": 4.74
    },
    {
        "text": "there are structures within local region",
        "start": 2502.619,
        "duration": 3.301
    },
    {
        "text": "for example you want to look at just",
        "start": 2504.42,
        "duration": 4.23
    },
    {
        "text": "onto one pixel but rather other pixels",
        "start": 2505.92,
        "duration": 5.58
    },
    {
        "text": "around in this pixel and then images are",
        "start": 2508.65,
        "duration": 4.35
    },
    {
        "text": "stationary meaning that some features",
        "start": 2511.5,
        "duration": 3.15
    },
    {
        "text": "that we can find in one place in the",
        "start": 2513.0,
        "duration": 2.5
    },
    {
        "text": "image we also",
        "start": 2514.65,
        "duration": 2.71
    },
    {
        "text": "can find another part of the image like",
        "start": 2515.5,
        "duration": 4.56
    },
    {
        "text": "edges if you have an engine this pretty",
        "start": 2517.36,
        "duration": 4.38
    },
    {
        "text": "much you might have the same edge in",
        "start": 2520.06,
        "duration": 3.24
    },
    {
        "text": "other parts so the same feature you",
        "start": 2521.74,
        "duration": 4.19
    },
    {
        "text": "might reuse and have to do that",
        "start": 2523.3,
        "duration": 6.54
    },
    {
        "text": "basically these layers of the neurons",
        "start": 2525.93,
        "duration": 7.45
    },
    {
        "text": "they are reorganized in the 3d volume so",
        "start": 2529.84,
        "duration": 6.51
    },
    {
        "text": "instead of being just one array of you",
        "start": 2533.38,
        "duration": 5.49
    },
    {
        "text": "know linear layers you put them in the",
        "start": 2536.35,
        "duration": 4.62
    },
    {
        "text": "box that has the width the height and",
        "start": 2538.87,
        "duration": 4.59
    },
    {
        "text": "the depth and how do you do that and why",
        "start": 2540.97,
        "duration": 6.06
    },
    {
        "text": "you do that there are two or three two",
        "start": 2543.46,
        "duration": 8.13
    },
    {
        "text": "reasons for that or two tricks the first",
        "start": 2547.03,
        "duration": 7.74
    },
    {
        "text": "is you connect each neuron to only a",
        "start": 2551.59,
        "duration": 6.03
    },
    {
        "text": "local region of the input volume so this",
        "start": 2554.77,
        "duration": 7.29
    },
    {
        "text": "neuron that is you know one of the depth",
        "start": 2557.62,
        "duration": 7.88
    },
    {
        "text": "depth blocks in the in the neuron block",
        "start": 2562.06,
        "duration": 6.03
    },
    {
        "text": "each neuron is only looking to the",
        "start": 2565.5,
        "duration": 4.03
    },
    {
        "text": "corresponding part of the image it",
        "start": 2568.09,
        "duration": 3.27
    },
    {
        "text": "doesn't look to the whole image so it",
        "start": 2569.53,
        "duration": 3.6
    },
    {
        "text": "doesn't have to have the parameters for",
        "start": 2571.36,
        "duration": 4.23
    },
    {
        "text": "pieces here pixels they're only for this",
        "start": 2573.13,
        "duration": 4.41
    },
    {
        "text": "little part in this can be four by four",
        "start": 2575.59,
        "duration": 5.49
    },
    {
        "text": "by three so that you'd only have time",
        "start": 2577.54,
        "duration": 5.52
    },
    {
        "text": "sixteen times three parameters instead",
        "start": 2581.08,
        "duration": 4.56
    },
    {
        "text": "of having 32 by 32 by three parameters",
        "start": 2583.06,
        "duration": 6.96
    },
    {
        "text": "and basically that was the area that he",
        "start": 2585.64,
        "duration": 6.63
    },
    {
        "text": "sees it's called receptive field or",
        "start": 2590.02,
        "duration": 5.01
    },
    {
        "text": "filter size because it basically how",
        "start": 2592.27,
        "duration": 4.82
    },
    {
        "text": "many parameters you have for neuron and",
        "start": 2595.03,
        "duration": 7.41
    },
    {
        "text": "that depth the depth of this receptive",
        "start": 2597.09,
        "duration": 7.33
    },
    {
        "text": "field is always equal to the depths of",
        "start": 2602.44,
        "duration": 3.42
    },
    {
        "text": "your image so for example if you have",
        "start": 2604.42,
        "duration": 3.87
    },
    {
        "text": "RGB you have three colors you might have",
        "start": 2605.86,
        "duration": 4.26
    },
    {
        "text": "medical image that is volume that has",
        "start": 2608.29,
        "duration": 4.17
    },
    {
        "text": "bigger depth so it's always going to see",
        "start": 2610.12,
        "duration": 5.28
    },
    {
        "text": "the full depth but limited X&Y dimension",
        "start": 2612.46,
        "duration": 6.18
    },
    {
        "text": "and depth of the output volume so the",
        "start": 2615.4,
        "duration": 5.7
    },
    {
        "text": "size of this block is actually a hyper",
        "start": 2618.64,
        "duration": 3.39
    },
    {
        "text": "parameter",
        "start": 2621.1,
        "duration": 3.69
    },
    {
        "text": "corresponds to how many filters how many",
        "start": 2622.03,
        "duration": 5.43
    },
    {
        "text": "this neurons that would look at the same",
        "start": 2624.79,
        "duration": 4.44
    },
    {
        "text": "area in the image you want to have so",
        "start": 2627.46,
        "duration": 4.41
    },
    {
        "text": "this is like five features of five",
        "start": 2629.23,
        "duration": 4.5
    },
    {
        "text": "filter that would all of them will look",
        "start": 2631.87,
        "duration": 3.57
    },
    {
        "text": "only in this area but they will learn",
        "start": 2633.73,
        "duration": 3.84
    },
    {
        "text": "different things from that area so this",
        "start": 2635.44,
        "duration": 3.9
    },
    {
        "text": "might learn an edge this might learn",
        "start": 2637.57,
        "duration": 3.42
    },
    {
        "text": "some circle structure this might learn",
        "start": 2639.34,
        "duration": 4.02
    },
    {
        "text": "some noise or something like that so all",
        "start": 2640.99,
        "duration": 5.84
    },
    {
        "text": "of them are looking only in one area",
        "start": 2643.36,
        "duration": 3.47
    },
    {
        "text": "it's a step it's a next so originally",
        "start": 2656.13,
        "duration": 6.1
    },
    {
        "text": "with CNN's you're assumed it consumes",
        "start": 2659.95,
        "duration": 5.55
    },
    {
        "text": "all the depth the whole hold that they",
        "start": 2662.23,
        "duration": 6.99
    },
    {
        "text": "will take for my four by 50 slices that",
        "start": 2665.5,
        "duration": 7.68
    },
    {
        "text": "you have in the image definitely you can",
        "start": 2669.22,
        "duration": 6.6
    },
    {
        "text": "you can go fancy and start making the",
        "start": 2673.18,
        "duration": 4.53
    },
    {
        "text": "propagation and dep you can do four by",
        "start": 2675.82,
        "duration": 4.56
    },
    {
        "text": "four by four and then snap only slide",
        "start": 2677.71,
        "duration": 4.47
    },
    {
        "text": "because this guy is going to be sliding",
        "start": 2680.38,
        "duration": 3.75
    },
    {
        "text": "across the beam across the image but you",
        "start": 2682.18,
        "duration": 3.78
    },
    {
        "text": "can also start sliding in three",
        "start": 2684.13,
        "duration": 5.01
    },
    {
        "text": "dimensions so then that's like a next",
        "start": 2685.96,
        "duration": 6.27
    },
    {
        "text": "development and the second trick is",
        "start": 2689.14,
        "duration": 5.52
    },
    {
        "text": "parameter sharing and this one is maybe",
        "start": 2692.23,
        "duration": 4.26
    },
    {
        "text": "a little tricky to understand but this",
        "start": 2694.66,
        "duration": 4.8
    },
    {
        "text": "is basically saying since images are",
        "start": 2696.49,
        "duration": 5.1
    },
    {
        "text": "stationary so the same features that is",
        "start": 2699.46,
        "duration": 3.93
    },
    {
        "text": "useful in one location can be useful",
        "start": 2701.59,
        "duration": 5.34
    },
    {
        "text": "another location let's limit for each of",
        "start": 2703.39,
        "duration": 6.06
    },
    {
        "text": "this filter or for each this depth",
        "start": 2706.93,
        "duration": 6.36
    },
    {
        "text": "plains or depth slices let's link the",
        "start": 2709.45,
        "duration": 7.5
    },
    {
        "text": "parameters for all neurons in one slice",
        "start": 2713.29,
        "duration": 5.97
    },
    {
        "text": "so basically all neurons in one slice",
        "start": 2716.95,
        "duration": 4.35
    },
    {
        "text": "are going to be have the same set of",
        "start": 2719.26,
        "duration": 5.49
    },
    {
        "text": "parameters so humans are looking here",
        "start": 2721.3,
        "duration": 5.37
    },
    {
        "text": "mutants are looking hit in different",
        "start": 2724.75,
        "duration": 4.11
    },
    {
        "text": "like one urine is looking at this for",
        "start": 2726.67,
        "duration": 4.68
    },
    {
        "text": "another one is looking at this for that",
        "start": 2728.86,
        "duration": 5.31
    },
    {
        "text": "look here and here all of them are going",
        "start": 2731.35,
        "duration": 5.34
    },
    {
        "text": "to have one set of parameters so because",
        "start": 2734.17,
        "duration": 4.47
    },
    {
        "text": "they're looking at the at the areas of",
        "start": 2736.69,
        "duration": 4.8
    },
    {
        "text": "the same side of the same size all of",
        "start": 2738.64,
        "duration": 5.82
    },
    {
        "text": "them are going to have for two by two by",
        "start": 2741.49,
        "duration": 4.89
    },
    {
        "text": "three number of parameters and the set",
        "start": 2744.46,
        "duration": 3.09
    },
    {
        "text": "of parameters is going to be shared",
        "start": 2746.38,
        "duration": 3.66
    },
    {
        "text": "between all the locations basically what",
        "start": 2747.55,
        "duration": 4.71
    },
    {
        "text": "it means each each filter is learning",
        "start": 2750.04,
        "duration": 5.79
    },
    {
        "text": "something corresponding to different",
        "start": 2752.26,
        "duration": 5.67
    },
    {
        "text": "locations in the image so it can learn",
        "start": 2755.83,
        "duration": 4.56
    },
    {
        "text": "an edge but it would try to learn edge",
        "start": 2757.93,
        "duration": 4.47
    },
    {
        "text": "here edge here edge here edge here and",
        "start": 2760.39,
        "duration": 4.56
    },
    {
        "text": "it will learn it would work like an edge",
        "start": 2762.4,
        "duration": 5.66
    },
    {
        "text": "detector so that corresponds actually",
        "start": 2764.95,
        "duration": 5.73
    },
    {
        "text": "when you do this it can be computed as a",
        "start": 2768.06,
        "duration": 5.26
    },
    {
        "text": "convolution when you take this weights",
        "start": 2770.68,
        "duration": 4.53
    },
    {
        "text": "or you take the neuron and then you",
        "start": 2773.32,
        "duration": 4.26
    },
    {
        "text": "measure you convolve it with the area",
        "start": 2775.21,
        "duration": 4.53
    },
    {
        "text": "here then you look here look at the area",
        "start": 2777.58,
        "duration": 4.17
    },
    {
        "text": "here then here and then here",
        "start": 2779.74,
        "duration": 5.31
    },
    {
        "text": "slide that neuron across the image so",
        "start": 2781.75,
        "duration": 4.92
    },
    {
        "text": "that corresponds to convolution that",
        "start": 2785.05,
        "duration": 3.84
    },
    {
        "text": "what people in signal processing and",
        "start": 2786.67,
        "duration": 5.1
    },
    {
        "text": "image now this will do before what it",
        "start": 2788.89,
        "duration": 5.37
    },
    {
        "text": "does is fairly simple operation and it's",
        "start": 2791.77,
        "duration": 4.5
    },
    {
        "text": "not going to work unfortunately what it",
        "start": 2794.26,
        "duration": 4.71
    },
    {
        "text": "does when you look at the one area you",
        "start": 2796.27,
        "duration": 5.22
    },
    {
        "text": "take so big numbers are image values and",
        "start": 2798.97,
        "duration": 5.13
    },
    {
        "text": "the small numbers are your weight values",
        "start": 2801.49,
        "duration": 4.53
    },
    {
        "text": "or your fuel filter Mouse what you do",
        "start": 2804.1,
        "duration": 5.16
    },
    {
        "text": "you multiply element-wise your values of",
        "start": 2806.02,
        "duration": 4.68
    },
    {
        "text": "the image by your corresponding weight",
        "start": 2809.26,
        "duration": 4.32
    },
    {
        "text": "weight values and then you make a",
        "start": 2810.7,
        "duration": 5.49
    },
    {
        "text": "summation and you output then what you",
        "start": 2813.58,
        "duration": 6.54
    },
    {
        "text": "do your move your filter to the next to",
        "start": 2816.19,
        "duration": 6.81
    },
    {
        "text": "the next 3x3 part of the image and you",
        "start": 2820.12,
        "duration": 6.33
    },
    {
        "text": "repeat the same with the same values of",
        "start": 2823.0,
        "duration": 5.75
    },
    {
        "text": "the filters so for example this filter",
        "start": 2826.45,
        "duration": 4.89
    },
    {
        "text": "actually learns a cross pattern because",
        "start": 2828.75,
        "duration": 5.68
    },
    {
        "text": "it has 0 here 0 here here and here and",
        "start": 2831.34,
        "duration": 6.18
    },
    {
        "text": "once on the agonist so it learns the and",
        "start": 2834.43,
        "duration": 6.12
    },
    {
        "text": "then it learns the cross pattern and",
        "start": 2837.52,
        "duration": 8.4
    },
    {
        "text": "this value somehow represents the how",
        "start": 2840.55,
        "duration": 9.12
    },
    {
        "text": "much of the similarity this part of the",
        "start": 2845.92,
        "duration": 6.45
    },
    {
        "text": "image represents to a to this cross",
        "start": 2849.67,
        "duration": 4.89
    },
    {
        "text": "pattern if there is a cross pattern it's",
        "start": 2852.37,
        "duration": 4.26
    },
    {
        "text": "going to get bigger number if there are",
        "start": 2854.56,
        "duration": 4.17
    },
    {
        "text": "if the part of the image has exactly",
        "start": 2856.63,
        "duration": 4.08
    },
    {
        "text": "zeros on the diagonals and once",
        "start": 2858.73,
        "duration": 4.53
    },
    {
        "text": "otherwise that would actually have 0 so",
        "start": 2860.71,
        "duration": 4.95
    },
    {
        "text": "no cross pattern so that's and then you",
        "start": 2863.26,
        "duration": 4.44
    },
    {
        "text": "keep sliding this across the image and",
        "start": 2865.66,
        "duration": 10.95
    },
    {
        "text": "then you fill up the control feature no",
        "start": 2867.7,
        "duration": 11.34
    },
    {
        "text": "it's it's hyper parameter they're",
        "start": 2876.61,
        "duration": 4.98
    },
    {
        "text": "usually fairly small from 2 by 2 I think",
        "start": 2879.04,
        "duration": 4.86
    },
    {
        "text": "the smallest and then you can go the",
        "start": 2881.59,
        "duration": 4.74
    },
    {
        "text": "people go to like 7 by 7 if you have a",
        "start": 2883.9,
        "duration": 4.47
    },
    {
        "text": "large image because you want to slide",
        "start": 2886.33,
        "duration": 3.87
    },
    {
        "text": "quickly and then pick up the big",
        "start": 2888.37,
        "duration": 3.99
    },
    {
        "text": "patterns if you have a big image and you",
        "start": 2890.2,
        "duration": 3.54
    },
    {
        "text": "have to buy to filter it will take",
        "start": 2892.36,
        "duration": 5.45
    },
    {
        "text": "forever to compute all the convolutions",
        "start": 2893.74,
        "duration": 4.07
    },
    {
        "text": "there is a nice explanation of the",
        "start": 2898.59,
        "duration": 14.7
    },
    {
        "text": "convolution so here yes",
        "start": 2902.56,
        "duration": 13.25
    },
    {
        "text": "not fitting everything so basically",
        "start": 2913.29,
        "duration": 6.34
    },
    {
        "text": "that's a dynamic dynamic demo so you",
        "start": 2915.81,
        "duration": 5.89
    },
    {
        "text": "have input volume and then this volume",
        "start": 2919.63,
        "duration": 4.26
    },
    {
        "text": "is just basically the depths of the",
        "start": 2921.7,
        "duration": 4.8
    },
    {
        "text": "volume these three colors that cut them",
        "start": 2923.89,
        "duration": 4.65
    },
    {
        "text": "different different pieces so you can",
        "start": 2926.5,
        "duration": 4.89
    },
    {
        "text": "just look at one color for example you",
        "start": 2928.54,
        "duration": 6.63
    },
    {
        "text": "have the one filter corresponding to a",
        "start": 2931.39,
        "duration": 6.06
    },
    {
        "text": "set of neurons in first depth slice when",
        "start": 2935.17,
        "duration": 3.72
    },
    {
        "text": "you when you have the volume of the",
        "start": 2937.45,
        "duration": 3.36
    },
    {
        "text": "filters this is the first depth slice",
        "start": 2938.89,
        "duration": 4.62
    },
    {
        "text": "and this filter these filters are for",
        "start": 2940.81,
        "duration": 4.71
    },
    {
        "text": "all neurons in there what you do you",
        "start": 2943.51,
        "duration": 4.26
    },
    {
        "text": "take the filter and then you slide it",
        "start": 2945.52,
        "duration": 4.29
    },
    {
        "text": "across the image and then you compute",
        "start": 2947.77,
        "duration": 4.44
    },
    {
        "text": "this convolution that we just discussed",
        "start": 2949.81,
        "duration": 6.09
    },
    {
        "text": "by element-wise multiplication and you",
        "start": 2952.21,
        "duration": 5.58
    },
    {
        "text": "put the output to the corresponding",
        "start": 2955.9,
        "duration": 4.74
    },
    {
        "text": "output volume and again because you have",
        "start": 2957.79,
        "duration": 6.09
    },
    {
        "text": "filtered filter number one and filter",
        "start": 2960.64,
        "duration": 6.21
    },
    {
        "text": "number two in your output volume they're",
        "start": 2963.88,
        "duration": 5.1
    },
    {
        "text": "going to correspond to depth slices so",
        "start": 2966.85,
        "duration": 4.8
    },
    {
        "text": "it's going to be you know first slice of",
        "start": 2968.98,
        "duration": 5.67
    },
    {
        "text": "that depth output and the second slice",
        "start": 2971.65,
        "duration": 5.82
    },
    {
        "text": "of the depth output so when you perform",
        "start": 2974.65,
        "duration": 4.35
    },
    {
        "text": "this convolution when you finish this",
        "start": 2977.47,
        "duration": 12.66
    },
    {
        "text": "convolution yeah you can you can do know",
        "start": 2979.0,
        "duration": 13.26
    },
    {
        "text": "you can do this in real life you can go",
        "start": 2990.13,
        "duration": 3.96
    },
    {
        "text": "by one step you can jump one you can",
        "start": 2992.26,
        "duration": 4.65
    },
    {
        "text": "jump to for example if you have a very",
        "start": 2994.09,
        "duration": 5.46
    },
    {
        "text": "large image again but your features are",
        "start": 2996.91,
        "duration": 4.92
    },
    {
        "text": "relatively sparse you might not go",
        "start": 2999.55,
        "duration": 4.26
    },
    {
        "text": "everywhere every single step but you",
        "start": 3001.83,
        "duration": 3.48
    },
    {
        "text": "actually wanted to jump and find",
        "start": 3003.81,
        "duration": 8.28
    },
    {
        "text": "something or if if that make sense yeah",
        "start": 3005.31,
        "duration": 9.75
    },
    {
        "text": "so what they actually learn if you take",
        "start": 3012.09,
        "duration": 6.18
    },
    {
        "text": "some of this so so that green input that",
        "start": 3015.06,
        "duration": 6.27
    },
    {
        "text": "was you know showing it's actually",
        "start": 3018.27,
        "duration": 5.07
    },
    {
        "text": "slices here so one filter here another",
        "start": 3021.33,
        "duration": 4.47
    },
    {
        "text": "filter here when we learn those values",
        "start": 3023.34,
        "duration": 4.08
    },
    {
        "text": "and we want to look what they actually",
        "start": 3025.8,
        "duration": 4.86
    },
    {
        "text": "learning in a multi-layer network you",
        "start": 3027.42,
        "duration": 6.21
    },
    {
        "text": "can look at the output of the you know a",
        "start": 3030.66,
        "duration": 7.29
    },
    {
        "text": "few first layer filters and these are on",
        "start": 3033.63,
        "duration": 6.72
    },
    {
        "text": "the RGB natural images so like",
        "start": 3037.95,
        "duration": 4.35
    },
    {
        "text": "photographs so they learn some",
        "start": 3040.35,
        "duration": 4.2
    },
    {
        "text": "structures and patterns and some",
        "start": 3042.3,
        "duration": 3.92
    },
    {
        "text": "speckles and stuff stuff like that",
        "start": 3044.55,
        "duration": 3.9
    },
    {
        "text": "you know doesn't make any sense but you",
        "start": 3046.22,
        "duration": 3.82
    },
    {
        "text": "can see how there are some different",
        "start": 3048.45,
        "duration": 3.389
    },
    {
        "text": "colors in different patterns so it does",
        "start": 3050.04,
        "duration": 4.53
    },
    {
        "text": "learn different things so it converges",
        "start": 3051.839,
        "duration": 5.131
    },
    {
        "text": "to learning different patterns if you",
        "start": 3054.57,
        "duration": 4.529
    },
    {
        "text": "look at the second layer already start",
        "start": 3056.97,
        "duration": 3.57
    },
    {
        "text": "showing some of the more complex",
        "start": 3059.099,
        "duration": 3.901
    },
    {
        "text": "structures some of this I don't know",
        "start": 3060.54,
        "duration": 5.19
    },
    {
        "text": "what's this some circles of some",
        "start": 3063.0,
        "duration": 6.839
    },
    {
        "text": "particles like that some other more fine",
        "start": 3065.73,
        "duration": 8.129
    },
    {
        "text": "green textures and if you look highl a",
        "start": 3069.839,
        "duration": 5.731
    },
    {
        "text": "network for example fourth or fifth",
        "start": 3073.859,
        "duration": 3.421
    },
    {
        "text": "convolutional layer you can see that's",
        "start": 3075.57,
        "duration": 4.56
    },
    {
        "text": "much much or complex patterns are",
        "start": 3077.28,
        "duration": 5.91
    },
    {
        "text": "learned in in the higher levels so what",
        "start": 3080.13,
        "duration": 4.739
    },
    {
        "text": "it does because it's again using the",
        "start": 3083.19,
        "duration": 4.32
    },
    {
        "text": "combinations of the previous layer",
        "start": 3084.869,
        "duration": 5.341
    },
    {
        "text": "output to come to combine them and build",
        "start": 3087.51,
        "duration": 7.079
    },
    {
        "text": "more complex representations it that",
        "start": 3090.21,
        "duration": 7.23
    },
    {
        "text": "combines these guys to build these guys",
        "start": 3094.589,
        "duration": 5.28
    },
    {
        "text": "and it uses different combinations of",
        "start": 3097.44,
        "duration": 4.71
    },
    {
        "text": "these with different coefficients to",
        "start": 3099.869,
        "duration": 3.901
    },
    {
        "text": "build different combinations of this and",
        "start": 3102.15,
        "duration": 5.04
    },
    {
        "text": "etc etc so deeper your network more",
        "start": 3103.77,
        "duration": 8.27
    },
    {
        "text": "complicated structures you can learn and",
        "start": 3107.19,
        "duration": 7.409
    },
    {
        "text": "then if you take a imagine if you take a",
        "start": 3112.04,
        "duration": 5.799
    },
    {
        "text": "simple you know not very deep network is",
        "start": 3114.599,
        "duration": 4.801
    },
    {
        "text": "like what 12 layers or something like",
        "start": 3117.839,
        "duration": 6.48
    },
    {
        "text": "that you visualize that filters but",
        "start": 3119.4,
        "duration": 6.89
    },
    {
        "text": "activations the outputs of the layers",
        "start": 3124.319,
        "duration": 4.861
    },
    {
        "text": "you can see how in a very in the first",
        "start": 3126.29,
        "duration": 5.829
    },
    {
        "text": "layer using the random set of parameters",
        "start": 3129.18,
        "duration": 6.54
    },
    {
        "text": "it does get sort of detect like a",
        "start": 3132.119,
        "duration": 6.391
    },
    {
        "text": "silhouette of the car and then when you",
        "start": 3135.72,
        "duration": 6.0
    },
    {
        "text": "apply the rail in a nonlinear function",
        "start": 3138.51,
        "duration": 6.72
    },
    {
        "text": "to it you get output some kind of",
        "start": 3141.72,
        "duration": 5.82
    },
    {
        "text": "features and you continue doing that and",
        "start": 3145.23,
        "duration": 4.109
    },
    {
        "text": "you continue combine the outputs or",
        "start": 3147.54,
        "duration": 5.19
    },
    {
        "text": "previous layers to to the next ones in",
        "start": 3149.339,
        "duration": 5.49
    },
    {
        "text": "the end you get some kind of output of",
        "start": 3152.73,
        "duration": 5.639
    },
    {
        "text": "very specific features that correspond",
        "start": 3154.829,
        "duration": 5.671
    },
    {
        "text": "to these images and then the specific",
        "start": 3158.369,
        "duration": 4.081
    },
    {
        "text": "features go on the output layer that",
        "start": 3160.5,
        "duration": 4.2
    },
    {
        "text": "produces probabilities so what sort of",
        "start": 3162.45,
        "duration": 6.6
    },
    {
        "text": "produces the original image to the most",
        "start": 3164.7,
        "duration": 7.32
    },
    {
        "text": "informative image the features that it",
        "start": 3169.05,
        "duration": 5.58
    },
    {
        "text": "was able to pick up from here so it's",
        "start": 3172.02,
        "duration": 4.2
    },
    {
        "text": "not using the full image in the end it",
        "start": 3174.63,
        "duration": 4.14
    },
    {
        "text": "uses only what was filter down so it",
        "start": 3176.22,
        "duration": 3.619
    },
    {
        "text": "sort of doesn't like",
        "start": 3178.77,
        "duration": 3.349
    },
    {
        "text": "automatic feature selection not only",
        "start": 3179.839,
        "duration": 3.66
    },
    {
        "text": "feature extraction but also feature",
        "start": 3182.119,
        "duration": 4.021
    },
    {
        "text": "selection based on the training and then",
        "start": 3183.499,
        "duration": 4.62
    },
    {
        "text": "as a regard as before so that puts the",
        "start": 3186.14,
        "duration": 4.859
    },
    {
        "text": "bunch of probabilities and then for five",
        "start": 3188.119,
        "duration": 4.291
    },
    {
        "text": "class classification would be something",
        "start": 3190.999,
        "duration": 3.451
    },
    {
        "text": "like that that outputs probability for a",
        "start": 3192.41,
        "duration": 4.379
    },
    {
        "text": "car probability for track so none of",
        "start": 3194.45,
        "duration": 4.919
    },
    {
        "text": "them are zero actually but if you look",
        "start": 3196.789,
        "duration": 4.141
    },
    {
        "text": "at the maximum probability it would",
        "start": 3199.369,
        "duration": 3.811
    },
    {
        "text": "correspond to one of only one class so",
        "start": 3200.93,
        "duration": 4.26
    },
    {
        "text": "that's why it's actually a regression if",
        "start": 3203.18,
        "duration": 3.119
    },
    {
        "text": "you want to do regression it is a",
        "start": 3205.19,
        "duration": 4.889
    },
    {
        "text": "regression but in probabilities but if",
        "start": 3206.299,
        "duration": 5.28
    },
    {
        "text": "you want the classification just pick",
        "start": 3210.079,
        "duration": 4.76
    },
    {
        "text": "one that has the highest permeability",
        "start": 3211.579,
        "duration": 17.22
    },
    {
        "text": "and how deep is the deep it is smaller",
        "start": 3214.839,
        "duration": 20.89
    },
    {
        "text": "so these are the it is smaller so every",
        "start": 3228.799,
        "duration": 10.95
    },
    {
        "text": "next book of activations these guys that",
        "start": 3235.729,
        "duration": 5.61
    },
    {
        "text": "you get they're getting smaller and",
        "start": 3239.749,
        "duration": 3.661
    },
    {
        "text": "smaller and smaller because you come off",
        "start": 3241.339,
        "duration": 4.17
    },
    {
        "text": "and when you come off you get the output",
        "start": 3243.41,
        "duration": 6.149
    },
    {
        "text": "that is equal to your original image",
        "start": 3245.509,
        "duration": 7.651
    },
    {
        "text": "times your step with which you convolve",
        "start": 3249.559,
        "duration": 6.631
    },
    {
        "text": "and then negative one so it gets smaller",
        "start": 3253.16,
        "duration": 7.169
    },
    {
        "text": "and smaller every time yeah you can look",
        "start": 3256.19,
        "duration": 6.419
    },
    {
        "text": "it up there is a it's it because because",
        "start": 3260.329,
        "duration": 3.96
    },
    {
        "text": "of the convolution operation it gets",
        "start": 3262.609,
        "duration": 6.69
    },
    {
        "text": "smaller and smaller every time you can",
        "start": 3264.289,
        "duration": 10.53
    },
    {
        "text": "do padding and then I'm gonna finish so",
        "start": 3269.299,
        "duration": 7.891
    },
    {
        "text": "how deep is the deep for example 2014",
        "start": 3274.819,
        "duration": 4.381
    },
    {
        "text": "was very deep commercial neural network",
        "start": 3277.19,
        "duration": 4.77
    },
    {
        "text": "who's nineteen layers and and these days",
        "start": 3279.2,
        "duration": 4.68
    },
    {
        "text": "last year for example that was a paper",
        "start": 3281.96,
        "duration": 5.909
    },
    {
        "text": "that was state-of-the-art 2016 a 1200",
        "start": 3283.88,
        "duration": 6.119
    },
    {
        "text": "layers and this is important for very",
        "start": 3287.869,
        "duration": 4.65
    },
    {
        "text": "specific Asian problems is very large",
        "start": 3289.999,
        "duration": 7.891
    },
    {
        "text": "datasets and then as revolution of depth",
        "start": 3292.519,
        "duration": 7.921
    },
    {
        "text": "was negatively correlated is the error",
        "start": 3297.89,
        "duration": 6.359
    },
    {
        "text": "on one of the famous datasets the big",
        "start": 3300.44,
        "duration": 5.25
    },
    {
        "text": "challenge and the computer vision",
        "start": 3304.249,
        "duration": 4.08
    },
    {
        "text": "recognition it has I think few million",
        "start": 3305.69,
        "duration": 5.22
    },
    {
        "text": "images and a thousand classes so the",
        "start": 3308.329,
        "duration": 5.02
    },
    {
        "text": "error was 2010 was 28",
        "start": 3310.91,
        "duration": 4.99
    },
    {
        "text": "dropped significantly and now it's below",
        "start": 3313.349,
        "duration": 5.7
    },
    {
        "text": "4% and 4% is like a huge human error on",
        "start": 3315.9,
        "duration": 5.419
    },
    {
        "text": "that data set so now this",
        "start": 3319.049,
        "duration": 4.68
    },
    {
        "text": "lepers perform better than people on",
        "start": 3321.319,
        "duration": 6.01
    },
    {
        "text": "this specific data set so I think I'm",
        "start": 3323.729,
        "duration": 7.08
    },
    {
        "text": "gonna skip structure the summary there",
        "start": 3327.329,
        "duration": 5.97
    },
    {
        "text": "are some challenges associated is using",
        "start": 3330.809,
        "duration": 3.96
    },
    {
        "text": "these models because they require lots",
        "start": 3333.299,
        "duration": 3.72
    },
    {
        "text": "of data and then you can address this a",
        "start": 3334.769,
        "duration": 3.96
    },
    {
        "text": "little bit with some specificity",
        "start": 3337.019,
        "duration": 4.53
    },
    {
        "text": "specific techniques but still you have",
        "start": 3338.729,
        "duration": 4.1
    },
    {
        "text": "to have a decent amount of data",
        "start": 3341.549,
        "duration": 3.48
    },
    {
        "text": "unsupervised learning doesn't work so if",
        "start": 3342.829,
        "duration": 4.66
    },
    {
        "text": "you have very little number of layers",
        "start": 3345.029,
        "duration": 4.351
    },
    {
        "text": "that might create a lot of problems for",
        "start": 3347.489,
        "duration": 4.8
    },
    {
        "text": "you it's not a silver bullet doesn't",
        "start": 3349.38,
        "duration": 5.31
    },
    {
        "text": "solve any problem and then a lot of",
        "start": 3352.289,
        "duration": 6.601
    },
    {
        "text": "people tend to use it as the overkill so",
        "start": 3354.69,
        "duration": 5.929
    },
    {
        "text": "they could solve something using similar",
        "start": 3358.89,
        "duration": 4.589
    },
    {
        "text": "algorithms like linear regression or and",
        "start": 3360.619,
        "duration": 4.69
    },
    {
        "text": "the first or something so you know it",
        "start": 3363.479,
        "duration": 3.421
    },
    {
        "text": "not necessarily to get all that",
        "start": 3365.309,
        "duration": 4.141
    },
    {
        "text": "complicated you should try the simple",
        "start": 3366.9,
        "duration": 5.07
    },
    {
        "text": "things first at the same time it's not",
        "start": 3369.45,
        "duration": 5.599
    },
    {
        "text": "Nautilus works in other words to try and",
        "start": 3371.97,
        "duration": 6.059
    },
    {
        "text": "you know you need GPUs to train them and",
        "start": 3375.049,
        "duration": 5.8
    },
    {
        "text": "then there was a large body of work in",
        "start": 3378.029,
        "duration": 4.47
    },
    {
        "text": "theory visualization interpretation on",
        "start": 3380.849,
        "duration": 4.62
    },
    {
        "text": "how to interpret the outputs of these",
        "start": 3382.499,
        "duration": 7.85
    },
    {
        "text": "models there is a lot of lot of",
        "start": 3385.469,
        "duration": 7.05
    },
    {
        "text": "different frameworks you can use and",
        "start": 3390.349,
        "duration": 4.27
    },
    {
        "text": "then I'm gonna skip most of the",
        "start": 3392.519,
        "duration": 3.96
    },
    {
        "text": "applications because the amount of time",
        "start": 3394.619,
        "duration": 9.321
    },
    {
        "text": "just want to mention 2016 so we can do",
        "start": 3396.479,
        "duration": 10.62
    },
    {
        "text": "you can do the histology images you can",
        "start": 3403.94,
        "duration": 6.19
    },
    {
        "text": "do CT scans this is a 3d CT scan when",
        "start": 3407.099,
        "duration": 4.95
    },
    {
        "text": "they slide the filters of three",
        "start": 3410.13,
        "duration": 3.75
    },
    {
        "text": "dimensions instead of two dimensions and",
        "start": 3412.049,
        "duration": 5.19
    },
    {
        "text": "then you know have a very good organ",
        "start": 3413.88,
        "duration": 8.959
    },
    {
        "text": "organ segmentation quality so again 3d",
        "start": 3417.239,
        "duration": 9.36
    },
    {
        "text": "so yeah for example this 2016 twenty",
        "start": 3422.839,
        "duration": 6.22
    },
    {
        "text": "seven twenty seven layer CNN trained on",
        "start": 3426.599,
        "duration": 4.26
    },
    {
        "text": "the budget extracted for whole slide",
        "start": 3429.059,
        "duration": 4.41
    },
    {
        "text": "images of these cells I think it was",
        "start": 3430.859,
        "duration": 4.86
    },
    {
        "text": "breast cancer yeah and then a you see of",
        "start": 3433.469,
        "duration": 5.07
    },
    {
        "text": "neural network was or ninety two and a",
        "start": 3435.719,
        "duration": 5.701
    },
    {
        "text": "half and then board of pathologist also",
        "start": 3438.539,
        "duration": 5.73
    },
    {
        "text": "did evaluate them and then average",
        "start": 3441.42,
        "duration": 5.649
    },
    {
        "text": "pathologist was ninety six and six",
        "start": 3444.269,
        "duration": 5.111
    },
    {
        "text": "and then when the combined your network",
        "start": 3447.069,
        "duration": 3.69
    },
    {
        "text": "and pathologist because they were not",
        "start": 3449.38,
        "duration": 3.089
    },
    {
        "text": "very well correlated they actually were",
        "start": 3450.759,
        "duration": 5.82
    },
    {
        "text": "able to push the accuracy up because so",
        "start": 3452.469,
        "duration": 6.12
    },
    {
        "text": "pathologist and neural network perform",
        "start": 3456.579,
        "duration": 4.051
    },
    {
        "text": "better than just pathologist there",
        "start": 3458.589,
        "duration": 3.72
    },
    {
        "text": "aren't 85 percent reduction in this",
        "start": 3460.63,
        "duration": 7.439
    },
    {
        "text": "reality of humor rate and if this is",
        "start": 3462.309,
        "duration": 7.47
    },
    {
        "text": "interesting work also from last year",
        "start": 3468.069,
        "duration": 6.061
    },
    {
        "text": "they combined this detection of of",
        "start": 3469.779,
        "duration": 8.881
    },
    {
        "text": "something oh I think it was lung in lung",
        "start": 3474.13,
        "duration": 8.79
    },
    {
        "text": "there they were detected lesions or I",
        "start": 3478.66,
        "duration": 7.829
    },
    {
        "text": "guess that here I don't remember so but",
        "start": 3482.92,
        "duration": 5.609
    },
    {
        "text": "the fact they were training together",
        "start": 3486.489,
        "duration": 4.08
    },
    {
        "text": "with the vocabulary that doctor is used",
        "start": 3488.529,
        "duration": 3.51
    },
    {
        "text": "to describe images so when it was",
        "start": 3490.569,
        "duration": 3.72
    },
    {
        "text": "detecting these things they would find",
        "start": 3492.039,
        "duration": 4.53
    },
    {
        "text": "the lesions inside the neural network is",
        "start": 3494.289,
        "duration": 4.97
    },
    {
        "text": "outputting the response did the",
        "start": 3496.569,
        "duration": 4.92
    },
    {
        "text": "probability but also the description so",
        "start": 3499.259,
        "duration": 4.661
    },
    {
        "text": "this is machine generated text that",
        "start": 3501.489,
        "duration": 5.04
    },
    {
        "text": "would describe what the does machine see",
        "start": 3503.92,
        "duration": 4.649
    },
    {
        "text": "in human language so when it comes to",
        "start": 3506.529,
        "duration": 3.93
    },
    {
        "text": "interpretation this is very exciting",
        "start": 3508.569,
        "duration": 4.14
    },
    {
        "text": "work that you know doctor doctor not",
        "start": 3510.459,
        "duration": 4.35
    },
    {
        "text": "only sees where it found it in the",
        "start": 3512.709,
        "duration": 4.11
    },
    {
        "text": "probability but it also describes in the",
        "start": 3514.809,
        "duration": 4.23
    },
    {
        "text": "human language some features of that",
        "start": 3516.819,
        "duration": 5.54
    },
    {
        "text": "couple of Google papers from last year",
        "start": 3519.039,
        "duration": 6.3
    },
    {
        "text": "detecting diabetic retinopathy in eyes",
        "start": 3522.359,
        "duration": 7.831
    },
    {
        "text": "with 99% accuracy with 48 layer CNN and",
        "start": 3525.339,
        "duration": 7.7
    },
    {
        "text": "this this year paper in nature",
        "start": 3530.19,
        "duration": 4.869
    },
    {
        "text": "dermatologist level classification of",
        "start": 3533.039,
        "duration": 3.97
    },
    {
        "text": "skin cancer so they took two different",
        "start": 3535.059,
        "duration": 4.94
    },
    {
        "text": "types of skin cancer compare it to",
        "start": 3537.009,
        "duration": 5.671
    },
    {
        "text": "25:21 board-certified dermatologist and",
        "start": 3539.999,
        "duration": 6.52
    },
    {
        "text": "then CNN was performing on par with 21",
        "start": 3542.68,
        "duration": 6.269
    },
    {
        "text": "dermatologist so we can take 21 people",
        "start": 3546.519,
        "duration": 4.77
    },
    {
        "text": "you can take 48 layers of CN and it get",
        "start": 3548.949,
        "duration": 5.991
    },
    {
        "text": "pretty much the same result so that",
        "start": 3551.289,
        "duration": 5.641
    },
    {
        "text": "provides a lot of excitement in the",
        "start": 3554.94,
        "duration": 4.599
    },
    {
        "text": "industry there are a lot of stars that",
        "start": 3556.93,
        "duration": 4.019
    },
    {
        "text": "startups they're doing this kind of",
        "start": 3559.539,
        "duration": 4.651
    },
    {
        "text": "stuff and I think I'm gonna skip my work",
        "start": 3560.949,
        "duration": 5.42
    },
    {
        "text": "I'm gonna go straight to questions",
        "start": 3564.19,
        "duration": 4.589
    },
    {
        "text": "it's okay what work is unpublished so",
        "start": 3566.369,
        "duration": 7.42
    },
    {
        "text": "too early anyway sorry I got a little",
        "start": 3568.779,
        "duration": 7.611
    },
    {
        "text": "overtime",
        "start": 3573.789,
        "duration": 2.601
    },
    {
        "text": "that means age at the at the edge of",
        "start": 3583.61,
        "duration": 9.66
    },
    {
        "text": "your speaker is more difficult to learn",
        "start": 3588.93,
        "duration": 10.02
    },
    {
        "text": "second when you do convolutions you are",
        "start": 3593.27,
        "duration": 10.66
    },
    {
        "text": "drinking yeah so does that means stuff",
        "start": 3598.95,
        "duration": 11.55
    },
    {
        "text": "at the edge of your input layer is in",
        "start": 3603.93,
        "duration": 8.1
    },
    {
        "text": "the middle right right",
        "start": 3610.5,
        "duration": 6.9
    },
    {
        "text": "so wait where is the convolution yeah so",
        "start": 3612.03,
        "duration": 7.68
    },
    {
        "text": "when you when you have this edges of the",
        "start": 3617.4,
        "duration": 5.82
    },
    {
        "text": "image as it was correct mentioned before",
        "start": 3619.71,
        "duration": 5.37
    },
    {
        "text": "you can path with zeros here",
        "start": 3623.22,
        "duration": 4.44
    },
    {
        "text": "so when you start the first convolution",
        "start": 3625.08,
        "duration": 5.61
    },
    {
        "text": "it would it would Center right on right",
        "start": 3627.66,
        "duration": 5.28
    },
    {
        "text": "on this pixel and as you move you would",
        "start": 3630.69,
        "duration": 4.47
    },
    {
        "text": "Center on the each pixel neighboring to",
        "start": 3632.94,
        "duration": 5.52
    },
    {
        "text": "it or even if you don't pad these values",
        "start": 3635.16,
        "duration": 7.35
    },
    {
        "text": "are still contributing to this so there",
        "start": 3638.46,
        "duration": 6.18
    },
    {
        "text": "is a contribution from edge values here",
        "start": 3642.51,
        "duration": 4.26
    },
    {
        "text": "so when you start the next layer and you",
        "start": 3644.64,
        "duration": 4.35
    },
    {
        "text": "start involving with this this already",
        "start": 3646.77,
        "duration": 4.77
    },
    {
        "text": "have like a representation of the edge",
        "start": 3648.99,
        "duration": 5.55
    },
    {
        "text": "information from earlier stages so it",
        "start": 3651.54,
        "duration": 4.98
    },
    {
        "text": "sort of does like a summarization of",
        "start": 3654.54,
        "duration": 6.51
    },
    {
        "text": "these patterns into the next edge edge",
        "start": 3656.52,
        "duration": 7.65
    },
    {
        "text": "parts but yeah in in the demo that I was",
        "start": 3661.05,
        "duration": 6.33
    },
    {
        "text": "showing this actually does the actually",
        "start": 3664.17,
        "duration": 7.62
    },
    {
        "text": "does the padding you can see this image",
        "start": 3667.38,
        "duration": 7.2
    },
    {
        "text": "is actually has zeros on the on the",
        "start": 3671.79,
        "duration": 4.41
    },
    {
        "text": "outside so that's that's the padding",
        "start": 3674.58,
        "duration": 4.38
    },
    {
        "text": "because they actually image here but if",
        "start": 3676.2,
        "duration": 4.86
    },
    {
        "text": "you want there is to have more",
        "start": 3678.96,
        "duration": 4.05
    },
    {
        "text": "information about edges you can do this",
        "start": 3681.06,
        "duration": 16.68
    },
    {
        "text": "kind of trick necessarily I'm actually a",
        "start": 3683.01,
        "duration": 17.82
    },
    {
        "text": "couple of models that I was using there",
        "start": 3697.74,
        "duration": 5.1
    },
    {
        "text": "in different frameworks but you know",
        "start": 3700.83,
        "duration": 6.6
    },
    {
        "text": "there are so many of them so as and then",
        "start": 3702.84,
        "duration": 6.479
    },
    {
        "text": "gets one of the researchers tweeted this",
        "start": 3707.43,
        "duration": 4.679
    },
    {
        "text": "this month he said but like we saw 2012",
        "start": 3709.319,
        "duration": 5.371
    },
    {
        "text": "Catholics of 2013 theano is 14 torches",
        "start": 3712.109,
        "duration": 5.551
    },
    {
        "text": "15 and tensorflow is already sub 2016",
        "start": 3714.69,
        "duration": 4.77
    },
    {
        "text": "because there is no library just came",
        "start": 3717.66,
        "duration": 4.53
    },
    {
        "text": "out last month my torch and that",
        "start": 3719.46,
        "duration": 6.24
    },
    {
        "text": "combines Python and torch which is was",
        "start": 3722.19,
        "duration": 6.81
    },
    {
        "text": "Facebook deep learning library and that",
        "start": 3725.7,
        "duration": 6.03
    },
    {
        "text": "was very fast very easy to use so",
        "start": 3729.0,
        "duration": 3.809
    },
    {
        "text": "tensorflow",
        "start": 3731.73,
        "duration": 3.18
    },
    {
        "text": "is going to be a big player in the game",
        "start": 3732.809,
        "duration": 4.201
    },
    {
        "text": "and then yesterday actually Google",
        "start": 3734.91,
        "duration": 4.5
    },
    {
        "text": "released version 1.0 so it's like an",
        "start": 3737.01,
        "duration": 4.859
    },
    {
        "text": "official release now so it will be",
        "start": 3739.41,
        "duration": 4.29
    },
    {
        "text": "definitely still big player in the game",
        "start": 3741.869,
        "duration": 3.811
    },
    {
        "text": "but you know the landscape in changing",
        "start": 3743.7,
        "duration": 3.629
    },
    {
        "text": "and people are using many different",
        "start": 3745.68,
        "duration": 3.87
    },
    {
        "text": "libraries the Charis library was",
        "start": 3747.329,
        "duration": 3.841
    },
    {
        "text": "developed separately from tons of flow",
        "start": 3749.55,
        "duration": 3.15
    },
    {
        "text": "but yesterday they announced they're",
        "start": 3751.17,
        "duration": 3.54
    },
    {
        "text": "going to merge these two ones so it's",
        "start": 3752.7,
        "duration": 3.75
    },
    {
        "text": "going to be chaos inside of tons and",
        "start": 3754.71,
        "duration": 5.04
    },
    {
        "text": "tons of flow and I used I used CDN NIU",
        "start": 3756.45,
        "duration": 5.55
    },
    {
        "text": "storage I used I didn't use standard",
        "start": 3759.75,
        "duration": 4.619
    },
    {
        "text": "form for practical purposes carries for",
        "start": 3762.0,
        "duration": 4.5
    },
    {
        "text": "crack caris I used a little bit yeah I",
        "start": 3764.369,
        "duration": 4.651
    },
    {
        "text": "would say tons of flow is definitely if",
        "start": 3766.5,
        "duration": 4.47
    },
    {
        "text": "you want to like start from your you get",
        "start": 3769.02,
        "duration": 3.539
    },
    {
        "text": "familiar with now you know network",
        "start": 3770.97,
        "duration": 3.089
    },
    {
        "text": "standard flow is definitely a safe bet",
        "start": 3772.559,
        "duration": 2.971
    },
    {
        "text": "is going to be around for a long time",
        "start": 3774.059,
        "duration": 3.99
    },
    {
        "text": "and it's already like a stable version",
        "start": 3775.53,
        "duration": 5.93
    },
    {
        "text": "stable API so it's a good place to start",
        "start": 3778.049,
        "duration": 6.961
    },
    {
        "text": "good place to try using it for for your",
        "start": 3781.46,
        "duration": 6.609
    },
    {
        "text": "problem also my torch worth looking at I",
        "start": 3785.01,
        "duration": 11.45
    },
    {
        "text": "think all right",
        "start": 3788.069,
        "duration": 8.391
    }
]