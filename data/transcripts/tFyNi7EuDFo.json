[
    {
        "text": "he good",
        "start": 0.399,
        "duration": 2.4
    },
    {
        "text": "afternoon uh it's my pleasure to",
        "start": 2.959,
        "duration": 6.521
    },
    {
        "text": "introduce our speaker today uh Dr Jin y",
        "start": 5.64,
        "duration": 6.52
    },
    {
        "text": "Jessica Lee professor of statistics and",
        "start": 9.48,
        "duration": 4.48
    },
    {
        "text": "data science from",
        "start": 12.16,
        "duration": 5.64
    },
    {
        "text": "ucra Jessica's research lies in the at",
        "start": 13.96,
        "duration": 6.8
    },
    {
        "text": "the interface of statistics and biology",
        "start": 17.8,
        "duration": 5.52
    },
    {
        "text": "with a focus on developing statistical",
        "start": 20.76,
        "duration": 4.56
    },
    {
        "text": "methods to understand large scale",
        "start": 23.32,
        "duration": 3.959
    },
    {
        "text": "genomic and transomics",
        "start": 25.32,
        "duration": 4.359
    },
    {
        "text": "data Jessica earned her bachelor's",
        "start": 27.279,
        "duration": 4.8
    },
    {
        "text": "degree in biological sciences at the",
        "start": 29.679,
        "duration": 5.4
    },
    {
        "text": "chinua University and PhD in biost",
        "start": 32.079,
        "duration": 6.681
    },
    {
        "text": "statistics from UC Berkeley she joined",
        "start": 35.079,
        "duration": 7.041
    },
    {
        "text": "ucra in 2013 at The Faculty since then",
        "start": 38.76,
        "duration": 5.92
    },
    {
        "text": "she had made significant contributions",
        "start": 42.12,
        "duration": 4.88
    },
    {
        "text": "to computational biology especially in",
        "start": 44.68,
        "duration": 5.24
    },
    {
        "text": "creating tools for single cell R",
        "start": 47.0,
        "duration": 3.92
    },
    {
        "text": "sequency",
        "start": 49.92,
        "duration": 4.279
    },
    {
        "text": "analysis she's a leader in guarding",
        "start": 50.92,
        "duration": 6.279
    },
    {
        "text": "rigor and transparency in toour",
        "start": 54.199,
        "duration": 6.36
    },
    {
        "text": "development a strong advocate for using",
        "start": 57.199,
        "duration": 4.88
    },
    {
        "text": "synthetic data to Benchmark",
        "start": 60.559,
        "duration": 4.761
    },
    {
        "text": "computational algorithms and a creative",
        "start": 62.079,
        "duration": 6.161
    },
    {
        "text": "tireless scientist that we come to know",
        "start": 65.32,
        "duration": 5.32
    },
    {
        "text": "and appreciate she collaborates broadly",
        "start": 68.24,
        "duration": 4.68
    },
    {
        "text": "to pursue topics uh such as",
        "start": 70.64,
        "duration": 4.96
    },
    {
        "text": "classification problems and multispecies",
        "start": 72.92,
        "duration": 5.48
    },
    {
        "text": "Joint analysis her record of innovation",
        "start": 75.6,
        "duration": 5.0
    },
    {
        "text": "has earned her numerous prestigious",
        "start": 78.4,
        "duration": 4.8
    },
    {
        "text": "award other than ones you know such as",
        "start": 80.6,
        "duration": 6.32
    },
    {
        "text": "SF career HR",
        "start": 83.2,
        "duration": 8.04
    },
    {
        "text": "35 and those from SLO K or c C she has",
        "start": 86.92,
        "duration": 7.36
    },
    {
        "text": "also received just bear with me here",
        "start": 91.24,
        "duration": 4.879
    },
    {
        "text": "received the Overton prize from the",
        "start": 94.28,
        "duration": 3.72
    },
    {
        "text": "international Society for computational",
        "start": 96.119,
        "duration": 4.521
    },
    {
        "text": "biology the emerging leader award from",
        "start": 98.0,
        "duration": 4.72
    },
    {
        "text": "the committee of President of",
        "start": 100.64,
        "duration": 5.24
    },
    {
        "text": "statistical societies MIT Technology",
        "start": 102.72,
        "duration": 6.759
    },
    {
        "text": "reviews award for 35 innovators under",
        "start": 105.88,
        "duration": 6.839
    },
    {
        "text": "35 Johnson Johnson women in stem scholar",
        "start": 109.479,
        "duration": 6.401
    },
    {
        "text": "award and I've got a chance to he to",
        "start": 112.719,
        "duration": 5.241
    },
    {
        "text": "hear her radically Fellowship from",
        "start": 115.88,
        "duration": 4.879
    },
    {
        "text": "Harvard that gave her whole year of",
        "start": 117.96,
        "duration": 5.519
    },
    {
        "text": "visiting professorship to connect with",
        "start": 120.759,
        "duration": 5.72
    },
    {
        "text": "Scholars from a exceptionally wide range",
        "start": 123.479,
        "duration": 4.92
    },
    {
        "text": "of uh disciplines in Natural Sciences",
        "start": 126.479,
        "duration": 3.041
    },
    {
        "text": "and",
        "start": 128.399,
        "duration": 4.721
    },
    {
        "text": "Humanities so it's again my pleasure and",
        "start": 129.52,
        "duration": 6.359
    },
    {
        "text": "I want to thank the seminar committee",
        "start": 133.12,
        "duration": 5.28
    },
    {
        "text": "led by Ami king and the King for",
        "start": 135.879,
        "duration": 5.36
    },
    {
        "text": "inviting Jessica now please join me in",
        "start": 138.4,
        "duration": 6.08
    },
    {
        "text": "welcoming Jessica",
        "start": 141.239,
        "duration": 6.201
    },
    {
        "text": "Lee thank you so much Professor Jun Le",
        "start": 144.48,
        "duration": 5.399
    },
    {
        "text": "for the very generous introduction and",
        "start": 147.44,
        "duration": 4.96
    },
    {
        "text": "and also thanks to Kimi and Mii for",
        "start": 149.879,
        "duration": 4.921
    },
    {
        "text": "inviting me to give this talk and visit",
        "start": 152.4,
        "duration": 5.16
    },
    {
        "text": "this very vibrant department and it's my",
        "start": 154.8,
        "duration": 5.079
    },
    {
        "text": "great pleasure to talk about some of our",
        "start": 157.56,
        "duration": 4.72
    },
    {
        "text": "recent work just from the perspective of",
        "start": 159.879,
        "duration": 4.921
    },
    {
        "text": "permutation it's just it's a new talk",
        "start": 162.28,
        "duration": 4.879
    },
    {
        "text": "actually I reorganize my old stuff into",
        "start": 164.8,
        "duration": 5.2
    },
    {
        "text": "this new talk to provide I will say",
        "start": 167.159,
        "duration": 5.561
    },
    {
        "text": "hopefully a simple but useful",
        "start": 170.0,
        "duration": 5.239
    },
    {
        "text": "perspective and which I hope everyone",
        "start": 172.72,
        "duration": 5.08
    },
    {
        "text": "can integrate into your method",
        "start": 175.239,
        "duration": 5.841
    },
    {
        "text": "development or data analysis so just as",
        "start": 177.8,
        "duration": 5.92
    },
    {
        "text": "Professor Le introduced my research has",
        "start": 181.08,
        "duration": 5.079
    },
    {
        "text": "been we try to sit at a junction of",
        "start": 183.72,
        "duration": 4.76
    },
    {
        "text": "statistics and biology so that's how I",
        "start": 186.159,
        "duration": 4.08
    },
    {
        "text": "designed the logo if you pay attention",
        "start": 188.48,
        "duration": 5.36
    },
    {
        "text": "to it GSB and the two colors one for",
        "start": 190.239,
        "duration": 6.041
    },
    {
        "text": "statistics one for biology so I also use",
        "start": 193.84,
        "duration": 4.36
    },
    {
        "text": "the same two colors to highlight the two",
        "start": 196.28,
        "duration": 4.64
    },
    {
        "text": "concepts my title permutation and",
        "start": 198.2,
        "duration": 5.8
    },
    {
        "text": "genomics so just to G your interest from",
        "start": 200.92,
        "duration": 5.36
    },
    {
        "text": "the beginning yesterday you might have",
        "start": 204.0,
        "duration": 4.36
    },
    {
        "text": "seen the big news for the Nobel Prize in",
        "start": 206.28,
        "duration": 4.64
    },
    {
        "text": "physics and I noticed this very",
        "start": 208.36,
        "duration": 6.159
    },
    {
        "text": "interesting figure in Chinese news media",
        "start": 210.92,
        "duration": 6.039
    },
    {
        "text": "and I kindly asked my post of hand to",
        "start": 214.519,
        "duration": 4.961
    },
    {
        "text": "help me translate this to English so",
        "start": 216.959,
        "duration": 4.28
    },
    {
        "text": "basically if you don't look at",
        "start": 219.48,
        "duration": 3.039
    },
    {
        "text": "everything right it's just two",
        "start": 221.239,
        "duration": 5.241
    },
    {
        "text": "Dimensions one is the object we study so",
        "start": 222.519,
        "duration": 5.36
    },
    {
        "text": "we can go from",
        "start": 226.48,
        "duration": 5.039
    },
    {
        "text": "fundamentalism to liberalism so it can",
        "start": 227.879,
        "duration": 7.841
    },
    {
        "text": "be very broad or very specific and we",
        "start": 231.519,
        "duration": 6.601
    },
    {
        "text": "can also have the method going from",
        "start": 235.72,
        "duration": 5.2
    },
    {
        "text": "fundamental to Liberal so what it means",
        "start": 238.12,
        "duration": 5.52
    },
    {
        "text": "is that we will have less requirement in",
        "start": 240.92,
        "duration": 5.44
    },
    {
        "text": "defining what is physics so this is",
        "start": 243.64,
        "duration": 5.0
    },
    {
        "text": "where we are neuron networks is also",
        "start": 246.36,
        "duration": 4.879
    },
    {
        "text": "physics but this",
        "start": 248.64,
        "duration": 5.84
    },
    {
        "text": "liberalism made me wonder the connection",
        "start": 251.239,
        "duration": 5.441
    },
    {
        "text": "to our research right if we think about",
        "start": 254.48,
        "duration": 4.759
    },
    {
        "text": "genomics I would call it a liberal",
        "start": 256.68,
        "duration": 5.279
    },
    {
        "text": "discipline in four ways first of all",
        "start": 259.239,
        "duration": 5.601
    },
    {
        "text": "it's interdisciplinary nature second it",
        "start": 261.959,
        "duration": 5.481
    },
    {
        "text": "has a very data driven Focus instead of",
        "start": 264.84,
        "duration": 4.88
    },
    {
        "text": "theory driven or hypothesis driven",
        "start": 267.44,
        "duration": 5.08
    },
    {
        "text": "driven if it's data driven you may end",
        "start": 269.72,
        "duration": 4.72
    },
    {
        "text": "up getting many interesting findings",
        "start": 272.52,
        "duration": 4.84
    },
    {
        "text": "from data third we have a very rapid",
        "start": 274.44,
        "duration": 5.52
    },
    {
        "text": "evolution of methods as you can see",
        "start": 277.36,
        "duration": 4.16
    },
    {
        "text": "which I will show very soon for single",
        "start": 279.96,
        "duration": 5.239
    },
    {
        "text": "cell data analysis fourth it has allow",
        "start": 281.52,
        "duration": 6.519
    },
    {
        "text": "flexible analytical approaches so we",
        "start": 285.199,
        "duration": 4.56
    },
    {
        "text": "have them if you have real day",
        "start": 288.039,
        "duration": 3.88
    },
    {
        "text": "experience you will know how many steps",
        "start": 289.759,
        "duration": 4.88
    },
    {
        "text": "are involved in your data analysis and",
        "start": 291.919,
        "duration": 4.84
    },
    {
        "text": "different people may do data analysis in",
        "start": 294.639,
        "duration": 5.641
    },
    {
        "text": "a different way so this can be SE in",
        "start": 296.759,
        "duration": 7.681
    },
    {
        "text": "this website the Single Cell RNA tools.",
        "start": 300.28,
        "duration": 7.479
    },
    {
        "text": "org so you can see the quick Evolution",
        "start": 304.44,
        "duration": 5.08
    },
    {
        "text": "right of methods we ended up getting",
        "start": 307.759,
        "duration": 5.241
    },
    {
        "text": "more than a thousand or 1500 methods for",
        "start": 309.52,
        "duration": 5.16
    },
    {
        "text": "just single cell data",
        "start": 313.0,
        "duration": 5.919
    },
    {
        "text": "analysis so facing this success huge",
        "start": 314.68,
        "duration": 6.959
    },
    {
        "text": "success in AI right Nobel pricing",
        "start": 318.919,
        "duration": 5.521
    },
    {
        "text": "physics even and also this data analysis",
        "start": 321.639,
        "duration": 6.0
    },
    {
        "text": "liberalism in our field so the very",
        "start": 324.44,
        "duration": 4.879
    },
    {
        "text": "fundamental question I would say this is",
        "start": 327.639,
        "duration": 4.441
    },
    {
        "text": "the question I had even when I was a PhD",
        "start": 329.319,
        "duration": 4.921
    },
    {
        "text": "student at Berkeley so is statistics",
        "start": 332.08,
        "duration": 4.92
    },
    {
        "text": "still relevant because statistics itself",
        "start": 334.24,
        "duration": 5.28
    },
    {
        "text": "is a such an old discipline more than",
        "start": 337.0,
        "duration": 5.479
    },
    {
        "text": "100 years old given we have so many",
        "start": 339.52,
        "duration": 5.48
    },
    {
        "text": "tools so many flexible approaches and we",
        "start": 342.479,
        "duration": 5.041
    },
    {
        "text": "been so liberal in doing genomics is it",
        "start": 345.0,
        "duration": 5.68
    },
    {
        "text": "still relevant in my opinion statistics",
        "start": 347.52,
        "duration": 5.08
    },
    {
        "text": "is a discipline that ensures rigor in",
        "start": 350.68,
        "duration": 4.359
    },
    {
        "text": "data analysis so to me as long as we",
        "start": 352.6,
        "duration": 5.319
    },
    {
        "text": "care about rigor it still matters so how",
        "start": 355.039,
        "duration": 4.88
    },
    {
        "text": "do I why do I say this let's let's go",
        "start": 357.919,
        "duration": 4.881
    },
    {
        "text": "back to the history to the origin so one",
        "start": 359.919,
        "duration": 5.041
    },
    {
        "text": "of the most famous and fundamental",
        "start": 362.8,
        "duration": 4.32
    },
    {
        "text": "method you guys learn from intro to",
        "start": 364.96,
        "duration": 5.239
    },
    {
        "text": "stack course is two sample te test I",
        "start": 367.12,
        "duration": 6.04
    },
    {
        "text": "happen to had the opportunity to visit",
        "start": 370.199,
        "duration": 5.481
    },
    {
        "text": "the the origin of this method is",
        "start": 373.16,
        "duration": 5.0
    },
    {
        "text": "actually in the Guinness bu Buy in",
        "start": 375.68,
        "duration": 5.919
    },
    {
        "text": "Dublin in Ireland so this guy whose name",
        "start": 378.16,
        "duration": 7.2
    },
    {
        "text": "is goette he actually work at this buy",
        "start": 381.599,
        "duration": 6.241
    },
    {
        "text": "and he was interested in comparing the",
        "start": 385.36,
        "duration": 5.76
    },
    {
        "text": "quality of their be here in two batches",
        "start": 387.84,
        "duration": 6.079
    },
    {
        "text": "to see whether the new technique improve",
        "start": 391.12,
        "duration": 5.359
    },
    {
        "text": "the overall quality so to answer this",
        "start": 393.919,
        "duration": 5.481
    },
    {
        "text": "question he needed to compare two sets",
        "start": 396.479,
        "duration": 7.041
    },
    {
        "text": "of numbers from each batch so because he",
        "start": 399.4,
        "duration": 6.639
    },
    {
        "text": "was so humble he named himself using the",
        "start": 403.52,
        "duration": 4.16
    },
    {
        "text": "studo name of student that's why it's",
        "start": 406.039,
        "duration": 4.241
    },
    {
        "text": "called students T Test but basically",
        "start": 407.68,
        "duration": 4.639
    },
    {
        "text": "what this is about is that we want to",
        "start": 410.28,
        "duration": 5.56
    },
    {
        "text": "compare the two the two averages or two",
        "start": 412.319,
        "duration": 6.201
    },
    {
        "text": "means of the two sets of numbers so even",
        "start": 415.84,
        "duration": 4.24
    },
    {
        "text": "without statistics we could do the",
        "start": 418.52,
        "duration": 3.519
    },
    {
        "text": "comparison right as an elementary school",
        "start": 420.08,
        "duration": 4.559
    },
    {
        "text": "student and there should always be some",
        "start": 422.039,
        "duration": 5.241
    },
    {
        "text": "nonzero difference but the question is",
        "start": 424.639,
        "duration": 5.481
    },
    {
        "text": "is the difference big enough to say yes",
        "start": 427.28,
        "duration": 4.68
    },
    {
        "text": "we have a better quality with a new",
        "start": 430.12,
        "duration": 3.519
    },
    {
        "text": "technique this is the question",
        "start": 431.96,
        "duration": 3.799
    },
    {
        "text": "statistics help answer so you can see",
        "start": 433.639,
        "duration": 4.521
    },
    {
        "text": "even back then it's about the rigor in",
        "start": 435.759,
        "duration": 4.761
    },
    {
        "text": "data analysis how do we interpret data",
        "start": 438.16,
        "duration": 3.879
    },
    {
        "text": "how do we make",
        "start": 440.52,
        "duration": 4.399
    },
    {
        "text": "conclusions continue from here I really",
        "start": 442.039,
        "duration": 4.921
    },
    {
        "text": "like this article and I strongly",
        "start": 444.919,
        "duration": 5.12
    },
    {
        "text": "recommended to people here it's about",
        "start": 446.96,
        "duration": 6.04
    },
    {
        "text": "this summary of important statistical",
        "start": 450.039,
        "duration": 5.801
    },
    {
        "text": "ideas in the past 50 years and Andrew",
        "start": 453.0,
        "duration": 4.84
    },
    {
        "text": "Gilman is a renounced station at",
        "start": 455.84,
        "duration": 4.68
    },
    {
        "text": "Columbia and there's an even I'll say",
        "start": 457.84,
        "duration": 6.16
    },
    {
        "text": "simpler summary on medium about this",
        "start": 460.52,
        "duration": 7.04
    },
    {
        "text": "article so I will draw my talk from a",
        "start": 464.0,
        "duration": 7.36
    },
    {
        "text": "connection in this article because it",
        "start": 467.56,
        "duration": 6.479
    },
    {
        "text": "mentioned I think it listed eight",
        "start": 471.36,
        "duration": 5.559
    },
    {
        "text": "important ideas and the second of it is",
        "start": 474.039,
        "duration": 4.521
    },
    {
        "text": "called bootstrapping and simulation",
        "start": 476.919,
        "duration": 4.361
    },
    {
        "text": "based inference under this idea there's",
        "start": 478.56,
        "duration": 5.8
    },
    {
        "text": "a quote in permutation testing resample",
        "start": 481.28,
        "duration": 5.52
    },
    {
        "text": "data sets are generated by breaking",
        "start": 484.36,
        "duration": 4.76
    },
    {
        "text": "possible dependency between the",
        "start": 486.8,
        "duration": 4.6
    },
    {
        "text": "predictors and Target by randomly",
        "start": 489.12,
        "duration": 4.32
    },
    {
        "text": "shuffling the target values so",
        "start": 491.4,
        "duration": 4.639
    },
    {
        "text": "permutation is the theme of my talk and",
        "start": 493.44,
        "duration": 5.319
    },
    {
        "text": "I will show you three examples of how it",
        "start": 496.039,
        "duration": 6.72
    },
    {
        "text": "can help with my research so to begin I",
        "start": 498.759,
        "duration": 6.601
    },
    {
        "text": "will first show you two scenarios about",
        "start": 502.759,
        "duration": 6.361
    },
    {
        "text": "permutation so Bri briefly right in many",
        "start": 505.36,
        "duration": 5.6
    },
    {
        "text": "me machine learning problems data",
        "start": 509.12,
        "duration": 4.24
    },
    {
        "text": "analysis problems we have a supervised",
        "start": 510.96,
        "duration": 4.759
    },
    {
        "text": "learning setting in which you have a",
        "start": 513.36,
        "duration": 4.76
    },
    {
        "text": "matrix and here I'm just denoting the",
        "start": 515.719,
        "duration": 5.24
    },
    {
        "text": "rows as samples columns as features and",
        "start": 518.12,
        "duration": 5.12
    },
    {
        "text": "you have a company response or outcome",
        "start": 520.959,
        "duration": 4.44
    },
    {
        "text": "Vector called y that's called supervis",
        "start": 523.24,
        "duration": 4.8
    },
    {
        "text": "setting and for unsupervised learning",
        "start": 525.399,
        "duration": 6.321
    },
    {
        "text": "you just have this Matrix x no y and in",
        "start": 528.04,
        "duration": 6.44
    },
    {
        "text": "my talk I'll give you some respective",
        "start": 531.72,
        "duration": 6.92
    },
    {
        "text": "examples for B for B data which I'll",
        "start": 534.48,
        "duration": 6.08
    },
    {
        "text": "talk about it's actually under this",
        "start": 538.64,
        "duration": 4.759
    },
    {
        "text": "setting so here the features are genes",
        "start": 540.56,
        "duration": 5.2
    },
    {
        "text": "samples are just biological samples and",
        "start": 543.399,
        "duration": 5.241
    },
    {
        "text": "why are some sample condition labels",
        "start": 545.76,
        "duration": 5.319
    },
    {
        "text": "binary for the unsupervised learning",
        "start": 548.64,
        "duration": 4.52
    },
    {
        "text": "setting my single set examples will be",
        "start": 551.079,
        "duration": 4.521
    },
    {
        "text": "under this setting here the samples are",
        "start": 553.16,
        "duration": 5.32
    },
    {
        "text": "cells and the features are",
        "start": 555.6,
        "duration": 6.12
    },
    {
        "text": "genes so the three examples I will give",
        "start": 558.48,
        "duration": 5.84
    },
    {
        "text": "you the first one is for bulk",
        "start": 561.72,
        "duration": 4.799
    },
    {
        "text": "differential expression analysis second",
        "start": 564.32,
        "duration": 4.32
    },
    {
        "text": "for single cell data visualization and",
        "start": 566.519,
        "duration": 4.601
    },
    {
        "text": "the third the most recent ongoing one is",
        "start": 568.64,
        "duration": 4.319
    },
    {
        "text": "the aggregation of single cells into",
        "start": 571.12,
        "duration": 4.6
    },
    {
        "text": "meta cells so the first two Works were",
        "start": 572.959,
        "duration": 6.401
    },
    {
        "text": "published so this B arnc de analysis is",
        "start": 575.72,
        "duration": 6.6
    },
    {
        "text": "one of the most commonly performed D",
        "start": 579.36,
        "duration": 6.76
    },
    {
        "text": "analysis in our single sorry in our Arn",
        "start": 582.32,
        "duration": 5.639
    },
    {
        "text": "data analysis I should say so",
        "start": 586.12,
        "duration": 4.56
    },
    {
        "text": "differential expression is defined as oh",
        "start": 587.959,
        "duration": 5.041
    },
    {
        "text": "if a gene has different expression",
        "start": 590.68,
        "duration": 5.279
    },
    {
        "text": "levels under between two conditions I",
        "start": 593.0,
        "duration": 4.959
    },
    {
        "text": "call the gene a de Gene differential",
        "start": 595.959,
        "duration": 4.481
    },
    {
        "text": "Express Gene so this cartoon shows a",
        "start": 597.959,
        "duration": 4.961
    },
    {
        "text": "typical scenario in which you have",
        "start": 600.44,
        "duration": 5.12
    },
    {
        "text": "collected experimental replicates under",
        "start": 602.92,
        "duration": 5.52
    },
    {
        "text": "each condition because RN in those days",
        "start": 605.56,
        "duration": 5.44
    },
    {
        "text": "was quite expensive so people typically",
        "start": 608.44,
        "duration": 5.8
    },
    {
        "text": "collected three replicates per condition",
        "start": 611.0,
        "duration": 5.839
    },
    {
        "text": "and this comparison is done Gene by Gene",
        "start": 614.24,
        "duration": 5.2
    },
    {
        "text": "you look at one gene at a time so",
        "start": 616.839,
        "duration": 4.601
    },
    {
        "text": "intuitively we want to call the left",
        "start": 619.44,
        "duration": 4.639
    },
    {
        "text": "Gene which shows greater differences",
        "start": 621.44,
        "duration": 5.079
    },
    {
        "text": "between the two conditions a de Gene the",
        "start": 624.079,
        "duration": 5.44
    },
    {
        "text": "right Gene maybe not so much so the",
        "start": 626.519,
        "duration": 4.88
    },
    {
        "text": "statistically it was done as a",
        "start": 629.519,
        "duration": 4.041
    },
    {
        "text": "hypothesis testing problem one",
        "start": 631.399,
        "duration": 4.68
    },
    {
        "text": "statistical test per Gene with no",
        "start": 633.56,
        "duration": 5.0
    },
    {
        "text": "hypothesis that the gene has equal",
        "start": 636.079,
        "duration": 5.521
    },
    {
        "text": "expression under the two conditions so",
        "start": 638.56,
        "duration": 5.44
    },
    {
        "text": "if we decide to reject the no hypothesis",
        "start": 641.6,
        "duration": 6.32
    },
    {
        "text": "we call a g a DG that's a setup for this",
        "start": 644.0,
        "duration": 6.399
    },
    {
        "text": "analysis if you want to learn how to do",
        "start": 647.92,
        "duration": 4.76
    },
    {
        "text": "it the two popular methods you will",
        "start": 650.399,
        "duration": 5.841
    },
    {
        "text": "definitely encounter are Edge R and dc2",
        "start": 652.68,
        "duration": 5.48
    },
    {
        "text": "they were both developed more than 10",
        "start": 656.24,
        "duration": 4.08
    },
    {
        "text": "years ago and they were designed for",
        "start": 658.16,
        "duration": 4.4
    },
    {
        "text": "small sample sizes like the three versus",
        "start": 660.32,
        "duration": 5.519
    },
    {
        "text": "three scenario so it's a small sample",
        "start": 662.56,
        "duration": 5.56
    },
    {
        "text": "size and we know that it will be a big",
        "start": 665.839,
        "duration": 3.841
    },
    {
        "text": "challenge to get some reasonable",
        "start": 668.12,
        "duration": 2.88
    },
    {
        "text": "statistical power because your",
        "start": 669.68,
        "duration": 3.88
    },
    {
        "text": "information is so little so therefore",
        "start": 671.0,
        "duration": 4.72
    },
    {
        "text": "they have to compensate the small sample",
        "start": 673.56,
        "duration": 5.279
    },
    {
        "text": "size by assuming a distribution of data",
        "start": 675.72,
        "duration": 5.96
    },
    {
        "text": "R data are counts if you summarize the",
        "start": 678.839,
        "duration": 5.521
    },
    {
        "text": "data right one gen get one count in one",
        "start": 681.68,
        "duration": 5.56
    },
    {
        "text": "sample so they assume both methods",
        "start": 684.36,
        "duration": 4.919
    },
    {
        "text": "assume this negative binomial",
        "start": 687.24,
        "duration": 4.12
    },
    {
        "text": "distribution for count data which is",
        "start": 689.279,
        "duration": 3.56
    },
    {
        "text": "more General than the passon",
        "start": 691.36,
        "duration": 2.919
    },
    {
        "text": "distribution because you get an",
        "start": 692.839,
        "duration": 4.0
    },
    {
        "text": "additional variance parameter which",
        "start": 694.279,
        "duration": 4.881
    },
    {
        "text": "could be bigger than the pong mean if",
        "start": 696.839,
        "duration": 4.281
    },
    {
        "text": "it's pong you just have one parameter",
        "start": 699.16,
        "duration": 3.919
    },
    {
        "text": "and furthermore both methods will count",
        "start": 701.12,
        "duration": 4.44
    },
    {
        "text": "for the fact that different R examples",
        "start": 703.079,
        "duration": 4.681
    },
    {
        "text": "have different sequence in depth and",
        "start": 705.56,
        "duration": 5.64
    },
    {
        "text": "Sample Library sizes so in this",
        "start": 707.76,
        "duration": 6.12
    },
    {
        "text": "formulation essentially we assume one",
        "start": 711.2,
        "duration": 5.4
    },
    {
        "text": "negative binomial per condition and we",
        "start": 713.88,
        "duration": 4.639
    },
    {
        "text": "assume that the condition has its mean",
        "start": 716.6,
        "duration": 5.64
    },
    {
        "text": "parameter me one and mu2 multiply by the",
        "start": 718.519,
        "duration": 6.56
    },
    {
        "text": "sample specific size Factor so given",
        "start": 722.24,
        "duration": 4.719
    },
    {
        "text": "that then you have some Independence",
        "start": 725.079,
        "duration": 4.641
    },
    {
        "text": "assumption for the counts of the samples",
        "start": 726.959,
        "duration": 5.721
    },
    {
        "text": "under each condition so for these",
        "start": 729.72,
        "duration": 5.88
    },
    {
        "text": "methods both methods the no hypothesis",
        "start": 732.68,
        "duration": 5.839
    },
    {
        "text": "is that mu1 is equal to mu2 that is",
        "start": 735.6,
        "duration": 5.919
    },
    {
        "text": "subject to sample size differences or",
        "start": 738.519,
        "duration": 5.841
    },
    {
        "text": "size factors which believe that the two",
        "start": 741.519,
        "duration": 4.641
    },
    {
        "text": "negative binomials should share the same",
        "start": 744.36,
        "duration": 4.839
    },
    {
        "text": "mean parameter I will say this is a",
        "start": 746.16,
        "duration": 6.119
    },
    {
        "text": "reasonable hypothesis only if the",
        "start": 749.199,
        "duration": 4.961
    },
    {
        "text": "negative binomial assumption holds",
        "start": 752.279,
        "duration": 3.761
    },
    {
        "text": "because it's what you assume for the",
        "start": 754.16,
        "duration": 5.64
    },
    {
        "text": "data so our surprising discovery was by",
        "start": 756.04,
        "duration": 6.44
    },
    {
        "text": "coincidence in a collaboration project",
        "start": 759.8,
        "duration": 5.599
    },
    {
        "text": "with Professor way at us Irvine with his",
        "start": 762.48,
        "duration": 5.84
    },
    {
        "text": "former postto yum who is now a faculty",
        "start": 765.399,
        "duration": 5.401
    },
    {
        "text": "herself in China and my former student",
        "start": 768.32,
        "duration": 4.519
    },
    {
        "text": "shinjo who is now a faculty himself at",
        "start": 770.8,
        "duration": 4.599
    },
    {
        "text": "Oran state so this collaboration Started",
        "start": 772.839,
        "duration": 5.041
    },
    {
        "text": "From The Journey where you may applied",
        "start": 775.399,
        "duration": 6.401
    },
    {
        "text": "dc2 HR to this data set which is about",
        "start": 777.88,
        "duration": 7.04
    },
    {
        "text": "immunotherapy of patients blood samples",
        "start": 781.8,
        "duration": 5.88
    },
    {
        "text": "before therapy which is 51 patients and",
        "start": 784.92,
        "duration": 5.84
    },
    {
        "text": "58 patients on therapy and it was",
        "start": 787.68,
        "duration": 5.76
    },
    {
        "text": "published in the cell paper the blood R",
        "start": 790.76,
        "duration": 4.879
    },
    {
        "text": "examples from patients so her first",
        "start": 793.44,
        "duration": 5.72
    },
    {
        "text": "finding was that D2 and HR actually give",
        "start": 795.639,
        "duration": 5.601
    },
    {
        "text": "quite different numbers of De gen at the",
        "start": 799.16,
        "duration": 5.72
    },
    {
        "text": "same false Discovery threshold say 5% so",
        "start": 801.24,
        "duration": 6.039
    },
    {
        "text": "Edge are found almost twice as many as",
        "start": 804.88,
        "duration": 6.04
    },
    {
        "text": "theic to and they're over lab is not so",
        "start": 807.279,
        "duration": 6.24
    },
    {
        "text": "big so the intersection is not so big so",
        "start": 810.92,
        "duration": 4.719
    },
    {
        "text": "this motivate you may to look into this",
        "start": 813.519,
        "duration": 4.481
    },
    {
        "text": "question and ask this question why do",
        "start": 815.639,
        "duration": 5.2
    },
    {
        "text": "dc2 and HR identify just drastically",
        "start": 818.0,
        "duration": 5.76
    },
    {
        "text": "different D gen because here the sample",
        "start": 820.839,
        "duration": 6.761
    },
    {
        "text": "sizes are 51 and 58 then I had this",
        "start": 823.76,
        "duration": 6.16
    },
    {
        "text": "question so in this cases can we do some",
        "start": 827.6,
        "duration": 4.12
    },
    {
        "text": "permutation because there are many ways",
        "start": 829.92,
        "duration": 4.32
    },
    {
        "text": "we can permute the data so just imagine",
        "start": 831.72,
        "duration": 4.96
    },
    {
        "text": "that if you are just want to randomly",
        "start": 834.24,
        "duration": 6.36
    },
    {
        "text": "assign a total of nine samples into two",
        "start": 836.68,
        "duration": 7.04
    },
    {
        "text": "groups of 51 and 58 you can do 109",
        "start": 840.6,
        "duration": 5.039
    },
    {
        "text": "choose 51 right that can give you a",
        "start": 843.72,
        "duration": 4.32
    },
    {
        "text": "large number of permutations so let's",
        "start": 845.639,
        "duration": 4.521
    },
    {
        "text": "just try that and I call this condition",
        "start": 848.04,
        "duration": 4.0
    },
    {
        "text": "label permutation because once we",
        "start": 850.16,
        "duration": 4.239
    },
    {
        "text": "permute the labels there's no more",
        "start": 852.04,
        "duration": 4.88
    },
    {
        "text": "relationship between the condition label",
        "start": 854.399,
        "duration": 4.281
    },
    {
        "text": "and the J",
        "start": 856.92,
        "duration": 4.2
    },
    {
        "text": "expression then we get a more surprising",
        "start": 858.68,
        "duration": 5.079
    },
    {
        "text": "result that is if you look at here so",
        "start": 861.12,
        "duration": 5.32
    },
    {
        "text": "from the permedia data each permutation",
        "start": 863.759,
        "duration": 5.801
    },
    {
        "text": "we can apply a method and get a d DG",
        "start": 866.44,
        "duration": 5.28
    },
    {
        "text": "number and we can do many many",
        "start": 869.56,
        "duration": 4.48
    },
    {
        "text": "permutations and then get a distribution",
        "start": 871.72,
        "duration": 5.6
    },
    {
        "text": "of DG numbers whose average is the",
        "start": 874.04,
        "duration": 5.52
    },
    {
        "text": "height here and whose standard deviation",
        "start": 877.32,
        "duration": 4.36
    },
    {
        "text": "is half of the arrow bar here and you",
        "start": 879.56,
        "duration": 5.44
    },
    {
        "text": "can see that from many permitted data we",
        "start": 881.68,
        "duration": 5.839
    },
    {
        "text": "get even more D genes than we get from",
        "start": 885.0,
        "duration": 4.279
    },
    {
        "text": "the original data which is more",
        "start": 887.519,
        "duration": 3.921
    },
    {
        "text": "surprising and how could that be",
        "start": 889.279,
        "duration": 6.36
    },
    {
        "text": "possible the reason is the negative B",
        "start": 891.44,
        "duration": 5.92
    },
    {
        "text": "binomial assumption does not hold for",
        "start": 895.639,
        "duration": 4.361
    },
    {
        "text": "this data set so just as a mention the",
        "start": 897.36,
        "duration": 4.44
    },
    {
        "text": "Assumption was first assumed for",
        "start": 900.0,
        "duration": 3.56
    },
    {
        "text": "replicate data right experimental",
        "start": 901.8,
        "duration": 4.44
    },
    {
        "text": "replicate data under a control condition",
        "start": 903.56,
        "duration": 4.639
    },
    {
        "text": "so we can have some assumption about the",
        "start": 906.24,
        "duration": 5.64
    },
    {
        "text": "sample variance due to pong and then we",
        "start": 908.199,
        "duration": 5.481
    },
    {
        "text": "can assume that maybe there's some",
        "start": 911.88,
        "duration": 4.16
    },
    {
        "text": "biological variance in the three",
        "start": 913.68,
        "duration": 4.92
    },
    {
        "text": "replicates subtle rep variance and then",
        "start": 916.04,
        "duration": 5.84
    },
    {
        "text": "POS give us the count but and together",
        "start": 918.6,
        "duration": 5.52
    },
    {
        "text": "like for example gamma for biological",
        "start": 921.88,
        "duration": 5.24
    },
    {
        "text": "variance and pone for sample variance",
        "start": 924.12,
        "duration": 5.24
    },
    {
        "text": "together we get a negative binomial but",
        "start": 927.12,
        "duration": 3.959
    },
    {
        "text": "these people these samples are from",
        "start": 929.36,
        "duration": 4.039
    },
    {
        "text": "patients and patients could be more",
        "start": 931.079,
        "duration": 5.041
    },
    {
        "text": "heterogeneous than they should be than",
        "start": 933.399,
        "duration": 5.481
    },
    {
        "text": "experimental replicates so that's why",
        "start": 936.12,
        "duration": 6.04
    },
    {
        "text": "our first check is to use the 51 data",
        "start": 938.88,
        "duration": 6.079
    },
    {
        "text": "points and 58 data points to check",
        "start": 942.16,
        "duration": 5.599
    },
    {
        "text": "whether the negative binomial fit well",
        "start": 944.959,
        "duration": 5.641
    },
    {
        "text": "to data and these are based on the edge",
        "start": 947.759,
        "duration": 5.64
    },
    {
        "text": "RS fited negative binomial so these are",
        "start": 950.6,
        "duration": 6.0
    },
    {
        "text": "called Quant Quant QQ plot so if the",
        "start": 953.399,
        "duration": 5.721
    },
    {
        "text": "model fits well we should expect to have",
        "start": 956.6,
        "duration": 4.76
    },
    {
        "text": "some like the dots should lie on the",
        "start": 959.12,
        "duration": 4.519
    },
    {
        "text": "straight line but that's not the case we",
        "start": 961.36,
        "duration": 3.839
    },
    {
        "text": "have some clear",
        "start": 963.639,
        "duration": 4.961
    },
    {
        "text": "outliers that's for HR and for dc2",
        "start": 965.199,
        "duration": 5.44
    },
    {
        "text": "another Gene we also see the similar",
        "start": 968.6,
        "duration": 5.12
    },
    {
        "text": "pattern so basically what's indicated",
        "start": 970.639,
        "duration": 4.64
    },
    {
        "text": "here is that the negative bomal",
        "start": 973.72,
        "duration": 3.88
    },
    {
        "text": "assumptions not reasonable for this data",
        "start": 975.279,
        "duration": 5.601
    },
    {
        "text": "from Human patients not replicate and",
        "start": 977.6,
        "duration": 6.12
    },
    {
        "text": "furthermore we divided the genes into",
        "start": 980.88,
        "duration": 5.519
    },
    {
        "text": "two groups two extreme groups one group",
        "start": 983.72,
        "duration": 4.359
    },
    {
        "text": "includes the genes that are rarely",
        "start": 986.399,
        "duration": 4.521
    },
    {
        "text": "identified from per data the okay genes",
        "start": 988.079,
        "duration": 4.721
    },
    {
        "text": "and the second group contains the genes",
        "start": 990.92,
        "duration": 4.12
    },
    {
        "text": "that are frequently identified from per",
        "start": 992.8,
        "duration": 5.2
    },
    {
        "text": "data the bad or problematic genes so for",
        "start": 995.04,
        "duration": 6.479
    },
    {
        "text": "between the two groups we check whether",
        "start": 998.0,
        "duration": 6.04
    },
    {
        "text": "the fitness or goodness of fit for",
        "start": 1001.519,
        "duration": 5.081
    },
    {
        "text": "negative binomial for genes in each",
        "start": 1004.04,
        "duration": 5.2
    },
    {
        "text": "group have some differences and this",
        "start": 1006.6,
        "duration": 4.96
    },
    {
        "text": "shows that okay let me just explain the",
        "start": 1009.24,
        "duration": 5.519
    },
    {
        "text": "y axis is the negative lock P value for",
        "start": 1011.56,
        "duration": 5.68
    },
    {
        "text": "goodness of fit test the smaller the P",
        "start": 1014.759,
        "duration": 4.921
    },
    {
        "text": "value the worse the fit",
        "start": 1017.24,
        "duration": 5.32
    },
    {
        "text": "and because we take negative log we have",
        "start": 1019.68,
        "duration": 5.92
    },
    {
        "text": "larger value to indicate worse fitting",
        "start": 1022.56,
        "duration": 5.759
    },
    {
        "text": "so in this case it confirms our guess",
        "start": 1025.6,
        "duration": 4.88
    },
    {
        "text": "that is for the problematic group",
        "start": 1028.319,
        "duration": 5.281
    },
    {
        "text": "negative binomial fitting was poorer and",
        "start": 1030.48,
        "duration": 5.64
    },
    {
        "text": "I want to give a credit to my current",
        "start": 1033.6,
        "duration": 4.479
    },
    {
        "text": "Master student who did her undergrad",
        "start": 1036.12,
        "duration": 4.24
    },
    {
        "text": "research in Chinese University of Hong",
        "start": 1038.079,
        "duration": 5.081
    },
    {
        "text": "Kong after she read our paper published",
        "start": 1040.36,
        "duration": 5.479
    },
    {
        "text": "she pointed out an issue in our goodness",
        "start": 1043.16,
        "duration": 5.36
    },
    {
        "text": "of P value calculation and we fixed that",
        "start": 1045.839,
        "duration": 5.401
    },
    {
        "text": "so the I'm we I'm presenting here is the",
        "start": 1048.52,
        "duration": 5.039
    },
    {
        "text": "uh post correction figure the good thing",
        "start": 1051.24,
        "duration": 4.799
    },
    {
        "text": "is that our conclusion qualitatively",
        "start": 1053.559,
        "duration": 5.441
    },
    {
        "text": "still holds but we have to fix the scale",
        "start": 1056.039,
        "duration": 5.961
    },
    {
        "text": "here to make it more accurate so coming",
        "start": 1059.0,
        "duration": 5.039
    },
    {
        "text": "from this conclusion like negative",
        "start": 1062.0,
        "duration": 3.96
    },
    {
        "text": "binomial assumption doesn't hold for",
        "start": 1064.039,
        "duration": 4.681
    },
    {
        "text": "this data set then the next try will be",
        "start": 1065.96,
        "duration": 5.52
    },
    {
        "text": "for us to use a test that doesn't assume",
        "start": 1068.72,
        "duration": 5.36
    },
    {
        "text": "negative binomial assumption and we use",
        "start": 1071.48,
        "duration": 5.04
    },
    {
        "text": "the will coxon rankon test which is a",
        "start": 1074.08,
        "duration": 5.12
    },
    {
        "text": "so-called nonparametric test it has a",
        "start": 1076.52,
        "duration": 3.84
    },
    {
        "text": "different n",
        "start": 1079.2,
        "duration": 3.599
    },
    {
        "text": "hypothesis basically I'm making it",
        "start": 1080.36,
        "duration": 3.84
    },
    {
        "text": "simple by assuming my data are",
        "start": 1082.799,
        "duration": 2.801
    },
    {
        "text": "normalized so I don't need to worry",
        "start": 1084.2,
        "duration": 3.32
    },
    {
        "text": "about the sample size differences with",
        "start": 1085.6,
        "duration": 4.959
    },
    {
        "text": "the normalized counts the no hypothesis",
        "start": 1087.52,
        "duration": 5.639
    },
    {
        "text": "of will coxen right some test is that if",
        "start": 1090.559,
        "duration": 6.321
    },
    {
        "text": "you just randomly pick one value one",
        "start": 1093.159,
        "duration": 6.441
    },
    {
        "text": "count normaliz count from condition one",
        "start": 1096.88,
        "duration": 4.52
    },
    {
        "text": "and you random pick another one from",
        "start": 1099.6,
        "duration": 3.68
    },
    {
        "text": "condition two then which of the two",
        "start": 1101.4,
        "duration": 4.72
    },
    {
        "text": "numbers is bigger is equally likely so",
        "start": 1103.28,
        "duration": 6.639
    },
    {
        "text": "you have 0% 0.5% chance that one is",
        "start": 1106.12,
        "duration": 5.919
    },
    {
        "text": "bigger than the other so it's based on",
        "start": 1109.919,
        "duration": 4.401
    },
    {
        "text": "the relative ranking of values instead",
        "start": 1112.039,
        "duration": 4.321
    },
    {
        "text": "of saying negative binomial and the same",
        "start": 1114.32,
        "duration": 4.16
    },
    {
        "text": "mean so this doesn't have the negative",
        "start": 1116.36,
        "duration": 5.0
    },
    {
        "text": "binomial assumption on this data set we",
        "start": 1118.48,
        "duration": 4.92
    },
    {
        "text": "look at even though the result may be",
        "start": 1121.36,
        "duration": 4.319
    },
    {
        "text": "disappointing because will coxen didn't",
        "start": 1123.4,
        "duration": 5.159
    },
    {
        "text": "give us any discoveries however it's not",
        "start": 1125.679,
        "duration": 6.0
    },
    {
        "text": "self self-contradictory because it also",
        "start": 1128.559,
        "duration": 5.6
    },
    {
        "text": "didn't produce anything from perm data",
        "start": 1131.679,
        "duration": 4.681
    },
    {
        "text": "so it's consistent so I just want to say",
        "start": 1134.159,
        "duration": 4.0
    },
    {
        "text": "that this is not a benchmark study",
        "start": 1136.36,
        "duration": 3.64
    },
    {
        "text": "because we didn't include all possible",
        "start": 1138.159,
        "duration": 3.841
    },
    {
        "text": "methods I know there are there's maybe",
        "start": 1140.0,
        "duration": 4.559
    },
    {
        "text": "like two dozen methods out there it's",
        "start": 1142.0,
        "duration": 4.6
    },
    {
        "text": "not Benchmark study we just want to say",
        "start": 1144.559,
        "duration": 4.24
    },
    {
        "text": "that the Assumption matters and if we",
        "start": 1146.6,
        "duration": 4.24
    },
    {
        "text": "just use popular methods without knowing",
        "start": 1148.799,
        "duration": 4.201
    },
    {
        "text": "their assumptions we may get false",
        "start": 1150.84,
        "duration": 5.959
    },
    {
        "text": "discoveries so surprisingly this me this",
        "start": 1153.0,
        "duration": 6.36
    },
    {
        "text": "analysis paper got a lot of attention",
        "start": 1156.799,
        "duration": 5.721
    },
    {
        "text": "especially on internet after publication",
        "start": 1159.36,
        "duration": 4.84
    },
    {
        "text": "and some people even tell me I mean",
        "start": 1162.52,
        "duration": 3.76
    },
    {
        "text": "someone from usel Medical School tell me",
        "start": 1164.2,
        "duration": 4.24
    },
    {
        "text": "oh this is your most famous work and I",
        "start": 1166.28,
        "duration": 4.399
    },
    {
        "text": "was very shocked because it's not a new",
        "start": 1168.44,
        "duration": 4.84
    },
    {
        "text": "method it's analysis but what tell what",
        "start": 1170.679,
        "duration": 5.401
    },
    {
        "text": "it tell me was that wow something we",
        "start": 1173.28,
        "duration": 4.84
    },
    {
        "text": "computational people took for granted",
        "start": 1176.08,
        "duration": 3.32
    },
    {
        "text": "like we need to check a methods",
        "start": 1178.12,
        "duration": 3.559
    },
    {
        "text": "assumption it's not as obvious to people",
        "start": 1179.4,
        "duration": 4.279
    },
    {
        "text": "who analyze data so that's why I feel",
        "start": 1181.679,
        "duration": 3.921
    },
    {
        "text": "like permutation is something people can",
        "start": 1183.679,
        "duration": 4.801
    },
    {
        "text": "Implement in their practice but the",
        "start": 1185.6,
        "duration": 4.72
    },
    {
        "text": "condition the discussion continues I",
        "start": 1188.48,
        "duration": 4.96
    },
    {
        "text": "would say so this is like how to say I",
        "start": 1190.32,
        "duration": 5.32
    },
    {
        "text": "shouldn't complain but basically our",
        "start": 1193.44,
        "duration": 3.64
    },
    {
        "text": "correspondence should have been",
        "start": 1195.64,
        "duration": 4.0
    },
    {
        "text": "published half a year ago as you mology",
        "start": 1197.08,
        "duration": 4.2
    },
    {
        "text": "but we're still waiting so this",
        "start": 1199.64,
        "duration": 4.159
    },
    {
        "text": "correspondence is actually well we're",
        "start": 1201.28,
        "duration": 5.0
    },
    {
        "text": "responding to two correspondences that",
        "start": 1203.799,
        "duration": 6.081
    },
    {
        "text": "pointed out either an issue in our",
        "start": 1206.28,
        "duration": 6.519
    },
    {
        "text": "analysis or a suggestion so one",
        "start": 1209.88,
        "duration": 5.84
    },
    {
        "text": "correspondent saying that we neced",
        "start": 1212.799,
        "duration": 5.721
    },
    {
        "text": "normalization in our analysis or in our",
        "start": 1215.72,
        "duration": 6.0
    },
    {
        "text": "simulation part so we",
        "start": 1218.52,
        "duration": 5.12
    },
    {
        "text": "definitely so I should say I shouldn't",
        "start": 1221.72,
        "duration": 4.199
    },
    {
        "text": "go to too much detail but we did some",
        "start": 1223.64,
        "duration": 5.2
    },
    {
        "text": "correction to analysis and said that we",
        "start": 1225.919,
        "duration": 5.841
    },
    {
        "text": "want to argue that if the semisynthetic",
        "start": 1228.84,
        "duration": 6.199
    },
    {
        "text": "data based on simulation does not assume",
        "start": 1231.76,
        "duration": 6.159
    },
    {
        "text": "any batch effects in its definition of",
        "start": 1235.039,
        "duration": 5.601
    },
    {
        "text": "true D gen then no normalization should",
        "start": 1237.919,
        "duration": 4.201
    },
    {
        "text": "be applied so this is something we",
        "start": 1240.64,
        "duration": 3.2
    },
    {
        "text": "should also pay attention to because",
        "start": 1242.12,
        "duration": 3.16
    },
    {
        "text": "real data analysis involves",
        "start": 1243.84,
        "duration": 3.719
    },
    {
        "text": "normalization but our simulations based",
        "start": 1245.28,
        "duration": 4.68
    },
    {
        "text": "on our assumed ground truth so in that",
        "start": 1247.559,
        "duration": 4.441
    },
    {
        "text": "case we have to be careful about whether",
        "start": 1249.96,
        "duration": 3.959
    },
    {
        "text": "normalization should or should not be",
        "start": 1252.0,
        "duration": 4.28
    },
    {
        "text": "used in our simulation second",
        "start": 1253.919,
        "duration": 4.441
    },
    {
        "text": "correspondents pointed out that if we",
        "start": 1256.28,
        "duration": 3.84
    },
    {
        "text": "app apply a procedure called",
        "start": 1258.36,
        "duration": 4.0
    },
    {
        "text": "winsorization it's just a fancy way to",
        "start": 1260.12,
        "duration": 5.32
    },
    {
        "text": "say outlier removal basically you remove",
        "start": 1262.36,
        "duration": 5.48
    },
    {
        "text": "the outliers from your data and then you",
        "start": 1265.44,
        "duration": 5.28
    },
    {
        "text": "can run the2 EDR so this correspondence",
        "start": 1267.84,
        "duration": 5.6
    },
    {
        "text": "shows that without the outliers the",
        "start": 1270.72,
        "duration": 5.16
    },
    {
        "text": "false discoveries we reported were not",
        "start": 1273.44,
        "duration": 5.359
    },
    {
        "text": "severe however we want to argue that",
        "start": 1275.88,
        "duration": 5.0
    },
    {
        "text": "even though this is true but in real",
        "start": 1278.799,
        "duration": 5.281
    },
    {
        "text": "data analysis throwing away outlier is",
        "start": 1280.88,
        "duration": 5.08
    },
    {
        "text": "not as straightforward as it is right",
        "start": 1284.08,
        "duration": 3.76
    },
    {
        "text": "because we need to set a threshold how",
        "start": 1285.96,
        "duration": 4.839
    },
    {
        "text": "much to throw away so basically in this",
        "start": 1287.84,
        "duration": 5.28
    },
    {
        "text": "correspondence we just showed that world",
        "start": 1290.799,
        "duration": 5.0
    },
    {
        "text": "coin remains to be robust even though we",
        "start": 1293.12,
        "duration": 5.48
    },
    {
        "text": "consider normalization or rization but I",
        "start": 1295.799,
        "duration": 4.36
    },
    {
        "text": "just want to say that there are many",
        "start": 1298.6,
        "duration": 4.12
    },
    {
        "text": "choices in real data analysis and",
        "start": 1300.159,
        "duration": 4.201
    },
    {
        "text": "whether you want to go with a robust",
        "start": 1302.72,
        "duration": 3.72
    },
    {
        "text": "method or whether you want to go with",
        "start": 1304.36,
        "duration": 5.04
    },
    {
        "text": "the delicate method specific for the STA",
        "start": 1306.44,
        "duration": 5.64
    },
    {
        "text": "set you need to make this Choice that's",
        "start": 1309.4,
        "duration": 5.279
    },
    {
        "text": "the first example second example single",
        "start": 1312.08,
        "duration": 5.76
    },
    {
        "text": "cell visualization so this is an example",
        "start": 1314.679,
        "duration": 5.761
    },
    {
        "text": "where I will show under the unsupervised",
        "start": 1317.84,
        "duration": 4.4
    },
    {
        "text": "learning setting we just have the Matrix",
        "start": 1320.44,
        "duration": 4.8
    },
    {
        "text": "X so the motivation is about the common",
        "start": 1322.24,
        "duration": 5.84
    },
    {
        "text": "use of tne and umap for visualizing",
        "start": 1325.24,
        "duration": 5.12
    },
    {
        "text": "single cell data this is a very well-",
        "start": 1328.08,
        "duration": 5.44
    },
    {
        "text": "written block for tne so in short when",
        "start": 1330.36,
        "duration": 5.6
    },
    {
        "text": "you use it you need to pay attention to",
        "start": 1333.52,
        "duration": 5.0
    },
    {
        "text": "the hyperparameter because both methods",
        "start": 1335.96,
        "duration": 5.24
    },
    {
        "text": "are nonlinear redu Dimension reduction",
        "start": 1338.52,
        "duration": 6.2
    },
    {
        "text": "methods for tney there is a there's an",
        "start": 1341.2,
        "duration": 6.04
    },
    {
        "text": "important parameter called perplexity",
        "start": 1344.72,
        "duration": 4.4
    },
    {
        "text": "perplexity roughly measures the",
        "start": 1347.24,
        "duration": 4.48
    },
    {
        "text": "neighborhood size so basically if you",
        "start": 1349.12,
        "duration": 5.12
    },
    {
        "text": "said the perplexity to be small you're",
        "start": 1351.72,
        "duration": 5.12
    },
    {
        "text": "assuming every cell's very close",
        "start": 1354.24,
        "duration": 4.52
    },
    {
        "text": "neighbors are neighbors and you want to",
        "start": 1356.84,
        "duration": 3.8
    },
    {
        "text": "preserve the neighborhood when you make",
        "start": 1358.76,
        "duration": 4.279
    },
    {
        "text": "the perplexity very big then you have a",
        "start": 1360.64,
        "duration": 5.039
    },
    {
        "text": "very large neighborhood size so both T",
        "start": 1363.039,
        "duration": 5.561
    },
    {
        "text": "and umap are good at emphasizing",
        "start": 1365.679,
        "duration": 5.321
    },
    {
        "text": "clusters so you can see clear clusters",
        "start": 1368.6,
        "duration": 4.6
    },
    {
        "text": "and this is a simulation example in",
        "start": 1371.0,
        "duration": 4.76
    },
    {
        "text": "which the real data the the the truth",
        "start": 1373.2,
        "duration": 4.32
    },
    {
        "text": "has just two Dimensions so this is the",
        "start": 1375.76,
        "duration": 3.2
    },
    {
        "text": "true visualization",
        "start": 1377.52,
        "duration": 3.36
    },
    {
        "text": "but you can see that when you vary",
        "start": 1378.96,
        "duration": 3.88
    },
    {
        "text": "perplexity sometimes you're closer to",
        "start": 1380.88,
        "duration": 3.679
    },
    {
        "text": "the truth sometimes you are very far",
        "start": 1382.84,
        "duration": 4.56
    },
    {
        "text": "away from the truth this issue was also",
        "start": 1384.559,
        "duration": 5.401
    },
    {
        "text": "reported recently by a nature method",
        "start": 1387.4,
        "duration": 6.0
    },
    {
        "text": "journalist saying how do we see data as",
        "start": 1389.96,
        "duration": 6.28
    },
    {
        "text": "tne and umap so basically there's one",
        "start": 1393.4,
        "duration": 5.04
    },
    {
        "text": "sentence in the abstract sometimes these",
        "start": 1396.24,
        "duration": 4.08
    },
    {
        "text": "methods take a second thought we have to",
        "start": 1398.44,
        "duration": 5.719
    },
    {
        "text": "use them carefully so the question my",
        "start": 1400.32,
        "duration": 6.719
    },
    {
        "text": "collaborator Professor Lucy sha who is a",
        "start": 1404.159,
        "duration": 5.841
    },
    {
        "text": "statistician in Kong and my student",
        "start": 1407.039,
        "duration": 5.361
    },
    {
        "text": "christe Lee the problem we try to tackle",
        "start": 1410.0,
        "duration": 5.039
    },
    {
        "text": "is to answer this question can we tell",
        "start": 1412.4,
        "duration": 5.32
    },
    {
        "text": "whether a cell's embedding is dubious or",
        "start": 1415.039,
        "duration": 6.561
    },
    {
        "text": "trustworthy in a Tey or umap plot our",
        "start": 1417.72,
        "duration": 6.52
    },
    {
        "text": "idea is to decide on this",
        "start": 1421.6,
        "duration": 5.04
    },
    {
        "text": "trustworthiness by looking at the sales",
        "start": 1424.24,
        "duration": 5.88
    },
    {
        "text": "neighbors before we run te or umap and",
        "start": 1426.64,
        "duration": 7.08
    },
    {
        "text": "after we run te and umap this is our",
        "start": 1430.12,
        "duration": 6.88
    },
    {
        "text": "intuition so basically we look at two",
        "start": 1433.72,
        "duration": 5.959
    },
    {
        "text": "sets of neighbors and this cell one is",
        "start": 1437.0,
        "duration": 4.64
    },
    {
        "text": "one good example I want to show using",
        "start": 1439.679,
        "duration": 4.841
    },
    {
        "text": "this cartoon that is before we do the",
        "start": 1441.64,
        "duration": 5.36
    },
    {
        "text": "embedding this we can Define some",
        "start": 1444.52,
        "duration": 6.0
    },
    {
        "text": "neighbors for the cell one after we do",
        "start": 1447.0,
        "duration": 5.679
    },
    {
        "text": "the tney embedding we can Define the",
        "start": 1450.52,
        "duration": 5.32
    },
    {
        "text": "neighbors so on both sides of this slide",
        "start": 1452.679,
        "duration": 5.961
    },
    {
        "text": "I'm showing the same tne embedding or",
        "start": 1455.84,
        "duration": 4.68
    },
    {
        "text": "umap embedding doesn't matter it's an",
        "start": 1458.64,
        "duration": 4.12
    },
    {
        "text": "illustration same embedding but I'm",
        "start": 1460.52,
        "duration": 5.24
    },
    {
        "text": "labeling two sets of neighbors one based",
        "start": 1462.76,
        "duration": 5.68
    },
    {
        "text": "on the pre-embedding space one based on",
        "start": 1465.76,
        "duration": 5.919
    },
    {
        "text": "the after embedding space so to just",
        "start": 1468.44,
        "duration": 6.479
    },
    {
        "text": "clarify because we hope to make some",
        "start": 1471.679,
        "duration": 6.48
    },
    {
        "text": "reliable conclusion about the relative",
        "start": 1474.919,
        "duration": 6.721
    },
    {
        "text": "locations of cell clusters to each other",
        "start": 1478.159,
        "duration": 6.601
    },
    {
        "text": "in this 2D plot we said the neighborhood",
        "start": 1481.64,
        "duration": 5.72
    },
    {
        "text": "size to be half of all cells because we",
        "start": 1484.76,
        "duration": 4.56
    },
    {
        "text": "want to capture or check the",
        "start": 1487.36,
        "duration": 5.12
    },
    {
        "text": "preservation of mid-range distances",
        "start": 1489.32,
        "duration": 5.28
    },
    {
        "text": "furthermore for the pre-embedding",
        "start": 1492.48,
        "duration": 5.0
    },
    {
        "text": "Neighbors we actually defined it in the",
        "start": 1494.6,
        "duration": 5.24
    },
    {
        "text": "principal component space because that's",
        "start": 1497.48,
        "duration": 4.28
    },
    {
        "text": "the standard practice in the Single Cell",
        "start": 1499.84,
        "duration": 4.64
    },
    {
        "text": "field that is you in you first do some",
        "start": 1501.76,
        "duration": 5.039
    },
    {
        "text": "lock transformation then PCA on your",
        "start": 1504.48,
        "duration": 5.919
    },
    {
        "text": "data then you keep about 20 to 50 pieces",
        "start": 1506.799,
        "duration": 6.24
    },
    {
        "text": "a number of your choice you then input",
        "start": 1510.399,
        "duration": 5.16
    },
    {
        "text": "those pieces as the input into T near",
        "start": 1513.039,
        "duration": 4.64
    },
    {
        "text": "map so in other words you're not using T",
        "start": 1515.559,
        "duration": 4.24
    },
    {
        "text": "near map to reduce the original",
        "start": 1517.679,
        "duration": 4.72
    },
    {
        "text": "thousands of Dimensions as gen but the",
        "start": 1519.799,
        "duration": 7.081
    },
    {
        "text": "PCS and also in both tne and umap they",
        "start": 1522.399,
        "duration": 7.361
    },
    {
        "text": "operate on the UK median distances of",
        "start": 1526.88,
        "duration": 6.0
    },
    {
        "text": "cells in the input space the PC space so",
        "start": 1529.76,
        "duration": 5.72
    },
    {
        "text": "therefore when we Define pre-embedded",
        "start": 1532.88,
        "duration": 5.32
    },
    {
        "text": "neighbors based on ukan distances in the",
        "start": 1535.48,
        "duration": 5.919
    },
    {
        "text": "PC space and we Define 2D embedding",
        "start": 1538.2,
        "duration": 5.359
    },
    {
        "text": "neighbors based on ukan distance in the",
        "start": 1541.399,
        "duration": 4.76
    },
    {
        "text": "2D space so then what do we do we want",
        "start": 1543.559,
        "duration": 4.921
    },
    {
        "text": "to check the consistency of two sets of",
        "start": 1546.159,
        "duration": 5.441
    },
    {
        "text": "neighbors by consistency we mean that",
        "start": 1548.48,
        "duration": 5.64
    },
    {
        "text": "whether the ordering of neighbors are",
        "start": 1551.6,
        "duration": 5.36
    },
    {
        "text": "consistent and also whether the",
        "start": 1554.12,
        "duration": 4.6
    },
    {
        "text": "distances between the neighbor neb to",
        "start": 1556.96,
        "duration": 4.199
    },
    {
        "text": "the cell one in the 2D spaces is",
        "start": 1558.72,
        "duration": 5.8
    },
    {
        "text": "consistent so here we just order these",
        "start": 1561.159,
        "duration": 6.561
    },
    {
        "text": "neighbors ukian distances in the 2D",
        "start": 1564.52,
        "duration": 6.68
    },
    {
        "text": "space to cell one by following their",
        "start": 1567.72,
        "duration": 6.839
    },
    {
        "text": "order in the respective space the reason",
        "start": 1571.2,
        "duration": 5.839
    },
    {
        "text": "is that we think when we look at this",
        "start": 1574.559,
        "duration": 5.521
    },
    {
        "text": "plot we are visualizing the 2D ukan",
        "start": 1577.039,
        "duration": 4.841
    },
    {
        "text": "distances so that's why that's what",
        "start": 1580.08,
        "duration": 5.079
    },
    {
        "text": "matters and then with these two ordered",
        "start": 1581.88,
        "duration": 6.48
    },
    {
        "text": "ukian distance vectors in the 2D space",
        "start": 1585.159,
        "duration": 5.441
    },
    {
        "text": "we check their consistency and we use",
        "start": 1588.36,
        "duration": 4.319
    },
    {
        "text": "the Pearson's correlation to measure the",
        "start": 1590.6,
        "duration": 4.76
    },
    {
        "text": "consistency because we need to care",
        "start": 1592.679,
        "duration": 5.161
    },
    {
        "text": "about the absolute values the magnitude",
        "start": 1595.36,
        "duration": 4.96
    },
    {
        "text": "of distances not just their ranks so",
        "start": 1597.84,
        "duration": 5.0
    },
    {
        "text": "this is a good trustworthy self",
        "start": 1600.32,
        "duration": 5.239
    },
    {
        "text": "embedding example this is a bad example",
        "start": 1602.84,
        "duration": 4.8
    },
    {
        "text": "I just want to show that the sales",
        "start": 1605.559,
        "duration": 4.24
    },
    {
        "text": "neighbors change a lot from",
        "start": 1607.64,
        "duration": 4.8
    },
    {
        "text": "pre-embedding space to the postembedding",
        "start": 1609.799,
        "duration": 5.321
    },
    {
        "text": "space okay so that's the intuition but",
        "start": 1612.44,
        "duration": 5.28
    },
    {
        "text": "we need to formalize this intuition and",
        "start": 1615.12,
        "duration": 5.279
    },
    {
        "text": "our question question is what is the no",
        "start": 1617.72,
        "duration": 5.8
    },
    {
        "text": "hypothesis in this problem so I would",
        "start": 1620.399,
        "duration": 6.201
    },
    {
        "text": "say it is that a sales neighbors half of",
        "start": 1623.52,
        "duration": 5.36
    },
    {
        "text": "all cells are just random after",
        "start": 1626.6,
        "duration": 4.76
    },
    {
        "text": "embedding so before and after embedding",
        "start": 1628.88,
        "duration": 5.12
    },
    {
        "text": "you have a random selection for half of",
        "start": 1631.36,
        "duration": 5.96
    },
    {
        "text": "the sales as neighbors but how do how do",
        "start": 1634.0,
        "duration": 5.6
    },
    {
        "text": "we obtain such a no hypothesis how do we",
        "start": 1637.32,
        "duration": 5.12
    },
    {
        "text": "realize that we can use permutation so",
        "start": 1639.6,
        "duration": 5.28
    },
    {
        "text": "in this case I'm just thinking about one",
        "start": 1642.44,
        "duration": 5.359
    },
    {
        "text": "case in which I can permute every Gene",
        "start": 1644.88,
        "duration": 5.96
    },
    {
        "text": "in dependently okay so then the cells",
        "start": 1647.799,
        "duration": 5.401
    },
    {
        "text": "became permuted cells because they're",
        "start": 1650.84,
        "duration": 5.04
    },
    {
        "text": "not no longer the real cells but every",
        "start": 1653.2,
        "duration": 6.56
    },
    {
        "text": "Gene still has its marginal distribution",
        "start": 1655.88,
        "duration": 5.84
    },
    {
        "text": "preserved because I'm just shuffling the",
        "start": 1659.76,
        "duration": 4.44
    },
    {
        "text": "numbers for each gene so I do preserve",
        "start": 1661.72,
        "duration": 5.52
    },
    {
        "text": "every Gene's marginal distribution but I",
        "start": 1664.2,
        "duration": 5.52
    },
    {
        "text": "didn't preserve Gene Gene correlations",
        "start": 1667.24,
        "duration": 4.84
    },
    {
        "text": "or cell cell relationships since I do",
        "start": 1669.72,
        "duration": 4.16
    },
    {
        "text": "the independent permutation for every",
        "start": 1672.08,
        "duration": 5.36
    },
    {
        "text": "Gene essentially I get a new population",
        "start": 1673.88,
        "duration": 6.84
    },
    {
        "text": "of this per cells and in this population",
        "start": 1677.44,
        "duration": 5.88
    },
    {
        "text": "the permuted cells have exchangeable",
        "start": 1680.72,
        "duration": 4.76
    },
    {
        "text": "relationships they're equally likely to",
        "start": 1683.32,
        "duration": 4.12
    },
    {
        "text": "be close to each other so that give me",
        "start": 1685.48,
        "duration": 4.64
    },
    {
        "text": "the no hypothesis and then we Implement",
        "start": 1687.44,
        "duration": 5.68
    },
    {
        "text": "that so the permuted data is just the",
        "start": 1690.12,
        "duration": 5.679
    },
    {
        "text": "permuted cells I talk about and I just",
        "start": 1693.12,
        "duration": 4.559
    },
    {
        "text": "need to do this permutation for once for",
        "start": 1695.799,
        "duration": 4.641
    },
    {
        "text": "my whole data set because every cell",
        "start": 1697.679,
        "duration": 5.84
    },
    {
        "text": "becomes a permuted cell and then I can",
        "start": 1700.44,
        "duration": 6.64
    },
    {
        "text": "calculate my score for every perer cell",
        "start": 1703.519,
        "duration": 5.961
    },
    {
        "text": "and these scores together give me a n",
        "start": 1707.08,
        "duration": 4.88
    },
    {
        "text": "distribution so this reflects what my",
        "start": 1709.48,
        "duration": 5.679
    },
    {
        "text": "score will behave if there's no real",
        "start": 1711.96,
        "duration": 6.719
    },
    {
        "text": "relationship among my C so under using",
        "start": 1715.159,
        "duration": 6.161
    },
    {
        "text": "this s distribution I can calibrate my",
        "start": 1718.679,
        "duration": 4.801
    },
    {
        "text": "score for the real sales whether I",
        "start": 1721.32,
        "duration": 4.719
    },
    {
        "text": "believe the score is high enough so I",
        "start": 1723.48,
        "duration": 4.64
    },
    {
        "text": "can call this real sales embedding",
        "start": 1726.039,
        "duration": 4.561
    },
    {
        "text": "trustworthy it must be much higher than",
        "start": 1728.12,
        "duration": 5.52
    },
    {
        "text": "the no scores or if the score for Real",
        "start": 1730.6,
        "duration": 5.439
    },
    {
        "text": "is so bad that is even worse than most",
        "start": 1733.64,
        "duration": 5.2
    },
    {
        "text": "of the no scores I call that dubious",
        "start": 1736.039,
        "duration": 4.921
    },
    {
        "text": "just one message is that here the",
        "start": 1738.84,
        "duration": 4.439
    },
    {
        "text": "percentages of dubious and trustworthy",
        "start": 1740.96,
        "duration": 5.719
    },
    {
        "text": "embeddings Are Not Just the Two Tales",
        "start": 1743.279,
        "duration": 5.841
    },
    {
        "text": "because I'm using the null distribution",
        "start": 1746.679,
        "duration": 4.921
    },
    {
        "text": "not the real sales distribution so in",
        "start": 1749.12,
        "duration": 5.84
    },
    {
        "text": "fact on a real cell population or real",
        "start": 1751.6,
        "duration": 6.4
    },
    {
        "text": "cell data set we found that majority of",
        "start": 1754.96,
        "duration": 4.64
    },
    {
        "text": "the real celles can be called",
        "start": 1758.0,
        "duration": 4.08
    },
    {
        "text": "trustworthy which is good and minority",
        "start": 1759.6,
        "duration": 4.48
    },
    {
        "text": "dubious and we don't have so much in the",
        "start": 1762.08,
        "duration": 4.52
    },
    {
        "text": "middle the gray zone so looking at one",
        "start": 1764.08,
        "duration": 4.719
    },
    {
        "text": "example in our paper and have multiple",
        "start": 1766.6,
        "duration": 3.88
    },
    {
        "text": "examples but this is the most striking",
        "start": 1768.799,
        "duration": 4.641
    },
    {
        "text": "one this is the first Hydro single",
        "start": 1770.48,
        "duration": 5.16
    },
    {
        "text": "aristic data set publishing science in",
        "start": 1773.44,
        "duration": 4.88
    },
    {
        "text": "2019 so this is the original",
        "start": 1775.64,
        "duration": 5.159
    },
    {
        "text": "visualization we were able to reproduce",
        "start": 1778.32,
        "duration": 3.239
    },
    {
        "text": "the",
        "start": 1780.799,
        "duration": 3.48
    },
    {
        "text": "visualization and we run our algorithm",
        "start": 1781.559,
        "duration": 5.161
    },
    {
        "text": "to detect dubious embeddings and they",
        "start": 1784.279,
        "duration": 4.601
    },
    {
        "text": "are labeled as red so see what's",
        "start": 1786.72,
        "duration": 4.24
    },
    {
        "text": "interesting here is that many dubious",
        "start": 1788.88,
        "duration": 4.56
    },
    {
        "text": "embeddings are those small clusters and",
        "start": 1790.96,
        "duration": 4.04
    },
    {
        "text": "if you don't know that if you just look",
        "start": 1793.44,
        "duration": 4.04
    },
    {
        "text": "at tne you might say oh these small",
        "start": 1795.0,
        "duration": 4.88
    },
    {
        "text": "clusters might be cell types or cell",
        "start": 1797.48,
        "duration": 4.76
    },
    {
        "text": "subtypes but if you know that their",
        "start": 1799.88,
        "duration": 4.519
    },
    {
        "text": "locations are dubious relative to other",
        "start": 1802.24,
        "duration": 4.48
    },
    {
        "text": "cells then we should be careful when we",
        "start": 1804.399,
        "duration": 5.76
    },
    {
        "text": "draw conclusions this is one use of s",
        "start": 1806.72,
        "duration": 6.16
    },
    {
        "text": "the method to detect dubious embeddings",
        "start": 1810.159,
        "duration": 6.161
    },
    {
        "text": "given one perplexity given one parameter",
        "start": 1812.88,
        "duration": 5.76
    },
    {
        "text": "and another use is that we can change",
        "start": 1816.32,
        "duration": 5.079
    },
    {
        "text": "the perplexity parameter and then for",
        "start": 1818.64,
        "duration": 5.36
    },
    {
        "text": "each parameter we detect dubious",
        "start": 1821.399,
        "duration": 5.081
    },
    {
        "text": "embeddings and count the number so we",
        "start": 1824.0,
        "duration": 5.24
    },
    {
        "text": "can minimize the number of embeddings",
        "start": 1826.48,
        "duration": 5.039
    },
    {
        "text": "and as a way to optimize the hyper",
        "start": 1829.24,
        "duration": 4.679
    },
    {
        "text": "parameter perplexity so for this data",
        "start": 1831.519,
        "duration": 4.801
    },
    {
        "text": "set we ended up getting the perplexity",
        "start": 1833.919,
        "duration": 5.921
    },
    {
        "text": "to be 230 instead of 40 so you see",
        "start": 1836.32,
        "duration": 5.76
    },
    {
        "text": "comparing this new visualization to the",
        "start": 1839.84,
        "duration": 5.36
    },
    {
        "text": "previous one we see some big differences",
        "start": 1842.08,
        "duration": 6.0
    },
    {
        "text": "in particular if we focus on these three",
        "start": 1845.2,
        "duration": 5.599
    },
    {
        "text": "colored cell clusters previously the",
        "start": 1848.08,
        "duration": 6.599
    },
    {
        "text": "blue and purple one green and blue ones",
        "start": 1850.799,
        "duration": 6.521
    },
    {
        "text": "were were like surrounding the orange",
        "start": 1854.679,
        "duration": 4.921
    },
    {
        "text": "one the orange one is a big cluster of",
        "start": 1857.32,
        "duration": 4.199
    },
    {
        "text": "stem cells so you see that these",
        "start": 1859.6,
        "duration": 4.64
    },
    {
        "text": "neuronal ectoderm cells were surrounding",
        "start": 1861.519,
        "duration": 6.04
    },
    {
        "text": "them the orange one in similar distances",
        "start": 1864.24,
        "duration": 5.88
    },
    {
        "text": "but after our visualization",
        "start": 1867.559,
        "duration": 5.401
    },
    {
        "text": "optimization these three clusters are",
        "start": 1870.12,
        "duration": 5.559
    },
    {
        "text": "unified down here very far away from the",
        "start": 1872.96,
        "duration": 5.64
    },
    {
        "text": "orange one so if you look at the datas",
        "start": 1875.679,
        "duration": 4.96
    },
    {
        "text": "how to say the Jinx bression data in",
        "start": 1878.6,
        "duration": 4.559
    },
    {
        "text": "heat map you can see that our",
        "start": 1880.639,
        "duration": 4.88
    },
    {
        "text": "visualization is more consistent with",
        "start": 1883.159,
        "duration": 4.921
    },
    {
        "text": "the data so these small clusters are are",
        "start": 1885.519,
        "duration": 4.721
    },
    {
        "text": "indeed not so different and they're very",
        "start": 1888.08,
        "duration": 4.12
    },
    {
        "text": "different from the orange cluster so",
        "start": 1890.24,
        "duration": 3.72
    },
    {
        "text": "that's my example Two showing that when",
        "start": 1892.2,
        "duration": 4.04
    },
    {
        "text": "you use permutation you may help you",
        "start": 1893.96,
        "duration": 4.319
    },
    {
        "text": "decide whether visualization is",
        "start": 1896.24,
        "duration": 5.159
    },
    {
        "text": "reasonable or not the third example is",
        "start": 1898.279,
        "duration": 5.201
    },
    {
        "text": "the news recent work which hasn't been",
        "start": 1901.399,
        "duration": 4.681
    },
    {
        "text": "published yet but we hope to post the",
        "start": 1903.48,
        "duration": 5.72
    },
    {
        "text": "preprint version so this is called The",
        "start": 1906.08,
        "duration": 6.16
    },
    {
        "text": "Meta cell concept which is very",
        "start": 1909.2,
        "duration": 5.76
    },
    {
        "text": "interesting because you change your data",
        "start": 1912.24,
        "duration": 5.36
    },
    {
        "text": "you no longer work with single cells but",
        "start": 1914.96,
        "duration": 6.0
    },
    {
        "text": "you aggregate them into meta cells for",
        "start": 1917.6,
        "duration": 6.199
    },
    {
        "text": "the purpose of having less sparse data",
        "start": 1920.96,
        "duration": 5.319
    },
    {
        "text": "so this in this cartoon if you just have",
        "start": 1923.799,
        "duration": 5.041
    },
    {
        "text": "the Single Cell Matrix this is from this",
        "start": 1926.279,
        "duration": 6.52
    },
    {
        "text": "paper and you basically can so basically",
        "start": 1928.84,
        "duration": 6.92
    },
    {
        "text": "you have the cells as the columns genes",
        "start": 1932.799,
        "duration": 4.681
    },
    {
        "text": "as the rows this is different from my",
        "start": 1935.76,
        "duration": 3.84
    },
    {
        "text": "original X Matrix because I regard",
        "start": 1937.48,
        "duration": 4.799
    },
    {
        "text": "celles as samples but anyway if you",
        "start": 1939.6,
        "duration": 5.28
    },
    {
        "text": "aggregate certain cells into one meta",
        "start": 1942.279,
        "duration": 5.12
    },
    {
        "text": "cell you get a smaller Matrix so here",
        "start": 1944.88,
        "duration": 4.679
    },
    {
        "text": "your columns are meta cells and you get",
        "start": 1947.399,
        "duration": 4.52
    },
    {
        "text": "fewer zeros because of the aggregation",
        "start": 1949.559,
        "duration": 6.321
    },
    {
        "text": "from 90% of zeros to 50% of zeros but",
        "start": 1951.919,
        "duration": 6.48
    },
    {
        "text": "from a statistical perspective this is",
        "start": 1955.88,
        "duration": 5.679
    },
    {
        "text": "interesting because the question is is",
        "start": 1958.399,
        "duration": 4.88
    },
    {
        "text": "there a reasonable definition for",
        "start": 1961.559,
        "duration": 4.0
    },
    {
        "text": "aggregating cells into meta cells",
        "start": 1963.279,
        "duration": 4.801
    },
    {
        "text": "obviously we cannot aggregate too much",
        "start": 1965.559,
        "duration": 4.161
    },
    {
        "text": "right if you aggregate everything into",
        "start": 1968.08,
        "duration": 4.439
    },
    {
        "text": "one meta cell there's no information or",
        "start": 1969.72,
        "duration": 4.799
    },
    {
        "text": "if you don't aggregate you should just",
        "start": 1972.519,
        "duration": 4.12
    },
    {
        "text": "work with single cell data so there must",
        "start": 1974.519,
        "duration": 4.441
    },
    {
        "text": "be some tradeoff there",
        "start": 1976.639,
        "duration": 5.481
    },
    {
        "text": "and this is the literature review so so",
        "start": 1978.96,
        "duration": 5.719
    },
    {
        "text": "far there are four meta cell methods",
        "start": 1982.12,
        "duration": 5.32
    },
    {
        "text": "published from the first one meta cell",
        "start": 1984.679,
        "duration": 4.401
    },
    {
        "text": "meta cell 2 they were from the same",
        "start": 1987.44,
        "duration": 3.8
    },
    {
        "text": "authors there's one called supercell and",
        "start": 1989.08,
        "duration": 4.079
    },
    {
        "text": "the most recent one is from Dana Pierce",
        "start": 1991.24,
        "duration": 3.919
    },
    {
        "text": "group called C cells on nature B",
        "start": 1993.159,
        "duration": 4.36
    },
    {
        "text": "technology so they all use the meta cell",
        "start": 1995.159,
        "duration": 4.721
    },
    {
        "text": "concept and aggregating cells into meta",
        "start": 1997.519,
        "duration": 5.241
    },
    {
        "text": "cells in different ways we also have",
        "start": 1999.88,
        "duration": 5.48
    },
    {
        "text": "many prominent application papers they",
        "start": 2002.76,
        "duration": 6.0
    },
    {
        "text": "are all nature papers right so basically",
        "start": 2005.36,
        "duration": 5.48
    },
    {
        "text": "this paper implement the meta cell",
        "start": 2008.76,
        "duration": 4.12
    },
    {
        "text": "concept even though they may or may not",
        "start": 2010.84,
        "duration": 4.6
    },
    {
        "text": "use the method here so it has been used",
        "start": 2012.88,
        "duration": 5.399
    },
    {
        "text": "in practice but if we read those papers",
        "start": 2015.44,
        "duration": 4.839
    },
    {
        "text": "there's no consensus on meta cell",
        "start": 2018.279,
        "duration": 4.601
    },
    {
        "text": "definition so this motivate us to think",
        "start": 2020.279,
        "duration": 3.601
    },
    {
        "text": "about this",
        "start": 2022.88,
        "duration": 4.039
    },
    {
        "text": "problem can we Define a meta cell in a",
        "start": 2023.88,
        "duration": 5.72
    },
    {
        "text": "rigorous way and this is the first meta",
        "start": 2026.919,
        "duration": 5.64
    },
    {
        "text": "cell paper I'm just mentioning the quote",
        "start": 2029.6,
        "duration": 5.84
    },
    {
        "text": "here it it defines a meta cell as a",
        "start": 2032.559,
        "duration": 5.761
    },
    {
        "text": "homogeneous collection of single cell",
        "start": 2035.44,
        "duration": 5.44
    },
    {
        "text": "profiles that could have been resampled",
        "start": 2038.32,
        "duration": 4.88
    },
    {
        "text": "from the same original",
        "start": 2040.88,
        "duration": 5.679
    },
    {
        "text": "cell so essentially we translate the",
        "start": 2043.2,
        "duration": 5.479
    },
    {
        "text": "sentence to say that we think the",
        "start": 2046.559,
        "duration": 5.241
    },
    {
        "text": "variation of celles within a meta cell",
        "start": 2048.679,
        "duration": 6.041
    },
    {
        "text": "should be attributed exclusively to",
        "start": 2051.8,
        "duration": 5.119
    },
    {
        "text": "measurement or technical error not",
        "start": 2054.72,
        "duration": 5.439
    },
    {
        "text": "biological differences and this concept",
        "start": 2056.919,
        "duration": 6.641
    },
    {
        "text": "or this question motivated me to look",
        "start": 2060.159,
        "duration": 5.561
    },
    {
        "text": "into or actually I read this paper",
        "start": 2063.56,
        "duration": 4.2
    },
    {
        "text": "before so I draw the connection here it",
        "start": 2065.72,
        "duration": 4.52
    },
    {
        "text": "was from Matthew stens group it's a very",
        "start": 2067.76,
        "duration": 4.28
    },
    {
        "text": "interesting paper very well-written",
        "start": 2070.24,
        "duration": 3.919
    },
    {
        "text": "paper on nature genetics about",
        "start": 2072.04,
        "duration": 4.359
    },
    {
        "text": "separating measurement and expression",
        "start": 2074.159,
        "duration": 5.841
    },
    {
        "text": "models in singles rna6 beta so this",
        "start": 2076.399,
        "duration": 7.401
    },
    {
        "text": "paper was originally for about the zero",
        "start": 2080.0,
        "duration": 6.52
    },
    {
        "text": "problem or imputation problem but it's",
        "start": 2083.8,
        "duration": 5.279
    },
    {
        "text": "very conceptual and it's related to this",
        "start": 2086.52,
        "duration": 5.44
    },
    {
        "text": "metasol concept so in this paper the",
        "start": 2089.079,
        "duration": 5.6
    },
    {
        "text": "authors proposed two layer models one",
        "start": 2091.96,
        "duration": 4.48
    },
    {
        "text": "for expression model that's the",
        "start": 2094.679,
        "duration": 3.561
    },
    {
        "text": "distribution of the true expression",
        "start": 2096.44,
        "duration": 4.0
    },
    {
        "text": "levels of genes in cells and the",
        "start": 2098.24,
        "duration": 4.28
    },
    {
        "text": "measurement model on top of that that is",
        "start": 2100.44,
        "duration": 4.32
    },
    {
        "text": "the distribution of The observed counts",
        "start": 2102.52,
        "duration": 4.839
    },
    {
        "text": "given the true expression levels so this",
        "start": 2104.76,
        "duration": 4.2
    },
    {
        "text": "is probably the most technical slide in",
        "start": 2107.359,
        "duration": 4.24
    },
    {
        "text": "my talk which is how to describe this",
        "start": 2108.96,
        "duration": 5.44
    },
    {
        "text": "two layer model so again we consider a",
        "start": 2111.599,
        "duration": 5.361
    },
    {
        "text": "cell to be an observation from one",
        "start": 2114.4,
        "duration": 4.88
    },
    {
        "text": "distribution cell I from one to n n",
        "start": 2116.96,
        "duration": 5.159
    },
    {
        "text": "cells we consider a gene to be a feature",
        "start": 2119.28,
        "duration": 6.12
    },
    {
        "text": "J from 1 to P so this two layer",
        "start": 2122.119,
        "duration": 5.96
    },
    {
        "text": "observation model has the first layer",
        "start": 2125.4,
        "duration": 6.52
    },
    {
        "text": "about the biological true expression so",
        "start": 2128.079,
        "duration": 6.081
    },
    {
        "text": "here I denot this to be Lambda I for",
        "start": 2131.92,
        "duration": 5.28
    },
    {
        "text": "cell I it is a multi-dimensional vector",
        "start": 2134.16,
        "duration": 5.32
    },
    {
        "text": "because we have all genes here and P",
        "start": 2137.2,
        "duration": 4.72
    },
    {
        "text": "dimensional Vector so expression model",
        "start": 2139.48,
        "duration": 5.72
    },
    {
        "text": "describes how the cells true expression",
        "start": 2141.92,
        "duration": 6.48
    },
    {
        "text": "levels for the P gen are distributed so",
        "start": 2145.2,
        "duration": 3.96
    },
    {
        "text": "in",
        "start": 2148.4,
        "duration": 2.679
    },
    {
        "text": "particular there's something to pay",
        "start": 2149.16,
        "duration": 5.28
    },
    {
        "text": "attention to Lambda i j is defined as",
        "start": 2151.079,
        "duration": 6.24
    },
    {
        "text": "the relative expression level of feature",
        "start": 2154.44,
        "duration": 6.08
    },
    {
        "text": "J like g j in cell I so it's the",
        "start": 2157.319,
        "duration": 5.961
    },
    {
        "text": "proportion and all the Lambda I JS",
        "start": 2160.52,
        "duration": 5.88
    },
    {
        "text": "across J sum up to one okay so it's the",
        "start": 2163.28,
        "duration": 5.039
    },
    {
        "text": "proportion why do we care about",
        "start": 2166.4,
        "duration": 5.12
    },
    {
        "text": "proportion because sequencing data only",
        "start": 2168.319,
        "duration": 5.201
    },
    {
        "text": "give us the relative proportion we don't",
        "start": 2171.52,
        "duration": 4.36
    },
    {
        "text": "have the absolute abundance level so",
        "start": 2173.52,
        "duration": 3.92
    },
    {
        "text": "that's why it's more reasonable to talk",
        "start": 2175.88,
        "duration": 3.959
    },
    {
        "text": "about Lambda i j than the absolute",
        "start": 2177.44,
        "duration": 4.32
    },
    {
        "text": "number of features in Celli because we",
        "start": 2179.839,
        "duration": 4.601
    },
    {
        "text": "don't observe this and this F is the",
        "start": 2181.76,
        "duration": 4.319
    },
    {
        "text": "distribution model which is p",
        "start": 2184.44,
        "duration": 3.44
    },
    {
        "text": "dimensional",
        "start": 2186.079,
        "duration": 4.0
    },
    {
        "text": "then given the Lambda i j the",
        "start": 2187.88,
        "duration": 4.92
    },
    {
        "text": "measurement model is used to describe",
        "start": 2190.079,
        "duration": 6.28
    },
    {
        "text": "how our observe count is distributed so",
        "start": 2192.8,
        "duration": 6.48
    },
    {
        "text": "Yi J is the count of feature J in cell I",
        "start": 2196.359,
        "duration": 6.0
    },
    {
        "text": "and Yi plus denotes the library size of",
        "start": 2199.28,
        "duration": 5.319
    },
    {
        "text": "cell I the sum of all accounts so",
        "start": 2202.359,
        "duration": 4.921
    },
    {
        "text": "basically the measurement model is like",
        "start": 2204.599,
        "duration": 5.161
    },
    {
        "text": "given the true proportions of features",
        "start": 2207.28,
        "duration": 4.76
    },
    {
        "text": "given the cell library size how would",
        "start": 2209.76,
        "duration": 5.0
    },
    {
        "text": "you allocate the total into individual",
        "start": 2212.04,
        "duration": 5.72
    },
    {
        "text": "features and this is a one-dimensional",
        "start": 2214.76,
        "duration": 5.12
    },
    {
        "text": "distribution just for the",
        "start": 2217.76,
        "duration": 5.16
    },
    {
        "text": "measurement so together if we want to",
        "start": 2219.88,
        "duration": 5.239
    },
    {
        "text": "have a statistical definition for meta",
        "start": 2222.92,
        "duration": 4.72
    },
    {
        "text": "cell we want to say that it is it should",
        "start": 2225.119,
        "duration": 5.2
    },
    {
        "text": "be a group of single cells that share",
        "start": 2227.64,
        "duration": 4.76
    },
    {
        "text": "the same Lambda because Lambda is a",
        "start": 2230.319,
        "duration": 3.8
    },
    {
        "text": "biological",
        "start": 2232.4,
        "duration": 4.6
    },
    {
        "text": "signal okay so we just want to have",
        "start": 2234.119,
        "duration": 5.841
    },
    {
        "text": "approach to check this definition if we",
        "start": 2237.0,
        "duration": 5.8
    },
    {
        "text": "think a meta cell satisfies this",
        "start": 2239.96,
        "duration": 5.0
    },
    {
        "text": "definition we want to call it a",
        "start": 2242.8,
        "duration": 4.799
    },
    {
        "text": "trustworthy meta cell otherwise we want",
        "start": 2244.96,
        "duration": 4.56
    },
    {
        "text": "to call it a dubious metas cell so we're",
        "start": 2247.599,
        "duration": 3.681
    },
    {
        "text": "borrowing the names from the previous",
        "start": 2249.52,
        "duration": 3.799
    },
    {
        "text": "project and we want to formulate this as",
        "start": 2251.28,
        "duration": 3.4
    },
    {
        "text": "a statistical",
        "start": 2253.319,
        "duration": 4.201
    },
    {
        "text": "problem so why would dubious meta cells",
        "start": 2254.68,
        "duration": 6.24
    },
    {
        "text": "matter so this is again from this review",
        "start": 2257.52,
        "duration": 6.2
    },
    {
        "text": "paper for meta cell at MSB molecular",
        "start": 2260.92,
        "duration": 4.439
    },
    {
        "text": "systems biology you can check out this",
        "start": 2263.72,
        "duration": 4.16
    },
    {
        "text": "paper so it has this nice cartoon",
        "start": 2265.359,
        "duration": 5.161
    },
    {
        "text": "showing that if your meta cell happens",
        "start": 2267.88,
        "duration": 5.68
    },
    {
        "text": "to include cells of two different types",
        "start": 2270.52,
        "duration": 6.44
    },
    {
        "text": "here then it will bias your ging Jing",
        "start": 2273.56,
        "duration": 6.08
    },
    {
        "text": "correlation analysis because this is",
        "start": 2276.96,
        "duration": 4.84
    },
    {
        "text": "what you get using single cells this is",
        "start": 2279.64,
        "duration": 5.24
    },
    {
        "text": "what you get using meta cells and if you",
        "start": 2281.8,
        "duration": 5.319
    },
    {
        "text": "have a dubious meta cell here you will",
        "start": 2284.88,
        "duration": 4.4
    },
    {
        "text": "get a different correlation",
        "start": 2287.119,
        "duration": 4.681
    },
    {
        "text": "calculation so how do we tackle this",
        "start": 2289.28,
        "duration": 5.2
    },
    {
        "text": "problem to check whether some meta cells",
        "start": 2291.8,
        "duration": 5.0
    },
    {
        "text": "are dubious some are trustworthy how can",
        "start": 2294.48,
        "duration": 5.4
    },
    {
        "text": "we screen out the trustworthy ones so my",
        "start": 2296.8,
        "duration": 6.92
    },
    {
        "text": "post. pan leted this project and our",
        "start": 2299.88,
        "duration": 7.0
    },
    {
        "text": "first question is can we achieve these",
        "start": 2303.72,
        "duration": 5.119
    },
    {
        "text": "goals so we hope to give you a",
        "start": 2306.88,
        "duration": 5.0
    },
    {
        "text": "statistical Criterion based on which we",
        "start": 2308.839,
        "duration": 5.081
    },
    {
        "text": "can identify dubious meta cells",
        "start": 2311.88,
        "duration": 3.68
    },
    {
        "text": "consisting of single cells from",
        "start": 2313.92,
        "duration": 3.08
    },
    {
        "text": "different cell States these are the",
        "start": 2315.56,
        "duration": 3.72
    },
    {
        "text": "problem ones we shouldn't use and we",
        "start": 2317.0,
        "duration": 5.079
    },
    {
        "text": "should also we also hope to nominate the",
        "start": 2319.28,
        "duration": 5.0
    },
    {
        "text": "top performing meta cell method out of",
        "start": 2322.079,
        "duration": 5.401
    },
    {
        "text": "the four and optimize hyperparameter so",
        "start": 2324.28,
        "duration": 5.2
    },
    {
        "text": "every method has a",
        "start": 2327.48,
        "duration": 4.16
    },
    {
        "text": "hyperparameter which can be translated",
        "start": 2329.48,
        "duration": 5.0
    },
    {
        "text": "into the granularity level that is on",
        "start": 2331.64,
        "duration": 5.28
    },
    {
        "text": "average how many single cells do you",
        "start": 2334.48,
        "duration": 4.76
    },
    {
        "text": "aggregate into a meta cell we help to",
        "start": 2336.92,
        "duration": 5.24
    },
    {
        "text": "choose this in a data specific way so",
        "start": 2339.24,
        "duration": 5.48
    },
    {
        "text": "our intuition here is that within a",
        "start": 2342.16,
        "duration": 5.36
    },
    {
        "text": "trustworthy meta cell features are",
        "start": 2344.72,
        "duration": 5.2
    },
    {
        "text": "approximately uncorrelated this is the",
        "start": 2347.52,
        "duration": 4.72
    },
    {
        "text": "most important part because we think the",
        "start": 2349.92,
        "duration": 4.8
    },
    {
        "text": "differences of these single cells in",
        "start": 2352.24,
        "duration": 4.24
    },
    {
        "text": "this meta cell are just due to the",
        "start": 2354.72,
        "duration": 4.0
    },
    {
        "text": "measurement error and the measurement",
        "start": 2356.48,
        "duration": 4.92
    },
    {
        "text": "error should be roughly independent for",
        "start": 2358.72,
        "duration": 4.8
    },
    {
        "text": "different features so they should just",
        "start": 2361.4,
        "duration": 4.719
    },
    {
        "text": "give us a rig correlation and I have to",
        "start": 2363.52,
        "duration": 4.28
    },
    {
        "text": "say it's not zero correlation but weak",
        "start": 2366.119,
        "duration": 3.681
    },
    {
        "text": "correlation the reason is that because",
        "start": 2367.8,
        "duration": 4.96
    },
    {
        "text": "we have the cell library size as a total",
        "start": 2369.8,
        "duration": 4.84
    },
    {
        "text": "so that will still induce some weak",
        "start": 2372.76,
        "duration": 4.68
    },
    {
        "text": "correlation but that's pretty much it so",
        "start": 2374.64,
        "duration": 5.8
    },
    {
        "text": "this is from like a real data we can",
        "start": 2377.44,
        "duration": 5.56
    },
    {
        "text": "show that for a trustworthy cell we",
        "start": 2380.44,
        "duration": 4.679
    },
    {
        "text": "found meta cell we found the gingin",
        "start": 2383.0,
        "duration": 4.88
    },
    {
        "text": "correlations does not show a block",
        "start": 2385.119,
        "duration": 5.401
    },
    {
        "text": "structure but from a dubious meta cell",
        "start": 2387.88,
        "duration": 5.479
    },
    {
        "text": "we do see some coration structure and",
        "start": 2390.52,
        "duration": 6.88
    },
    {
        "text": "our strategy is to propose a per meta",
        "start": 2393.359,
        "duration": 7.401
    },
    {
        "text": "statistic we call MCD standing for meta",
        "start": 2397.4,
        "duration": 7.959
    },
    {
        "text": "cell divergence and also we hope to get",
        "start": 2400.76,
        "duration": 7.0
    },
    {
        "text": "some reasonable null distribution by",
        "start": 2405.359,
        "duration": 4.521
    },
    {
        "text": "permutation so this is the third",
        "start": 2407.76,
        "duration": 4.839
    },
    {
        "text": "permutation I will introduce today first",
        "start": 2409.88,
        "duration": 5.479
    },
    {
        "text": "question previously for as did I use a",
        "start": 2412.599,
        "duration": 5.441
    },
    {
        "text": "within Gene permutation is it enough",
        "start": 2415.359,
        "duration": 5.561
    },
    {
        "text": "here so we permute every Gene",
        "start": 2418.04,
        "duration": 5.079
    },
    {
        "text": "independently right for that purpose so",
        "start": 2420.92,
        "duration": 3.919
    },
    {
        "text": "we can get a permuted sales with",
        "start": 2423.119,
        "duration": 3.641
    },
    {
        "text": "exchangeable relationship no real",
        "start": 2424.839,
        "duration": 3.24
    },
    {
        "text": "relationship",
        "start": 2426.76,
        "duration": 4.04
    },
    {
        "text": "but here the problem is that the cell",
        "start": 2428.079,
        "duration": 5.361
    },
    {
        "text": "library sizes are disrupted because once",
        "start": 2430.8,
        "duration": 5.16
    },
    {
        "text": "you permute Aver independently each",
        "start": 2433.44,
        "duration": 4.6
    },
    {
        "text": "permitted cell doesn't carry the",
        "start": 2435.96,
        "duration": 4.72
    },
    {
        "text": "original cells Library size but in the",
        "start": 2438.04,
        "duration": 4.799
    },
    {
        "text": "meta cell definition for the measurement",
        "start": 2440.68,
        "duration": 4.439
    },
    {
        "text": "model if you remember it involves the",
        "start": 2442.839,
        "duration": 3.961
    },
    {
        "text": "cell library size because we're working",
        "start": 2445.119,
        "duration": 4.0
    },
    {
        "text": "with counts not normalized counts so the",
        "start": 2446.8,
        "duration": 4.64
    },
    {
        "text": "cell library size should be preserved so",
        "start": 2449.119,
        "duration": 4.96
    },
    {
        "text": "then this is an interesting strategy Han",
        "start": 2451.44,
        "duration": 4.44
    },
    {
        "text": "came up with I call it double",
        "start": 2454.079,
        "duration": 3.401
    },
    {
        "text": "permutation",
        "start": 2455.88,
        "duration": 4.84
    },
    {
        "text": "which is like for this top row we",
        "start": 2457.48,
        "duration": 5.839
    },
    {
        "text": "Premier averaging independently just as",
        "start": 2460.72,
        "duration": 5.119
    },
    {
        "text": "a way to serve as the how to say",
        "start": 2463.319,
        "duration": 4.52
    },
    {
        "text": "negative case where genes are",
        "start": 2465.839,
        "duration": 4.48
    },
    {
        "text": "uncorrelated so based on the original",
        "start": 2467.839,
        "duration": 5.321
    },
    {
        "text": "Matrix and this permuted Matrix permuted",
        "start": 2470.319,
        "duration": 7.161
    },
    {
        "text": "Gene Matrix we can get our statistic MCD",
        "start": 2473.16,
        "duration": 6.8
    },
    {
        "text": "and I ignore the detail here because our",
        "start": 2477.48,
        "duration": 4.4
    },
    {
        "text": "work is still ongoing but also I want to",
        "start": 2479.96,
        "duration": 4.6
    },
    {
        "text": "give you the idea here so MCD is",
        "start": 2481.88,
        "duration": 5.68
    },
    {
        "text": "calculated from the top row but how do",
        "start": 2484.56,
        "duration": 5.279
    },
    {
        "text": "we get a reasonable no distribution for",
        "start": 2487.56,
        "duration": 5.32
    },
    {
        "text": "MCD so pan had this idea that we can",
        "start": 2489.839,
        "duration": 6.161
    },
    {
        "text": "permute the genes within each cell we do",
        "start": 2492.88,
        "duration": 5.6
    },
    {
        "text": "the within cell permutation so if I do",
        "start": 2496.0,
        "duration": 5.04
    },
    {
        "text": "it this way then every cell keeps the",
        "start": 2498.48,
        "duration": 5.44
    },
    {
        "text": "same Library size as the original cells",
        "start": 2501.04,
        "duration": 4.96
    },
    {
        "text": "but here the genes are permuted genes",
        "start": 2503.92,
        "duration": 4.56
    },
    {
        "text": "they're not the real genes so therefore",
        "start": 2506.0,
        "duration": 5.04
    },
    {
        "text": "to we need to give this Matrix its own",
        "start": 2508.48,
        "duration": 4.96
    },
    {
        "text": "negative control that is given the",
        "start": 2511.04,
        "duration": 4.96
    },
    {
        "text": "permuted genes we perform within Gene",
        "start": 2513.44,
        "duration": 5.48
    },
    {
        "text": "permutation again at in the top then",
        "start": 2516.0,
        "duration": 5.599
    },
    {
        "text": "these two matrices will give me the same",
        "start": 2518.92,
        "duration": 5.64
    },
    {
        "text": "statistic but under the N so the idea is",
        "start": 2521.599,
        "duration": 5.72
    },
    {
        "text": "that for the bottom row we are getting",
        "start": 2524.56,
        "duration": 4.32
    },
    {
        "text": "it from the case where the genes are",
        "start": 2527.319,
        "duration": 4.121
    },
    {
        "text": "indeed uncorrelated but the key thing",
        "start": 2528.88,
        "duration": 4.16
    },
    {
        "text": "why we need double permutation is",
        "start": 2531.44,
        "duration": 4.44
    },
    {
        "text": "because for each Matrix we need to keep",
        "start": 2533.04,
        "duration": 5.76
    },
    {
        "text": "it genes but disrupt their correlations",
        "start": 2535.88,
        "duration": 4.8
    },
    {
        "text": "that's what we did in the top row right",
        "start": 2538.8,
        "duration": 3.92
    },
    {
        "text": "within Gene permutation preserves every",
        "start": 2540.68,
        "duration": 4.399
    },
    {
        "text": "Gene's marginal distribution removes G",
        "start": 2542.72,
        "duration": 4.48
    },
    {
        "text": "Gene correlations it removes cell",
        "start": 2545.079,
        "duration": 4.161
    },
    {
        "text": "Library sizes what when we do within",
        "start": 2547.2,
        "duration": 4.119
    },
    {
        "text": "cell permutation we preserve the cell",
        "start": 2549.24,
        "duration": 4.64
    },
    {
        "text": "library sizes but we remove the genes",
        "start": 2551.319,
        "duration": 4.921
    },
    {
        "text": "marginal distributions so these genes",
        "start": 2553.88,
        "duration": 4.679
    },
    {
        "text": "the per genes are no longer the original",
        "start": 2556.24,
        "duration": 4.52
    },
    {
        "text": "genes so that's why they need their own",
        "start": 2558.559,
        "duration": 5.28
    },
    {
        "text": "negative control as here we have here so",
        "start": 2560.76,
        "duration": 5.12
    },
    {
        "text": "that's the double permutation strategy",
        "start": 2563.839,
        "duration": 4.801
    },
    {
        "text": "and I can show you that it worked so",
        "start": 2565.88,
        "duration": 5.16
    },
    {
        "text": "this is a simulation in which we know",
        "start": 2568.64,
        "duration": 5.36
    },
    {
        "text": "the ground truth so these are the meta",
        "start": 2571.04,
        "duration": 5.36
    },
    {
        "text": "cells that are not pure so they are",
        "start": 2574.0,
        "duration": 4.559
    },
    {
        "text": "composed of different cell types and",
        "start": 2576.4,
        "duration": 4.56
    },
    {
        "text": "these are the meta cells that are pure",
        "start": 2578.559,
        "duration": 5.401
    },
    {
        "text": "so this is our statistic MCD so you can",
        "start": 2580.96,
        "duration": 5.159
    },
    {
        "text": "see that we do we are able to separate",
        "start": 2583.96,
        "duration": 4.32
    },
    {
        "text": "the two group using the statistic but",
        "start": 2586.119,
        "duration": 4.401
    },
    {
        "text": "how do we determine a threshold using",
        "start": 2588.28,
        "duration": 4.88
    },
    {
        "text": "permutation if you just do within gym",
        "start": 2590.52,
        "duration": 4.92
    },
    {
        "text": "permutation that is you just do the top",
        "start": 2593.16,
        "duration": 6.24
    },
    {
        "text": "row without this within sell permutation",
        "start": 2595.44,
        "duration": 6.52
    },
    {
        "text": "that's the problem you are being too how",
        "start": 2599.4,
        "duration": 5.919
    },
    {
        "text": "to say come too aggressive or actually",
        "start": 2601.96,
        "duration": 5.28
    },
    {
        "text": "should say too conservative so so",
        "start": 2605.319,
        "duration": 4.361
    },
    {
        "text": "basically you will call too many cells",
        "start": 2607.24,
        "duration": 4.68
    },
    {
        "text": "meta cells as dubious because the",
        "start": 2609.68,
        "duration": 4.919
    },
    {
        "text": "threshold is just too low but with that",
        "start": 2611.92,
        "duration": 5.24
    },
    {
        "text": "double permutation by preserving cell",
        "start": 2614.599,
        "duration": 4.841
    },
    {
        "text": "library sizes we get a much more",
        "start": 2617.16,
        "duration": 4.8
    },
    {
        "text": "reasonable threshold here and we can",
        "start": 2619.44,
        "duration": 5.52
    },
    {
        "text": "also see that here this left one shows",
        "start": 2621.96,
        "duration": 5.359
    },
    {
        "text": "how many dubious meta cells we detected",
        "start": 2624.96,
        "duration": 4.8
    },
    {
        "text": "using double permutation not so many but",
        "start": 2627.319,
        "duration": 5.04
    },
    {
        "text": "if we use within gy permutation half of",
        "start": 2629.76,
        "duration": 4.359
    },
    {
        "text": "the meta cells are dubious which is not",
        "start": 2632.359,
        "duration": 4.881
    },
    {
        "text": "possible and this also shows that using",
        "start": 2634.119,
        "duration": 5.921
    },
    {
        "text": "our double permutation approach there's",
        "start": 2637.24,
        "duration": 5.319
    },
    {
        "text": "a much better separation between the",
        "start": 2640.04,
        "duration": 5.2
    },
    {
        "text": "pure meta cells the trustworthy ones and",
        "start": 2642.559,
        "duration": 4.961
    },
    {
        "text": "the dubious ones then we use just within",
        "start": 2645.24,
        "duration": 5.68
    },
    {
        "text": "G peration due to time constraint I'll",
        "start": 2647.52,
        "duration": 5.36
    },
    {
        "text": "just quickly show you two real data",
        "start": 2650.92,
        "duration": 4.56
    },
    {
        "text": "applications to demonstrate how this",
        "start": 2652.88,
        "duration": 4.959
    },
    {
        "text": "method we propos MCU riger can be",
        "start": 2655.48,
        "duration": 4.879
    },
    {
        "text": "helpful the first one is to help with",
        "start": 2657.839,
        "duration": 6.041
    },
    {
        "text": "Gene coexpression analysis so this is a",
        "start": 2660.359,
        "duration": 6.041
    },
    {
        "text": "a real data application from this from",
        "start": 2663.88,
        "duration": 5.16
    },
    {
        "text": "this let me see from this nature",
        "start": 2666.4,
        "duration": 4.88
    },
    {
        "text": "medicine paper and it was first",
        "start": 2669.04,
        "duration": 4.64
    },
    {
        "text": "implemented in the using the meta cell",
        "start": 2671.28,
        "duration": 5.52
    },
    {
        "text": "method paper so we reproduce the result",
        "start": 2673.68,
        "duration": 6.56
    },
    {
        "text": "so the left column shows the Gan Gene",
        "start": 2676.8,
        "duration": 6.279
    },
    {
        "text": "correlation Matrix you get from single",
        "start": 2680.24,
        "duration": 5.64
    },
    {
        "text": "cells okay so it's from patient data",
        "start": 2683.079,
        "duration": 5.76
    },
    {
        "text": "healthy patient or covid-19 patient we",
        "start": 2685.88,
        "duration": 5.0
    },
    {
        "text": "hope to see some gen Co expression",
        "start": 2688.839,
        "duration": 4.041
    },
    {
        "text": "differences between the two groups of",
        "start": 2690.88,
        "duration": 4.16
    },
    {
        "text": "patients but you can see that with",
        "start": 2692.88,
        "duration": 5.12
    },
    {
        "text": "single cell data the Cor ations are very",
        "start": 2695.04,
        "duration": 6.12
    },
    {
        "text": "sparse because data are sparse and if we",
        "start": 2698.0,
        "duration": 5.72
    },
    {
        "text": "use all meta cells and this is from the",
        "start": 2701.16,
        "duration": 6.28
    },
    {
        "text": "original paper we can see that for this",
        "start": 2703.72,
        "duration": 6.08
    },
    {
        "text": "group of genes block of genes so these",
        "start": 2707.44,
        "duration": 4.8
    },
    {
        "text": "genes are basically adaptive immune",
        "start": 2709.8,
        "duration": 4.68
    },
    {
        "text": "response genes we see some strong",
        "start": 2712.24,
        "duration": 5.04
    },
    {
        "text": "correlations in both groups of patients",
        "start": 2714.48,
        "duration": 6.28
    },
    {
        "text": "both healthy and covid-19 but using our",
        "start": 2717.28,
        "duration": 6.279
    },
    {
        "text": "selected trustworthy meta cells using MC",
        "start": 2720.76,
        "duration": 5.28
    },
    {
        "text": "riger we can see some big difference so",
        "start": 2723.559,
        "duration": 4.681
    },
    {
        "text": "the same group of G have weak",
        "start": 2726.04,
        "duration": 5.039
    },
    {
        "text": "correlation in healthy or weak",
        "start": 2728.24,
        "duration": 5.28
    },
    {
        "text": "coexpression healthy patients but strong",
        "start": 2731.079,
        "duration": 5.28
    },
    {
        "text": "coexpression in covid-19 patient this is",
        "start": 2733.52,
        "duration": 4.72
    },
    {
        "text": "consistent with the the nature of these",
        "start": 2736.359,
        "duration": 4.121
    },
    {
        "text": "genes being adaptive immune response",
        "start": 2738.24,
        "duration": 4.28
    },
    {
        "text": "they should be active in covid patients",
        "start": 2740.48,
        "duration": 4.68
    },
    {
        "text": "but not healthy patients and looking at",
        "start": 2742.52,
        "duration": 5.44
    },
    {
        "text": "the data we can also see why we have",
        "start": 2745.16,
        "duration": 5.919
    },
    {
        "text": "those fake positive correlations using",
        "start": 2747.96,
        "duration": 5.879
    },
    {
        "text": "all meta cells so here the blue meta",
        "start": 2751.079,
        "duration": 5.48
    },
    {
        "text": "cells are the questionable dubious ones",
        "start": 2753.839,
        "duration": 5.24
    },
    {
        "text": "the red that are the trustworthy ones so",
        "start": 2756.559,
        "duration": 5.321
    },
    {
        "text": "you can see this blue one here here here",
        "start": 2759.079,
        "duration": 4.76
    },
    {
        "text": "drove the correlation to be very high",
        "start": 2761.88,
        "duration": 3.6
    },
    {
        "text": "but they should be removed and if you",
        "start": 2763.839,
        "duration": 4.0
    },
    {
        "text": "look at a single cell data original data",
        "start": 2765.48,
        "duration": 4.68
    },
    {
        "text": "you don't see such strong correlation",
        "start": 2767.839,
        "duration": 5.361
    },
    {
        "text": "there so that's one example the second",
        "start": 2770.16,
        "duration": 5.199
    },
    {
        "text": "example is for single so multi-owned",
        "start": 2773.2,
        "duration": 6.359
    },
    {
        "text": "data we have RNA seek and attack seek so",
        "start": 2775.359,
        "duration": 6.441
    },
    {
        "text": "basically we are trying to draw some",
        "start": 2779.559,
        "duration": 5.161
    },
    {
        "text": "associations between enhancers and G",
        "start": 2781.8,
        "duration": 6.6
    },
    {
        "text": "expression for inferring G regulation so",
        "start": 2784.72,
        "duration": 5.76
    },
    {
        "text": "we can actually see that using our",
        "start": 2788.4,
        "duration": 5.08
    },
    {
        "text": "trustworthy meta cell versus using all",
        "start": 2790.48,
        "duration": 5.4
    },
    {
        "text": "meta cells we see some differences and",
        "start": 2793.48,
        "duration": 4.8
    },
    {
        "text": "this is a data analysis from the C cell",
        "start": 2795.88,
        "duration": 4.84
    },
    {
        "text": "paper and nature bch so we see that",
        "start": 2798.28,
        "duration": 5.16
    },
    {
        "text": "there's one Edge used to be found by all",
        "start": 2800.72,
        "duration": 5.16
    },
    {
        "text": "meta cells that disappeared so if you",
        "start": 2803.44,
        "duration": 5.24
    },
    {
        "text": "look at the raw single cell data by cell",
        "start": 2805.88,
        "duration": 5.56
    },
    {
        "text": "type we do not see a peak here so which",
        "start": 2808.68,
        "duration": 5.52
    },
    {
        "text": "suggest that this Association was indeed",
        "start": 2811.44,
        "duration": 5.879
    },
    {
        "text": "spous but if we look at this additional",
        "start": 2814.2,
        "duration": 5.72
    },
    {
        "text": "one we found using just trustworthy meta",
        "start": 2817.319,
        "duration": 4.24
    },
    {
        "text": "cells but missed by the previous",
        "start": 2819.92,
        "duration": 4.159
    },
    {
        "text": "analysis we see that this is indeed a",
        "start": 2821.559,
        "duration": 5.76
    },
    {
        "text": "small peak in all the cell types here by",
        "start": 2824.079,
        "duration": 5.76
    },
    {
        "text": "aggregating cells into meta cells we can",
        "start": 2827.319,
        "duration": 5.04
    },
    {
        "text": "discover this but if you don't use just",
        "start": 2829.839,
        "duration": 4.76
    },
    {
        "text": "trustworthy meta cells You by using all",
        "start": 2832.359,
        "duration": 4.76
    },
    {
        "text": "meta cells you will miss this one so",
        "start": 2834.599,
        "duration": 4.641
    },
    {
        "text": "that's just a two of the main examples",
        "start": 2837.119,
        "duration": 4.641
    },
    {
        "text": "we hope to show in this work so in",
        "start": 2839.24,
        "duration": 5.72
    },
    {
        "text": "summary I talk about three ways of doing",
        "start": 2841.76,
        "duration": 5.799
    },
    {
        "text": "peration the first one is a simplest one",
        "start": 2844.96,
        "duration": 5.159
    },
    {
        "text": "for the de analysis we just permute the",
        "start": 2847.559,
        "duration": 5.401
    },
    {
        "text": "labels so we can get a negative case the",
        "start": 2850.119,
        "duration": 5.641
    },
    {
        "text": "second one within G permutation is for",
        "start": 2852.96,
        "duration": 5.599
    },
    {
        "text": "us to get some permuted cells whose cell",
        "start": 2855.76,
        "duration": 5.44
    },
    {
        "text": "cell relationships are disrupted and",
        "start": 2858.559,
        "duration": 4.361
    },
    {
        "text": "here we don't need to preserve cell",
        "start": 2861.2,
        "duration": 4.04
    },
    {
        "text": "library sizes because in this",
        "start": 2862.92,
        "duration": 5.199
    },
    {
        "text": "visualization thing our score is defined",
        "start": 2865.24,
        "duration": 5.64
    },
    {
        "text": "as pearon correlation whose range from",
        "start": 2868.119,
        "duration": 4.841
    },
    {
        "text": "negative 1 to positive one so we have",
        "start": 2870.88,
        "duration": 4.439
    },
    {
        "text": "the same range that's comparable between",
        "start": 2872.96,
        "duration": 4.639
    },
    {
        "text": "the cells and per cells and for the",
        "start": 2875.319,
        "duration": 4.401
    },
    {
        "text": "third one is the most tricky one because",
        "start": 2877.599,
        "duration": 4.48
    },
    {
        "text": "we had to do this double permutation to",
        "start": 2879.72,
        "duration": 4.8
    },
    {
        "text": "preserve cell library sizes when we need",
        "start": 2882.079,
        "duration": 4.841
    },
    {
        "text": "a negative control so I just want to say",
        "start": 2884.52,
        "duration": 5.0
    },
    {
        "text": "that even the permutation may seem as a",
        "start": 2886.92,
        "duration": 4.48
    },
    {
        "text": "easy technique but when you want to",
        "start": 2889.52,
        "duration": 4.2
    },
    {
        "text": "implement in your analysis it's very",
        "start": 2891.4,
        "duration": 4.24
    },
    {
        "text": "important to think what you want to",
        "start": 2893.72,
        "duration": 3.92
    },
    {
        "text": "preserve what you want to disrupt and",
        "start": 2895.64,
        "duration": 4.439
    },
    {
        "text": "then design the permutation accordingly",
        "start": 2897.64,
        "duration": 5.8
    },
    {
        "text": "so finally I want to thank my",
        "start": 2900.079,
        "duration": 5.721
    },
    {
        "text": "collaborators and my students and post",
        "start": 2903.44,
        "duration": 4.2
    },
    {
        "text": "off for the three project project in",
        "start": 2905.8,
        "duration": 4.4
    },
    {
        "text": "particular the first project which is a",
        "start": 2907.64,
        "duration": 5.6
    },
    {
        "text": "surprise finding has actually led to",
        "start": 2910.2,
        "duration": 6.04
    },
    {
        "text": "many how to say feedback and interesting",
        "start": 2913.24,
        "duration": 4.839
    },
    {
        "text": "discussion about how to do differential",
        "start": 2916.24,
        "duration": 3.879
    },
    {
        "text": "expression analysis an analysis of 10",
        "start": 2918.079,
        "duration": 4.361
    },
    {
        "text": "years old and for the visualization we",
        "start": 2920.119,
        "duration": 4.561
    },
    {
        "text": "hope that we can help people visualize",
        "start": 2922.44,
        "duration": 5.56
    },
    {
        "text": "or have a better more reliable TN and um",
        "start": 2924.68,
        "duration": 5.76
    },
    {
        "text": "plot by optimizing the hyp parameter",
        "start": 2928.0,
        "duration": 4.839
    },
    {
        "text": "finally for the MC riger we're hoping to",
        "start": 2930.44,
        "duration": 4.32
    },
    {
        "text": "submit the pre-print very soon so you",
        "start": 2932.839,
        "duration": 4.201
    },
    {
        "text": "can see the whole idea there thank you",
        "start": 2934.76,
        "duration": 4.44
    },
    {
        "text": "very",
        "start": 2937.04,
        "duration": 2.16
    },
    {
        "text": "much hi Dr thank you for your inspiring",
        "start": 2953.92,
        "duration": 6.399
    },
    {
        "text": "talk so uh because I see some people",
        "start": 2957.16,
        "duration": 5.28
    },
    {
        "text": "doing like a per perturbation based",
        "start": 2960.319,
        "duration": 4.601
    },
    {
        "text": "method like introduce noises to the data",
        "start": 2962.44,
        "duration": 5.24
    },
    {
        "text": "and to see whether what differential CH",
        "start": 2964.92,
        "duration": 5.159
    },
    {
        "text": "I remain the same so what's real",
        "start": 2967.68,
        "duration": 5.879
    },
    {
        "text": "difference of the Coe The Code idea",
        "start": 2970.079,
        "duration": 5.401
    },
    {
        "text": "between this one and permutation so you",
        "start": 2973.559,
        "duration": 4.361
    },
    {
        "text": "said other paper use permutation to",
        "start": 2975.48,
        "duration": 4.359
    },
    {
        "text": "introduce noise peration mean",
        "start": 2977.92,
        "duration": 4.96
    },
    {
        "text": "perturbation yeah adding some noises to",
        "start": 2979.839,
        "duration": 5.321
    },
    {
        "text": "the data you have good question actually",
        "start": 2982.88,
        "duration": 4.32
    },
    {
        "text": "that's related to another project in my",
        "start": 2985.16,
        "duration": 5.399
    },
    {
        "text": "group I would say if you have heard",
        "start": 2987.2,
        "duration": 6.84
    },
    {
        "text": "about this bias variance tradeoff thing",
        "start": 2990.559,
        "duration": 6.8
    },
    {
        "text": "in statistics that's it so basically",
        "start": 2994.04,
        "duration": 6.16
    },
    {
        "text": "here our peration is to provide provide",
        "start": 2997.359,
        "duration": 6.801
    },
    {
        "text": "a negative control for removing the bias",
        "start": 3000.2,
        "duration": 6.48
    },
    {
        "text": "and when you add noise to your data you",
        "start": 3004.16,
        "duration": 5.08
    },
    {
        "text": "are evaluating the variance of a result",
        "start": 3006.68,
        "duration": 6.32
    },
    {
        "text": "how robust or how stable your result is",
        "start": 3009.24,
        "duration": 6.24
    },
    {
        "text": "the riger is about you know the bias",
        "start": 3013.0,
        "duration": 4.24
    },
    {
        "text": "bias yeah so what I'm introducing here",
        "start": 3015.48,
        "duration": 3.48
    },
    {
        "text": "is all about the bias we're getting a",
        "start": 3017.24,
        "duration": 3.72
    },
    {
        "text": "negative control case but I totally",
        "start": 3018.96,
        "duration": 3.72
    },
    {
        "text": "agree I think variance is another thing",
        "start": 3020.96,
        "duration": 6.119
    },
    {
        "text": "to consider yeah thank you",
        "start": 3022.68,
        "duration": 7.159
    },
    {
        "text": "it's very interesting talk and um um I'm",
        "start": 3027.079,
        "duration": 7.121
    },
    {
        "text": "curious uh for the B de uh differential",
        "start": 3029.839,
        "duration": 6.921
    },
    {
        "text": "expression analysis one um your",
        "start": 3034.2,
        "duration": 5.48
    },
    {
        "text": "observation is for some data sets that",
        "start": 3036.76,
        "duration": 5.28
    },
    {
        "text": "like NE to binomial distribution doesn't",
        "start": 3039.68,
        "duration": 4.72
    },
    {
        "text": "hold but currently on single cell it's",
        "start": 3042.04,
        "duration": 4.64
    },
    {
        "text": "like plus and netive distribution on",
        "start": 3044.4,
        "duration": 3.84
    },
    {
        "text": "their main distributions I wonder have",
        "start": 3046.68,
        "duration": 3.399
    },
    {
        "text": "you look at them and how good are they",
        "start": 3048.24,
        "duration": 4.48
    },
    {
        "text": "and how does that like I don't",
        "start": 3050.079,
        "duration": 6.24
    },
    {
        "text": "know just effect I mean gener yeah very",
        "start": 3052.72,
        "duration": 5.8
    },
    {
        "text": "good question question I would say that",
        "start": 3056.319,
        "duration": 6.0
    },
    {
        "text": "um um for single cell data and we have",
        "start": 3058.52,
        "duration": 6.559
    },
    {
        "text": "abundant evidence we have a genome",
        "start": 3062.319,
        "duration": 5.0
    },
    {
        "text": "biology paper called statistics or",
        "start": 3065.079,
        "duration": 4.48
    },
    {
        "text": "biology the zero inflation controversy",
        "start": 3067.319,
        "duration": 4.76
    },
    {
        "text": "it's a review paper so in that paper we",
        "start": 3069.559,
        "duration": 5.161
    },
    {
        "text": "actually found and I think Shan Jo at",
        "start": 3072.079,
        "duration": 4.641
    },
    {
        "text": "Michigan here he also had another paper",
        "start": 3074.72,
        "duration": 4.399
    },
    {
        "text": "showing that negative bomia is a good",
        "start": 3076.72,
        "duration": 4.639
    },
    {
        "text": "distribution if your cells are",
        "start": 3079.119,
        "duration": 4.921
    },
    {
        "text": "homogeneous in one cell type so with in",
        "start": 3081.359,
        "duration": 5.401
    },
    {
        "text": "that case you can safely say oh no Al",
        "start": 3084.04,
        "duration": 5.16
    },
    {
        "text": "the Single Cell technology has the Umi",
        "start": 3086.76,
        "duration": 4.599
    },
    {
        "text": "unique molecular identifier for removing",
        "start": 3089.2,
        "duration": 4.599
    },
    {
        "text": "PCR bias so for that case negative",
        "start": 3091.359,
        "duration": 4.921
    },
    {
        "text": "binomial is reasonable which is also",
        "start": 3093.799,
        "duration": 4.8
    },
    {
        "text": "related to that two layer model right",
        "start": 3096.28,
        "duration": 5.0
    },
    {
        "text": "the measurement model can be reasonable",
        "start": 3098.599,
        "duration": 5.2
    },
    {
        "text": "assume AS Plus on and if the expression",
        "start": 3101.28,
        "duration": 4.6
    },
    {
        "text": "model is gamma the negative binomial",
        "start": 3103.799,
        "duration": 3.76
    },
    {
        "text": "would be good and because you know gamma",
        "start": 3105.88,
        "duration": 4.0
    },
    {
        "text": "is a very flexible distribution for",
        "start": 3107.559,
        "duration": 4.441
    },
    {
        "text": "describing positive real values it has",
        "start": 3109.88,
        "duration": 5.479
    },
    {
        "text": "two parameters anyway so I think for the",
        "start": 3112.0,
        "duration": 5.799
    },
    {
        "text": "question we report here for both AR",
        "start": 3115.359,
        "duration": 4.801
    },
    {
        "text": "examples I think here just the the",
        "start": 3117.799,
        "duration": 4.481
    },
    {
        "text": "patients are very heterogeneous and they",
        "start": 3120.16,
        "duration": 3.8
    },
    {
        "text": "may have different genetic backgrounds",
        "start": 3122.28,
        "duration": 3.559
    },
    {
        "text": "right so when you pull them together you",
        "start": 3123.96,
        "duration": 4.32
    },
    {
        "text": "don't use other patient coars just",
        "start": 3125.839,
        "duration": 5.561
    },
    {
        "text": "simply do the two condition de test you",
        "start": 3128.28,
        "duration": 5.4
    },
    {
        "text": "might have false positives but in here",
        "start": 3131.4,
        "duration": 4.6
    },
    {
        "text": "we can discover that negative binomial",
        "start": 3133.68,
        "duration": 4.639
    },
    {
        "text": "doesn't fit well because of the large",
        "start": 3136.0,
        "duration": 5.96
    },
    {
        "text": "sample sizes 51 58 but previously if you",
        "start": 3138.319,
        "duration": 6.081
    },
    {
        "text": "only have three values there's no way to",
        "start": 3141.96,
        "duration": 4.56
    },
    {
        "text": "check you just have to assume that",
        "start": 3144.4,
        "duration": 3.76
    },
    {
        "text": "negative binomial I think that's another",
        "start": 3146.52,
        "duration": 4.24
    },
    {
        "text": "difference but for single cell data I",
        "start": 3148.16,
        "duration": 4.52
    },
    {
        "text": "wouldn't be too worried because we",
        "start": 3150.76,
        "duration": 5.28
    },
    {
        "text": "usually have at least maybe aund cells",
        "start": 3152.68,
        "duration": 5.0
    },
    {
        "text": "to look at right so we have a large",
        "start": 3156.04,
        "duration": 4.72
    },
    {
        "text": "number to check the distribution",
        "start": 3157.68,
        "duration": 7.48
    },
    {
        "text": "yeah any other questions thank",
        "start": 3160.76,
        "duration": 4.4
    },
    {
        "text": "you hi uh I'm very curious about the",
        "start": 3168.68,
        "duration": 6.08
    },
    {
        "text": "time complexity of your methods usually",
        "start": 3171.92,
        "duration": 5.6
    },
    {
        "text": "permutation based methods are question I",
        "start": 3174.76,
        "duration": 4.48
    },
    {
        "text": "I would like to say that the good thing",
        "start": 3177.52,
        "duration": 3.839
    },
    {
        "text": "about all the permutation I introduced",
        "start": 3179.24,
        "duration": 4.359
    },
    {
        "text": "here is that except the first one where",
        "start": 3181.359,
        "duration": 5.881
    },
    {
        "text": "we did 1,000 permutation just to get to",
        "start": 3183.599,
        "duration": 6.401
    },
    {
        "text": "show the distribution of the deeg number",
        "start": 3187.24,
        "duration": 4.72
    },
    {
        "text": "but for the second and the third one we",
        "start": 3190.0,
        "duration": 5.319
    },
    {
        "text": "only permuted once yeah the reason is",
        "start": 3191.96,
        "duration": 6.28
    },
    {
        "text": "that we actually after the permutation",
        "start": 3195.319,
        "duration": 5.881
    },
    {
        "text": "every permuted cell will give you one n",
        "start": 3198.24,
        "duration": 5.0
    },
    {
        "text": "value and they together give me a",
        "start": 3201.2,
        "duration": 4.08
    },
    {
        "text": "distribution so I don't have to repeat",
        "start": 3203.24,
        "duration": 4.559
    },
    {
        "text": "this again and again and the reason is",
        "start": 3205.28,
        "duration": 5.799
    },
    {
        "text": "because for both s and MC riger we're",
        "start": 3207.799,
        "duration": 6.52
    },
    {
        "text": "looking at a per sale statistic so we're",
        "start": 3211.079,
        "duration": 5.601
    },
    {
        "text": "not looking at the STA for ourselves so",
        "start": 3214.319,
        "duration": 4.201
    },
    {
        "text": "that's why we don't need to get many",
        "start": 3216.68,
        "duration": 3.6
    },
    {
        "text": "many rounds of permutation which is a",
        "start": 3218.52,
        "duration": 3.68
    },
    {
        "text": "good thing to me yeah so that's why it's",
        "start": 3220.28,
        "duration": 3.36
    },
    {
        "text": "not complicated you're just doing",
        "start": 3222.2,
        "duration": 5.159
    },
    {
        "text": "permutation for once",
        "start": 3223.64,
        "duration": 3.719
    },
    {
        "text": "yeah thank you Dr so my question is",
        "start": 3229.319,
        "duration": 5.881
    },
    {
        "text": "about like for the like some single cell",
        "start": 3232.559,
        "duration": 5.961
    },
    {
        "text": "R uh data analysis if we want to focus",
        "start": 3235.2,
        "duration": 6.04
    },
    {
        "text": "focus on like some Downstream tasks like",
        "start": 3238.52,
        "duration": 4.839
    },
    {
        "text": "earning velocity something like this how",
        "start": 3241.24,
        "duration": 4.599
    },
    {
        "text": "important do you think we need to do the",
        "start": 3243.359,
        "duration": 6.281
    },
    {
        "text": "like the laboring uh inviting just",
        "start": 3245.839,
        "duration": 6.28
    },
    {
        "text": "trustworthy check definitely that is",
        "start": 3249.64,
        "duration": 5.04
    },
    {
        "text": "something we are definitely interested",
        "start": 3252.119,
        "duration": 5.081
    },
    {
        "text": "in yeah and I would say that's why I",
        "start": 3254.68,
        "duration": 5.919
    },
    {
        "text": "feel like probably this talk can inspire",
        "start": 3257.2,
        "duration": 5.04
    },
    {
        "text": "you to think about how it can be in",
        "start": 3260.599,
        "duration": 3.601
    },
    {
        "text": "covered in your analysis right some",
        "start": 3262.24,
        "duration": 4.319
    },
    {
        "text": "negative control where you know that",
        "start": 3264.2,
        "duration": 5.04
    },
    {
        "text": "nothing should be found as a as a sanity",
        "start": 3266.559,
        "duration": 4.921
    },
    {
        "text": "check to see because I think for single",
        "start": 3269.24,
        "duration": 5.04
    },
    {
        "text": "cell data analysis the reality is that",
        "start": 3271.48,
        "duration": 5.48
    },
    {
        "text": "it has a lot of steps involved right we",
        "start": 3274.28,
        "duration": 4.92
    },
    {
        "text": "are not just doing a one-step analysis",
        "start": 3276.96,
        "duration": 5.52
    },
    {
        "text": "so we need to be sure that some signals",
        "start": 3279.2,
        "duration": 5.399
    },
    {
        "text": "we found are not due to the previous",
        "start": 3282.48,
        "duration": 4.0
    },
    {
        "text": "steps but are should be really be in the",
        "start": 3284.599,
        "duration": 4.361
    },
    {
        "text": "data yeah so I think Professor Jun Lee",
        "start": 3286.48,
        "duration": 4.48
    },
    {
        "text": "when he introduced me he mentioned that",
        "start": 3288.96,
        "duration": 4.119
    },
    {
        "text": "I'm also advocating the synthetic no",
        "start": 3290.96,
        "duration": 4.44
    },
    {
        "text": "data idea to be added to data analysis",
        "start": 3293.079,
        "duration": 4.72
    },
    {
        "text": "and I also permit just one approach to",
        "start": 3295.4,
        "duration": 6.439
    },
    {
        "text": "implement the idea yeah thank",
        "start": 3297.799,
        "duration": 10.0
    },
    {
        "text": "you do we have online questions online",
        "start": 3301.839,
        "duration": 5.96
    },
    {
        "text": "question I'll ask couple still have a",
        "start": 3311.48,
        "duration": 8.639
    },
    {
        "text": "few minutes yeah uh um so for the meta",
        "start": 3315.2,
        "duration": 7.839
    },
    {
        "text": "cell right uh you mentioned the hyper",
        "start": 3320.119,
        "duration": 6.48
    },
    {
        "text": "parameter yeah um have you thought about",
        "start": 3323.039,
        "duration": 5.401
    },
    {
        "text": "making it different depending on whether",
        "start": 3326.599,
        "duration": 4.24
    },
    {
        "text": "it's a dense area or the sparse area",
        "start": 3328.44,
        "duration": 7.0
    },
    {
        "text": "very good question I have to say that",
        "start": 3330.839,
        "duration": 7.361
    },
    {
        "text": "yes so right now for this work alone",
        "start": 3335.44,
        "duration": 5.119
    },
    {
        "text": "we're just proposing a Criterion and",
        "start": 3338.2,
        "duration": 5.68
    },
    {
        "text": "we're trying to optimize each of the",
        "start": 3340.559,
        "duration": 5.081
    },
    {
        "text": "four methods in terms of their",
        "start": 3343.88,
        "duration": 4.56
    },
    {
        "text": "hyperparameter and then we can compare",
        "start": 3345.64,
        "duration": 4.959
    },
    {
        "text": "these four methods to see which one is",
        "start": 3348.44,
        "duration": 4.04
    },
    {
        "text": "better for this data set there are some",
        "start": 3350.599,
        "duration": 4.48
    },
    {
        "text": "results I'm not showing but I would say",
        "start": 3352.48,
        "duration": 5.319
    },
    {
        "text": "ultimately we are dividing we are",
        "start": 3355.079,
        "duration": 5.641
    },
    {
        "text": "devising our own algorithm so when we",
        "start": 3357.799,
        "duration": 5.161
    },
    {
        "text": "devise an algorithm for defining meta",
        "start": 3360.72,
        "duration": 4.92
    },
    {
        "text": "cell I think it should consider density",
        "start": 3362.96,
        "duration": 4.639
    },
    {
        "text": "so maybe the neighborhood size shouldn't",
        "start": 3365.64,
        "duration": 4.479
    },
    {
        "text": "be the same thing so this gamma is just",
        "start": 3367.599,
        "duration": 4.841
    },
    {
        "text": "a way to summarize existing methods",
        "start": 3370.119,
        "duration": 4.041
    },
    {
        "text": "hyper parameter maybe it's not directly",
        "start": 3372.44,
        "duration": 3.32
    },
    {
        "text": "gamma but basically have another",
        "start": 3374.16,
        "duration": 3.76
    },
    {
        "text": "parameter that controls the gamma we are",
        "start": 3375.76,
        "duration": 5.76
    },
    {
        "text": "just using for Simplicity but how do we",
        "start": 3377.92,
        "duration": 6.48
    },
    {
        "text": "divide single cells into trustworthy",
        "start": 3381.52,
        "duration": 6.039
    },
    {
        "text": "meta cells by optimizing our Criterion",
        "start": 3384.4,
        "duration": 4.399
    },
    {
        "text": "that is something we're working",
        "start": 3387.559,
        "duration": 5.201
    },
    {
        "text": "separately as a new meta algorithm",
        "start": 3388.799,
        "duration": 6.881
    },
    {
        "text": "yeah thank",
        "start": 3392.76,
        "duration": 5.52
    },
    {
        "text": "you thank you for your nice talk um I'm",
        "start": 3395.68,
        "duration": 5.639
    },
    {
        "text": "just curious um whether the SD is",
        "start": 3398.28,
        "duration": 5.72
    },
    {
        "text": "applicable to even image based special",
        "start": 3401.319,
        "duration": 4.681
    },
    {
        "text": "transcript to mix which is also single",
        "start": 3404.0,
        "duration": 4.559
    },
    {
        "text": "resolution yeah for sure because as did",
        "start": 3406.0,
        "duration": 4.52
    },
    {
        "text": "is just a visualization thing right it's",
        "start": 3408.559,
        "duration": 4.201
    },
    {
        "text": "just a it's an add-on it's an add-on to",
        "start": 3410.52,
        "duration": 5.12
    },
    {
        "text": "T near umap as long as you can use T",
        "start": 3412.76,
        "duration": 4.799
    },
    {
        "text": "near umap to visualize your data you can",
        "start": 3415.64,
        "duration": 5.52
    },
    {
        "text": "tryd great yeah thank",
        "start": 3417.559,
        "duration": 3.601
    },
    {
        "text": "you another one so on your second part",
        "start": 3423.24,
        "duration": 8.2
    },
    {
        "text": "about um embedding credible versus dupus",
        "start": 3426.44,
        "duration": 8.44
    },
    {
        "text": "embedding uh there is a parameter U 50%",
        "start": 3431.44,
        "duration": 6.28
    },
    {
        "text": "of the cells yeah yes if you dial that",
        "start": 3434.88,
        "duration": 5.0
    },
    {
        "text": "down to 20% would you get more of the",
        "start": 3437.72,
        "duration": 5.2
    },
    {
        "text": "rare clusters to be credible good",
        "start": 3439.88,
        "duration": 5.64
    },
    {
        "text": "question yes in this paper we did try",
        "start": 3442.92,
        "duration": 4.679
    },
    {
        "text": "varying that neighborhood percentage",
        "start": 3445.52,
        "duration": 5.68
    },
    {
        "text": "right from 50 down to like 10% or 20% up",
        "start": 3447.599,
        "duration": 4.841
    },
    {
        "text": "to 80%",
        "start": 3451.2,
        "duration": 4.96
    },
    {
        "text": "90% exactly because tney and umap are",
        "start": 3452.44,
        "duration": 6.32
    },
    {
        "text": "designed to preserve local neighborhood",
        "start": 3456.16,
        "duration": 5.48
    },
    {
        "text": "if this sale neighborhood size is small",
        "start": 3458.76,
        "duration": 4.92
    },
    {
        "text": "then you will find more sales to be",
        "start": 3461.64,
        "duration": 3.959
    },
    {
        "text": "trustworthy yeah more embedding to be",
        "start": 3463.68,
        "duration": 3.2
    },
    {
        "text": "trustworthy because it's that's what",
        "start": 3465.599,
        "duration": 4.361
    },
    {
        "text": "it's designed for but we chose this 50%",
        "start": 3466.88,
        "duration": 5.479
    },
    {
        "text": "because we want to talk about mid-range",
        "start": 3469.96,
        "duration": 5.2
    },
    {
        "text": "relationship like here oh sorry I should",
        "start": 3472.359,
        "duration": 4.641
    },
    {
        "text": "go back",
        "start": 3475.16,
        "duration": 3.24
    },
    {
        "text": "where is",
        "start": 3477.0,
        "duration": 5.0
    },
    {
        "text": "it yeah so I should go here so basically",
        "start": 3478.4,
        "duration": 5.28
    },
    {
        "text": "want to say whether the relationship",
        "start": 3482.0,
        "duration": 4.039
    },
    {
        "text": "between these clusters are reliable so",
        "start": 3483.68,
        "duration": 5.359
    },
    {
        "text": "that's why we go larger 50% but the good",
        "start": 3486.039,
        "duration": 5.481
    },
    {
        "text": "thing is that we in our how to say",
        "start": 3489.039,
        "duration": 4.881
    },
    {
        "text": "stability analysis or sensitivity",
        "start": 3491.52,
        "duration": 4.48
    },
    {
        "text": "analysis we found that if you change the",
        "start": 3493.92,
        "duration": 5.159
    },
    {
        "text": "percentage to 40% to 60% it doesn't",
        "start": 3496.0,
        "duration": 6.52
    },
    {
        "text": "change that much yeah so that's what we",
        "start": 3499.079,
        "duration": 7.361
    },
    {
        "text": "did yeah",
        "start": 3502.52,
        "duration": 4.799
    },
    {
        "text": "last",
        "start": 3506.44,
        "duration": 5.8
    },
    {
        "text": "one okay I'll ask last one um so the in",
        "start": 3507.319,
        "duration": 7.04
    },
    {
        "text": "the final part yeah there the assumption",
        "start": 3512.24,
        "duration": 4.4
    },
    {
        "text": "that Library size is purely technical",
        "start": 3514.359,
        "duration": 5.801
    },
    {
        "text": "yeah and then when the size is bigger or",
        "start": 3516.64,
        "duration": 7.439
    },
    {
        "text": "smaller the features are un correlated",
        "start": 3520.16,
        "duration": 6.199
    },
    {
        "text": "yeah biologically I think it's a",
        "start": 3524.079,
        "duration": 4.121
    },
    {
        "text": "profound question whether different cell",
        "start": 3526.359,
        "duration": 4.281
    },
    {
        "text": "types genuinely have original size been",
        "start": 3528.2,
        "duration": 5.08
    },
    {
        "text": "different and something propagated into",
        "start": 3530.64,
        "duration": 4.959
    },
    {
        "text": "exactly yeah that's a very good question",
        "start": 3533.28,
        "duration": 4.88
    },
    {
        "text": "um I I think here we are in a simpler",
        "start": 3535.599,
        "duration": 5.161
    },
    {
        "text": "situation because we hope that the meta",
        "start": 3538.16,
        "duration": 5.36
    },
    {
        "text": "cell definition should just include",
        "start": 3540.76,
        "duration": 5.4
    },
    {
        "text": "cells of the same type right every meta",
        "start": 3543.52,
        "duration": 4.839
    },
    {
        "text": "cell if it's trustworthy it should only",
        "start": 3546.16,
        "duration": 4.679
    },
    {
        "text": "include cells in the same type so that's",
        "start": 3548.359,
        "duration": 4.521
    },
    {
        "text": "why the cell type differences shouldn't",
        "start": 3550.839,
        "duration": 4.921
    },
    {
        "text": "be biological that's what we hope",
        "start": 3552.88,
        "duration": 4.959
    },
    {
        "text": "yeah thank you",
        "start": 3555.76,
        "duration": 6.0
    },
    {
        "text": "yeah thank you",
        "start": 3557.839,
        "duration": 3.921
    }
]