high resolution structure of a modeling of HIV spiked protein tomorrow nuts at midday downstairs and 2036 and our tools and tech he's having our series so Chyna please take it away thank you thank you thank you for the for the invite and for that flattering introduction it shows my age we accumulate this as the integral of little bits of effort all along the way but structural biology has definitely become a it started off as an avocation and now it's become a full-fledged area of research especially by automatics but this talk that I'm gonna give to you is is you know couched in some computer science some mathematics that's what I know and of course it's tuned to this application of prediction and part of the message is also a framework that I've been developing this past year with my PhD student mo Heba are she following on the footsteps of lots of other people who've been working in similar areas and that's on providing you know proof certificates for your prediction something we call quantified uncertainty so the problem itself is perhaps explanatory by these two pictures that I've drawn here this is if you're given a bunch of flexible tiles you can think of them as molecules can you predict this multi molecular assembly so as I'll show you pretty soon it could be akin to to create 3d jigsaw puzzles together and the other problem is in some sense a special case and that is you're given a single tile but you are given unlimited copies of it can you predict if it can close up to forms vertical shells and circle shells of just different sizes and can you predict what are the other sizes meeting a certain kind of us you know a stability function or a scoring function of a proper assembly but like all work you know one always rests on this on the UM the shoulders of giants so I'd like to acknowledge people beforehand so biology and math are kind of intertwined and and many of the names that you see here are senior people who have interacted with over time and I owe them a lot of thanks because because you know they've been nice for their time to first humor you into in trying to understand what are the basic principles of fair science and also willing to learn as to what our science is so some people that you see here is just a recent math PhD student of mine andrew gillette now he moved to Arizona so he and so his goal young-chul now he's in Beijing and Rizal Charlie is another student who is a sunny story book so this is a mix of past students as well as lots of senior investigators I've met over time and today's work I guess impinges a lot on stuff I used to do with arc Olsen at ESRI also and EEMA Cannon and a little bit with my co-host and tomorrow's talk I'll tell you a little bit of what I used to do with watch you and Steve lucky and Tim Baker and also recently with Manfred owl this is the current snapshot of people in my lab as you can see it's all these fluxes and you know wanes but the students I've had are from computer science sometimes from mechanical engineering or chemical engineering or mathematics so what's the problem I'm actually I could use this I'm saying well let's draw the analogy first to 2d jigsaw puzzles you've given a bunch of pieces can you go back and fill a hole and in this classic version you know the final picture with no gaps and it is kind of a visible proof that you've got thing right if you have any piece out of place you can also notice it you know locally or globally score it same thing is for you know oops I should know at a point yeah so is for 3d jigsaw puzzles so you can go back and assemble if you have a artifact you know architectural piece and that gets broken up and you have all the pieces and can you put it back together and suppose you don't have some piece that problem can also be counted as well so at least get some kind of a scoring function that tells you that the pieces that you have are glued in the right combination and and a different version of this problem is in building 3d navigational maps or layouts that we all now getting familiar with with with mobile phones but you can go back and put buildings together or artifacts together by taking point cloud scans or laser scans or lidar scans and then it's a question of fusing them all together to build up a three-dimensional multi-view reconstruction so all of these problems are reducible to the problem that I will be posing the the special case of course or the cases that are for structural bioinformatics are these are proteins or maybe even a combination of proteins and nucleic acids can you go back and predict a multi molecular assembly or macro molecular assembly and also if you're given multiple copies can you go back and predict how these assemble together with some kind of a global symmetry that is requisite in MO spiracle tilings so with that background let me jump right in and and try to formalize this problem in a common statement and that is suppose you are given say n components and these components are flexible of course the rigid case is a special case but they are some other flexibility what is the problem then the problem is to really come up with the set of transformations that you can apply to each component such that you can go back and transform them and map them into this complex so this is like taking each component and mapping it under its Granger rigid body and flexible transformation and so this complex is the assembly problem but what we have to do is since it's a model we have to come up with a scoring function how do we score or what this complex is what is truth and so we have to also come up with a plausible and well-defined scoring function and once we have that then the problem becomes complete the set of transformations such that the corresponding complex maximizes the score and the scoring function is monotonic in the sense that if you have some missing pieces the scoring function will only improve if you put all the missing pieces in there so when you get the final complex the score of the final complex should be much higher than if you had some pieces which you're missing and since most scoring functions are going to be highly non-convex and every time you're casting a problem into it as an optimization problem you don't want to report just a single answer what we will do is come up with a distribution of answers and this is what we call we want to come up with K such solutions ranked by their scores and additionally by something I have introduced call uncertainties and in general if you were just to think of this the sports face is clearly exponential I mean you've got all of these you know components and each one of them has some order of confirmation so if L is the average number of internal degrees of freedom for each component it can have their own choice azar each component each conformation then this is really an NL space so you can see that this thing goes very very fast and hence it becomes a combinatorial nightmare what about the two body problem and this two body problem is you know well-known in in structural bioinformatics and that's coming up with what we call the molecular docking problem so if I give you two molecules saying can you go back and search for the best relative conservation and relative conformation that yields a complex with the minimum binding energy so URI your scoring function is also an optimization or coming up with something we call the relative binding energy or that are they a lot of free energy of this molecule and what this is is roughly described here but I'll come back and give you more details about it but this Delta e or sometimes uses Delta G is the scoring function for the complex - the scoring function of in each of the individual components and if this difference is in some sense as negative as possible then these two molecules are better off being in close proximity and also docked with each other and so they have higher binding affinity because they can minimize their free energy by coming together rather than being isolated and so this is then a plausible scoring function at least in the case of molecular components in the case of rigid bodies or other kinds of applications you have to come up with similar scoring functions and I won't go into that much detail because I only have an hour today so we worked on this problem actually several years ago and in here was a way we mathematize the problem and also came up with the solution and today's talk will be live on so it's good to a little bit understand the details behind these so what we did was we built a scoring function which was somewhat asymmetric and so very soon you'll understand why that is it's an asymmetric so we took a molecule a and we thought of a grown layer we call it the skin of the molecule well that molecule a was a collection of atoms with the covalent bonded structure and this molecule me we took the first layer of solvent exposed atoms and we called it that as the skin and we said that if these two molecules have to come as closer together as possible in a dark positions and for biophysical term called the van der Waal energy that's what you would like to have then the best overlap would be that the skin of this and the skin of this dock together very similar to what people often refer to molecules come together as a lock and key and we were like to penalize if they were sterically interfere with each other because the electron density of this and the electron density of lists repelled each other so hence we don't want any skin core clashes or core cold flashes so one way to do this and set it up as a scoring function was to think of a complex function defined of these skin and coal so if you take a real value and associate it with a skin and you take a complex valued function and associate with the core the pink then real-world product would be real royal complex would be complex and complex complex would be a negative real so you can then integrate over this product and set up your scoring function to be this complex valued affinity function producted with the complex valued affinity function over the space of transformations relative transformations are molecule B with respect to model a so and you are summing this up over the entire domain of skin skin skin core overlap and so this itself is nice for two reasons a you've been able to capture exactly what is the configuration you want but being you can search over this over all possible relative transformations and if you look at this function it looks very much like a convolution function if the convolution though not over the spatial variables but over the transformation space we call this convolutions over motion space or transformation space but that having done that gives you the idea that oh I can speed this up because convolution can be done very fast you think fast Fourier transform and that's the thing we exploit so we're going to go back and go back and compute this optimization function by sampling the entire space of transformation because we don't know where the solution is we can bias the sampling some saying clearly don't sample in the middle of this molecule if you're moving this molecule around so sample in some region like a narrow band around this molecule so that kind of banding and adaptation can be done but within that we don't know the exact configuration that would maximize this function but we're going to use the fast for this scoring function what we choose is actually a biophysical and also a little bit of a knowledge base potential biophysical is is Paul in the biophysics literature as an MN salvation energy so the MM is the energy of the molecule in its bonded structure the internal energy this is the fact that the molecules are all in solvent which is predominantly water with some salts you know ionize potassium chlorine and so on the positive and negative ion so this is the internally of the molecule this is the energy of interaction of the molecules with the solvent and this is the thermo dynamical where T is the temperature and s is the entropy and the fact that these terms can be then further parameterised in terms of the internal energy is the fact that things are bonded for this bond stretching there is a certain de Hedra Langille energy there is also something with the portion about a bond and so these terms are expressed as the bonded terms and then there's the fact that molecules when they are bonded together develop a partial charge distribution they can be both a electrostatic and a van der Waal interaction already an atom consists of an electronic loud centered around a nucleus which is positively charged so a neutral atom same amount of charge around it protons inside the nucleus but the protons of one can attract the electrons of the other positive attracts negative but not too close if they get too close and the electronic clouds will interact so this is often expressed in this parameterised function called the Vander Waal energy which is that you get a net attraction of one over R to the sixth of the positive attracting the negative but then you get a repulsion which if they come too close so two atoms can come very close start interfering with the other this energy will go off to infinity and of course these exponents you know our empirical that's why this is often called an empirical scoring assumption you can have different types of coefficients to balance out and these coefficients have out here are depending on the atomic types or biomaterials these are nitrogen oxygen and and hydrogen maybe some sulfur but there's the columbic term called the electrostatic term which is also to be calculated so what's unique about the mechanical energy is that this is automatically a a pairwise summation that has to be estimated and that's quadratic time and if you're going to score this for every conformation this is probably in a loop of your calculation and so quadratic time for most average proteins is prohibitive and so what you want to do is start to make that as fast as possible so a lot of research went in early on in the computational biophysics literature and algorithms were to make this linear time calculations because these are configuration dependencies RI J's are the distance between the atoms so every time the atoms configurations relative configurations change you have to recalculate this energy to make matters worse the solvation term is oops I didn't press that it moved okay to make is this salvation' term is the interaction with water and that's equally important about you know roughly 50% total energy and they involve terms some of them easy to compute but these last two terms the dispersion energy and the polarization energy have various theories and models of computing them one of them requiring you to solve the Poisson Boltzmann equation which is saying the total potential is equal to the charge density these are the fixed charges and these are the mobile charges which are caused by the salt ions around it and they are in a Boltzmann like distribution and solving this equation which has a varying dielectric which is given by epsilon R is fairly complicated it's not a called an elliptic pde it's a model with this varying electric a nonlinear effect so solving this poison Boltzmann becomes this thing that lots of people worked on and so did we and what you're seeing here is for this binding for a much super virus binding affinity the Lu is negative and the red is positive or the other way around but it's showing you the petrol static potential which is far-reaching potential this is why molecules when they interact it's like you know a potential pool that causes them to close encounters and so the far-reaching effects are as equally important an alternative to this is a summation formula called the bond energy calculation and today's technology is that roughly you can do this much much faster in linear time you can also solve this in linear time constant in front of this is much larger than most of the time we prefer to use generalize born if you want very large estimations but if you want more accurate binding affinity estimations in you go and switch to portable and there's not the ultimate if you want to do even more accurate then you have to do some quantum interactions as well these are empirical models but they're good enough for the case of docking what's the other problem besides the scoring is this huge combinatorial search and the reason why we are interested in science most computer science Mattox use with search and score it and so this is what attracted us to the originally and also for this assembly problem so what is the search space between two rigid objects or one way you can think of it is you can parameterize it by six dimensions three rotations and three translations so you keep this molecule fixed and you rotate this oriented relative to this in three degrees and you can translate this relatively in Xyz but there are other parameterizations of this and as I'll show you in just in a second a different parameterization is actually better off or fast Fourier transform this is thinking of six dimensions can be reaper a maternity end this molecule in we orientation direction that's rotation in Ori and this also in create but one degree of orientation or rotation of this molecule is dependent on the rotation here exactly the rotation about the line joining the centroid so this has only two independent degrees of freedom and the last is the distance between them that's the translational degree so there's six dimensional rigid body transformations with five rotations and one why am I getting into liver detail because there are two things that we are always interested a accuracy of a prediction and then after that we are interested in the speed of Aqaba and just really this choice impacts the speed the accuracy comes from how well do we sample and how well do we score which captures reality another problem that we have to take into account is not molecules are not rigid and so we have to identify what are the rigid domains and what are the connections between them and this by itself this flexibility model is a huge prediction model so we make some approximation here which I have you about but even all of that suppose it had some L degrees of freedom in the total space of flexible docking or just a pair becomes three outlets thickness and for n molecules it becomes anytime three of them goes very fast over when you have symmetries of your overall structure and this Oh out of what I you know and a nice discussion earlier on with both Barry and yang bang was how do you go back and take a structure and find out which parts are rigid relative to each other which group of atoms are rigid in which are a nod their mating methods out there what we adopted is something a hierarchical partitioning based on simple normal mode analysis but it's hierarchical in that we don't group these atoms that atom basis we can move them into the bottom line is that you can identify these rigid domains those where the group of atoms are moving all with somewhat the same mean motion and having very small variance in their motion under a certain model of energetic and once you identify these domains you then have to worry about what is the parametrization of the overall molecule is what is the inter domain motion and we cast that as a motion graph and based on the connections between these rigid domains there can be multiple linkers as well as flexible loops and other kinds of living domain in place or she obeys one comes up with a ocean parameterization saying this has that many translational degrees of freedom and that many rotation of the those are the degrees of freedom that I was talking about when we talk about flexible for example for this molecule these are the primal in some sense parameters that we make this oh oh this is the flexibility model we've chosen for this tile now this might not be accurate but it's the dis model that we are building up and there's some uncertainty that will be attached to this model one can improve this by doing some reachability analysis saying I have the structure in two different confirmations with this model can I go from this table here under this parametrization to this other structure and minimize the distance then you're saying you've got some confidence that these two models are this flexibility model is reasonable but you can also be you know more conservative and happy would agree the freedom or you can be more liberal all and have many more degrees of freedom if you want to go down to the atomistic level and the total number of degrees of freedom would be three times the number of atom that becomes prohibitively large so you're always looking for the reduced space which you want to search the next little notion about is although this is a pretty sampling of motions since we're going to do this company you know computation who go back and do it we want to do it on the computer we do it it's great ask for your grandpa so every motion even though it's a whole we want to digitize how do we choose what are the discrete spacings between them how translation spaces are easy to do you can come up with uniform spacing in Cartesian grid and this will have what we call low discrepancy which I'll define in a second but this is a nice challenge problem also in Hilbert's lists of problems how do you take orientation space which is the special orthogonal group which is like akin to the solid sphere how do you distribute points on the solid sphere such that the distance between any two points or if you take the distribution of distances between all neighbors they are all equal all uniform sampling of the solids and coming up with different schemes to doing that if you did this simple naive Euler angle which are the three Euler angles which is the rotation about the x y&z axis and sampled them uniformly you would get what I call biased distribution or not what up although discredits but other non-uniform adaptive samplings will give you better distribution the good news is you don't have to Aion randomization you can actually come up with deterministic methods of call homily and so bowls include what is discrepancy saying that I get some measure of uniformity and how do I measure this uniformity saying let me take any subset in my space and my overall space is you know some X and my little subset is are a number of points that lie inside are divided by the total number of points should roughly be equal the ratio of the volumes of the subspace so the measure on R is like though in two dimensions would be the area in three dimensions the volume and so on so if I count the number of points that lines ID are divided by the total number of points and this difference from the ratio of the volume should be small if it's zero then you've got perfect you know uniformity but this should be true for any subset of R in a certain family of subsets if I took all boxes all and big but they were all parallel or I synthetic but this family of are I will find to find the ones which are the difference between these this ratio and if I can show that this total value this suit is small very small and I said I've got a uniform sampling of so the other good news is it's a good measure but it's also that you can do this not by randomly but by deterministic has been the advent of what is known as quasi Monte Carlo sampling so there's a fixed sequence and I don't have time to tell you but Alton and Hamersley points but you can already see I take points generated randomly as you do in Monte Carlo sampling from some pseudo-random generator then you get a certain nice distribution of points but I can give you a deterministic sequence where the discrepancy is really very low you see all of these volatiles don't know or annoy or annoyed tessellations are you decompose given the set of points into cells such that every cell surrounding a point these points are closest to this point than to any other point well cells are all partitioned by their bisectors so touched you know grouping and taking Hemisphere points are low discrepancy point reason we want these they have bounded distances between them they have close to uniformity and we need these low discrepancy bounds to produce all uncertainty bounds in a prediction it's how do i sample if I pick most number of samples and my arrow shoes go down and but if I take more number of samples I should have them with low discrepancy just increasing the number of samples is not going to do the trick because you could all be clustered in a certain corner but you want no discrepancy bounded large number of course your errors will go down probably that's what then we come down to this last rigid body and I'll go a little faster because on but this is the detail in the in the speed-up but like I said accuracy is more important speed up but speed up is also essential is when I'm computing this I'd compute this convolution naively over the motion space then I have a description for a and a description for me and I sample this with M sample n cube samples and M cube samples I'll get something like Oh em to vi in my computation but I can do things faster using more refined competition math and saying oh suppose I did part of this convolution as a as a career some so then I can go and transform this into clear space kind of a convolution and final spaces product it for your space I can once and for all convert this into the Fourier transform of a and B compute the product and map it back the problem with this is that I can either do the convolution over translation space if I use Cartesian because they are the invariant under translation but then I can't do rotations so I get a speed-up of only part of it I get a speed-up of MQ but not for the rotations I have to do each rotation and call a 3d FFT for the translation so the next idea was what if I grouped all my parameters into the same space so if I have five dimensional rotations then I can do five dimensional Fourier transforms and hence I can get speed-up in five dimensions and the answer is yes the details are how do you choose the right basis to use farrakhan harmonics they are only a two dimensional basis so you have to do a three dimensional regular basis multiply them show them that these are invariant and this is something where we want rotational invariance these are called Wigner functions again I'm not trying to impress you by just flashing these things there I'm saying there's a little bit of math to improve the efficiency most of it has been worked out how do you apply it to your domain problem was the kind of computational exercises that we had to do everything with a bounded discrepancy and so the overall complexity the worst case even though it might look like empty the six is far less than the product of a product of the eight random they're doing many things very fast and this becomes the vein of a lot of framework every time you are doing geometric optimization of nonlinear convex convex functions you know that fine the global minimum is very hard but you can apply fast Fourier analysis to exhaustively search it certain reason and this is what has been the bane of a lot of my that makes them the software that we develop one other thing is remember that I showed you the scoring function that we had used at many terms and I will have to compute each of these terms how do I not all of these terms of other way independent how do I add them together and so any scoring function you know one has to also go through the exercise you don't want to go trial and error so whenever you have a multi-term scoring function which has something based on vander waals something raters electrostatics you want to come up with a weighted sum as your scoring function then one can use a machine learning method to go back and set these weights this machine learning method says suppose I give you a training set which has both correct confirmations and also a set of decoys then I can reduce this choice of the optimal weights as an optimization quadratic program so I can minimize this to norm on the weights with some regularization where all my weights are shifted away correctly with respect to the error so choice of weights to passion scores are very important because they calibrate your scoring functions we have to do so so far what I've told you is only the two body docking but I'll quickly lift this into the combinatorial so what you're seeing this very busy chart is you know the fact that each of these terms does make a difference this was the electrostatics and the Mosman interaction take away from that little video was positive and negative cancel each other hence maximize or minimize the IE and so this is you know the reason that positive attracts negative events so it is complementarity in the potential that's very good for the mining but it's also evident why this chart here which is color-coded and maybe difficult to see back there so I'll just read out a few of this term this is on some benchmark there are certain benchmarks would be in community we looked at several of them and in the papers that I'm mentioning here are many more benchmarks and tests and comparisons with other people but so bars are representing how many predictions that we get which were very close to the true solution so in some of the exercises you know the molecule a and B and also you know them in a complex form and so you start out with a and B you predict the solution and then you compare it with the true result under the bed and by doing this what we found that as we added various terms and just called them based on just the end of all or just the electrostatics or the hydrogen bonding or the interface propensity that we kept improving our predictability and this predictability improves up to a certain limit where it starts to plateau why because these terms are not all independent and the choice of weights is something also some terms are not present like the entropy when a binding affinity we have not estimated the relative entropy so they are still what ought to be done and but at least you're getting to a pretty high level of predictability but when these benchmarks was not satisfactory so one of the things that to work on this last year is saying you know we do a lot of validation of our model we also do lots of verification of the accuracy of the model the process of validation and verification in most predictions but we should start doing a lot of uncertainty quantification it relates both to the validation and the variability what is uncertainty quantification it's like saying can I take a uncertainties which are known or unknown for measurable quantities in the physical process of how the input data was created and propagate that building a certain bound on the uncertainty of our result so for example in the molecule when I was looking at these functions I now have functions which have unbounded in certainties propagated from the uncertainties in the input so the input was PDB structures they are not only their positions but they also have a certain thing called me a temperature factors the larger the temperature factor the more uncertainty there is in the in the diffraction analysis or the position of each atom so we want to take these uncertainties in the input and propagate them to the uncertainties of a function and then look at the process of our integration and bound the uncertainty in our in our calculation so both a miracle and and and I mean the verification and validation should be qualified with an uncertainty variance power and this is not new this is what time they've been looking at the likelihood of a value that they achieve or they looking at confidence intervals also measure what's called the standard error of their sampling but what we in to be the science at leas and others have started to do this is the certificate that the over mathematical and computational model which guarantees that the probability that I have of my solution on the true solution is bounded founded for a certain distance specialty what that means is that my prediction if if you give me a value of T then I will also compute the probability of how good is my probability with respect to this epsilon or vice-versa you give me an epsilon of confidence that you want you said give this to me with 90% probability I will tell you what T does my process of a community so I'll prove this Chernoff bound which is something that I would like to attach to my prediction and such uncertainty V quantifications or certificates I think is essential for both our calibration of how we achieved the best we can or can we do better and it's also a way to compare between so there are various ways to compute these two not bound and one simple way that we've adopted is known bad ideas of medium it is the method of bounded differences of concentration it's saying over I can compute this uncertain or Chernoff bound by going back and computing the maximum deviation in each of the variables and then summing it up so you compute the maximum variance with respect to first you think of all of your input variables to be random variable but when you compute e CK's you can fix all the others and just look at the variance with respect to X K and if you can compute them for each one of the variables one at a time and the univariate variances can be lifted to a multivariate variance bound by this expression some of the SI K squared over the two and see that the greater a distance a you know the higher the probability of your of your prediction but the tighter you want to make your predictions from the resolve smaller the probability based on these variances individually weighted - and it's computing me CK is is where there's low discrepancy sampling plays a major role if you have bounded discrepancy or quasi Monte Carlo sampling then I can determine all of the docking so with that where I've spent a lot of time in explaining you know where the body docking problem which is now the foundations let me quickly jump and tell you about the n-body problem and also the assembly problem so what was the remaining tasks to be done I say it's now reduced to a combinatorial search so suppose you're given the set of components which was y original puzzle pieces I'm trying to predict this those well and I already have a ability of quantifying you know what is a pairwise interactions between every pair I have developed so don't ye quantify err wise docking I can put all of this into an assembly graph I compute a graph each component is the vertex and each edge R is giving you a confirmation a transformation and a score with a certain uncertainty and multiple edges between every pair is saying there are many choices of different confirmations which all very high and with low uncertainty high scores with low uncertainty just because the polarize docking problem is also non convex I can't give you a unique answer I'm going to choose over various confirmation this is important as you'll see in a minute he were to take the best configuration between a and B and the best configuration between a and stay if I not get the right out what you have to do is search over possible high ranking for search with the risk so this is what I'm showing you in this little piece and then I can go back there's the best configuration between a and C is 12 and this configuration another completion of 12 when I put these two together I choose these two Reds I get this so what is the puzzle problem reduced it says once you have this multi glad any choice of a spanning tree is a solution because once I fix the configuration between a and C and B and C I fix the configuration between all if you have n you have to choose n minus 1 edges such a graph or a tree all the spanning tree on one edge between every pair and you have to have the entire set of vertices connected that's called a spanning tree so the long statement reduces to given an assembly graph computed by a pairwise docking a multi graph with all your inner scores and you Q's find a spanning tree such that the global score for the assembly is maximized and well this might sound easy the problem is that the number of spanning trees is huge it's exponential and it's exponential so if you're going to search you're going to have an exponential time algorithm and this is already can be guaranteed because even simpler versions of this in particular the monkey puzzle is including is np-hard means there's no no no improvable polynomial time out what is this monkey puzzle suppose you're given you know this by three arrangement with little pictures of monkey and think of this as a molecule with some attribute functions on them and you want to fill this entire complex I said all of the pictures are completed what one can show is that this monkey puzzle problem reduces to our combinatorial assembly problem and hence this problem being entry hard combinatorial and assembly is also empty but we don't stop there what we do is we search for what are called polynomial time approximation scheme and the bad news is that even polynomial time approximation schemes for or multigraphs is hard both there's no known solution and three solutions are very well they can't guarantee you bound from there just to show you two methods that people have in the past one method is that of in bar I mean you know how and Wilson back in 2005 saying let's do the greedy search and see what kind of solutions we get they said we'd take a fixed number of solutions let's say some D and we'll grow these clusters incremental I start off with individual components for my puzzle a B and C here is a three but I'm gonna keep intermediate clusters up to five so each time I take the best configurations and since B and C had pairwise docking there were two of them they will form a group and if I continue on I go back and get at the end I just panicked rate and the best ranked solution I thought was this notice here what happens is that you have taken the best ranked solution each step and not search for suboptimal solution because the problem is np-hard you can just take the media Pro the video push will not work but that's what they were doing another wines method is very similar for trying and one and tell you why this approach was not correct and that is here if you look at this configuration this configuration in between a and B was a suboptimal configuration it had a score of only 8 and this total sum of course sums up to the maximum score and this was a solution of the puzzle because I built this example to show you why the work but of course this example could occur in general so what we developed then was a algorithm which has three major parts we first did crepin see a candidate solutions to go back into solve me I was talking give you the valid pair wise solution I scores in low uncertainties we learnt our scoring terms both for the use z-scores that young had also worked on and also this uncertainty quantification we looked at each of the scores and we scored the sub crease with domains and edges and furthermore and most importantly we were widened our search space well home sub optimal solutions were built in as they came along so with this we went and implemented this and slide it on a few set of examples and right now this is something that we are constantly improving what I wish this is the case of a this is actually a complex I think that was years ago are also my interesting this is go back and build these clusters or grow these spanning trees with various related clusters where our weights are not only on e code but also we take various combinations and complete these clusters by taking some suboptimal and some optimal configurations the final goal is these all sub tree generator are not shown in this example by using my space finally we get to a solution we're pretty close in this case everything works very well and it works well with what we call the total RMS de calculation with something like is the problem completely solved answer is no because what we have done is still ate all the prune and search method we've increased our search phase but we're still doing all kinds of exhaustive searching of various sub trees so that we can calculate our sub trees as we grow them so that we are keeping the maximal solution in there but you know what's the guarantee so the guarantee is that we are trying to prove now is this uncertainty propagation in our step think what sampling of sub tree it's sufficient I said you give me an error I'll say ok I can give you with that error so that's the next step but before we solve that I wanted to put me tell you about the solution where we can come up with a guarantee already or a special case the special case is the case of when you have symmetries in your bundle so symmetries help it reduces the search space and finite thing is if you are fixing exactly what type of symmetries they are distinct for your shapes you can quickly go back and prove or derive these general bounds so this is just showing you you know if you restrict your search page there are many complexes with symmetry we have group like symmetries or even icosahedral symmetry these occur in nature in viruses and so even the solution of reduced search space is now that I once I fix a transformation I'm going to just use and storage from the generation of the symmetry group all the other copies they have to be consistent so I don't have to check for what combinations go where in the earlier puzzle you know a goes next to be but in what orientation and we could move next to see and so the commoner toilets of the Oh what goes in next to home is what makes this much space so exponentially large with symmetry we know exactly what the neighbors are going to be based on the neighborhood so a pretty rigorous answer but there's one little other twist and how do you get something where you can build these puzzles but it close up and build a complete spherical shell like what happened virus capsid so examples of virus capsid 140 copy news is the copy given time I can't go into the detail but brings into this nice theory of tiling and Madison and so the good news that you suddenly find starting with a problem every time you hit a problem where you have to go back and learn a little section on laughs and you can pull that back and giving you an efficient or good solution and that's the great way of learning acting and I enjoy that in my research exercises that I like to do so this got me into learning about two things one is something called periodic tiling the only columns are all tiling generated by regular polytopes and they are five platonic solids and why are they only five in three dimensions so on it's actually very easy to show that this is the only the other he cited gone regular gowns can come together hence in the plane for example the most confident together to form the internal angle of a Penta Pentagon such that it will not form integral multiple of 360 and so but there's also this nice world of a periodic tilings that people had observed that you could take you know h2h starlings which i have more than one type of semi-regular tiling and also form through a periodic colleges which are not invariant under translation but they have some kind of local symmetries about them and you can do them more than one time your cars are all done with a single tire the whole triangle child so the two of shots are there are five platonic solids and if you think of them as all you know the circumscribing sphere for each one of them they will give you a tiling on the sphere and if you took the thirteen Archimedean solids and induce the tiling on the sphere they give you a layout they give you like a blueprint on which you can place protein so the discretize or tessellate the spherical space and so what one can do is and I'll skip this you can go back and come up with the following algorithm so you first take and generate all possible layouts you can use a tiling or a layout and so you can enumerate the possible regular and semi regular spherical tiling you have five 13 comedian and so with this space you can go back and have a finite space of circle tiling you also have to take certain subdivisions of the tiling but you don't start off with just one but you can come divide them and that's way you can not only predict certain size but you can also predict bigger and bigger size texts what you do is look at the local symmetry is induced by that spherical so for example in an icosahedron you have fivefold symmetry that the vertices three poles in the triangles and twofold on the engine that is what I was but knowing the local symmetries used by the polyhedral or the comedian solid regular tiling you go back and first compute those configurations by your cyclic talking all these local tiles details of cyclic symmetry time next you decorate your layout the spherical layout that you're chosen by arranging the symmetric tiles in various corners decoration while they're locally consistent might not be globally so you check for global consistent so will the sea tile configurations and they are flexible sea tiles so there are many possibilities of configuration who have an uncertainty and each one of them you go back in arrange them on has a decoration on each one you compute a global score and check for their inter tile interfaces and you optimize the one that this algorithm does not require anything which is a combinatorial with the number of components that you have it's basically how many cancellations of the sphere that you require the overall algorithm is polynomial and the e in the number of components that are used but constant in the non exponential so this one is what gives us a polynomial time algorithm for searching scoring and ranking well a lot of it is lifting the two-dimensional and the cyclic symmetric with the uncertainty quantification so in our implementation you know what you're trying to do then is I'd only go back and predict very a solution but we now have measures of uncertainty also as a measure of similarity this was nature solution if the model bio capsid and we can have multiple prediction multiple rearrangement which can actually produce something similar and you can come up with the Chernoff bound how close this and go back and see that they are nature has one kind of rearrangement of the capsid which is how these things but there are other alternative rearrangements with slightly different confirmations which perhaps might be a slightly higher energy or not energetically favorable but in the hole they might actually give you a complete global consistency for something so similarly here's two different versions of the same tiling this is native model but the model so if you wanted to design these nano shell you could go back and come up and fabricate the more actual architectural models if you want hey we'll glue together they will have no gaps and they'll have good affinity but having this predictive tool and then you to see how those are they based on your model it's like a relation but it's best that's less about so that's a great question and I agree with that that kind of study can be learned yes possibly but our door here was to be comprehensive in the algorithm we're searching for all predictions of all spherical assembly and hence what we're doing is to make a comprehensive I I have to go back and enumerate each layout and each subdivision and then search for them true true so hence finding out which ones are the layouts which will marry to solutions in nature is good but for completeness of the algorithm if I'm just saying here's a tile tell me in shape which is spherical and closed and has good affinity amongst them and so on tell me all the possible shapes you can get so what I'm doing is I'm going to search through all of these layouts and grade them and I'm saying still the complexity is bounded it's independent of the number so but your point is well-taken in that some of these might not be feasible for viral capsids and spontaneous assemblies but all of them will give you different types of solutions and if you don't search for them might not discover certain assemblies that are also feasible furthermore you know by subdividing them I can grow shells which are much larger than what occur in nature in this assembly has a certain minimum size for the genome and so on but if I wanted to build and say if I give you three times the number of tiles can you give me a shell which is complete with the same arrangement and you can predict that size - or say yes or no so one can predict all the discrete sizes in this so with that I am a good segue to taking questions and more but I stopped here I haven't gone really over time it's not again it's chicken time Thank You Jenna any questions oh maybe I'll start off so your uncertainty quantification in Chernov bonds sounds like a really appealing way to compare different methods and I wasn't clear if you'd done that when you compared to the new stuff Wilson approach and the other so out of the docking and I did this for you know things like the GB calculation which is based on the molecular surface and so on but we haven't completed it for our assembly multi-protein assembly problem this is all part of mahi Brewers thesis so so it's work still to progress but we have a definitive way using these mcdermott the deviations to go back and prove these bounds the other exercise I think that you're alluding to is how do we compare this method with others we will have to do the same analysis and see what's the best bounds we can do for UQ for those methods that way we can see if their methods are better off with uncertainty propagation or ours so that's another line of work to be done yeah yes it's very nice talk I have two questions mm-hmm so first of all when you optimize your parameters you use the training set learning how do you over I mean avoid like overtraining overhearing right yes this is very often seen when we optimize our first field so the second when you do use this fossil-free or transformation for this conformational change you're talking what's it it's that a similar or different from this Wachowskis faster for your information okay so there's two questions there first is when are we using machine learning approaches and using supervised learning how do you make sure that you've got the right training set and you're not or training on Europe and this is you know a selection mechanism for what your training with respect to so what we were training with respect to was all of our various families of interactions between proteins you know antibody antigen pens I'm you know inhibitors and and so on so from the benchmarks that we had taken we first separated how many of those complexes are grouped in each one and what we did was we took a small fraction now how do we choose a fraction we try to make it you know enough so that we could build not only positives from the benchmark but also decoys the decoys we generated you know using perturbations and the you know the fact that we wanted to be careful that and the goal was that for each subfamily of interactions we wanted to take a training set so that we can apply it to the predicting set that we were not really biasing the solution but we were doing the following that we were applying different weights to different subgroups now one other test we did was there was is the newly released Z dot four which had lots of mixture of things and so we went and did not take any complexes from that zero forth but we trained with only Z dot two and three and then applied it with zero four to come back and unequivocally show that this is you know the the optimal solution is where one can go back and say you know our weights that we are using optimal all I don't know an answer to that and that will then also be related to whether we you know kind of chose the right size of the training set plus also our procedure quadratic optimization is just one kind of case so for the uses that I have on what I'm saying is I I was I was careful in choosing not biasing my weights so that I would be or training them in some sense but I had no definitive way of pounding them because I was not making any predictions of optimality of my training set I was just saying I was just choosing them not by trial and error but by this learning method now coming to your second question which was related to the Chomsky cuts their method of all they were doing that you know FFTs so there are two differences and that's important one is they were doing FFTs in translation space only and applying for each rotation you have to do a three dimensional fifty the fact that they were also doing uniform f50s so they were taking a uniform grid and then doing the FFT in that uniform way our approach is using what's called non-uniform fast Fourier transforms or irregular fast Fourier transforms because we and also the other reason as we are doing this over rotational space so it's called polar f50s and sampling uniformly and polar are fifties is not feasible so hence we are doing low discrepancy sampling hence we have to use also a non uniform f of T so coupling those two things together we get both the speed up as well as some bounded calculations in our Fourier transform so those are the main differences between our method but no-no-no so the icosahedron and dodecahedron are duo so you can use one or the other so you don't need to book yeah but then so is like the octahedron and the cube so you can you don't have to use separate one no no you can predict shell assemblies which are perhaps not obtainable in nature and so and then you can evaluate them for the strength of materials for nano shells and so hence this is a generic truth yeah so I'm yeah so I was just showing you the viral capsids just because we have good models and people who have actually you know this is AJ structure from cryo um and so on through a good testing cases but having gone back and build a general procedure for spherical assemblies one can build other kinds of capsid and so there's another extension of course is to not look at non spherical and that's another way of you know generalizing so maybe I can ask so you had your normal mode kind of decomposition of the flexibility does that really contribute and you get to these higher-order structures yeah so they give you so they in these in some sense it's increasing the space of what kind of arrangements you have and so when you're testing them out of how close can you get to in an rmsd sense also to a model for a viral capsid it just flexibility does make it improve because small changes in the local rearrangements do occur and if you are if you're gonna just take rigid tiles or very low dimension then you will not be getting very close RMS these to the predicted solution or some of these configurations you can't get if you don't include that we haven't tested that out as sophisticatedly as we should what we did do for when we're doing pairwise talking's we did a lot I'll tell you how so you go back and you when you choose a model a reduce model of you know flexibility for a single protein or for each pair protein protein interactions and like in rf3 dog then we first went and calibrated them with respect to any other known structures of that protein independently in their April forms in Crystal in you know or it may be in complexes with other structures so we have a and B C D structures available or we also did this with you know when we running passer when we generate different confirmations then what we want to go back and see can you start from one and reach the other and minimize the RMS days when you say reach the other so if your flexibility model is not large enough or not flexible enough then you will not be able to come very close to the other structures so that's a way of calibrating how good is the reduced space now coming up with a minimum reduce space given a structure is a nice problem and yeah you know I don't have a pool or answer to that so that's the kind of thing you have to do but it's a good question you know you can apply it to assemblies and we are trying to mimic some structures that people had given you then one can see if we increase the flexibility space then can become closer to the true solution we should be another nice thing in cases where you have multiple very close solutions sort of think of those as perhaps being you know multiple local minima and some large non convex bays is there anything in nature you sort of say well you've got two candidates for nature solution is that is there any phenomenon like these could possibly be correct and incorrect foldings in the sense that you know when you look at a suboptimal solution it might be at the on the lip of the boundary between the two troughs in the energy landscape and yeah that could be it could be but I you know what I didn't mention in the details are when we do the pairwise stockings we cluster our solutions so what we do is you know even though you're doing bounded low discrepancy sapling there's so many little tiny variations that are clustering around the same score same low minimum energy we just picked one of that cluster so we do distance based clustering of the solutions in our predictions so when we are reporting K edges for each pair those K edges are pretty distinctive so there's a cluster distance between them so we don't take too many of the same but you know one way of testing this hypothesis would be maybe take not just one from each cluster but take maybe two or three from each cluster and then put it into the engine for the assembly and see if you suddenly get something right on the cusp where you get for one set of configurations and the choices of the spanning tree you'll get something which is nice and stable but for the other one it says now because the local configurations that you've chosen doesn't add up properly because there's a ripple effect so that could occur but I haven't checked that and that's because you know nowadays we take each independent pairwise transformation and that's set to be you know one unique candidate from each cluster of the transformation space it does the actual I don't know how much you know time series you have on stuff like this because it probably a very small scale but you know in nature that camp is somehow gonna form it's not gonna somehow all come together at one time so there has to be some you know no I you're in some abstract geometric space or geometric possibility so I'm not I'm not mirroring the assembly process and so I'm not looking at the kinetics of assembly either I'm solving this not also in a dynamical system setting I'm doing this is an optimization problem so I'm saying here is the whole space and I'm gonna naively search but I can search very fast you think that's free transform and I tried to give you where the hot spots are minima are other than iteratively go from one to the next to the next to the next so another way would be you know I first build up pairwise and then triples and quadruples and and see how these things aggregate and cluster to make up the whole that would be kind of doing it in a done you know doing power dynamics and mirroring somewhat the kinetics but optimization cuts right through that and saying finally the minimum solutions and because it's non convex find me the best ones and so what we're saying is a little sample and then quickly score and pick up the best because if you don't sample well enough you can't have any bounds on how good your score is so that's the approach we've taken but the good you know approaches to consider if you are also trying to predict the assembly pathways and you know the other dating or quantifying that could be people have taken snapshots of you know and once they can maybe microscopy will get to that level or even tomography because yeah electron tomography because that way they can see the maturation process of virus capsid assemblies happening in the cell and see them at different snapshots and then looking at the configurations which are predominant saying okay well they all form in tumors and they all on hexamers and then these hexamers and printables come together and then they expand out or something like that that would be a way of predicting the pathway and I'm not doing that right thank you very much for your attention