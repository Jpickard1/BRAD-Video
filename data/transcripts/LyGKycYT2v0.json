[
    {
        "text": "[\"Ode to Joy\", by Beethoven, plays to the end of the piano.] Traditionally, ",
        "start": 16.58,
        "duration": 3.807
    },
    {
        "text": "dot products are something that's introduced really early on in a linear algebra course, ",
        "start": 20.387,
        "duration": 4.46
    },
    {
        "text": "typically right at the start.",
        "start": 24.847,
        "duration": 1.453
    },
    {
        "text": "So it might seem strange that I've pushed them back this far in the series.",
        "start": 26.64,
        "duration": 2.94
    },
    {
        "text": "I did this because there's a standard way to introduce the topic, ",
        "start": 29.58,
        "duration": 3.143
    },
    {
        "text": "which requires nothing more than a basic understanding of vectors, ",
        "start": 32.723,
        "duration": 3.191
    },
    {
        "text": "but a fuller understanding of the role that dot products play in math can only really be ",
        "start": 35.914,
        "duration": 4.239
    },
    {
        "text": "found under the light of linear transformations.",
        "start": 40.153,
        "duration": 2.287
    },
    {
        "text": "Before that, though, let me just briefly cover the standard way that dot products are ",
        "start": 43.48,
        "duration": 3.612
    },
    {
        "text": "introduced, which I'm assuming is at least partially review for a number of viewers.",
        "start": 47.092,
        "duration": 3.528
    },
    {
        "text": "Numerically, if you have two vectors of the same dimension, ",
        "start": 51.44,
        "duration": 3.594
    },
    {
        "text": "two lists of numbers with the same lengths, taking their dot product means ",
        "start": 55.034,
        "duration": 4.494
    },
    {
        "text": "pairing up all of the coordinates, multiplying those pairs together, ",
        "start": 59.528,
        "duration": 4.133
    },
    {
        "text": "and adding the result.",
        "start": 63.661,
        "duration": 1.319
    },
    {
        "text": "So the vector 1, 2 dotted with 3, 4 would be 1 times 3 plus 2 times 4.",
        "start": 66.86,
        "duration": 6.32
    },
    {
        "text": "The vector 6, 2, 8, 3 dotted with 1, 8, 5, 3 would be ",
        "start": 74.58,
        "duration": 4.528
    },
    {
        "text": "6 times 1 plus 2 times 8 plus 8 times 5 plus 3 times 3.",
        "start": 79.108,
        "duration": 4.612
    },
    {
        "text": "Luckily, this computation has a really nice geometric interpretation.",
        "start": 84.74,
        "duration": 3.92
    },
    {
        "text": "To think about the dot product between two vectors, v and w, ",
        "start": 89.34,
        "duration": 3.66
    },
    {
        "text": "imagine projecting w onto the line that passes through the origin and the tip of v.",
        "start": 93.0,
        "duration": 4.98
    },
    {
        "text": "Multiplying the length of this projection by the length of v, ",
        "start": 98.78,
        "duration": 3.706
    },
    {
        "text": "you have the dot product v dot w.",
        "start": 102.486,
        "duration": 1.974
    },
    {
        "text": "Except when this projection of w is pointing in the opposite direction from v, ",
        "start": 106.42,
        "duration": 3.716
    },
    {
        "text": "that dot product will actually be negative.",
        "start": 110.136,
        "duration": 2.024
    },
    {
        "text": "So when two vectors are generally pointing in the same direction, ",
        "start": 113.72,
        "duration": 2.846
    },
    {
        "text": "their dot product is positive.",
        "start": 116.566,
        "duration": 1.294
    },
    {
        "text": "When they're perpendicular, meaning the projection of one ",
        "start": 119.24,
        "duration": 3.08
    },
    {
        "text": "onto the other is the zero vector, their dot product is zero.",
        "start": 122.32,
        "duration": 3.24
    },
    {
        "text": "And if they point in generally the opposite direction, their dot product is negative.",
        "start": 125.98,
        "duration": 3.62
    },
    {
        "text": "Now, this interpretation is weirdly asymmetric.",
        "start": 131.62,
        "duration": 2.94
    },
    {
        "text": "It treats the two vectors very differently.",
        "start": 134.8,
        "duration": 1.7
    },
    {
        "text": "So when I first learned this, I was surprised that order doesn't matter.",
        "start": 136.88,
        "duration": 3.12
    },
    {
        "text": "You could instead project v onto w, multiply the length of ",
        "start": 140.96,
        "duration": 3.599
    },
    {
        "text": "the projected v by the length of w, and get the same result.",
        "start": 144.559,
        "duration": 3.661
    },
    {
        "text": "I mean, doesn't that feel like a really different process?",
        "start": 150.4,
        "duration": 2.44
    },
    {
        "text": "Here's the intuition for why order doesn't matter.",
        "start": 155.32,
        "duration": 2.44
    },
    {
        "text": "If v and w happened to have the same length, we could leverage some symmetry.",
        "start": 158.44,
        "duration": 3.74
    },
    {
        "text": "Since projecting w onto v, then multiplying the length of that projection ",
        "start": 163.08,
        "duration": 4.264
    },
    {
        "text": "by the length of v, is a complete mirror image of projecting v onto w, ",
        "start": 167.344,
        "duration": 4.092
    },
    {
        "text": "then multiplying the length of that projection by the length of w.",
        "start": 171.436,
        "duration": 3.804
    },
    {
        "text": "Now, if you scale one of them, say v, by some constant like 2, ",
        "start": 177.28,
        "duration": 3.597
    },
    {
        "text": "so that they don't have equal length, the symmetry is broken.",
        "start": 180.877,
        "duration": 3.483
    },
    {
        "text": "But let's think through how to interpret the dot product between this new vector, ",
        "start": 185.02,
        "duration": 4.157
    },
    {
        "text": "2 times v, and w.",
        "start": 189.177,
        "duration": 0.863
    },
    {
        "text": "If you think of w as getting projected onto v, ",
        "start": 190.88,
        "duration": 3.377
    },
    {
        "text": "then the dot product 2v dot w will be exactly twice the dot product v dot w.",
        "start": 194.257,
        "duration": 5.463
    },
    {
        "text": "This is because when you scale v by 2, it doesn't change the length of the ",
        "start": 200.46,
        "duration": 4.246
    },
    {
        "text": "projection of w, but it doubles the length of the vector that you're projecting onto.",
        "start": 204.706,
        "duration": 4.814
    },
    {
        "text": "But on the other hand, let's say you were thinking about v getting projected onto w.",
        "start": 210.46,
        "duration": 3.74
    },
    {
        "text": "Well, in that case, the length of the projection is the thing that gets scaled when we ",
        "start": 214.9,
        "duration": 4.003
    },
    {
        "text": "multiply v by 2, but the length of the vector that you're projecting onto stays constant.",
        "start": 218.903,
        "duration": 4.097
    },
    {
        "text": "So the overall effect is still to just double the dot product.",
        "start": 223.0,
        "duration": 3.66
    },
    {
        "text": "So even though symmetry is broken in this case, ",
        "start": 227.28,
        "duration": 2.393
    },
    {
        "text": "the effect that this scaling has on the value of the dot product is the same ",
        "start": 229.673,
        "duration": 3.84
    },
    {
        "text": "under both interpretations.",
        "start": 233.513,
        "duration": 1.347
    },
    {
        "text": "There's also one other big question that confused me when I first learned this stuff.",
        "start": 236.64,
        "duration": 3.7
    },
    {
        "text": "Why on earth does this numerical process of matching coordinates, ",
        "start": 240.84,
        "duration": 3.571
    },
    {
        "text": "multiplying pairs, and adding them together have anything to do with projection?",
        "start": 244.411,
        "duration": 4.329
    },
    {
        "text": "Well, to give a satisfactory answer, and also to do full justice to ",
        "start": 250.64,
        "duration": 3.551
    },
    {
        "text": "the significance of the dot product, we need to unearth something a ",
        "start": 254.191,
        "duration": 3.552
    },
    {
        "text": "little bit deeper going on here, which often goes by the name duality.",
        "start": 257.743,
        "duration": 3.656
    },
    {
        "text": "But before getting into that, I need to spend some time talking about linear ",
        "start": 262.14,
        "duration": 3.664
    },
    {
        "text": "transformations from multiple dimensions to one dimension, which is just the number line.",
        "start": 265.804,
        "duration": 4.236
    },
    {
        "text": "These are functions that take in a 2D vector and spit out some number, ",
        "start": 272.42,
        "duration": 3.507
    },
    {
        "text": "but linear transformations are of course much more restricted than ",
        "start": 275.927,
        "duration": 3.31
    },
    {
        "text": "your run-of-the-mill function with a 2D input and a 1D output.",
        "start": 279.237,
        "duration": 3.063
    },
    {
        "text": "As with transformations in higher dimensions, like the ones I talked about in chapter 3, ",
        "start": 283.02,
        "duration": 4.06
    },
    {
        "text": "there are some formal properties that make these functions linear, ",
        "start": 287.08,
        "duration": 3.058
    },
    {
        "text": "but I'm going to purposefully ignore those here so as to not distract from our end goal, ",
        "start": 290.138,
        "duration": 4.061
    },
    {
        "text": "and instead focus on a certain visual property that's equivalent to all the formal stuff.",
        "start": 294.199,
        "duration": 4.061
    },
    {
        "text": "If you take a line of evenly spaced dots and apply a transformation, ",
        "start": 299.04,
        "duration": 4.468
    },
    {
        "text": "a linear transformation will keep those dots evenly spaced once ",
        "start": 303.508,
        "duration": 4.145
    },
    {
        "text": "they land in the output space, which is the number line.",
        "start": 307.653,
        "duration": 3.627
    },
    {
        "text": "Otherwise, if there's some line of dots that gets unevenly spaced, ",
        "start": 312.42,
        "duration": 2.983
    },
    {
        "text": "then your transformation is not linear.",
        "start": 315.403,
        "duration": 1.737
    },
    {
        "text": "As with the cases we've seen before, one of these linear transformations is ",
        "start": 319.22,
        "duration": 4.287
    },
    {
        "text": "completely determined by where it takes i-hat and j-hat, ",
        "start": 323.507,
        "duration": 3.215
    },
    {
        "text": "but this time each one of those basis vectors just lands on a number, ",
        "start": 326.722,
        "duration": 3.949
    },
    {
        "text": "so when we record where they land as the columns of a matrix, ",
        "start": 330.671,
        "duration": 3.497
    },
    {
        "text": "each of those columns just has a single number.",
        "start": 334.168,
        "duration": 2.652
    },
    {
        "text": "This is a 1x2 matrix.",
        "start": 338.46,
        "duration": 1.38
    },
    {
        "text": "Let's walk through an example of what it means ",
        "start": 341.86,
        "duration": 1.841
    },
    {
        "text": "to apply one of these transformations to a vector.",
        "start": 343.701,
        "duration": 1.959
    },
    {
        "text": "Let's say you have a linear transformation that takes i-hat to 1 and j-hat to negative 2.",
        "start": 346.38,
        "duration": 5.3
    },
    {
        "text": "To follow where a vector with coordinates, say, 4, 3 ends up, ",
        "start": 352.42,
        "duration": 4.07
    },
    {
        "text": "think of breaking up this vector as 4 times i-hat plus 3 times j-hat.",
        "start": 356.49,
        "duration": 4.53
    },
    {
        "text": "A consequence of linearity is that after the transformation, ",
        "start": 361.84,
        "duration": 3.713
    },
    {
        "text": "the vector will be 4 times the place where i-hat lands, 1, ",
        "start": 365.553,
        "duration": 3.591
    },
    {
        "text": "plus 3 times the place where j-hat lands, negative 2, ",
        "start": 369.144,
        "duration": 3.287
    },
    {
        "text": "which in this case implies that it lands on negative 2.",
        "start": 372.431,
        "duration": 3.349
    },
    {
        "text": "When you do this calculation purely numerically, it's matrix vector multiplication.",
        "start": 378.02,
        "duration": 4.34
    },
    {
        "text": "Now, this numerical operation of multiplying a 1x2 matrix by ",
        "start": 385.7,
        "duration": 3.522
    },
    {
        "text": "a vector feels just like taking the dot product of two vectors.",
        "start": 389.222,
        "duration": 3.638
    },
    {
        "text": "Doesn't that 1x2 matrix just look like a vector that we tipped on its side?",
        "start": 393.46,
        "duration": 3.34
    },
    {
        "text": "In fact, we could say right now that there's a nice association between 1x2 matrices ",
        "start": 397.96,
        "duration": 4.761
    },
    {
        "text": "and 2D vectors, defined by tilting the numerical representation of a vector on its side ",
        "start": 402.721,
        "duration": 4.929
    },
    {
        "text": "to get the associated matrix, or to tip the matrix back up to get the associated vector.",
        "start": 407.65,
        "duration": 4.93
    },
    {
        "text": "Since we're just looking at numerical expressions right now, ",
        "start": 413.56,
        "duration": 2.949
    },
    {
        "text": "going back and forth between vectors and 1x2 matrices might feel like a silly thing to do.",
        "start": 416.509,
        "duration": 4.351
    },
    {
        "text": "But this suggests something that's truly awesome from the geometric view.",
        "start": 421.46,
        "duration": 3.66
    },
    {
        "text": "There's some kind of connection between linear transformations ",
        "start": 425.38,
        "duration": 3.473
    },
    {
        "text": "that take vectors to numbers and vectors themselves.",
        "start": 428.853,
        "duration": 2.867
    },
    {
        "text": "Let me show an example that clarifies the significance, ",
        "start": 434.78,
        "duration": 2.778
    },
    {
        "text": "and which just so happens to also answer the dot product puzzle from earlier.",
        "start": 437.558,
        "duration": 3.822
    },
    {
        "text": "Unlearn what you have learned, and imagine that you don't ",
        "start": 442.14,
        "duration": 2.564
    },
    {
        "text": "already know that the dot product relates to projection.",
        "start": 444.704,
        "duration": 2.476
    },
    {
        "text": "What I'm going to do here is take a copy of the number line and place ",
        "start": 448.86,
        "duration": 3.549
    },
    {
        "text": "it diagonally in space somehow, with the number 0 sitting at the origin.",
        "start": 452.409,
        "duration": 3.651
    },
    {
        "text": "Now think of the two-dimensional unit vector whose ",
        "start": 456.9,
        "duration": 2.666
    },
    {
        "text": "tip sits where the number 1 on the number is.",
        "start": 459.566,
        "duration": 2.354
    },
    {
        "text": "I want to give that guy a name, u-hat.",
        "start": 462.4,
        "duration": 2.16
    },
    {
        "text": "This little guy plays an important role in what's about to happen, ",
        "start": 465.62,
        "duration": 2.704
    },
    {
        "text": "so just keep him in the back of your mind.",
        "start": 468.324,
        "duration": 1.696
    },
    {
        "text": "If we project 2d vectors straight onto this diagonal number line, ",
        "start": 470.74,
        "duration": 3.875
    },
    {
        "text": "in effect, we've just defined a function that takes 2d vectors to numbers.",
        "start": 474.615,
        "duration": 4.345
    },
    {
        "text": "What's more, this function is actually linear, ",
        "start": 479.66,
        "duration": 2.571
    },
    {
        "text": "since it passes our visual test that any line of evenly spaced dots remains evenly ",
        "start": 482.231,
        "duration": 4.54
    },
    {
        "text": "spaced once it lands on the number line.",
        "start": 486.771,
        "duration": 2.189
    },
    {
        "text": "Just to be clear, even though I've embedded the number line in 2d space like this, ",
        "start": 491.64,
        "duration": 4.562
    },
    {
        "text": "the outputs of the function are numbers, not 2d vectors.",
        "start": 496.202,
        "duration": 3.078
    },
    {
        "text": "You should think of a function that takes in two ",
        "start": 499.96,
        "duration": 1.959
    },
    {
        "text": "coordinates and outputs a single coordinate.",
        "start": 501.919,
        "duration": 1.761
    },
    {
        "text": "But that vector u-hat is a two-dimensional vector, living in the input space.",
        "start": 505.06,
        "duration": 3.96
    },
    {
        "text": "It's just situated in such a way that overlaps with the embedding of the number line.",
        "start": 509.44,
        "duration": 3.78
    },
    {
        "text": "With this projection, we just defined a linear transformation from 2d vectors to numbers, ",
        "start": 514.6,
        "duration": 4.918
    },
    {
        "text": "so we're going to be able to find some kind of 1x2 matrix that ",
        "start": 519.518,
        "duration": 3.442
    },
    {
        "text": "describes that transformation.",
        "start": 522.96,
        "duration": 1.64
    },
    {
        "text": "To find that 1x2 matrix, let's zoom in on this diagonal number ",
        "start": 525.54,
        "duration": 3.601
    },
    {
        "text": "line setup and think about where i-hat and j-hat each land, ",
        "start": 529.141,
        "duration": 3.431
    },
    {
        "text": "since those landing spots are going to be the columns of the matrix.",
        "start": 532.572,
        "duration": 3.888
    },
    {
        "text": "This part's super cool.",
        "start": 538.48,
        "duration": 0.96
    },
    {
        "text": "We can reason through it with a really elegant piece of symmetry.",
        "start": 539.7,
        "duration": 2.72
    },
    {
        "text": "Since i-hat and u-hat are both unit vectors, projecting i-hat onto the line ",
        "start": 543.02,
        "duration": 4.877
    },
    {
        "text": "passing through u-hat looks totally symmetric to projecting u-hat onto the x-axis.",
        "start": 547.897,
        "duration": 5.263
    },
    {
        "text": "So when we ask what number does i-hat land on when it gets projected, ",
        "start": 553.84,
        "duration": 3.533
    },
    {
        "text": "the answer is going to be the same as whatever u-hat lands on when it's projected ",
        "start": 557.373,
        "duration": 4.139
    },
    {
        "text": "onto the x-axis.",
        "start": 561.512,
        "duration": 0.808
    },
    {
        "text": "But projecting u-hat onto the x-axis just means taking the x-coordinate of u-hat.",
        "start": 562.92,
        "duration": 5.68
    },
    {
        "text": "So by symmetry, the number where i-hat lands when it's projected onto ",
        "start": 569.02,
        "duration": 3.883
    },
    {
        "text": "that diagonal number line is going to be the x-coordinate of u-hat.",
        "start": 572.903,
        "duration": 3.717
    },
    {
        "text": "Isn't that cool?",
        "start": 577.16,
        "duration": 0.5
    },
    {
        "text": "The reasoning is almost identical for the j-hat case.",
        "start": 579.2,
        "duration": 2.6
    },
    {
        "text": "Think about it for a moment.",
        "start": 582.18,
        "duration": 1.08
    },
    {
        "text": "For all the same reasons, the y-coordinate of u-hat gives us the ",
        "start": 589.12,
        "duration": 3.574
    },
    {
        "text": "number where j-hat lands when it's projected onto the number line copy.",
        "start": 592.694,
        "duration": 3.906
    },
    {
        "text": "Pause and ponder that for a moment.",
        "start": 597.58,
        "duration": 1.14
    },
    {
        "text": "I just think that's really cool.",
        "start": 598.78,
        "duration": 1.42
    },
    {
        "text": "So the entries of the 1x2 matrix describing the projection ",
        "start": 600.92,
        "duration": 3.252
    },
    {
        "text": "transformation are going to be the coordinates of u-hat.",
        "start": 604.172,
        "duration": 3.088
    },
    {
        "text": "And computing this projection transformation for arbitrary vectors in space, ",
        "start": 608.04,
        "duration": 4.215
    },
    {
        "text": "which requires multiplying that matrix by those vectors, ",
        "start": 612.255,
        "duration": 3.121
    },
    {
        "text": "is computationally identical to taking a dot product with u-hat.",
        "start": 615.376,
        "duration": 3.504
    },
    {
        "text": "This is why taking the dot product with a unit vector can be interpreted as ",
        "start": 621.46,
        "duration": 4.565
    },
    {
        "text": "projecting a vector onto the span of that unit vector and taking the length.",
        "start": 626.025,
        "duration": 4.565
    },
    {
        "text": "So what about non-unit vectors?",
        "start": 634.03,
        "duration": 1.76
    },
    {
        "text": "For example, let's say we take that unit vector u-hat, ",
        "start": 636.31,
        "duration": 2.61
    },
    {
        "text": "but we scale it up by a factor of 3.",
        "start": 638.92,
        "duration": 1.71
    },
    {
        "text": "Numerically, each of its components gets multiplied by 3.",
        "start": 641.35,
        "duration": 3.04
    },
    {
        "text": "So looking at the matrix associated with that vector, ",
        "start": 644.81,
        "duration": 3.148
    },
    {
        "text": "it takes i-hat and j-hat to three times the values where they landed before.",
        "start": 647.958,
        "duration": 4.432
    },
    {
        "text": "Since this is all linear, it implies more generally that the new matrix can be ",
        "start": 655.23,
        "duration": 4.18
    },
    {
        "text": "interpreted as projecting any vector onto the number line copy and multiplying where it ",
        "start": 659.41,
        "duration": 4.657
    },
    {
        "text": "lands by 3.",
        "start": 664.067,
        "duration": 0.583
    },
    {
        "text": "This is why the dot product with a non-unit vector can be ",
        "start": 665.47,
        "duration": 3.021
    },
    {
        "text": "interpreted as first projecting onto that vector, ",
        "start": 668.491,
        "duration": 2.604
    },
    {
        "text": "then scaling up the length of that projection by the length of the vector.",
        "start": 671.095,
        "duration": 3.855
    },
    {
        "text": "Take a moment to think about what happened here.",
        "start": 677.59,
        "duration": 1.96
    },
    {
        "text": "We had a linear transformation from 2D space to the number line, ",
        "start": 679.89,
        "duration": 3.191
    },
    {
        "text": "which was not defined in terms of numerical vectors or numerical dot products, ",
        "start": 683.081,
        "duration": 3.88
    },
    {
        "text": "it was just defined by projecting space onto a diagonal copy of the number line.",
        "start": 686.961,
        "duration": 3.929
    },
    {
        "text": "But because the transformation is linear, it was necessarily described by some 1x2 matrix.",
        "start": 691.67,
        "duration": 5.16
    },
    {
        "text": "And since multiplying a 1x2 matrix by a 2D vector is the same ",
        "start": 697.33,
        "duration": 3.545
    },
    {
        "text": "as turning that matrix on its side and taking a dot product, ",
        "start": 700.875,
        "duration": 3.489
    },
    {
        "text": "this transformation was inescapably related to some 2D vector.",
        "start": 704.364,
        "duration": 3.546
    },
    {
        "text": "The lesson here is that any time you have one of these linear transformations whose ",
        "start": 709.41,
        "duration": 4.298
    },
    {
        "text": "output space is the number line, no matter how it was defined, ",
        "start": 713.708,
        "duration": 3.225
    },
    {
        "text": "there's going to be some unique vector v corresponding to that transformation, ",
        "start": 716.933,
        "duration": 4.043
    },
    {
        "text": "in the sense that applying the transformation is the same thing as taking a dot ",
        "start": 720.976,
        "duration": 4.094
    },
    {
        "text": "product with that vector.",
        "start": 725.07,
        "duration": 1.28
    },
    {
        "text": "To me, this is utterly beautiful.",
        "start": 729.93,
        "duration": 2.1
    },
    {
        "text": "It's an example of something in math called duality.",
        "start": 732.73,
        "duration": 2.66
    },
    {
        "text": "Duality shows up in many different ways and forms throughout math, ",
        "start": 736.27,
        "duration": 3.511
    },
    {
        "text": "and it's super tricky to actually define.",
        "start": 739.781,
        "duration": 2.149
    },
    {
        "text": "Loosely speaking, it refers to situations where you have a natural ",
        "start": 742.67,
        "duration": 3.697
    },
    {
        "text": "but surprising correspondence between two types of mathematical thing.",
        "start": 746.367,
        "duration": 3.863
    },
    {
        "text": "For the linear algebra case that you just learned about, ",
        "start": 751.01,
        "duration": 3.16
    },
    {
        "text": "you'd say that the dual of a vector is the linear transformation that it encodes, ",
        "start": 754.17,
        "duration": 4.547
    },
    {
        "text": "and the dual of a linear transformation from some space to one dimension is a ",
        "start": 758.717,
        "duration": 4.325
    },
    {
        "text": "certain vector in that space.",
        "start": 763.042,
        "duration": 1.608
    },
    {
        "text": "So to sum up, on the surface, the dot product is a very useful ",
        "start": 766.73,
        "duration": 3.298
    },
    {
        "text": "geometric tool for understanding projections and for testing ",
        "start": 770.028,
        "duration": 3.193
    },
    {
        "text": "whether or not vectors tend to point in the same direction.",
        "start": 773.221,
        "duration": 3.089
    },
    {
        "text": "And that's probably the most important thing for you to remember about the dot product.",
        "start": 776.97,
        "duration": 3.82
    },
    {
        "text": "But at a deeper level, dotting two vectors together is a way ",
        "start": 781.27,
        "duration": 3.283
    },
    {
        "text": "to translate one of them into the world of transformations.",
        "start": 784.553,
        "duration": 3.177
    },
    {
        "text": "Again, numerically, this might feel like a silly point to emphasize.",
        "start": 788.67,
        "duration": 2.88
    },
    {
        "text": "It's just two computations that happen to look similar.",
        "start": 791.67,
        "duration": 2.82
    },
    {
        "text": "But the reason I find this so important is that throughout math, ",
        "start": 794.49,
        "duration": 3.557
    },
    {
        "text": "when you're dealing with a vector, once you really get to know its personality, ",
        "start": 798.047,
        "duration": 4.379
    },
    {
        "text": "sometimes you realize that it's easier to understand it not as an arrow in space, ",
        "start": 802.426,
        "duration": 4.489
    },
    {
        "text": "but as the physical embodiment of a linear transformation.",
        "start": 806.915,
        "duration": 3.175
    },
    {
        "text": "It's as if the vector is really just a conceptual shorthand for a certain transformation, ",
        "start": 810.73,
        "duration": 4.562
    },
    {
        "text": "since it's easier for us to think about arrows in space rather than ",
        "start": 815.292,
        "duration": 3.447
    },
    {
        "text": "moving all of that space to the number line.",
        "start": 818.739,
        "duration": 2.231
    },
    {
        "text": "In the next video, you'll see another really cool example of this duality in action, ",
        "start": 822.61,
        "duration": 4.7
    },
    {
        "text": "as I talk about the cross product.",
        "start": 827.31,
        "duration": 1.88
    }
]