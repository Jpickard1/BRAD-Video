[
    {
        "start": 62.449,
        "text": "so long as I pass with flying marks yeah this is how "
    },
    {
        "start": 159.53,
        "text": "resources to users like now like an incubator [Music] "
    },
    {
        "start": 244.39,
        "text": "[Music] [Music] [Music] all right so welcome everyone governments are settling in so we can go ahead and get started most of you guys have been here before tools and technology seminar series thanks for coming this is the first one of this semester hasn't been here and this is a regular series we meet on Thursdays at this time Anjali is a venue to discuss tools technologies methodologies either "
    },
    {
        "start": 305.85,
        "text": "currently being used by researchers maybe in development or recently developed sort of wide range their wide scope I'm so pleased to present today's speaker there's a sign in sheet going around if you haven't signed it please do so so today's speaker is Mikey and Franco and he is research assistant professor and life sciences institute an assistant professor in biological chemistry and I believe he just came to University of Michigan this summer okay great so thanks to the invitation to be here today so is be honest a new year I didn't know this tools and tech seminar existed I think it's very relevant that I'm here dicks I'll tell you about sort of what were what I'm building from my field of science but I think it's I think should be generally interesting to people here so today I'm gonna tell you about a field that's called cryo-electron microscopy so I'm gonna motivate why we care about protein structures and just into these people don't ya motivate that tell you about "
    },
    {
        "start": 366.57,
        "text": "this technique that we use it's very powerful here at Michigan but also sort of globally in that I just wanna point out that this was actually part of the Nobel Prize this year and 2017 in chemistry for three scientists who really helped just field but we sort of think of it as sort of the field won the Nobel Prizes we've sort of arrived as a technology that lets you solve protein structures and so you can read more on the Nobel Prize web play they have a lot of really nice summary material about the technology so today I want to tell you about why you want to use this technology called try room and then why if you want to use that you now are bumping into sort of high-performance computing follow next limitations and then how we address that because our user base and our field are not experts in command line and supercomputers so how do you deal with that sort of connection where you want to accelerate time to science for scientists while we're moving kind of most of the things that people probably here know how to do which is use high-performance computing you service job Commission all these different things so I'll tell you about two stories one using Amazon Web "
    },
    {
        "start": 427.2,
        "text": "Services sort of public clouds and also a science gateway that's using Exede resources that's a new supercomputing Center so like I OEM so I think the modern sort of part of biology research today is sort of we want understand how things go from atoms so organism and how that connects at its fundamental biology level but also how disease shows up and in the case where our changes and mutations and proteins and parts of the cell can be too regulation and so for us we want to know what's happening at the molecular level so to get us there we start at sort of the level of the human body when I really think about most the time ourselves so cells are really small compared to the human body these in the order of microns a cell is made up of many compartments these compartments are sort of packed together with lipids proteins DNA RNA and these are all moving around all the time the question is how's it all come together so if that's the cell if you were to zoom in even further you would see what in this case illustrations would look like of what the cytoplasm of the cell what the interior looks like and these are all "
    },
    {
        "start": 487.33,
        "text": "these different shapes proteins macro molecule complexes that have really important cellular function so down here this is sort of vesicles coming in and out of the cell this is a probably a virus being encapsulated and this is sort of the nucleus with DNA packed together in the shapes of these proteins is absolutely critical for what they do and that's the fundamental part of sort of where we are as a structural biologist which is part of it's like stamp collecting because you wanna know what they all look like they look part of it is sort of putting them all together and figuring out how they all come together to do these are amazing dynamic processes which is sort of cell biology so to give you sort of a flavor of this if you just take a subset of these and try and scale them with a cell they're these really small pieces relative to a giant cell this is a subset from the PDB and image from the PDB but these are all different shapes of proteins or macromolecules so things that you might see here that you might recognize if you're used to protein structures would be that's a virus there's a ribosome in here somewhere I think I saw this is a proteasome that can degrade things this is DNA shown right here so these are the protein "
    },
    {
        "start": 548.95,
        "text": "shapes that in these shapes determine protein function proteins are made of amino acids so amino acids are what you get from your food that you eat and those are get incorporated these specific shapes and that's the chemistry's of amino acids that determine shapes and that's sort of amazing part of biology is that chemistry determines shape and so we have about 21,000 genes that may be form you know on the order of hundred thousand different types of protein or macromolecular complexes perhaps and most of those structures are unknown out sailin that's a big gap more knowledge of understanding biology so the central dogma is that your DNA and code sequence of the proteins and that if you want to say this in the context of medicine its mutations in DNA that then results in proteins that are doing the wrong tasks that then did leads to some kind of disease so cryo-em is a tool that lets us solve protein structures using a technology that's called cryo-electron microscopy and that's what we'll be talking about today so why cry OEM I'll keep talking about "
    },
    {
        "start": 609.07,
        "text": "why what the technology is but really the problem that we have is that the technology we use gives us low signal-to-noise ratio data which I feels like which I feel like is everybody's problem today which the arrays data is not as high signal as you'd like and so that sort of you now have to collect a lot of it or analyze a lot of it to overcome the low signal so I'll tell you about why we're are low signals ratio comes from and why it's like that so how do we use criteria or cryo-electron microscopy to solve protein structures so if this really is is cryo-em is transmission electron microscopy taking images of proteins so normally you think of this when you think of looking at cells or things in textbooks but it turns out you can also just take purified proteins out of cells and image them with a transmission electron microscope and so this is what the old technology old platform used to look like this has been changing very quickly in the field and so now we have much better optics much better microscopes that are now kind of look more like a box form fortunately but these this type of platform here is what we use to platter samples in to do transmission "
    },
    {
        "start": 669.55,
        "text": "electron microscopy of proteins if you look at it you'd see there's actually probably over 50 years of physics behind the optics of focusing electrons taking images recording images in a way that actually maintains coherent electrons for imaging but we're skipping over all of that so let's assume that he have microscope and so Michigan has these this equipment here that's also let's go talk about it here is that your work it's a great platform just for a machine in general okay so chromium is the idea you're gonna take a support sample matrix film here like a copper grid you're going to deposit onto this protein of choice so it could be a ribosome virus whatever you're interested setting but a purified protein in this case you put it here you put a very thin film of water in that in that water you actually flash freeze it so quickly that doesn't actually form crystalline water it just stops moving that's called vitreous solid it's a form of matter index like glass so glass is a victory of solid where it's all crystalline but solid and so cryo Liam does that so if "
    },
    {
        "start": 730.42,
        "text": "you freeze a thin film of water really quickly just stops moving so if your protein is purified in that buffer what you now have is just a protein stuck in a in sort of some depth so if you look at the support film and zoom in you'd have your protein of choice so proteins are about a couple hundred angstroms diameter about you know plus or minus you get a film of water that's gonna be around that thickness so what you're trying to make is water that's about as deep as a hundred to 500 angstroms which is sort of amazing that we can even do this because that's like it's a fine number of water molecules deep thank you a hundred water molecules are 300 water molecules so it's amazing we can do this we can do this you pull this thin film you get this protein embedded within reason what's that how's the freezing dog so the freezing is done great you take your your proteins are degraded you blot it then you put into liquid ethane so liquid ethane is it so ethane will cool and liquid nitrogen temperatures so essentially have a liquid nitrogen bath when the connection is not very good at "
    },
    {
        "start": 790.6,
        "text": "it has a very low specific heat it doesn't really freeze things that quickly so here to do this with liquid nitrogen usually crystal nice because it had the water has time to reorient to crystallize but with liquid ethane this passivity is much higher and so it actually would have stopped the water so at they proclaim that type of gas but in liquid form is about you know minus minus 170 Celsius around 70 Kelvin so you so you can you take your protein of choice you've been in this liquid let's say you can get there enough to take images of this with electrons and so electrons behave the same way photons do and sort of light your eyes but with magnets as lenses you take images of this you now get where you're slow signals ratio problem comes from so electrons are extremely damaging if you put a biological material in for an electron that's a higher the electron and you wait long enough it distorts melting because of ionizing damage and all these different things so you can just take your example anyone who's doing this will do at some point where you just leave your sample on the microscope too long and suddenly you sir while bull star formation starts melting "
    },
    {
        "start": 851.79,
        "text": "away and so we take a really fast snapshot before it melts and so that's fast snapshot gives us the low signal problem that we have to overcome with high-performance computing it's pretty contrast material so you could use contrast material but then what you lose is actually the internal details of the protein so in this case the just to finish that so what we have is we're looking at the scattering of electrons to the protein into the buffer you can until the intervening space with heavy metals that scatter more but then you lose all the internal details of protein so it gives you that we can do that all the time actually but we just can't solve atomic structures of proteins that way because it's essentially a shell and this is also worth saying the field is we've kind of arrived we have now better we can solve doctors or proteins but to be honest we're in the middle probably of our growth we're still there's always new sort of algorithm sample prep all these things are sort of under development I would say AK unit is common so this is what we do today doesn't mean we'll be doing this in ten years it'll be the same ideas but all again change so this is your low signal "
    },
    {
        "start": 914.07,
        "text": "image that you take so it's a it's a micrograph now proteins that pixel size of your image is about an angstrom per pixel so which is about a carbon-carbon bond so very small pixel size what you really have is this so this is what it looks like but then you when you add when you actually take the images a lot of noise is added and so the problem that we have in cryo e/m is when you take low signal data compare all these different projections of a 3d object together to recover signal to actually see where all the atoms are in your protein so the idea is you're gonna extract these regions with these extracted regions you're gonna align them together to make them all in the same orientation you're really gonna average them together or classify them then you'll average them together to increase the signal and then eventually go into three dimensions to get a 3d protein structure and so this process here is we have pipelines that this is where a lot of people are thinking now is how do you do this better and faster and in more exhaust yeah exactly so very good point "
    },
    {
        "start": 975.45,
        "text": "he's pointing out that so what you get here is the density map you have a bottle in the actual atomic coordinates and this is a model and that's a just modeling into density is still ended up itself a problem they were trying to deal with why do you why can't we classify them or why do we classify them so why are you why is possible to classify them if they are like orientation is strengthened continuously then you don't have disgraced stays where you yeah I mean so the classification here in this case we're showing in two dimensions we will actually classifying three dimensionally doesn't have arbitrarily exampled space but so that your question is how is even possible cuz consuming it's discrete projections but a continuous 3d object and so that's where the samples a you see published lay that require instructors are ones that can withstand this this test of very fine sampling of imaging a route of projections around a 3d object so there are some samples "
    },
    {
        "start": 1036.62,
        "text": "where you don't get equal sampling across the whole 3d space and you won't solve a structure of it and that's actually sort of common and we just not published so we lost circles like that will you get a sort of in an isotropic view or distribution of views of your sample and then you're just stuck and then you have to change your buffer and start doing these things to fix it but it works for some of these and so the sampling ends up being enough true but I think we all just like the surprise that I worked as well as it does now I would say because it is true it's discrete so part of what we part of this so damaged they deal with damage we can take any short movies but turns out victory these ice or this frozen ice isn't actually solid it's not like glass where it's actually not moving when svit race and being kept in this cold temperature it still moves a little bit and so it turns out the big revolution in cryo am about three or four years ago was the ability to actually take a really fast movie so it's not just an image with a massive movie where you're can go up to you know 100 frames per "
    },
    {
        "start": 1096.89,
        "text": "second or 40 frames per second and so instead of an image take a movie and with that movie you can now reconstruct any kind of movements that happened so you can imagine that if you try to reconstruct where atoms are in your protein if that moved by you know an atom or two that's gonna blow out that part of the structure and you won't be able to actually solve that work and so part of what we do now is we take these movies that sort of also have you put ourselves in the closer the big data fields so if this is what you're imaging this is a same protein structure you're imaging we take a really fast movie where every frame is actually very low signal and what we're gonna do is align these movies together to actually recover signal so in this case if you were to sort of go through and you're there's different ways of doing this but you align these together and you actually will recover the signal and what you get is a higher quality version of your image whereas before you probably had a snapshot that was moving now this is more crisp details and that's what lets us solve structures at high resolution and so what that means that are the movies we get from the microscope are around either 4000 pixels by 4,000 pixels or 8,000 pickles by "
    },
    {
        "start": 1157.87,
        "text": "eight thousand pixels times forty frames and we collect that about one per minute 24 hours a day seven days a week and so each one of these movies is about one to ten gigabytes and then you do that data set that you need is about 3,000 of those just for a pile of data set so for instance yesterday we were collecting a pile of data set overnight I was about two terabytes just to see what's gonna happen and it's not even a final they said that's just like a screening data set and so that's sort of where were clumping it as a field into this problem of like data storage we're not deleting these movies because we can still analyze these better differently with new algorithms so there's every crime lab that's about you know half a petabyte of data sitting around that usually not on cold storage and like active storage because we don't know what to do with it okay so just to kind of recap that the idea is you take your protein you're putting in a so this is a side-on view of a sample here so your protein suspended it's some sort of you know 200 300 eggs from thick water that's frozen you're taking an image with electrons you get this is now a real image from the "
    },
    {
        "start": 1218.14,
        "text": "microscope so what this is this is actually a very pure protein and this looks like you can't see anything because it's low signal and because it sort of crowded and proteins aren't perfect spheres they all kind of touch each other and stick sometimes and so in white here are shown example proteins that you can isolate other things you see that's like a crystal nice contaminant so that's what looks like when its crystalline there's other things that can show up so to walk you through these steps this is what the raw data looks like you're gonna collect about half a million to a million of these subset of these sort of regions proteins or we also call them particles you cryo am you take this you're going to subtract them extract them out and then you're going start averaging them together with high-performance computing resources so that can be CPU based or we also have GPU accelerated soft code that can run on GPU workstations but you now take these in your hour cream together and when it's actually you're operating the same object it's working you get things that look like this or you're increasing the signal-to-noise of your of your data of your image compared to the input data so "
    },
    {
        "start": 1279.16,
        "text": "things are working these are in two dimensions and you really you really want this in three dimensions that's what you will then we'll do is go into 3d analysis and so we can also do classification in 3d so the idea that you sort of a computational purification of your data to find sub states of your of your protein of choice because people who maybe know those proteins that are usually not static usually they're changing confirmations or they're doing something so usually a lot of what time do you care about what's doing so he means how does it go from state a to state B so cryo-em can let you actually answer that from the same data set in the sense that you can come in and say well here's a global view of it and now let's sort it into sub states and then you with that you can sort of interpret what's going on so in this case let's say you show us sort of some starting point some starting model you then start grouping into three dimensions using different types of analysis now let's algorithms you take this and you can do sort of subsequently so if anyone who looks like Riley in papers today what you'll see a these sort of hierarchical trees in some little figures because they're sort of using computers to purify the data into sub States what you see is that when you start off they're "
    },
    {
        "start": 1340.06,
        "text": "sort of its kind of blobby and lower feature anyone who's used to looking at protein structures down here you now start seeing alpha helixes and you're actually kind of getting at height you're sorting things correctly and increasing the precision of each model as far as representing a single state because you can imagine that this state heater was a mixture of substates and you actually separated them out into three different states and then you can get higher higher resolution so you can go through this further interactively in many many steps and you can get the structures where this is an atomic this one is a real data set that's harder to analyze but what you see these are all out but HeLa season you see these bumps on the Alpha helix use different features of amino acid side chains so this is filtered at a number here called 5 angstroms which means we can't see all the atoms but we can see sort of secondary structure elements and this gets at you question which is modeling protein density tricky if you don't know where every amino acid side chain actually is that's where software programs like your lab works on is really helpful to have sort of I Taser or rosetta or different pipelines to sort of help you kind of use chemical knowledge to model your protein besides just the density map and "
    },
    {
        "start": 1401.679,
        "text": "so we get besides having a structure we actually can infer a sort of stable regions or flexible regions because you can imagine we're reconstructing a state these are serve this structure here's probably from 50,000 different images you can imagine that some images have a static part of the structure and some where may be flexible you actually will get that in your map where you can color it by local resolution we call which is that you know this part down here is more flexible which is low resolution whereas some parts that are more stable got averaged together constructively our higher resolution so it's similar to be factoring crystallography but the the actual scale of the differences can be much more I think a B factor in crystallography is pretty narrow it serves an uncertainty parameter around where one atom is or one sidechain is in this case this is saying the uncertainty here is on the order of an alpha helix almost of moving back and forth which is I think a much bigger displacement than you could probably have it be factored but analogous idea unfortunately it's not as well formulated as a B factor in "
    },
    {
        "start": 1461.92,
        "text": "them which is sort of just saying it's this is a local resolution where's it B factors that you actually can go your diffraction spots and understand just there while I watch there's some other examples of raw data so you kind of see what it looks like so on the left here is an amazing data set of beta glucan is a sample called beta galactosidase it was published a couple years ago from a lab at the NIH what this is again a purified protein can each one of these here is a protein of interest in that on the right here actually an image where we've selected out the areas that will take to further analysis so you can see these are clumping together for instance this they're not they're not mono dispersed but either way we still can start analyzing it another image so again really the images that we get are noisy but we can sort of try to overcome that sorry you are picking those particles you are gonna find so some some people still do that manually most people do it automatically no or some kind of semi automated way but there are you can look in some methods and something will just will do it manually because they're sort of resistant to change but so because you can imagine "
    },
    {
        "start": 1523.52,
        "text": "AIT's the yet so part of our field is actually how to identify sub-regions automatically reproducibly and sort of avoid sort of pitfalls where these are probably pretty good images but they lost images where there's like a big arrogant or there's contamination there's sort of things you don't want to pick but a lot of algorithms actually picks they can't discern it very well so there's a lot of people who are fielders focusing on this problem it's it's like you notice a lot of things that you see today in cryo my chart like half solve like we definitely proof of concept works but to actually make them more robust is sort of the next phase I think so if you took this data and then sort of in this case it was about a hundred and thirty thousand particles from 1,500 of these sort of micrographs you start averaging them together you get things that are now higher signal-to-noise ratio where you can start seeing features that are related to the secondary structure of the protein began to show you the raw data this is different this image here these are actually aligned projections that went into this exact so you kind of can tell like you know these bottom two are about the right shape but these are it's harder to see so that's part of what the "
    },
    {
        "start": 1585.8,
        "text": "analysis is doing finally if you sort of push that all the way to the end and solve and determine this protein structure this one goes to one of the highest ones that we have in the field which is run two point two angstroms which means that you see densities that looks like this which is near perfect when it comes to this efficient for this field and you can start seeing water molecules and sort of coordinating ions it's very impressive most don't go this high so that just a caveat but this is a sort of a model sample that's been a really nice proof of concept for all of these beta galactosidase so it's also fourfold symmetric which helps a lot so it's just sort of a rock of a sample okay so now I'll kind of talk about the approaches that we took to dealing with this so let's say you can imagine the problem now so there are lots of biochemists or like I have a really pure protein I want us all the protein structure I don't need to grow crystals I need to take it from the microscope and gives me a structure but the problem is you're like oh well here's your 15 terabytes of data you don't have a data server probably because you're a biochemist or maybe absurd crystallographer also don't not a data "
    },
    {
        "start": 1647.63,
        "text": "intensive field so that they have maybe a terabyte of storage for everything and you're like your first dataset out of like probably five is gonna be 15 terabytes start analyzing it so the problem is this for us which is that people have symbols that probably can be solved and they want to learn it but they don't have the data into the cyber infrastructure to do it themselves so the question is do you make everybody purchase and buy in a data storage are there other ways that sort of approaching this from up a cloud or remote resource perspective and so I'm on the side of trying to push this to remote resources but there are also labs where you can do it in-house and to spend the money but the question if you doing the money what can you do or the other ways to do they're more cost effective so I would say state-of-the-art cryo-em is this which is that you have these amazing microscopes one of which we have here at Michigan and you just go through this sort of mass of Linux based custom scripts custom data movement things and then you get a structure so you get amazing cultures but in between there's all these homemade things that start stand on the wage and the users coming into the field at the same time so "
    },
    {
        "start": 1711.18,
        "text": "number of users are going up I would say expert level of knowledge is going down and then probably cyber infrastructure available for that users also going down and so this is the problem the return of things they try to deal with so how do you do this so if you think about this you could be a really wealthy Institute laboratory and just be like hey we're gonna drop you know a hundred thousand dollars per thousand dollars on really nice data storage that's backed up and then computing resources and pay for an admin to maintain it and all those things that's just very expensive and so this is possible and most clarium labs are like this right now because they're sort of the first one Thor University but then when you have so here in Michigan we've talked to probably 20 different labs and want to do this we don't have storage for everybody or computing currently and so again that's the issue so university-wide yeah this is okay like Michigan has a computing center here but it turns out we have really high memory requirements and pretty much the super cocoon the high-performance computing here is not great for us it's also you cost money which is it's like "
    },
    {
        "start": 1771.93,
        "text": "maybe cheaper than Amazon or other resource but it's not so cheap that it makes you want to use it other things could be national supercomputing centers through the XE or NSF the nice thing there you can get free computing if you kind of can stay on top of the applications and get approved but there's no data stores no long-term data storage exactly through that so one of the other ways so one way is using cloud computing through Amazon or Google in today I'll talk about Amazon and so what I did as part of the postdoc and now transitioning to my independent lab here at Michigan is building a software package to mostly handle Amazon Web Services for analyzing cryo-em data so moving your data up and running it in at the end I'll talk about a web-based platform that I built that's we're about to release hopefully soon that's built on NSF's supercomputers the astronomy or particle physics communities have any insulin things I do or not do you know it would be nice to talk to more of them I think I'm not talking you enough for them I've talked a few physicists it seems like they mostly use NSF supercomputers they don't usually invest "
    },
    {
        "start": 1833.28,
        "text": "in their own supercomputer I think but I think you're right though that they're really data intensive and so we can look down the fields for how to deal with this I think we're like the order of magnitude isn't quite as big as them so we're sort of somewhere between those two I've seen owns one of the regional along with Michigan State there between I think you're right though that we need to cause we know that the problem is sort of practical one yeah for making the problems a practical one because we all want to publish papers and not spend a time to actually think about infrastructure - yeah the intensives are also dangerous so just make sure on the "
    },
    {
        "start": 1893.76,
        "text": "same page I put his audience count noses but so the cloud does everything sort of our world around us today but for people don't know Amazon Web Services one of the longest ones and most established cloud providers for everything that relates to the website's data storage and computing and so you know we all up here what they call the application cloud when you're watching Netflix is like you're just looking at things the question is can you plug into the bottom just go straight to the machines and do your tasks yourself the answer is yes you have to sort of do it and figure out how to do it and pay for it Michael yes get a message from Brian great okay he says it's a COPI ion the NSF computing resource okay great thanks okay so the reasons you might use cloud computing or you're paying per minute there's no cost it's in this case it's per minute pay as you go you're paying for everything though in some academics don't like that but essentially industries moved to cloud a while ago academics are really "
    },
    {
        "start": 1955.239,
        "text": "resistant to this reliable backup storage I think there's actually this is hard to capture how reliable this is relative to your your local machines are duplicated in the same room I think that's not very reliable most people in have for reliable storage this is extremely reliable because it's all over the world usually duplicated they measure how reliable it is the nice thing here is flexibility so if you build a software tool into Amazon you can then distribute it all over the world and put it in data centers next to other scientists and so it makes it actually easy to distribute really complicated workflows routines whereas otherwise it's sort of always custom installation things for people don't know Amazon's all over the world they keep adding new data centers so in the US there's Ohio Virginia Oregon San Francisco and then Seattle but the other thing to keeping all of it that all of the world these data centers have all different types of virtual machines in them and it just keeps changing and growing and getting cheaper so just make sure people know it's something I think kind of formative for the audience to see what they actually have there are different extremes in Amazon for the type of machines that you want there is "
    },
    {
        "start": 2016.529,
        "text": "a sort of half CPU machines that are tiny that can do like barely a task but that's there's a very cheap those are less than a penny per hour then you have very fancy ones that are you know in the order of 100 CPUs machines with a lot of RAM and these cost $13 an hour and so these prices are what you pay for yeah oh my god Brian a business agreement AWS now right so yeah I'm hopefully taking advantage of that as I am using the assistant level agreement with Amazon but yeah that's okay so an example here so those are two extremes there's like lots of in between computing resources here so there's more CPUs versus RAM there's more RAM versus CPUs they also have lots of GPUs in this case these are k80 GPUs they also have the people here GPUs there's lots of things here so there's many different types you sort of pick your type of project it's there that's what's nice "
    },
    {
        "start": 2077.34,
        "text": "for don't you for hiring workflows we have lots of different requirements per tasks that we're doing it's not just a single machine and so you can sort pick and choose two flavors so the giving example the order of battery prices here the most expensive is about thirteen to fourteen dollars an hour and the cheapest is less than a penny per hour just to point out storage they're sort of the storage on the machine that you're using there are sort of SSD type storage you can move around the pace or some number per gigabyte per month I just want to point out this name here it's called s3 or simple storage service this is one that's extremely reliable and you pay this amount per gigabyte place Alex I'll mention his name later but just so we know there's different types of storage details don't matter that much so we originally showed a few years ago this actually works for high OEM we could this case we were spinning up CPU clusters on Amazon it worked people it was maybe slightly expensive but also definitely worked and so but since then we realized this old system we did was really manual and kind of cumbersome and so what we did was we wrap this up into sort of a software "
    },
    {
        "start": 2138.24,
        "text": "package so cumbersome Linux experience required so honestly if you think about the cloud no one really cares about how what's actually happening in the cloud you just care about the results that you get or what you're doing and so if you're using Dropbox you're using whatever you just want to do it from your computer and not care it not really know what's going on the back end so that's sort of what we built they took that as inspiration this idea is you as a user just want to send your laptop or your computer and some of their jobs and something else has all the the magic of using the cloud for you and brings the results back to you and that's what we did we sort of won't use the command line language at Amazon to write this interface here and so what we did is we took those most popular software concurrently and it's called rely on it's very powerful software this is already set up to run on top of computing clusters and so we can kind of come and sort of plug into their cluster submission routine you say like instead of going to a cluster like call our code and just go to Amazon so it's actually very easy to integrate directly into the user interface so users don't have to know what's going on they see this the exact same output files everything is "
    },
    {
        "start": 2199.71,
        "text": "the same but it's actually going to Amazon now and so again there's this cluster submission feature here and now user just says yes they select our software and they hit run and then just goes Amazon so I think it's a nice paradigm also for sort of you know data intensive fields which is that I think users really like this worth it's not changing anything about their their day-to-day computing but that in this in this case it's all sinking back in real time so everything is is local but also running on Amazon so there's many steps is these are all the steps in the pipeline to solve a structure there's a gazillion steps so I took every step here and put into the software code so you depending on what you pick it it hits go it goes my software it says oh you're trying to do this task oh we want these types of machines or these types of machines sort of does I'm kind of deciding for users because again you don't know the computing on Amazon how to move dated Amazon fast all these sort of things and we can handle that for them yeah yeah yeah so it's all in the nice things you know there's a command-line language for Amazon so it's "
    },
    {
        "start": 2260.82,
        "text": "really easy to just sort of learn it but then yeah it's sort of monitoring things monitoring when the job finishes moving data around and so relate to that is that's you run the job from your computer you start the virtual machine or what they call them instances but it turns out the fastest way to move data is through these through s3 they're sort of block storage options and this is this link is more if you were an end-user you probably wouldn't figure out how to move data to s3 and down to your machine you'd probably just SCP it or something to your instance that's the slowest way to move data at Amazon so it's nice if this sort of work works in a sort of probably the best date of movement practices you could get so limited by your your networking so bad networking means slow uploads Michigan has good networking as long as your computer's in the right backbone kind of times we talked like 15 terabytes of data so here but usually for for me here it's about a terabyte an hour upload which is about 300 megabytes per second multi file uploads to s3 so bad it's like pretty good I still take would take like a day or "
    },
    {
        "start": 2321.59,
        "text": "like an afternoon depending honestly if you were fully into the Amazon system every file is about a gigabyte takes about a minute to collect your part take about a minute to move it up to Amazon so you probably could do it in real time if you actually really thinking about it no one's there yet right now but you could probably just do it on the fly it would be smack so that's the the high level thinking I sort of walk you through sort of what actually happens in the back end to show you the idea is you're on your laptop and you're pumping out laptop hopefully not Wi-Fi hopefully like a real network connection and you have let's say 15 terabytes of data you hit run for this step called the movie alignment or the raw data alignment stuff and this is like I was telling you it's about a terabyte an hour you know this type of speed if I'm getting on good networking it goes to s3 and in this case this is a GPU or sorry this one is actually a CPU accelerated step it's 1,500 files of movies from Verona and so you want to sort of do it as fast as possible so that means spit up five of these really big machines and just run it and so in this case you can run "
    },
    {
        "start": 2381.8,
        "text": "this step that normally takes you know 24 hours locally in an hour on Amazon you upload it there but it does it really fast because you can get as many resources yeah what's the advantage of making this like a local client rather than web base the advantage is mostly that the user so the question I guess with the audience and online is the question is why why of a client versus a web interface it's because the software that everybody uses it is comfortable with is a local client so you can extract it all the way that's the later part of my talk but I think you're gonna get people who like you're not changing anything about the software that people are using so it's nice that they can learn the software and not to learn my sophomore as well or it's sort of like single I think that's a web-based so yeah these diagrams are all detailed to believe the Amazon so there's sort of security settings all these things that I can sort of set up on the fly when you launch it so in this case it took two hours whereas normally this probably takes 36 hours locally so ran really nice so I apologize for some of the "
    },
    {
        "start": 2442.43,
        "text": "cryo-em terms that are gonna be sprinkled in here you do different steps in this case you're gonna select the areas out this is a CPU based step there's other steps now that are going to be gpu-accelerated where there's different types of classification alignment we can use the there's many GP machines you use those GP machines and again all these cases are syncing it back to a computer think every 10 or 20 seconds and sync me back so it's it sort of it feels like it's running locally a few more steps or CPU base it's worth pointing out there's a couple steps here where we have to take the whole data set which is 15 terabytes downloaded to an instance and then extract all these sub regions out and they have machines that can hold up to 42 terabytes per machine so you kind of can like actually handle really data intensive steps that this actually gives a bottleneck for local machines usually is this type of sentence and so this is sort of the overall idea is that this software packages all different steps my software can sort of do any of it that you need and can run it without users ever sort of seeing what's going on in the background they just see the output to "
    },
    {
        "start": 2502.54,
        "text": "give you a sense of what this is so someone had analyzed the same data set about to talk about using a GPU accelerated workstation so this is a for GPU machine with GTX 10 70s with 16 CPU cores so it's it it's that they can do it 115 hours every step of this pipeline so that's that's really impressive the fact that GPU accelerated really helped a lot before word this had to be in a CPU cluster it's a big step just have a GPU but everybody's buying this so the question is how does Amazon compare to this and so I took the same data set and you can do it in sort of less than half the time it's not your thought it'd be nice if it was faster if it was an order of magnitude faster that'd be better half as fast is still nice but the really the take-home point here for cryo-em is that we can do it faster than you can a local machine but we also can do it sort of arbitrarily scalable whereas this machine is sort of one person one job at a time this can be as many jobs at a time and so that's sort of the take-home point not that Amazon's necessarily gonna solve everything faster because the code itself is sort of limiting in a lot of different steps I guess it's five hours what's the price "
    },
    {
        "start": 2569.02,
        "text": "point at the bottom so the first point for this so I'm not even hello it has always pricing there's these sort of bidding things not using the bidding part so the price can always be cheaper but these are ticket price for this would be as well as I think Iran our dollars so it's not like it's 20 bucks take a chunk of money it's also the full pipeline here and to be honest you probably would use the full pipeline but hundreds of dollars sort of per attempt at this and you could still spend thousands of dollars on Amazon and so that's sort of where the field is right now we're trying to figure out like when is it worth it to do this when is it worth to buy a local resource I think there's probably the solution is like both you buy some more oil and then we but you don't wait so the the points here every step here the most expensive steps here cost maybe $13 an hour that's cheaper than all of our our lease our leaves South salaries so if you're going to wait overnight for someone else's job to complete it could have finished on Amazon that time and you would have saved yourself so I think that gives me these economic arguments and academics have hate and so it's kind of this it's "
    },
    {
        "start": 2629.65,
        "text": "in this sort of murky area right now I think industry it loves us idea because they already are on board the cloud they know to not maintain local resources and so so I think we are the biggest target audience here are gonna be big labs big facilities there's some chromium facilities in the country that are a hundred users like you how do you you buy hundred workstations and that's so won't be enough you know so they like the cloud as well I think that's the you see if you like a one personal lab and you're like I'm in my own lab I'm gonna solve my own structure so I do multiple structures at once and doing one at a time serial processing followed by GPU workstation or something like that well worth pointing out the code is running is developing really quickly and last year a new software package came out that's a cpu based and so if you would have invested in GPUs you'd be slower for you to use the CPU ones so it's sort of this whole thing that feels moving so quickly that actually using the cloud is kind of nice because it means you have to commit to some sort of big infrastructure investment so besides the actual cryo-em structure determination people like modeling atomic density atomic models into the density Maps and "
    },
    {
        "start": 2691.119,
        "text": "so we picked rosetta because this is one of the software packages that we also had experience with so Rosetta is a very powerful modeling software actually really hard to use if anyone's try to use it's kind of awful to use and so part of the added value can give out is you can take normal input files from users in run tasks to them whereas Rosetta requires a lot of sort of dense things not very well-documented that we can sort of wrap that up and make it easy to use and so part of this so this Rosetta same idea this is arbitrarily scalable code sort of one CPU gets one task it's not as complicated crime analysis so means we can scale it really well I think the bigger reason to use this is honestly that this is actually maybe more affordable it gets done much faster than you doing yourself locally so it can go way faster because you can sort just ask for the Macy's you want but what you get is our software package doing steps for you that are easy step that or you would do anyway but it does it for you so that's kind of X I'm starting that's where the cloud also steps in where you can add value to "
    },
    {
        "start": 2751.579,
        "text": "pipelines and things that users probably wouldn't try and do it their their own okay so this is sort of the for me the take home point was this that you wanted if you're running software for people who aren't experts in the field you want to do something like this where you're sort of abstract away things that are not actually related to the science being done because people will just want to do science faster I think it's the take-home point and so this lets you do that you have to pay for it and people don't like that we have to pay for it but you can do it faster and that's sort of where we are we're sort of expanding it to other software packages and thinking about how this goes in the future okay so the last thing I'll talk about is like just a few slides is to highlight this other approach of sort of remote cloud computing based project here and so this is so I started a project that we're calling cosmic 2 or cosmic squared for a long acronym that mostly the idea is it's a cryo iam website that will analyze your data on a supercomputer all without seeing command-line and so that's sort of the task that you're charged with is the users going to show up with 15 terabytes of data you need to be able to upload it "
    },
    {
        "start": 2813.169,
        "text": "ingest it and run it without with it so you can't use any normal data moving protocols it do something else you have to move it to a supercomputing Center and so this something that the I kids are people of already doing the science gateway so science gateway is a term that's sort of becoming popular which is it's a website that links users to super computing resources so we're serving the same same idea as I Taser so the idea is users come in with their data we're going to move it in this case that there's a first case to the San Diego supercomputing Center and then with this will be executing algorithms and choices if ever they want to do so short term we just wanted to get people you to the super commune center to run their jobs without us coming in and telling what to do just here's a resource through website run it and that required sort of removing all these job commands and sort of centralizing software packages because it turns out if you look at cryo-em listserv right now most of the questions just like computing problems and like like the workstation issues job running issues are sort of like just new users are learning how to run things so we can remove all that and "
    },
    {
        "start": 2875.12,
        "text": "this would also be free because using an ascetic seed resource so you academics was like a free resource so long term you really connect people to storage and computing be a place where you could actually implement new algorithm and new pipelines so it's like sort of Clearing House cryo-em software and also probably integrate educational materials as people are coming into the field sort of teach them about the different steps so we're at the point now where we've built it and we're just sort of debunking it really debugging it right now but the big thing that we implemented was through so through Exede we got developer time to really integrate the software package called Globus which follow people here of hurdles because Michigan uses them but people don't know Globus is a big data moving platform but also sharing as well but the idea is this can handle this can integrate into websites and users can interact with the data through a website and then sort of let Globus and move the data between the servers so to walk you through this idea is a user goes to a website they already have this software an endpoint connector "
    },
    {
        "start": 2935.3,
        "text": "installed on their local laptop data server through the website they then can interact with their data storage and say move this directory file you click it Globus then goes grabs it and move it to where it's going so in this case for our website users don't choose where they couldn't go they just say like send it to cosmic for me because then we'll grab it and just it but the idea is users come in click Globus and then we can ingest it we can adjust you know as much data as you throw at us you can upload you know there's no web us can move petabytes if they need to so it's not an issue of actually gaining moving the nice thing with global is that it's doing all the file checking and integrity checks that you'd want in a data moving service so it's really nice you can outsource all that bonus customized and robust call so in this case we had to today but there's a Globus API that we did that we edited and so I didn't do that myself we use someone at sdsu do that but so everything we build is online and the github repo but well this is very supported because because Globus is kind of a half academic industry so is "
    },
    {
        "start": 2997.83,
        "text": "Development University Chicago so they kind of like use cases and like people getting into it so it's an API and they have all the documentation online you can sort of learn how to use it seems not that what's that what we're golden perhaps I'm not sure that is but but yeah so I think it's pretty straightforward to use it's actually very nice we're just using the data moving part of the data sharing there's all these different things you can do with it they also connect to cloud storage so Google box Amazon they can connect to as well so lets you move data at different locations so one top sdsc because the Gateway is actually being hosted on a virtual machine at sdsc necessarily started once you're in the exeed network you're am now really amazing networking between all the supercomputers in the country so the ideas that we're just going to start putting jobs other places depending on what we need and what the type of job is so again there's high speed networking between supercomputing centers so it to be really nice people to choose where this goes next currently just stsc though so the idea is users are gonna "
    },
    {
        "start": 3058.1,
        "text": "land and be able to log into our website in that once they're in here they then can have a really rudimentary website for interacting with their data so I think I'd cut out let me just sort of show you a few slides over here so I cut these out so I wasn't sure how long this gonna take I'll just show you one thing so the nice thing about Globus is that when you click login they use the it's called the it's called as a password sharing service for allowing you to authenticate with remote with sort of Google universities other Authenticator authentication sites so idea is if you click login you say well indicate with University of Michigan it drops you off and here in Michigan you know normal Kerberos web site here once they're sort of a a handshake that says Mike authenticated okay come on in and then that's our website so it also removes all user management password management security tickle which means moving it to the University so it's the other nice thing with Globus that it removes that problem for us to deal with so the actual web interface is "
    },
    {
        "start": 3119.19,
        "text": "very bare-bones just to show you it looks like when you move your datasets and you'd be coming in as sort of these kind of files analyst this is all where we're gonna be improving in the future but then users can come in they can execute the navigate along which is the type of rely on jobs I hit run and it submits to comments and blames them back sort of output text files for now and they can sync it back down so Globus is syncing things so you sure she's syncing your home directory to our computer supercomputing gateway then it's gonna sync it all back so you the files are sort of changing and updating so it's very nice and so is it available yet we're about to make it available for you yeah oh this is very powerful software so it's very nice so everything we did we evaporation then using this globus integration I think people would like "
    },
    {
        "start": 3179.97,
        "text": "that but you know we can talk more later I mean but so now we're getting to the point now where it's gonna be the user management side of this project which is that we're gonna get an exit allocation to the cosmic science gateway and week it's now distributed and so we currently have you know 25,000 GPU hours and so the question is going to be how do you let users you just don't want you to force them to do anything they want to go to chooser than themselves but you also probably have to have some hand to make sure they don't submit things that are going to use up all of their allocation or all of yours so that's where we are right now is kind of you know setting this up so users can come in authenticate with Globus then we have sort of a database we're keeping track of everything and seeing how much time the user is using and so it's nice because it lets you divide up time but then we're gonna have cut off people I think but because of our next seed if the user comes to us our with an exceed allocation we can use an allocation as well so it's it's nice in that respect so we're gonna remove something that's rated ball index by just giving them time for free but then if it is a big "
    },
    {
        "start": 3241.05,
        "text": "crime lab or facility is like oh we want to have many hours on the exede we already have it we just plug it in a no big deal we're also talking about is this is so the Gateway right now is being printed through NSF sort of grant mechanism from SDSC long term that's not sustainable and so you have to turn your gateway into some kind of sustainable business consortium platform and so it's nice with Exede is that you can paper industry can pay for time or perfect computing computing cycle and so we're thinking about how to bring on industry partners who wanna do cry OEM they can they can pay and they can we sort of can figure out a way to have them get access to sort of our expertise and cryo-em but also sort of fund the Gateway to stay open so that's where I think we need to think next but you know where does it work in progress I think this is an exciting time I think it's gonna really move a lot of people here because the people who were trying exam is on trying to do it locally I think people would rather do like this and gets get it done and not worry about any of them so you tell us for lated mixture in the same "
    },
    {
        "start": 3304.89,
        "text": "sample right so the question is lay the protein structure determination what can we tell so I took an image of your sort of symmetric mixture within that phosphorylation the raw image we couldn't tell because it's so noisy but when we start analyzing it if there's any structural difference we could see it if it's honestly just a phosphate that's been added you might be able to see it but it also might get a scream it's just very very discrete yeah I know so in principle yes in practice it depends on like what signals in its place isit form so can you find the splice isoforms probably again it gets that sort of the analysis algorithms are using are really doing kind of template based alignment of things so the bigger the more dominant part of the protein is gonna drive alignments and so if you're looking for something that's really subtle and you don't know where it is for instance in the structure you might miss it if it's like really flexible you wouldn't see it so I'd say in principle "
    },
    {
        "start": 3365.46,
        "text": "yes I mean if you look at papers coming out on source splices ohms or ribosomes where they're sort of they get a structure a part of it and they sort of say oh this parts flexible moving we can then classify that and it gets into mostly sort of the sign your project but in principle yes these are all doable this still would be hard it's all doable so you can say GPO Alice but how much stolen you'll ask for yeah so the the storage part we're starting off with ten terabytes uncomment and the reality is I'm talking with this but we're we're most going to feed into the later stages of analysis so you just only have only have fifty two hundred gigabytes per user so it means they can't give you the volume eight yet it must give you the get on destined map they must give a sort of a pre-processed data where it's been extracted in a line the goal is to do the whole thing so that's where rich in the scratch store in comment because that's petabyte "
    },
    {
        "start": 3425.7,
        "text": "scratch storage sort of dumping data in there while it's being sort of processed and then and getting a nap back out but you're right we have to sort of figure out how to do this because we pay so we actually have storage on comment that's ten terabytes we pay to use that we pay to reserve that for ourselves but we need to figure out what that should be you should be a hundred terabytes shared and that's the next ya administrate things you're non-trivial part of its figuring how to actually do it so you can buy glasses yeah so it's on oasis we have sort of our own little corner that's ten terabytes that's ours for now own you pay you pay for it didn't seem that bad how much I can tell you about it I can tell you later I'm just curious how many proteins you can analyze with the current resources so with with this yeah yeah it seems like about let's say one run of your analysis I see you have your extracted data and you're gonna run it it's probably in the order of 200 to 400 GPU hours for one attempt isn't it I'm not "
    },
    {
        "start": 3488.76,
        "text": "like you actually final so this is gonna be in the hopefully in the order of like structures hopefully like I mean it is like yeah a few hundred there's a pilot allocation because I'm gonna go back to exceed like look how popular we were give us like Timberland yeah yeah so right now this is sort of a funding decision you off the ground for sure and then we're gonna keep I think the ideas we're going to keep some of this aside and be able to have our own kind of supplemental application for users because again the application process the exede is like they really try to help you do it but still certain times of the year it's kind of cumbersome and so we want to try and like remove that cumbersome aspect without but the problem if they have our own checks in place make sure people aren't wasting our time so I think that's where we will have in our pipeline there's gonna be steps there gonna be like this job looks like it failed done and then probably because we can't just let them run things forever because most people just run Java that are just like let's just see what happens or I think it failed let's let it keep going or because "
    },
    {
        "start": 3550.6,
        "text": "there's no deterministic and point for our jobs so that's can be a big part of this so that's it I suppose I say last slide fast-growing field we need computer scientists people interested in infrastructure and all these things really excited open the class so Michigan also has a great facility here so it's a great place for anyone who is interested in sort of this is we can connect you straight to the raw data to the pipeline to the routines but also to the analysis side so thanks thank you thank you "
    }
]