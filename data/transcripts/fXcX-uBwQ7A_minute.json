[
    {
        "start": 0.0,
        "text": "welcome everybody uh we got a couple people in the room here and i saw at least uh 12 or so online i'm guessing that number is going to increase uh over the next few minutes so welcome to the tools and technology seminar series uh we are trying a new microphone technology today so if people are having any trouble hearing please uh mention in the chat box if that is the case uh if you've got questions and you're online you know you can put them through the chat we'll be monitoring that and uh we'll pass it on to the speaker if you got a question in the room raise your hand and same thing uh you can speak up and we can get those questions so that i'm going to turn it over to our speaker jonathan golov and he's going to introduce himself hello um my name is sean tagalob i am a professor in the division of infectious disease i'm a physician scientist a little unusually my background is in engineering biomedical engineering and computer engineering so my laboratory does host microbe interaction studies and part of that i develop and evaluate "
    },
    {
        "start": 61.68,
        "text": "some of the software so i'd like to talk with you about two things i've been working on to address a sticky problem okay i think many of you are going to be aware of microbiome science this idea of trying to understand how the microbes that live within this and are on this effector disease um this has really been a field that's exploded in the last couple decades mostly facilitated by the advent of next generation sequencing the ability to do large-scale dna sequencing some innovative techniques and then finally some software to handle the results of those analytic techniques um there's been a backlash over the last couple years one of the challenges in microbiome science has been one of generalizability and reproducibility of the findings one can find many many studies in the literature of the finite association between some attribute of this microbial community that lives on us within us "
    },
    {
        "start": 123.28,
        "text": "and something health disease treatment response but even when the same condition is studied twice it's been very difficult to end up with the same results likewise um even studies where there's an effort made to have a cohesive strategy one central laboratory there's there's been a consistent challenge in getting been getting results repeated again so so this kind of falls more into the level of precision the techniques have been a little bit imprecise um which has led to frustration among funders and critical evaluators of this research i want to talk about the mainstay technique for microbiome research by no means is this the only technique that's used in microbiome research but it's probably the dominant one because it's efficient it's it's it's tractable and it works well um and that is to find a gene that is present universally in the microbes in this case the bacteria that has both highly conserved domains "
    },
    {
        "start": 185.599,
        "text": "as well as highly variable domains and use those variable domains as sort of r codes to identify which microbes are present i've been using the conserved domains that flank these variable domains as um as a primer site it's an amplified pcr across a variable domain amplify up those barcodes and then sequence them um the gene that is used most often is the 16s rrnag as a catalytic rna the conserved regions tend to be very conserved every base pair matters there's no codon degeneracy like you would find in a protein coding conversely there are some protein coding genes that are considered things like cpn 60 which is a heat shock protein gene the cxms rna gene is not a requirement but it's the most common one and it has well characterized variable domains um and for gut microbiome studies before b5 has sort of become the favorite region but certainly there are other variable domains in other studies for example v1 2 and 3 are "
    },
    {
        "start": 248.56,
        "text": "favored among people studying the skin number biome if it's a little bit better or more entropy is contained in that variable region for the microbes that are typically done in the skin how do we do this well this is the same gene now just schematized out flat and so you pick the gene and you do pcr across at least one variable region and you get a whole bunch of pcr and um they go off to a sequencer they get sequenced then you get back essentially depending on the technology but at the end of the day you get a long list of these reads base pairs starting from the primer across the variable region and ending at the electronic these then can be sorted or grouped or categorized into blocks based on the actual sequence identity and then depending on what you want to do you can go on to decorate these blocks of similar or identical sequences with things like taxonomy this is this species versus that species so this technique "
    },
    {
        "start": 308.639,
        "text": "it seems relatively straightforward but like most things the devil is in the details how are the primers designed how are they selected what kind of polymerase is used what sequencing technology is used how big is this thing in your sequencing are you doing paired or reading it essentially twice one forward and one for reverse um and even just the software side of things there's an enormous degree of variability and the ideal approach here would give an accurate representation of the composition structure and similarity of microbial human beings what do i knew that so composition it can be like there's this this species is present and it's in this amount of dominance um versus that one and this often can be expressed in a matrix or an account table um which then can go on to a lot of different statistical techniques that have been developed for that that style of data you can also look at a little bit more colligative measures of the type community alpha diversity which is a measure of both richness how many distinctive "
    },
    {
        "start": 369.68,
        "text": "microbes are there and unevenness or the the relative proportion of microbes um is it's become a favored data favorite attribute that homocochlear community to look at and then finally even just in n-dimensional space how similar are two microbial communities to each other in composition and structure and there are a lot of different ways of doing that um and then finally the thing that's kind of been lost is the generalizability it would be nice if i develop a table on my data set if it's meaningful to someone else working on their own population of patients um and and and could be immediately reviewed how did i evaluate some of the tools when i started out as a postdoc i want there's a lot of different ways to do this i developed a packard called package called deckard that uses that generates in silico data which is this block i've just shown that shares the composition of healthy human gut microbiota but because it's in silico data i know the origin of every "
    },
    {
        "start": 432.319,
        "text": "single in silicon reed pair and so i can go back and do the catalog how accurate are different software packages at recovering and classifying these sequences and if you just think about this from the perspective of trying to decorate with taxonomy it can be correct it can be under called and that's not a bad thing sometimes you're just not enough entrepreneur to tell and i would argue it's better for the you say hey give me the species of this and the software package is you know the best i can do is family or genius level confidently and not give me just an overconfident false answer um and that these are both desirable outcomes you can also have something that's kind of a near miss a miscalled sibling so in this case if the goal is species you get the right genus but you get called to a different member of that genius and sometimes this is just the nature of taxonomy um as a clinical infectious disease doctor taxonomist or my enemy and so this is where my i can get to give a "
    },
    {
        "start": 493.039,
        "text": "little bit of shade towards my nemesis and then you can overtly miss cold and what i mean by overtly miss cold is you're at least one full taxonomic rainfall it is genius a and and the classifier confidently calls it as a species and genius being that is really undesirable it's okay for things to be uncertain but you don't want it to be overconfident and then depending on the nature of the filtering and the overall approach sometimes true reads from true specimens can just be can disappear um and be filtered out inappropriately by overly aggressive filtering techniques so there's a bunch of different techniques that we evaluated um using different approaches and different software and you can get the rough sense of how many fell into each category this is for species level classification some people would argue species level classification is not possible um with this technique um and you can kind of see certainly some of the software packages "
    },
    {
        "start": 553.279,
        "text": "will under call we'll we'll say look it's in the right genus or fan or whatever else but not actually achieve correct species level identification and so the thing that i'm often again looking at is the percentage of missed calls um i care about the blog and then the other thing that i've always heard about is when there's a lot of true data that is filtered out by the standard operating procedure of the particular software product alpha diversity like i mentioned again shannon is a integrated metric of both evenness and richness how many things are and how are they this is true to true and so it's a spearmint r squared of one uh perfect correlation from true to true here's all these different approaches and you can see some of them do a remarkably good job of estimating this parameter despite all of the challenges and complexities you talk about you can see the bootstrap confidence some of these approach "
    },
    {
        "start": 614.8,
        "text": "95 are squareds um in spearmint so again we're looking for monotonicity not necessarily true one-to-one but because again we often use these as a comparative metric so i would argue to monitor this proportionality is really the objective a reasonable objective here so the scale won't change although a lot of times it's quite good um this is a lot much more estimatable so in the real world why is this such a challenge well getting back to my original premise why is there been such a generalized ability a reproducibility problem like this well there's a lot of ways to get false amplicons attack polymerases are not perfect i think it's become a standard use error correcting tap which does help and there are studies that have demonstrated that folks on construction communities um but it's really true tech areas are really tricky to do the polymerase is the most likely to run into trouble on the first round of "
    },
    {
        "start": 675.6,
        "text": "amplification when it is facing template that is a lot of the non-standard bases and modified bases that occur in the actual underlying genome and then once that mistake is made it is fixed in each subsequent cycle of pcr and amplified up so these false snips can look like real real data and be really heavily represented in a specimen because of this early error that gets fixed um we worry a lot historically i've worried a lot about sequencing error sequencing technology has gotten very good um it's still a problem and the sequencer also will give us sort of confidence based on the actual law chemistry and fluorescence data which is quite helpful and useful here and then again a lot of contemporary sequencing techniques will sequence the same amplicon twice once from the four once in the reverse since you have that also it's another failsafe and so compared to pcr errors but yeah that's that polymerase errors it's a little bit easier to catch sequence of errors there could just be "
    },
    {
        "start": 737.44,
        "text": "primer dimers kind of mirrors so there's a jump over and then the really stickiest wicket when you're trying to compare from study to study is what exact primers are you picking even if everyone is like all right we're just going to work on v4 we're going to filter down to b4 there's no agreement certainly in particular historic data about what exact primers are the best times how degenerate are they what is the exact amplicon that was used and for a lot of techniques based on clustering that's a real problem you can try to trim and throw out legitimate entropy down to the common region but even that's really tricky because indels and and and inserts do happen in these variable regions and so it can be extremely challenging from a generalizability perspective so let's talk about what people do people will remove schoolable signal things so you get one read of one amplifon it's probably a sequence where you could just filter that away and that's the heuristic you can likewise just a little general and say "
    },
    {
        "start": 798.56,
        "text": "this is a sequence grant that was only found in one out of hundreds of specimens we studied even if it's real it's not going to be that useful as far as understanding the underlying biology um you can filter against a set it could be a low stringent function you're not looking for very high identity or chromology but you can have sort of a catal what you think is right and then you can use those um and then you can cluster at a percent identity that kind of smooths out some of these incidental snips if it's only one base pair you can also do this clustering on the local sequence identity i showed earlier on the software package swarm that does that and then you can use statistical modeling to deconvolute out there so you and this is some of the more recent packages that do this um so traditional techniques and a lot of published data is based upon sort of the combination of clustering at a percent identity just in the variable region usually 97 identity and then "
    },
    {
        "start": 859.76,
        "text": "adding a filtering step against a reference of known good alleles it's the low stringency so i call this a semi-close technique you are creating these sort of clusters de novo from kin software based on your own pair of data but then you kind of give them a little look over by saying and they gotta look like something you've seen before in a global does remove some through entropy and the spirit of getting rid of this erroneous entropy and so it does worsen the experimental resolution of the study and then again this does absolutely nothing to solve the problem of primer selection so on the basis of these sort of preliminary studies and evaluations and thinking through it i assemble together in a workflow a next flow based workflow i'm about to explain a little bit more in detail what that is it's not a software i think that performs particularly well for this analysis and starts to "
    },
    {
        "start": 920.24,
        "text": "systematically address some of these challenges so malianka uses the combination of deconvoluting a pcr and submitting errors with a extent software package.2 part of that is to remove global singletons and do a little bit of filtering to look for things that just don't make a lot of sense and then finally there's this phylogenetic placement on the tree of known good full-length 16s rna wheels and i will explain what this is a little bit this is excellent software that has been developed by the phylogeny people computer scientists interested in phylogenetic analysis but this is the magic that i think can start to solve some of the primer selection of generalizability problems so what is dotted to dotted two you give the reads you give all the sequencers confidence of the weeds we give it a large step blocks of breeds that were cohesively processed in the same batch of physical chemicals so i often want to say you have a 96 volt plate where you're all doing the dna extraction the library "
    },
    {
        "start": 980.8,
        "text": "that is a batch there's going to be that attack on that day and that is thermal cycler that had that you know the everything is as close as you can do you have a set of pair of controls with it dotted two will make based on the quality score how comfortable the sequencer is as well as where the base pair is in is from the five prime to three prime to end of what was sequence um and then we'll also start to think a little bit about the forward and reverse pairs and does that make a separate model for the forward reason reverse reads and then apply these statistical models to it so there's a lot of variables to fit but the one thing we're not lacking in any of these experiments is a lot of data there is always a lot a lot of data the order is on the order of probably tens of thousands of reads per specimen so even a modestly sized study of a couple dozen specimens you're talking about an in that each we typically dies on the order of 300 to 600 base pairs when you start to accomplish in reverse so there's a lot "
    },
    {
        "start": 1041.36,
        "text": "of phases with a lot of data to fit so this is even though there's a fair number of variables you are fitting here a prediction of mutation for every base pair based on quality score um for every position in each to form a reverse reads those number of variables is worth by the shear volume of data often to the point where data two randomly will select reads to try to sample but will not use all the leads because it's not necessarily convergence much more quickly um there's a nice review that i rather like um that compares there's multiple packages that do this data two the reason why i select the dotted two and i've confirmed some of these on my somewhere in civility analysis it finds more low abundance organisms without the cost of significant increase in the number of non-references so it accomplishes both goals of filtering out gunk that was not actually there about preserving the low abundance accident right a lot of times there can be some real biology that's going on and it's important to capture that's why i picked onto there's certainly other software that can do it "
    },
    {
        "start": 1101.76,
        "text": "this is just to get the first steps here it gets rid of sequencing errors get through the most pcr eras most time eras this skill as i mentioned earlier cannot solve the problem with generalizability or primer selection so let's talk about phylogenetic and what i mean well the first thing i need is a tree to place on and i use to make my true from references a software package called ari xml it is a maximum a likelihood maximization software it is the bane of my existence i am literally going broke running this it is it is i i they are vt runs up spins up a coal plant every single time i start running on this um like any software if it has mpi it you know for a worldwide so it's there it works there's a new implementation of the rxmlng that's supposed to be faster i'm not so convinced on it but it is a very beloved really well liked by people who do study phylogenetic cells and then once you have the tree i have "
    },
    {
        "start": 1162.0,
        "text": "this placement business and there's some really lovely places p places which was developed by frederick matson who's now in seattle before hodge and then has subsequently been re-implemented as epa and g solves this simplifies this problem down this is actually quite fast once you have the true complacency so let's talk about that now i'm going to go through step by step where they've been saying this multiple times and explain what that comes off you have your sequence rules this is the output from data two i use it from data two this easily could be optical output from mother if this is agnostic to how you get these sequence areas these can be ocu's these could be exact sequence variants it doesn't matter that stuff is independent i have i like data too but swarm you there's use optical use whatever you want and we have a repository of full length six and s alleles what do i mean by forward counter compass the full g both conservative and variable dominance have no ambiguous basis and then often by your end goal is taxonomy i'd like to have them have a "
    },
    {
        "start": 1222.24,
        "text": "cohesive coherent sort of by majority rules taxonomic declaration but that's not necessary that's just great if i really want to do it and start with that subset um there's a separate topic that i um have developed that on honestly was first started at the hunch by some of the lab medicine people there um that can call these directly from instantly int and filter them and bring them up i am up to millions of following six and sr rna alleles um and they just just meet the sequence criteria and then there are there are now approaching 100 000 that have actual proper full length full down to speed at least species that sound like an innovation so um most of them are even live that's fine um and so what do you do well i use uh i search just do a very low strategy search my objective is to get at least one representative for every sequence variant um and then i filter these out just for "
    },
    {
        "start": 1283.44,
        "text": "sure that can get an enormous number even at like 90 identity or relatively stringent for some identities so i do the top hitfilter give me everything that matches at least as well as our best kit and across the reference database or maybe give a little bit of room for it and then if i get a non-hit i can go back and get some less annotated sequences depending on my objective what you end up is the recruits these are full-length systems rrna award recruits then i let ri xml take these recruits and assemble them into a denoble final object other software packages do make phylogenes but they make them on the sequence which is just one variable region this accounts for the entropy and all of the variable regions in the six units are only allele which may or may not agree with the bespoke variable region that was selected for a particular experiment and this is important later on because they're full length alleles it's a lot less dependent on the primer "
    },
    {
        "start": 1344.88,
        "text": "selection in fact it's not at all depending on the cover the topography of the tree that it's generated is not dependent at all in a large experiment this will be on the order of 10 000 reference sequences that will be into the truth the goal is to have it be a little bit um bushy have a lot of leads a lot of branches have lots of bad choices to place actual sequencers on so that you can be a little bit more confident that algorithms have had a chance to kind of nibble on less good ideas before it gets to the one that it should um and smaller experiences can be in the order of thousands um this is a little bit of a trickiness because the full length system's rrna allele is about 1200 base pairs so there is a limit to how much true entropy is available to actually make the topography of the truth so that's a limitation that i often struggle with and that's why it's a trade-off between how complex the tree is and representative the tree is versus how tractable it is to ascend assemble okay and then the placement "
    },
    {
        "start": 1406.48,
        "text": "so this is what p placer epa and g does it uses and i'm not going to do it justice and i highly recommend you just read the papers um if you are so inclined but it essentially takes these variable regions which are just a subset and tries them out on all of the leaves of the tree quickly settling down to subclades that seem likely and eventually then gives you a likelihood for every node of the truth most of them are zero so they don't mention those any reports have any note of the truth leap or internal whether there's any likelihood that this particular um sequencer came from rarely does it just center on one length often the probability is distributed over a sub code rarely because of convergence if you have a variable region you can have a variation here and here that look almost identical even though overall they're quite different so we'll beautifully apply the the the probability across multiple plates of the tree and report that out and this is the end the data type that is used "
    },
    {
        "start": 1467.279,
        "text": "for everything from here and so it reflects the uncertainty if you don't have a good leaf or good clay for your your um sequencer it'll be at some distance from its closest leaf which is another metric so you get a weighted probability you get a distance from each node in the tree um and you can go ahead and use that and you can use this to calculate all the things we started off with taxonomy alpha diversity or just diversity structure of the community composition of the humidity and overall distance of how dissimilar or similar to movements from one another but you're reflecting the uncertainty all the way to the terminal analysis which means you don't have to make so many horse choices up front about say directly picking a reference or okay it's a little bit tricky i'm going to stop and make sure there are no questions on this point and then and then i'll i can set the wall in here a little bit i have questions so far uh in the chat "
    },
    {
        "start": 1527.84,
        "text": "box if anybody has questions online uh please feel free to enter them in the chat box or on youtube that works too yeah yeah nothing so far thank you so how does this work in the real world let's apply it i did some things again in silico validation but i think it's much more interesting to talk about a hard real world application of this approach so this is in collaboration ucsf in the march of dimes it's a dream challenge this is a machine learning ai challenge the objective of the challenge is to have people be able to predict the date of collection of specimen as well as whether or not that pregnancy ended in pre-turning later um based on the urogenital or vaginal microbiome and what are we trying to do we're trying to combine nine studies with over 1300 participants um divided over and one of the "
    },
    {
        "start": 1588.88,
        "text": "objectives here is to have enough specimens and enough variance to start to get into some of these narrower questions about like distribution of how ethnicity and race affect some of these relationships um pre-term labor is obviously a horrible problem that we still are struggling to really understand wow um and how to and how they would reduce the incidence of um and so important problem a real challenge to combine all these different datasets they've been clearly heavily studied each study has a slightly different outcome that it is found and so can we successfully use something like an approach that i implemented in malliambi to um combine all these case studies into a cohesive dataset okay so if we just generate sequence values these are 100.2 sequence variants these are six studies for the sake of the sacred simplicity here these are all immune analysis data you can almost kind of see which studies kind of use similar "
    },
    {
        "start": 1650.399,
        "text": "primers these two same basic primer design these two have the same basic primary design this one's different and so sorry these are organization plots these are disney plots so the two the closer things are in this 2d space the closer they are in n-dimensional space i use cosine distance which is actually a little bit more forgiving i really like it as opposed to euclidean distance to sort of um try to start to calculate without having to build the spot which i'm but you can clearly see these studies start to really segregate more by the study than by the biological characteristic when was the specimen collected and and did this pregnancy of determine or what was the race or ethnicity of the person this is after hylogenic placement and we've gone through this business and then now recalculated distance between the communities based on the weight distribution and violation of the trees and so there's a nice distance metric that's been developed they are this "
    },
    {
        "start": 1711.279,
        "text": "balance which is a derivative of the weighted unit practices for people who care about it but can account for this uncertainty and placement and wow same tisney plots the studies are far more overlapping now it's totally fair for them to have a slightly different distribution of peaks and these contour plots because some studies focused latent pregnancies some studies focus early on pregnancy some had more of different outcomes but at the end of the day the two-dimensional space and distance is far more overlapped and cohesive after this following replacement again this is a umap another sort of ordination plot same basic idea these are five studies now where we have a little bit more metadata available and so umap is just a different coordination plot this kind of you can kind of get the structure but each of these are different colors you can see they're nice and integrated there's not like an island that is all yellow or an island that is all blue and you can kind of see different cities "
    },
    {
        "start": 1771.44,
        "text": "contribute different specimens but it's really hard to say any one of these is different from any of the other this is at least to my mind very satisfying cohesive integration with different studies with very different primary designs into all each covering the same biologic site and roughly corresponding to the sort of um sound um i just want to point out some of the um now i'm going to talk about structure so that's overall distance overall community distance and composition structural alpha diversity it's really difficult i feel like this is under considered how things like normalization techniques like rarefaction and primer selection and sequencing depth and all of the rest even though i showed earlier that you know a lot of programs can fairly well um attribute it it's a real challenge i really like dr willis's who um approach to this i'm just going to make a shout out without going into too much detail about her essay and pointing out why "
    },
    {
        "start": 1832.64,
        "text": "it is important to think about the impact of variable sequencing data and alpha diversity but our go-to approach which is very different than this phylogenetic placement business of rarefaction makes things even worse okay same story not normalized shannon alpha diversity estimations just based on the sequence variance and you can see and they're now stratified by did the pregnancy and determinant in each study and you can see they each have a different baseline they're all kind of jittered all over the place and that fits again very consistently with different studies different study designs different sequencing and primer approaches as you can synthesize this is shannon alpha diversity after this phylogenetic normalization and here it's the baseline the mode is almost identical in every single study and then you can start to see again some higher alpha diversity in in the pre-term versus term that fits with some "
    },
    {
        "start": 1893.919,
        "text": "of the excellent leverage there's another one you can start to see that pattern and at least those sub-populations but again it's much much more cohesive two clear modes that start to be fit from study to study after this normalization and then finally i would like to have something for composition and we could just do taxonomic assignment and there's a way to do that the way the taxonomic assignment says is all the way the probability of the sequencers is on the clay that is assigned to gardner mellow vaginalis and so therefore it's got to be guarding all of that alex and then like i said it's in the gardnerella genius but there's a little bit of some other one so whatever it's probably vaginalis but i'm going to say guardian a lot this is how key placer does and those sort of phylogenetic hyphalogene placement deals with taxonomy we also will counter the uncertainty as i've mentioned taxonomy particularly in some of the microbes i care about in the human god is unless um you will have organisms reassigned from one complete phylum to another basically at the highest "
    },
    {
        "start": 1955.039,
        "text": "level you can go from aside from bacteria or not um the things get reassigned constantly taxonomy is fraud um and so what if we use the distance in phylogenetic space between between sequence very much so one thing i want to point out to you is you have two different sequence variants again different primers different studies but from the same unbiological from the same underlying organism they're going to end up in phylogenetic placement space basically should be overlapping roughly into the same portion of the tree so in phylogenetic space even though in sequence space these are going to be completely distinct from one another um they're going to converge and have a very short biogenic distance from one another so you can go walk up the tree and walk back down the tree and get the final electric distance between this group of seasonal variants and that sequencer and you can use these paired light distances to cluster with things like "
    },
    {
        "start": 2015.519,
        "text": "clustering and so you can get what i would call thyroids you group the sequence variance by their phylogens and sending others um again um i made a software package that can do this as a python based command line utility you can give it one of these placement files that the output from key placer epng and you give it a desired phylogenetic distance and it will give you the grouping um at that phylogenetic distance um one thing i've been working on in the branch is to be able to do this iteratively so you can add data in as vsi this is available like molly as well molly on people works on great lakes and aws and spu and pbs tour i do a lot of collaboration everything i work on is portable so it's implemented you can try it out if you really get curious about it what does it do "
    },
    {
        "start": 2076.56,
        "text": "so pick the final existence so let's start over here at sequencer in space we have each row is a study we're in six studies again just to keep this a little bit simple this is detected not detected so each column here is a sequence variable and you can see yeah again w2 does a pretty darn good job in the studies that have similar primary design they're kind of saying yeah these are these should these are the same so there's a nearest set but most of them are really completely non-overlapped from starting to study this is the problem final forms other types and uh 0.5 final object distance grouping these sequence variants highly conserved across studies and it's not unexpected that there's going to be some paratypes are going to be one study or another and when you start to consider the total grievance how how how dominar are these between these tend to be these are concurrently shared across all six studies despite very different primary designs and sequencing of different "
    },
    {
        "start": 2138.0,
        "text": "centers and different studies and so it starts to achieve one of our desirable principles that we have things that are more or less reckless i would argue reference roots that are modestly dependent upon the topography of the following reference tree um but do not need an exact match of anything in the tree um and they're not dependent upon taxonomy but can can basically be turned into a code account table um i ran through this quickly because i wanted to see if there's any discussion i have another block that's going to talk about um whole genomic sequencing shotgun sequencing um if people like to i'm going to jump into that but before i do i'm going to pause again for some questions or follow us on this particular 16s processing technique so far there are no questions that have come in but again if anyone's online uh feel free to put a question in the chat box or underneath yourself "
    },
    {
        "start": 2198.56,
        "text": "not seeing anything it also doesn't look like there are any questions at the moment so one of the other proposed solutions to this problem is just ditch the primer selection sequence randomly primed dna that's been required acquired from the site sequence like there's no tomorrow sequence like moore's law has not been killed by illumina sequence till the end of time sequence like you're working on an hpc that is not cost recovered and you get because there are no primer selection you're basically random primers everything should be funky dory and and completely easily or comparable and what can you do with it well you can get composition out of this certainly by aligning these random genomic weeds as you can imagine from post-associated microbiota mostly what you get is depending on the conditions you can get a whole heck of a lot of human dna which is not as useful in the settings you have to throw that out um in in disease states where there's a "
    },
    {
        "start": 2259.44,
        "text": "lot of cell death that can be a lot of dna that leaks out that's fine you can fix that you if you sequence enough you can always make that better again not that not fully killed you can map these reads to reference databases you can assemble them into context which would be like little bits of genome and the world's worst and most horrible jigsaw puzzle and then you can bend those together using these little pieces of jigsaw puzzles from different specimens to try to get genome-like material out of all of it and that's what people do and you have something you can associate with it's a little bit harder to get estimates of alpha diversity although it certainly can be done by that sort of like taxonomic assignment business as you can imagine random confounded with hosts that can be a variable percentage you have even worse of the sort of limit of detection issues and even in a study comparability and having to think about things like usually again rarefaction for that would be problematic "
    },
    {
        "start": 2323.359,
        "text": "bacterial genomes are not are structured into little genomic ions that tend to be conserved in these clades of life and even be vertically or horizontally transmitted from organisms to organisms this is a set of genes that do something useful with their associated regulatory domains that do something the microbe needs or contributes a phenotype up to the microbe that is beneficial deleterious adaptive to an environment the one my colleagues and i d fight about all the time that these little genomic ions whether plasma mediated or actually within a genome that confer resistance antimicrobials again from the regulatory domain often have multiple different antibiotics that they confirm resistance genes that confirm resistance from multiple different antibiotics altogether she did this horrible quick selection business but when you start to think about the big big big big picture objective we want to associate microbial functional capability when you're talking about genomes phenotype health disease treatment response "
    },
    {
        "start": 2385.04,
        "text": "pre-term labor it doesn't matter what this is probably the minimal useful unit to start to think about and you can think about individual genes but that is pretty a lot of times genes really do need their other partners and the microbes have already nicely assembled them into functional units to be able to control and regulate those so why not try to identify those microbial genomic ions well again in collaboration with some of my dear colleagues we developed a software package gene shop that does exactly that using some a couple techniques to sort of make this a little bit more computational a little bit more computational gene china is horrifically computationally intense i'm if you're going to think about seriously doing this apply for an exceed credit now um but regardless here's what it does in the catalog and if you excuse so you get the raw raw raw raw little itty-bitty reads that have been randomly fine quality control filter check out all the "
    },
    {
        "start": 2446.319,
        "text": "human reads because or mouse it's a anything that is non-prokaryotic you you you chuck it and then you assemble into consoles okay you take these contexts so this is again the jigsaw puzzle part of this and there's a bunch of different software packages that do this none of them do it perfectly that's life is all about working with them precision and then you can extract actual protein-coding genes out of it there's a bunch of software that can identify if something is likely to be a protein coding um and make a little kettle of it and you can deduplicate it you can kind of try to figure out what the genes are by dumping it into an atom annotation engines but that's not required and so you still have now the mapping that this centroid came from these genes were identified here you do a little bit of duplication to make it and then you go back to the raw reads and align them against the centroids you developed you why in order to do the jigsaw puzzle you need not just one representative reed "
    },
    {
        "start": 2507.52,
        "text": "from that genomic region you need a lot you need a lot of layered up piled up evidence there in a paper i'll i'll show but not go into great detail what we found as a group is that when you're using these assembly-based techniques we're trying to make the contacts there's a limited detection issue that varies by how predominant a particular genomic piece was in the underlying microbial movement it's in the tail it's going to be there if you really look for it but it's not going to be assemblable by these algorithms because there just isn't enough overlapping data so here what we're taking we're not looking at a specs and we're looking at an entire set of specimens so one of these specimens if it's really relevant it's going to be a little bit more dominant we'll assemble it and then you go back to the raw reads and you realign them and filter these alignments with a software package we call we develop to kind of handle multi-mapping reads and then you can actually kind of flatten the limited detection curve out quite a bit where if the readout for that particular "
    },
    {
        "start": 2567.68,
        "text": "organism is anything above about point um you can actually detect it whereas for context you really need four or five copies of that genome to really start to pull out these contexts right and so you end up with the big gigantic matrix of doom gene sample count um and it is candid we're talking probably the order of tens of millions to hundreds of millions of genes um in a in a decent sized set of specimens hundreds of thousands of specimens um disease states tend to be even more diffuse than health and so this actually gets worse as you start to do it so how do we dimensionality reduce a little bit more and find genomic ions why not look for co-abundance look for coal bunnies across all our specimens super things are on the same bit of dna they're going to co-vary and so by looking for coherent groups we can find bits of um these scenes that do we can backfill from the information we had in the real "
    },
    {
        "start": 2629.28,
        "text": "world we found these two genes next to each other from the context we generated earlier but the co-op and the genes also can capture things that co-segregate because they're functionally selected to go microbe a creates favorable in or genomic island a creates favorable environmental conditions for genomic island b they're going to co-vary even though they're different maybe different organisms and different genomes are different mobile genetic elements and so this is more of a conceptual functional co-segregation in that set of disease or health you can quantify those cove on the gene groups and that often reduces the dimensionality by an order of a thousand or more so you go from tens of millions to hundreds of thousands which is still a bad problem but less bad you have a shot with power to do it and you can kind of go through and annotate these cold energy groups and so forth um and i've kind of gone through um this is this in silico software where we also talk about the um dealing with the "
    },
    {
        "start": 2690.8,
        "text": "multi-mapping root problem which i'm not going to go into a tremendous detail but there's a real problem again if you just take a read and blast best hit it against the reference from these random you're going to be wrong it microbial proteins like all proteins all g protein coding genes are kind of made like legos and so if you have conservative domain because they're doing a conservative and a sequence from one conserved domain is going to match the sequence of ones that are concerned with many many many many many other genes and so this really needs to be thought of um and i'm just in the spirit of time i'm not going to believe it it's just to say our software package iteratively filters these out quite nicely to get them to the one true the one true source okay right this is just a little bit i'm going to also skip over this in the spirit of time we apply ginger again in the meta analysis this is another extremely hot area of incredible clinical importance there's a new form of therapy for cancer that is now first line therapy for over "
    },
    {
        "start": 2752.4,
        "text": "half of cancer diagnosis in the united states immune checkpoint inhibitor therapy there's been some really lovely work they clearly associate functionally um the structure and composition of the gut microbiome with whether or not the treatment works secondarily the treatment can cause bad side effects including colitis and the microbiomes related to that as well what i'm doing functionally using like adoptive transfer studies and sometimes you show the mice and also get an adoptively transfer tumor you take a microbiome from a person whose tumor responded to this therapy and you take the microbiome from a person this tumor did not respond to the therapy you put the same tumor into the two different mice and given them through microbiota the mouse who got the responders microbiome will respond whereas the mouse took out the microbiota from the non-responders so clearly involved nobody can agree why or what these studies all have a similar phenomenon but cannot agree about why what is the organism that is there um each one of these senior authors will be "
    },
    {
        "start": 2814.079,
        "text": "glad to tell you which one it is but they can't agree and so let's try it let's pull them all together and let's see what we get we ran it through g-shock my favorite kind of sequencing someone else's it 108 specimens billion reads duplicate down to on the order of a million genes further down to so this is seven million down to one million coefficients and then we do our regression against response or not you get down to three thousand coordinate gene groups that seem to have any relationship to response it's not surprising a lot of these combination groups are before not necessarily interesting and then these you can start to now do some annotation one of the big problems in metagenomics is the annotation databases are expensive and difficult to make and maintain computationally expensive nobody wants to pay for them so they get made once and then left to faster i'd be a little bit too editorializing but this is a problem a challenge for this sort of data so right "
    },
    {
        "start": 2874.64,
        "text": "you can at least get some sense of what they are and go through all this right um you can kind of get at least at least some sense of how well this works and that there is at least some degree of linearity um but the key thing i want to point out is it's just there's just so much to level the way you need to go through some of this business here then um okay let me show you a couple caps this is the percentage between progression versus response where this cag was above the limited detection and then this is when it was above the limit of intersection how much of it was detected so this cag and again we're comparing two different studies a little bit more compelling for this one is positively associated with response what is it you look at the members of this gene is that crispr [Music] operon in f transmission which is an organism that is one of the few organisms universally present in the healthy human gut "
    },
    {
        "start": 2935.44,
        "text": "microbiome how about this this tag is the opposite relationship again it's a little bit more driven by this study but this is a character that is negatively associated with response it's more abundant than people who progress what is this one well member genes of this tag are in a phage that is found same species so same organism you would not be able to detect this in any sort of taxonomy based identification but if president say has one island versus the other and if you put these together you can you you basically do the ratio of the phage versus crispr pad you really start to get something that's quite discriminatory between the outcome and start to understand that really you do need to start thinking down at these minimal limits of genetic operons or genetic ions um so gene shot is my answer to the question well can't we just use shotguns to do meta-analysis or get to learn a little bit about it um "
    },
    {
        "start": 2995.599,
        "text": "and so i'm going to stop there and just acknowledge all the amazing people that they get to work with and continue to work with and the people who foolishly pain to do some of this not much it's hard to get payment i think that people know to do total development and evaluation but i'm endlessly grateful to the wonderful group of people to work with so with that i'll answer any questions otherwise i very much appreciate your time "
    }
]