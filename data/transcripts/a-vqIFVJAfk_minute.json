[
    {
        "start": 7.639,
        "text": "it's really a great pleasure to be here with you today and I'm excited to give a talk and give a graph AI to label precision and medicine I hope that those on Zoom can also hear me and see my slides and if not um feel free to uh to to let us know so uh throughout the talk I will highlight various projects and papers and data sets and all different materials and supporting resource resources are available to our live website so if you go to the club HMS hierarchy there you will find pointers as well as tutorials for those students in the audience who are interested in trying out some of the data set or working with some of the data sets or experimenting with microbiology and algorithms that will be highlighted and discussed today okay so with that uh let's uh let's begin um this works "
    },
    {
        "start": 71.7,
        "text": "okay great so um over the last few years we have seen tremendous search in interest of leveraging advanced analytic methods in particular machine learning techniques to learn from biological and medical data and so the kind of data and problems that um are of specific interest to us relate to modeling complex systems with a pervasive technology to multiple scales of organization and in complex systems we typically think of them as very rich Network structured graph structure data sets that can be used to represent sound cellular interactions proteins to interactions between them in complex systems higher level of organization to the level of individual what kind of diseases might exist within an individual person and only to the level of Healthcare systems and so those kinds "
    },
    {
        "start": 132.54,
        "text": "of multiple heterogeneous in independent data sets when represented with graphs one of the interesting questions of how can we leverage advances in machine learning generation intelligence to be able to learn and reason on those biomedical networks in order to produce management predictions patterns in inside and so the moving area of ground artificial intelligence sometimes also refers or graphical networks is the area that has been productive and supported the development and use of machine learning on ground structure data the use of geometrical networks have really skyrocketed over the last three years behind some of the most important advances and uses of AI Life Sciences one graph neural networks here I'm showing just three concrete examples um the top one showing um part of the background of other football network is actually I actually will grant neural "
    },
    {
        "start": 192.599,
        "text": "network which takes us input a one-dimensional sequence of amino acids and produces us as output two-dimensional um amount of our daily Nets or heavy atoms of a protein an important structure individual network is a graph neural network second is molecular graphic veterans organized surround neural networks have been successful used to prioritize molecular graphs in large chemical libraries and that has led to the development of normal antibiotics hallucinism most important and successful example of that and taking the rules of transforming across different biological contexts defined by salons and or chemical genetic perturbations have advanced understanding of cellular function the study of plyotropy induced and other cellular phenotypes so what is the key idea of graphical Network so the key idea behind all these "
    },
    {
        "start": 254.64,
        "text": "different applications is this notion of newer message passing or graph neural networks so let me try to illustrate that um to um one in this one slide so graph neural network is a non-linear mapping function here denoted as F the Texas in photograph shown on the left and that graph represents a complex system of interest and circle denote nodes or vertices and edges indicate links and uh graph neural networks will then mapped as interested in the graphs numbers that is typically referred to as an embedding and so the idea is that the one General Normal that it will do that in such a way that students in the input that have similar local network neighborhoods they will be represented by points in them that will expose that "
    },
    {
        "start": 314.94,
        "text": "are close together in back embedding space shown on the right part of the slide so the way junior is popular optimized is them such that the launch lengthened embedding space is optimized subject it reflects topology of the input graph so that once you are prefer routine the algebra operations Windows embedding connect vectors for nodes that reflects the structure of the input graph so a variety of different kinds of architectures in this area different specific algorithms have been developed to the adopt a work with different kinds of grafts include different kinds of network similarity in the input graph all those architectures are based on the idea of message passing so what you will do is Define messages that are distribution of vectors with the neural network will propagate along edges of the input graph and normally transform them in such a "
    },
    {
        "start": 376.74,
        "text": "way that the American embedding space will be most useful for Downstream applications so the problem indications of this field of research is that with graph neural networks you cannot apply deep learning markets much more budget and what was possible before so traditional response analysis of images so think of corrosia new networks or Transformer architectures and Analysis of sequence data through LST networks and other kinds of algorithms the geomets we can now use sleep learning very broadly and we can apply it to any connected graph structure data set so this creates exciting opportunities in biology stock to highlight a few of them and motivate the use of My Graph AI is specifically relevant and a productive strategy to use for certain kinds of problems in biology medicine so why do authorized what is particularly additional activity of those techniques "
    },
    {
        "start": 439.199,
        "text": "when you are thinking of modern problems that require multiple a structure geometry Symmetry and find biomedical knowledge that is naturally given okay so just a moment we need to figure out something um maybe turned up too loud on the microphone input under the report sorry to interrupt does that work okay just give us a second or two seems quite good now great okay great thank you so much for for "
    },
    {
        "start": 499.319,
        "text": "this technical help uh let's let's proceed so we left our with the discussion of what are the kind of problems where graph neural networks would Excel and make are particularly suitable to use and so the kind of problems that that they are particularly interested in and relevant to cons to to use ER those that require us to consider the structure geometry symmetry or prior knowledge of a problem that we care about so so in the context of modeling genome when we are interested in modeling regulatory relationships or genomic dependencies that's what we can effectively do with gnns questions related to modeling molecular graphs or protein structure that requires modeling spatial distance or bonds are also naturally represented as graphs as well as settings where there is an amount of Prior biological knowledge or principles or rationals that we want to incorp rate into the design of our machine learning "
    },
    {
        "start": 559.56,
        "text": "model to make the model before broadly generalizable our graph AI methods are Excel one of the second the second reason why we are excited about graphing AI is because it these models allow us to more readily translate predictions into actionable hypothesis and I will return to that in the throughout the stock the idea here is that what we care a lot about is not only achieving producing models that produce that achieve High measures of accuracy but to directly produce outputs that can lend themselves to actionable hypothesis in the laboratory so that for us means that predictions must be interpretable must be as specific as possible and must be contextualized in this in the context in in such a way that it is more readily clear what kind of Downstream experiment could be done to validate hypothesis produced by the model foreign "
    },
    {
        "start": 620.48,
        "text": "use of graph AI is in settings where we have large unlabeled data sets or where our data sets are readily small and and therefore not do not have sufficient amount of label data that would allow us to use standard supervised learning algorithms so we have seen that many recent advances over the last decade in these areas have depended on methods being trained on large data sets that would richly labeled inputs so an example here is imagenet a data set with millions of images that are richly labeled that have then enabled the use of many deep learning strategies in the context of images however what I what I would argue is that many problems at Frontier of biology and medicine and some of the problems that we care more about most about come with either very small data sets or data sets that are large but not a highly accurately labeled or not labeled or cruel "
    },
    {
        "start": 682.68,
        "text": "and so in that in those contexts graph AI can be really helpful and so in this this in in this talk today I want to to broadly talk about two topics that we are working on in my research group in the first part of the talk I will show how graph AI can be helpful in assisting with diagnosis of rare genetic diseases and in the second part of the talk I will show how graph AI in graph neural networks in particular can help us predict therapeutic use of normal molecules as well as small drugs that are already on the market and extend their therapeutic use for for underserved patient communities so the motivation for the first part of talk which is that of um is assisting diagnosis is the observation that one of the biggest challenges in medical diagnosis is being able to assist in the uh in the diagnosing patients that are otherwise very hard difficult cases and so the particular disease area that we "
    },
    {
        "start": 745.44,
        "text": "are interested in are rare diseases there are over 7 000 rare diseases in the U.S and they are defined as a diseases where each affects less than 200 000 individuals most of these diseases are phenotypically very diverse and heterogeneous and the challenges that many Frontline clinicians might not have prior experience with those uh individual diseases and that can contribute to um the difficulty in diagnosing patients in recognizing their diagnosis in patients and it often requires Specialist sub-specialists or multi-disciplinary referral to diagnose the patient and so the problem with that is that that leads to a diagnostic delay that can be so pervasive that it creates problems for patients patients might need to take a redundant test or undergo medical procedures that might not be necessary have no had we known their diagnosis early on substantial there might be substantial delay in providing appropriate "
    },
    {
        "start": 806.899,
        "text": "therapeutic interventions or managing the disease and sometimes the disease can progress in such a way that the the Judo for optimal intervention has been missed because of the delay in diagnosis and so this creates lots of questions whether AI techniques can be developed to help and assist with diagnosis of those patients so those who might work in this area might be wondering well but there are already many many methods that can assist with diagnosing providing medical diagnosis and that is true AI has demonstrated tremendous success in assisting diagnosis however much of that has depended on the availability of large label data sets one of most well-known example is a tool based that has been in February of this year approved by FDA is the first autonomous AI tool for diagnosing diabetic retinopathy and it's a neural network that was trained on over 110 "
    },
    {
        "start": 867.54,
        "text": "000 labeled images of patients with diabetic retinopathy and so that is great but in settings where we have so few patients for a condition it is unlikely or impossible to be able to train those models and use those models in the same way as it has been done for some of more common diseases so we have developed an algorithm that's called shepherd shepherd is the future of learning graph algorithm that can provide multifaceted diagnosis of patients with rare genetic diseases it can be integrated at multiple levels and stages of diagnostic process of rare genetic diseases in the context of utn let me give an intuition of how Shepherd works so Shepherd is a graph neural network that is trained in two stages stage one stage one is to construct a large embed a large biomedical Knowledge Graph so this is a multi-model knowledge graph of biological facts and known "
    },
    {
        "start": 929.3,
        "text": "relationships between variants their their phenotypic effects genes and biological pathways take that Knowledge Graph that has no patient level information using that to prior knowledge alone which we pre-train a graph neural network in a self-supervised manner such that it can capture the relationships or those known biological facts in the second stage we we do the following we take the model from the first stage and we use it as initialization on top of which we can now consider patient level information and so for every patient we represent a patient by a set of patient phenotypes that are HBO terms and a set of genes that are harboring variants found in that patient through genome genome sequencing so then a patient becomes then given to those sets describing a patient the patient is becomes a subgraph of the knowledge graph "
    },
    {
        "start": 989.779,
        "text": "activated by those nodes that represent phenotypes and genus harboring variants in that patient because we have prior embedding of Knowledge Graph then we can really very easily produce embedding vectors for individual genes phenotypes and patients and patients as a whole and we can do that in such a way that at when this second stage is completed the model can be easily queried um for novel patient and then calculating proximity between embedding vectors in the embedding space we can denominate what are the diagnosis uh and what are possible cause of genes that the patient might that are relevant for the patient the question that you might have had when I was describing the second stage is well but how can we train this patient and better in step two if there are so few patients available for each and every disease "
    },
    {
        "start": 1051.02,
        "text": "so here is the trick the shepherd introduced and so the main the main trick here was that we're actually training the model on a large number of simulated patients so we rely on databases of rare disease information such as orphanet and kebabing and others to simulate tens of thousands of patients that the model in the stage in step two is being optimized for and so on that step is concluded the model is fully trained and can be used directly to evaluate to evaluate and make predictions for real-world patients so in our case we were collaborating with the udn network which is undiagnosed diseases Network a nationwide Initiative for 12 clinical sites throughout the US that is admitting patients that have that are hard to diagnose and um it's it's it is helping them provide and find the correct diagnosis to a lengthy and complex diagnostic process and so we were wondering whether we can "
    },
    {
        "start": 1113.299,
        "text": "evaluate and take patients that were undiagnosed in utn and run them to train Shepherd model uh in an effort to provide medical diagnosis for those patients and so we evaluate the chap hurt on um two external cohorts in addition to the udn cohort of 465 patients in the udn with diagnosis across 12 clinical sites we also have another cohort my Gene tool that the model was independently evaluated on and so um results of that evaluations are have shown that the strategy of training graph neural network and bringing in prior knowledge it can really help us train models much more with less labeled data after having seen only a handful of patients with the given diagnosis the model can correctly prioritize causal Gene close to the top priority list and "
    },
    {
        "start": 1175.58,
        "text": "so there were a few different tasks three tasks in particular that we evaluated Shepherd on the first was causal Gene Discovery and the task there for Shepherd was given a patient represented by the set of their phenotypes and set of genus harboring variants in those patients can we prioritize and produce a ranked list of those of those genes in an effort that close to the top of the list would be would be causal chain for the occupation so what we found is that over 460 across 465 patients that were already diagnosed in udn for 75 percent of those patients Shepard was able to prioritize the correct Gene among top five genes that the model has prioritized and so this is really a good result because it shows that the correct the relevant Gene that was that was was found very close to the top of the list importantly those genes tend will tend "
    },
    {
        "start": 1237.26,
        "text": "to be relatively far away and not directly Associated to any of the phenotypes that patients have that's not surprising because those are very hard challenging cases to diagnose and it was graphs that that was a technique that enabled them to bring singly disconnected phenoceto phenotypes and genes closer together second what Shepherd can also do is to support patients like me queries so this uh in this way you can think of Shepherd as a as a query engine that takes us input a patient and produces an embedding for that patient take that embedding signature and use it as a query against embeddings of other patients in a database and therefore find what are other patients that are similar to The query patients and that is helpful because it can help inform diagnosis and downstream interventions and finally the third task is a novel disease characterization or characterization of diseases that have "
    },
    {
        "start": 1297.86,
        "text": "unusual representation where Shepherd can be of value because it can generate generate these low dimensional embedding spaces of both of of genes phenotypes and patients and therefore a query patient can be explained in the context of nearby phenotypic notes or points in the embedding space so key takeaway messages here the following so when Shepherd we were able for the first time to bring deep learning to rare disease community and that's really exciting to us because it shows an ex it shows the promise of deep learning and most recent advances in machine learning for this area that has been underserved by prior efforts the use of graphs is not only beneficial but it's necessary to solve these diagnosis tasks Shepherd overcomes limitations of traditional machine learning approaches in three ways first it can Infuse um "
    },
    {
        "start": 1357.919,
        "text": "prior knowledge and oh and of biology into model through embedding a Knowledge Graph it leverages self-supervised learning to align patient Gene and phenotype embeddings and it can be trained on a large cohort of simulated patients meaning that we can answer new queries in a zero shot learning manner it can generalize to normal phenotypes genes and diseases and more broadly it can demonstrate the potential of shortening the diagnostic Odyssey for rare disease patients which is an exciting opportunity for us and we are and we are working closely now with to the end to see to what extent we can provide Shepherd as a tool that can be routinely used by utn researchers so when the diagnosis of a patient is is it is reached and identified the obvious next question that the murderist is "
    },
    {
        "start": 1419.08,
        "text": "whether there exists therapeutic interventions or what kind of Novel therapeutic interventions can be developed to help um reverse the disease effect or or treat symptoms and so in the second part of the talk I want to illustrate how when you know we can when we can use we use graph neural networks in a thoughtful way we can advance the process of discovering safe and effective therapies for um important areas so we have recently started an initiative uh that serves as a meeting point between on one end uh AI machine learning scientists who are designing new algorithms and on the other hand biomedical and scientists and biochemists for identifying meaningful tasks and problems in drug Discovery development and that effort known as Therapeutics "
    },
    {
        "start": 1479.48,
        "text": "data Commons serves as a meeting point between two communities that have been otherwise quite separated from each other in an effort to facilitate algor both algorithmic innovation and advanced therapeutic science so Therapeutics data common Services of AI Foundation or platform for therapeutic science you can learn more about it if you go to TD Commons dot Ai and it comes with a very rich ecosystem of AR ready data stats ready to use Python packages with implementations of best-in-class algorithms for tasks as well as leaderboards benchmarks and so on the way we organize therapeutic data Commerce is by or recognizing that many uses of machine learning in this area can be decomposed into a typical workflow so a typical workload drug Discovery would require the following steps first would be for a machine learning "
    },
    {
        "start": 1540.08,
        "text": "scientists in collaboration with biomedical Partners to formulate meaningful machine learning tasks a question that is at the cost of benefiting from machine learning analysis those are questions that have sufficiently large and ready to use data sets that can be trained by the model and those are questions also for which we are are able to evaluate outputs predictions produced by the model and know how to go about evaluating them either quantitatively or experimentally once that is identified one needs to source and process harmonize the relevant data sets third develop the or Implement machine learning model in any of the popular Frameworks such as Pi G or Pi torch tensorflow checks hugging phase and once that models muscles are developed of course evaluate performance of the model understand its failures and successes compare it to alternatives to to decide what is the best in class algorithm that can be transitioned into "
    },
    {
        "start": 1601.34,
        "text": "practical implementation so TDC supports development of Novel ml Theory and methods with a strong bend on understanding what are the specific algorithms that are most suitable for specific tasks and why would that be the case so to do that it provides support throughout all these stages of machine learning workflows including different meaningful mathematical definitions of relevant tasks machine learning data sets at a harmonized standardized and can be easily linked to additional external databases that you care about a variety of functions that can split the data sets into training validation test sets in a in a way that is close to real world use of models various methods for model performance evaluations and various leaderboards where if you're interested in specific tasks you can go and see what is the currently best method for that test that the community has and use that method "
    },
    {
        "start": 1663.38,
        "text": "directly for your scientific question data sets in TDC color are range of therapeutic modalities the tools that are currently supported include small molecules most of data sets are cover small molecules however there are a handful of data sets that included into decent color biologics antibodies and and Gene editing therapies and those machine learning tasks 22 of them that have been mentioning mentioning that I mentioned they go across stages of drug Discovery and they cover tasks related to Target the discovery identity in the notification of candidate drug targets to modeling um activity and supporting High throughput screening uh to to predicting efficacy and safety of a candidate chemical compounds and matching their uh therapeutic signatures with with clinical outcomes and tasks related related to synthesis of Therapeutics it is an open science initiative so so those in the audience who might be interested in this in these "
    },
    {
        "start": 1725.84,
        "text": "questions you are very much welcome to check out uh TD Commons and and get engaged with us in in a way that works best for you what I want to mention in the next few minutes are some of the compelling use cases of the comments and so what we have seen over the last a year and a half is that um the broad user base of Therapeutics data commands have been using the resources that are provided by the comments for a variety of different tasks across stages of drug Discovery development that includes modeling and predicting molecular properties in early stages of drug design using models to prioritize therapeutic targets based on the strength on binding to to relevant to relevant compounds repurposing already available drugs for novel users and to tackle and provide "
    },
    {
        "start": 1786.22,
        "text": "treatments for emerging pathogens and be more broadly modeling and predicting indications and contraindications of drugs let me highlight some of those stocks some of those use cases so molecular property prediction is a test that is concerned with predicting various endpoints or given only molecular structure of a small molecule compound so typically that means that we're interested in building a model that will take us input a molecular graph and it will predict endpoints most relevant are widely considered are admit endpoints that they're concerned with properties around absorption distribution metabolism excretion and toxicity of that compound and so the idea here is that we if we would have such a highly accurate model for predicting those end points we could apply that model across millions of molecules in chemical libraries with the purpose of finding those molecules that have acceptable "
    },
    {
        "start": 1846.58,
        "text": "admit profile and those would be the molecules that would Advance the therapeutic Discovery process so in PDC we provide over 22 data sets that cover around 10 million chemical compounds across various admit endpoints and they are all very easily accessible and and essentially with just three lines of code one can get any of those data sets pre-processed and harmonized across those five types of endpoint and so what TDC then provides are implementations of graph neural network models that take us input those molecular graph structure and predict these multi-dimensional admit endpoints for millions of molecules implementations of all of those smart methods are already available to TDC and can be readily used by users in a direct manner or directly can be directly queried or can "
    },
    {
        "start": 1908.84,
        "text": "they be further fine-tuned based considering your your data sets Beyond those that are in TDC benchmarking rigorously methods for these problems we found that in fact there is no single method or type of architecture that performs the best across all scenarios certain endpoints but the general principle is that pre-training is very helpful in boost performance of the model so taking large standard chemical libraries and pre-training the model to capture molecular graph structure is then very helpful and can help then take the pre-trained model and fine-tuning it on small label data set and that can lead that typically leads to better performance uh for the small for the set of molecules of Interest and pre-trained graph models in particular yield produce stronger strongest predictors overall they outperform those expert curated models methods that have been developed through Decades of research in chemoinformatics "
    },
    {
        "start": 1969.86,
        "text": "and they can do that because they were developed and inspired by the way those classic fin molecular fingerprints such as Morgan or Smiles fingerprints have been designed over Decades of research in chemistry and then graph neural networks provide their differentiable counterpart of those methods second very widely relevant in your use case of the comments relates to the problem of predicting interactions and bindings in particular between a candidate drug and a therapeutic therapeutic Target and so the setup here is slightly different the way you can think of it is that our input is has two components the first input is the the structure of a candidate molecule that's given in the form of a graph where nodes represent atoms and it'll just indicate bonds between them and the the second input is the "
    },
    {
        "start": 2030.34,
        "text": "structure of therapeutic Target and so both of those inputs are embedded into low dimensional embedding vector by separate graph neural networks once those two low dimensional embedding vectors are produced a scoring function is then trained in such a way that the the dot product between those two embedding filters the blue and yellow one in this slide will be very high where the when the the Target and the compound are likely to bind and low and will have low value otherwise and so that score is then reflects The Binding affinity so this setup is then supported in TDC and a variety of Gene and architectures can be used to then to embed molecular graphs or or Target proteins the way we are evaluating those models is that we want to evaluate them in their performance in in settings that are challenging and settings that are "
    },
    {
        "start": 2092.5,
        "text": "indicative of more realistic use or most practically that means that we want to test models on structures that are very different than structures the model was trained on and so in machine learning language that means that we want our models to be robust and we want our models to be robust to distribution shifts so what what and so those distribution sheets are the distribution of molecular structures the model has seen during training might be very different than the kind of structures the model will be will see during test time and so we want to take the our data sets and split them into train validation and test sets in such a way that um molecules with similar scaffolds will be fully contained in either training set or in test set so that we in that way we can help Benchmark performance of the model when it is asked to generalize to "
    },
    {
        "start": 2154.66,
        "text": "molecular structures that are very different than what has been seen during training time so what is shown on this heat map on this slide is the following so we have in the in rows we have various methods that um are designed for handling distribution shift problems on the x axis in columns we have years and years indicate the data set split so for uh 2013 um the First Column indicates performance of the model in terms of buying Affinity prediction higher value indicates better performance when the model was trained on molecules from that year and tested on molecules from the same year and the further we go to the right part to the right of that heat map we see the performance of the models where the model was tested on data from between 2013 and 2018 but tested on data "
    },
    {
        "start": 2218.14,
        "text": "that was generated a year later or two years later or three years later so here's what we see so first we see that there is some variation in performance between rows that's not surprising some methods perform better than other methods in particular ERM or empirical risk volumization is a strategy that that works surprisingly well for handling distribution shift problem in in binding Affinity prediction the second observation is this is the one that we can see that the performance drops drastically when we go from left to right within a row and so what that indicates in in fact we could summarize this number we would see that out of distribution performance drops considerably on average from 43.6 to 33.9 percent and so that essentially tells us that in many ways when we are "
    },
    {
        "start": 2278.74,
        "text": "reading papers writing them and Reporting this AOC area under Precision recoil curves and other performance metrics that are near perfect that does not mean that those models also provide perform so well in when they are practically practically used and we see how the performance the grades considerably when the model is asked to make predictions that are truly novel for very different kinds of inputs the model was trained on so TDC support that kind of analysis and and provides practical insights into what are the specific algorithmic Innovation one can make in this area finally I would like to spend the next few minutes uh highlighting one of the study that is relevant for later stages of drug Discovery development and after the drugs are already approved and so that's uh the work that um we have done in in 2020 and expanded in 2021 as part "
    },
    {
        "start": 2338.98,
        "text": "of a Copic 19 task force to to use and integrate and bring together a machine learning Automation and novel data sources with the goal of providing support for Rapid therapeutic invasion so um in in many settings traditional iterative development experimental clinical testing and approval of new drugs completely from scratch is not feasible so the statistics is that Stakes on average between 11 and 16 years and it costs one to two billion dollars to design a new drug for completely from scratch and bring it to the market so when that is not practical or feasible which might be the case for certain therapeutic areas such as rare diseases or emerging pathogens where we need to develop and deploy therapeutic invention as quickly as possible an enticing alternative becomes that of drug repurposing can we take a drug that is already on the market that "
    },
    {
        "start": 2399.339,
        "text": "was initially designed for us for a specific purpose and extend extend its use figure out if any of those drugs could also be repurposed and used for in a different uh for for different indication and so that is the question that we are asking ourselves and what we demonstrated in in the study is that to careful use of of ML and Noble data sources and very close partnership with experimentalists it's possible to cut down the the time and resources needed for identifying drug purposing opportunities considerably for two months or even weeks in certain cases so let me tell you a bit more about this particular study so the motivation was um or the big goal in question was for us was to um find uh what am I what are the "
    },
    {
        "start": 2461.98,
        "text": "compounds um chemical compounds that could be used to intervene against um and the covid-19 infection so I brought a reasoning for that in the approach the strategy that we were considering was the following uh since uh disease diseases disrupt normal behavior of genes um we want to find what would be then the drugs that could intervene against the Disease by restoring the function of their disrupted genes but what does that mean in the context of this virus so in the context of the trial that means that we were primarily working with the viral interrectum and information on 330 uh human proteins that the virus attacks and then the knowledge of how those human proteins interact with other human proteins in the Cell by seeing where do they map when we overlay them on the human interactive or human protein protein traction Network which "
    },
    {
        "start": 2522.22,
        "text": "is a network where nodes are human proteins and edges would indicate physical interaction context between between those between those proteins so we can study that trial interact on then in the context of known targets for each and every of approved drugs and the the hypothesis then is that the proximity between targets of known drugs and proteins that are attacked by the virus can tell us some meaningful information about the potential for dead drug to be repurposed for the disease and so this this hypothesis is is not new to our study it's been founded and used by many many other repurposing efforts particularly Network medicine so our approach was considering the entire range of diseases and non-approved treatments for those diseases we want to prioritize compounds for this new pathogen and we want to do that in a zero shot manner zero short manner means "
    },
    {
        "start": 2583.599,
        "text": "that well we we want to find then the compound for that normal pathogen yet there are no examples of successful compounds that one could use as labels to train supervised models and so we need to do that with zero labels or in zero short manner so here's how we can do that so the project we use is few short learning this is a strategy that is um that uh that refers to a class of algorithms that are designed to work well when the amount of labeled examples is very small we typically refer those labeled examples as shots so few shots would mean you only have a few short labeled examples for each class that you care about so typical terminology and papers that you might have seen is when somebody refers that they have a three-shot problem that means that they are trying to classify um objects in into whether they belong to a "
    },
    {
        "start": 2643.9,
        "text": "class yes or no but but they only have three labeled examples for each of those class so it's a hard problem so a few short learning for allowed us to to um be able to identify repurposing opportunities for for this emerging pathogen in the following Commander it the the process proceeds in two stages the first stage is called meta training stage this is a stage where we intentionally expose machine learning model to the kind of really hard challenging tasks that are similar to what the model will be asked to do at test time so we designed a number of challenges that are called training tasks and each training task is centered around a different disease the disease the tests are more approved three to months for and so in training task one the model is asked to uh after given only a handful of this illustration here too examples of drugs approved for f for this one "
    },
    {
        "start": 2705.579,
        "text": "um do it's the best it can to discriminate between drugs approved for disease one and those that are not approved for this is one you can imagine that the model would perform really poorly after training test one but the purpose is we will take the model uh and its weights at the end of training task one and use them to initialize model weights for training task two and repeat that process now on different diseases and then repeat that many many times you repeated at twenty thousand around twenty thousand times and when this process has been completed around 20 000 diseases the uh we get we the model transitions into the second stage which is the meta testing phase and at that phase a phase the model can be directly used and queried against the pathogen without having seen any labeled example or example of successful treatment and produce a prioritized list of drugs for that pathogen the matter that can that can do that it's called gmata it's it's an approach "
    },
    {
        "start": 2767.38,
        "text": "that's very flexible and scales very easily to very large graphs um we have it's a detailed description characterization of the method in in an Europe's paper that we published in in the same year um and um I would highly encourage those who are interested to read it out as well as ask me questions about it but we were able to show that theoretically and from mathematical standpoint uh improvably that this algorithm can can perform well but not only that we were then interested in actually using the algorithm for prioritizing compounds for copy 90. so the first test that we have done is is just to check whether the prioritized list of drugs that the method produced um overlaps um with those drugs that were considered as clinical trials at the time and so that's a very very weak way of evaluating the model because the fact that there is a clinical trial and "
    },
    {
        "start": 2827.619,
        "text": "compound does not mean the debt would that that compound would actually be safe and effective for patient we have seen some enrichment of uh top ranked predictions um shown here in this Roc plot on the left of geometer models are relative to more classic drug purposing techniques that are based on network medicine Network proximity indirect overlap between therapeutic targets and disease Associated genes and so that has provided some confidence so that we were able to proceed in the neck to the next stage which was to share those predicted lists of compounds with our collaborators so our collaborators in the National emerging infectious disease Laboratories in Boston then took top of the ranked list made by the model and they they those were 918 compounds and then they screened those compounds for efficacy against the pathogen in African monkey cells followed by "
    },
    {
        "start": 2888.579,
        "text": "experiments in human cells so the results of that uh were the following by screening top-ranked predictions in human cells we obtained near 62 percent success rate and so success rate was defined as uh well since the successful defined as the rate at which the pathogen was its growth was inhibited when the drug under different dosage levels was administered to the cells in contrast if that screening process would not be guided by priority list produced by a RT meta algorithm the heat rate or success rate one would get would be around 0.8 percent so this is an order of magnitude hierarchy trade among top 100 trucks then alternative approach importantly what we were interested in knowing is is there something common to those drugs that the model predicted and it were experimentally verified are there some common biological mechanism or some common pattern that the model "
    },
    {
        "start": 2950.44,
        "text": "has relied on when making those predictions and so we used GNN explainers and explainability techniques that we've been developing in the group to probe graph neural networks distill their predictions and figure out what parts regions of the input graph the model is focusing on when making predictions founded 76 out of 77 predicted drugs that refuse reduced infection were such that their therapeutic targets did not coincide at all with those proteins attacked by the pathogen instead there was a series of cascading effects between targets of those drugs and then those indirect effects that eventually reached a proteins attacked by virus and so that plus the various additional analysis that we have done that can that we kind of Thoroughly described in the paper I'm happy to ask questions about them have motivated us to realize that that those drugs rely on network based "
    },
    {
        "start": 3010.98,
        "text": "action they they can they would not be could not be identified by more classic molecular docking strategies and we refer to them as Network drugs we have since then extended this approach more broadly Beyond um Beyond covid-19 to other disease areas in particularly to rare diseases where we are modeling then um uh the predicting therapeutic use of already available drugs in the context of perturbation experiments where we take response signatures of chemical perturbations and overlaid those Gene singers on top of biological Knowledge Graph and then measure proximity between signatures on the knowledge graph to prioritize drugs foreign so the results of that approach are "
    },
    {
        "start": 3073.74,
        "text": "particularly promising for those diseases that have a very few uh annotated treatments or for those diseases that um are um for for which their knowledge describing them is relatively sparse meaning they are found in parts of the knowledge graphs that are very sparsely connected to each other and in particularly because graph neural networks can um take into account those indirect effects not only Direct effects that are available about a specific molecule the performance of the model is particularly strong for those diseases that have few treatments or limited molecular understanding so here on the right I am showing a evaluation of the graph neural network in three specific scenarios performance is measure test area under Precision recoil curve so high rail indicates better performance and we can see that few short learning performs comparably well and performs actually on "
    },
    {
        "start": 3134.22,
        "text": "pair with standard supervised learning methods when the model is asked to make predictions around diseases with the with many annotated treatments and considerable probiological knowledge but the the gap between performance of standard approaches and future learning approaches increases drastically when we go in and use those methods for harder settings and in various valuations we've been doing we have seen these effects across different disease areas we have seen those effects for predict when the model was asked to predict indications so positive use of a drug towards treating a disease as well as contraindications um when we have since then been engaged in a big cloud or a larger effort uh on evaluating predictions made by the model cross disease areas against EHR data sets at at Mount Sinai Hospital in New York City and and practically that means that our users of our models are "
    },
    {
        "start": 3195.96,
        "text": "clinical researchers who interact with their models through Visual interfaces an example is shown here on the slide and they can ask questions in natural language for example where Clause a pin treat unipolar depression what could this treatment mechanism would be and what the model would provide us output our prediction scores visualization of embedding spaces and utilization of parts of the knowledge graph that the model has relied on when making the prediction and so that has been really helpful for our users to decide when to trust predictions or not trust predictions and we've done a recently a user-centric study uh with uh with also with our collaborators Niels gillenberg lab in my department showing that presenting the outputs of analysis to users who might have limited machine learning experience in that way can help them identify what are those predictions that are most trustworthy that are made by the model okay so we uh with that let me wrap up "
    },
    {
        "start": 3257.76,
        "text": "today's uh presentation my goal was to give an intuition behind the use of machine learning for complex systems and networks and graphs in particular I hope it I convinced you that crap neural networks and more broadly neural message passing and graph representation learning techniques can be helpful in areas of precision medicine with an illustration of facilitating diagnosis for aerogeneetic diseases and areas of therapeutic science through the various use cases of Therapeutics data Commons the papers codes data sets mentioned they are all available on our lab website also go to tdcommons.i Therapeutics data Commerce website we have a separate initiative called AI for science initiative which is a initiative of 27 academic groups around four continents and we are carefully asked child what are current uses of AI that are successful and more importantly are "
    },
    {
        "start": 3319.2,
        "text": "there examples or problems where the use of AI can be counterproductive harmful or not beneficial and what are the challenges and concerns there and more about that effort is also available at AI science for science Community GitHub IO my lab is hiring so if you have um if you are looking for postdoc positions or are there are outstanding pH students who would like to work with us reach out we have several open postdoctoral fellowships in AI for cancer drug discovering in neotherapy analysis as well as recruiting page students area um thank you so any questions uh and people on Zoom you can just ask questions directly we can hear you really well hello um so I think it's pretty fascinating to see the GNN application on the actually the "
    },
    {
        "start": 3382.079,
        "text": "party of your talk about the therapeutic and you has mentioned that um this the this the GNN can help to um just find like the the um the drug Target for the emergent like pathogens and I'm just wondering like how well like the like the the result of the prediction can resist the mutant of the pathogens or the variants of the disease great question uh we that's a question that we're asking ourselves as well so we have an ongoing project where we are studying mechanisms of resistance um with a particular focus on TB I'm leveraging some of the expertise uh um in in TB of in in the department uh Harvard where I am and initial results are promising in the sense of that we can once the large data sets are available we can identify the mod these "
    },
    {
        "start": 3444.18,
        "text": "models can predict accurately um resistance finding some patient samples whose response to certain antibiotic treatments uh will result in antibiotic resistance that said however we don't have a good understanding of those mechanisms for the emerging pathogens project that I was highlighted today that was completely independent um so yeah great talk um I guess um my question is so on slide 25 um when you were talking I think it's like 25 when you had that big chart um with all of those predictions um I guess my question is so you explain that the drop um in performances because of you know it's in distribution from 2013 to 2018 "
    },
    {
        "start": 3505.16,
        "text": "and then out of distribution from 2019 to 21 but why is it getting worse between 2019 and 2021 because it's like 2021 is far worse than anything else at least as far as I can tell on that chart so like why is it still getting worse like wouldn't we be improving a great question so so your interpretation is correct the worst performance would be uh is achieved uh by methods in this in last column uh so you can see that the average person correlation coefficient which is measuring the agreement between predicted binding Affinity scores and ground Through The Binding Affinity scores so the lower value indicate was performance we can see that for the last column performance of models no matter what models you pick is really poor so why is why does the performance go down "
    },
    {
        "start": 3565.92,
        "text": "as years go up so the problem has gone down because the model is asked to a predict further in the future and it's asked to do so in such a way that the test molecules for the 2021 year have different scaffold structure than those molecules that the model was trained on from year 2013 to 2018. and so the shift in the underlying data distributions of the structure of those molecules uh the um think of the the distribution of a occurrence of certain functional groups for motifs in those molecular graphs changes drastically and because many of these algorithms are still relying on those uh that's those that signal to make predictions they are performing increasingly poorly where they are asking where they are asked to generalize further in the future and further for the future for us it's a proxy for the the test molecules be "
    },
    {
        "start": 3627.359,
        "text": "having more different structure than those molecules the model was trained on so what this you could think of this slide is showing negative results so the current state of the art methods perform in such a way that when you expose them to challenging settings that are those that drug designers would care about they perform really poorly so like this project for us was particularly motivated by this observation that when you are reading various papers for binding Affinity prediction you see this those papers supporting near perfect accuracies and and so the question is why then those models not more broadly used in drug discovery why have they not transitioned into practical implementation and so what we realized doing detailed analysis of predictions is those models make really good predictions for inputs that are trivial those kinds of settings where a test molecule is very similar to a molecule that's in the training set plus "
    },
    {
        "start": 3688.44,
        "text": "minus one or two bonds but those are trivial predictions because drug designer biochemist who works in the therapeutic area is most likely to already think of that structure on the by themselves they would not need an air model for that the part the use cases where they would benefit the most from AI models would be to quickly sweep through areas of the chemical space it's not densely investigated by existing experimental data sets and in that areas find promising or non-promised in chemicals and what we see is that state-of-the-art models do not perform well in those cases which to us it expands it explains right some insight into why we don't see their broader adoption and more concurrently we are now working with uh with the drug designers on improving those uh improving some of these methods "
    },
    {
        "start": 3748.799,
        "text": "uh so I have a question on the Shepherd Mobile uh so you mentioned that uh because uh this is targeting for weird disease that's why you have rear samples to as inputs to your training GNN and we use like simulated patients to this model so uh well we have problem of overfitting if so how to avoid this great question short answer you see yes this can be a problem of overfitting so here's what we have done to address this problem in particularly in our evaluation studies we did two kinds of experiments so first is that uh the knowledge graph and simulated patients that were generated um up to um you know were generated using the data available uh up to June 2021 and the test set of patients that the model was evaluated on was that of patients that were admitted to the union and diagnosed later so that is one way how "
    },
    {
        "start": 3809.579,
        "text": "we were will try to mitigate the potential data leakage so that they kind of data liquid that one could happen is imagine a patient admitted that has not yet been diagnosed it seemingly has a diagnosis that's very hard to discern and they are admitted to udn however as parts of the insides and in terms of the relationship the phenotypic effects of their variants might have been characterized already and that data would be then uploaded to particularly orphanet which is this repository for every disease research if that information then fits into our generation of synthetic patients and training of the models that would feed that would lead to data leakage or overfeeding as you refer to it so the way we mitigated that is by doing this temporal split of when what the the data used to train the model was generated up to one a certain time point and the model was evaluated on patients later that were tested late a minute later on second we "
    },
    {
        "start": 3869.819,
        "text": "have also used udn on cohort of patients that are currently undiagnosed um and and we are following cop on uh predictions for a small number of patients for them with experimentalists and in third in addition to udn we have uh we have second independent cohort um of my gene2 patients which is a community which is a patient community portal where patients are volunteering their own data in an effort to um be diagnosed and biomedical Community provide a support for them and so that's a separate effort a separate cohort that will evaluate our model as well so yeah we are very much aware of this problem and we have lots of checks in the paper um uh that to to mitigate those effects thank you question for you yeah I'm interested to know if these graph "
    },
    {
        "start": 3930.0,
        "text": "neural networks are related to what around here is called hypergrabs we've been used quite extensively by one of our colleagues and deco rajapakson yes so the question was whether these graph neural networks are related to hypergraphs and the answer is yes so hyper graphs are higher order graphs that can model not only pairwise relationships but relationship between uh Triplets of notes or quadruplets or higher order interactions and are particularly useful for problems such as modeling poorly Pharmacy or drug combinations multigenetic interactions and so on so graph neural networks are are class of algorithms that can also work on hyper graphs which are specific type of graphs actually they have been recently a series of uh I think very interesting papers in uh where graph neural networks trained on hypergraphs were used to model higher order chromatin interactions that was done primarily by "
    },
    {
        "start": 3993.059,
        "text": "John ma from Carnegie Mellon University and his group he has published a series of papers demonstrating the use of graph neural networks trained on those higher order um graph structures um and they have they were able to successfully predict uh chromatin interactions and do also multi-way um learning with um chromatin data and um uh history modifications to what you've been doing uh yeah for us uh hypergraphs have been something that we were considering and and I have projects on uh for modeling drug combinations I didn't talk about that effort uh today but an important Direction in our group is to support with um AI post marketing surveillance of of drugs and so for us that means that we're working with data from FDA "
    },
    {
        "start": 4054.74,
        "text": "National adverse event Reporting System uh which is large observational data sets that we are trying to clean they confound them in different ways to identify if there are patient groups that um for which there are certain Adverse Events overrepresented or underrepresented when they're taking multiple drugs at the same time and so we are representing those um interactions between drugs for those patients as hypergraphs of drugs uh dead the patient is currently taking on and and then predicting labels for them which are adverse event outcomes very good thank you there's one more question from Zoom uh Joey can you ask the question okay yeah hi professor Jenny thanks for the great talk so I have a question regarding the distribution shift part so I'm wondering uh how did you divide this training environment for this "
    },
    {
        "start": 4115.339,
        "text": "environment risk minimization model yeah so the question was of how do we split the data set into training validation and test set for uh our distribution shift study and so for this uh for this study of by predicting binding Affinity under distribution shifts uh the data set was split based on molecular scaffolds so that most practically means that one can use chemoinformatics tools such as Rd kit to compare for a given pair of uh compounds uh their molecular backbone and so cluster compounds are those backbones and so then you take an entire cluster which is a cluster compounds with similar background or similar scaffold and you put the entire cluster either full in training set or fully intested and so in doing so what you end up producing is a training validation and test set where molecules with "
    },
    {
        "start": 4176.96,
        "text": "similar scaffolds are all introduced only for training or only for testing so that's one type of split which is called molecular scaffold split that is implemented in Therapeutics data common so you can readily use it just draw a few lines of code on your own data set the distribution shift was evaluated on this we have a few other kinds of data set splits and another one was temporal data split where we have obtained information on when certain patterns were filed and um finalized for therapeutic targets and then then we were splitting the data based on the years in which both patents were granted um so that's that's another type of split that we were considering as well it's also implemented interpretics data comments yeah thank you one more question this will be the last question "
    },
    {
        "start": 4240.199,
        "text": "hi um Excellence presentation um I did have several questions but I'll stick to one for here and maybe if you still have some time later I might want to ask a few more but um I am just wondering um for the second part of your talk where you talked about applying the models to you know repurposed Therapeutics or discovered new Therapeutics for treating diseases um I was just wondering um like have you tried to incorporate data on maybe natural product compounds or you know uncharacterized novel compounds because um I guess I'm trying to understand what is the Baseline information that you need for a single compound for a model to work effectively um how well does it perform with maybe compounds that are not as highly characterized as you know um already approved Therapeutics yeah great question uh so in short a couple of points so first is in the work so far we have not been considering natural "
    },
    {
        "start": 4300.679,
        "text": "compounds um we have a bigger collaboration with Partners where we're looking actually at food ingredients and when you that can be decomposed into chemical uh in in compound signatures and then thought of as some form of therapeutic intervention um in the right dosages so but that's a bit kind of unrelated to to the topic of today's presentation the the second appointed that I'd like to make is that one of the things that we are very excited in context of TDC is that we can stratify those machine learning tests along different stages of drug Discovery and so that means that we are then being thoughtful in the sense of if we are designing a model that could be that is helpful in stage one or drug discovery that model could only use the data that is relay readily available at that stage we're seeing many of the pitfalls of um a published literature in this area "
    },
    {
        "start": 4362.78,
        "text": "um be for example modeling molecular property prediction tests using data on toxicity that's only available when the molecule enters the animal testing mode and of course that that kind of use of the model or like developing a model to solve a task in stage one using the data that's only available in stage three the real Discovery it's it's really not particularly meaningful thing to do so um one of the efforts that we are then undertaking is to to systematically evaluate the utility of different data types for specific machine learning tasks that can be helpful and also all those insights are available to putting data Commons website what we have found so far that um some of the users of the commons have used that information to decide given limited resources available what kind of experimental screen might be most informative to perform for a specific "
    },
    {
        "start": 4423.86,
        "text": "group of compounds that they are considering okay since we're almost 20 minutes late so let's stop here let's thank marinka again for this wonderful talk thank you very very good "
    }
]