[
    {
        "start": 21.69,
        "text": "[Music] [Laughter] "
    },
    {
        "start": 93.56,
        "text": "after going get started welcome everyone to the technology seminar series probably notice we are testing out a new pizza vendor feedback is welcome so please feel free to let me know after the seminar or email me with your opinion on it it is just sort of a trial run so based on feedback will decide whether or not to stick with us or go back to Pizza house all right so today I'm pleased to present our speaker Karan deep Singh who is clinical assistant professor the Department of learning Health Sciences and he's gonna talk to us about our and machine learning thanks for having me just so I know these CCM be signs are outdated right you guys are DC MB now since when just okay just curious "
    },
    {
        "start": 155.94,
        "text": "thought I would ask so anyways I'm excited to be here thanks for inviting me and I am wanting to talk about and mainly demonstrating how to do supervised machine learning machine learning in our and the focus for the first part of the talk we'll be talking about just figuring out in the room kind of what the level of experience is with supervised machine learning so we'll kind of keep it interactive and do some QA and then then we'll dive into actually doing it and seeing what things we can do and what MLR can help us figure out from a variety of models so a little bit about me before I do this slide I'm relatively recent faculty recently joined faculty member in the department of learning Health Sciences which is also housed in that school I was previously here for undergrad med school left to sunny LA then to frigid Boston and then I'm back here so did "
    },
    {
        "start": 216.0,
        "text": "training in a couple places several weeks the Europe I practiced apology in the hospital but most of my time is spent doing kind of informatics research in which I use tools primarily an hour so just to nearly all messages I teach a class here LHS 610 which is a intro data science course where we teach our and it's specifically relates to health so we deal with a lot of health related issues and also get into some of this stuff in the course towards the end so just so I know who's in the room who's used our okay how about Python okay how about SAS so I won't Stata SPSS but just want to get a flavor for what people's kind of experiences with various tools how many people are familiar with machine learning okay and "
    },
    {
        "start": 279.45,
        "text": "how people new room know the difference between supervised not supervised machine learning okay how many believe every use a regression for an analysis I don't know I told my she's learning but just plain old regression to figure things out okay and then how machine learning algorithm for analysis for something you've done okay so given all the hands that have gone up there may be times where I see heads nodding and I'll move forward but I still want to have some discussions you know depending on where it's so but it's good to see good to see the level of experience in the room and I didn't ask this but how people use ml are in our and how people have used carrot in our okay so just here those are two packages that people use for missionary so we'll do like a very brief overview of machine learning I'll probably skip some of these slides there's some questions "
    },
    {
        "start": 339.78,
        "text": "we'll do around what data set we're gonna look at and we're actually gonna try to work through an example you know in the next hour I'll do a brief reduction that all ml are and just then will pretty much dive into and try to build a model and see if we can predict diabetes having data from a population of Pima Indian women and this is a pretty famous slash publicly available data set so it stays that you may have been covered at some point but if not we'll actually get into it a little bit and I'll show you what's in there and we'll try to figure out even before we build them all do we think our model will do well or not and how do we define success so machine learning you know is an area of study where primarily or teller by example this is unlike the idea that we're gonna tell model what to do in advance so we don't this is not a pre you know set set of rules this is show apples and you know have this machine "
    },
    {
        "start": 401.23,
        "text": "spit on algorithm for how its understanding and information the phrase is coined by Arthur Samuel Stanford who wrote a first self learning program to play checkers in 1959 and so unsupervised machine learning is well we don't have an outcome and so we're really just trying to understand the relationship between a set of variables and so we often do this kind of very early on when we have a very high dimensional data set just trying to figure out you know how are things related to one another and we may want to do this to defy subgroups or subtypes where we're clustering observations or like rows in the data or we might want to cluster variables and try to figure out which variables carry similar information so if you are working with like single nucleotide polymorphisms you want to figure out which snips are carrying similar information when it comes to relating it to a phenotype is exactly they might be familiar to you supervised machine learning which is "
    },
    {
        "start": 462.61,
        "text": "primarily the example we'll be working through today is where you have a set of predictors and outcome and really you want to learn the relationship between the predictors and the outcome and a successful machine supervised look model should be able to generalize and who knows whatever you might generalize your freeness just start talking so back there so we hand them back you say your familiars y'all yeah so and it realize you're eating pizza exactly so you want to make sure that the model is not just memorizing the the data that you show to it so that if you show it data that's that's generated in a similar way that it's going to fit that data well too because you know a bad pool is one that will just memorize the data and handle do perfectly on your data but it will not perform well on you data and so she will present you know "
    },
    {
        "start": 524.8,
        "text": "correctly predict the outcome not just on its own data set but really the measure of success is how it does on new data with consistency and not just the original data and so we called regional data oftentimes as the training set data and that new data we kind of all from lump together as kind of test set data that we set aside to see how it's going to do on that and how it does on that is how we think it will do on you know other data that we get down the road that we don't have access to yet so this is a little bit of a detour but for those of you who said that you have done the regression analysis people do regression analysis for cod2 reasons so when you were using regression and something that you were working on what was the reason that you were using regression what was the value or what was the number that was a kind of most relevant to you know whether you were gonna get excited or not so it could be "
    },
    {
        "start": 589.54,
        "text": "a certain type of value that starts with a letter or it could be our square so what could be a square yeah so so so the reason one of the reasons we do regression analyses is that we want to test a hypothesis and so sometimes we have a outcome we have a set of predictors and really only one of those predictors is one that we care about and the other predictors are kind of our covariance or our potential confounders and so in epidemiology we frequently will use a production Alice's to really figure out what's the relationship between one particular outcome adjusting for other predictors that we think are confounding that relationship between that exposure in the outcome and so example this is you know what heart disease and if we have one in mind and want to test it we would put that variable you know in the model we would put things like age sex things that we think you know cholesterol levels that "
    },
    {
        "start": 650.47,
        "text": "we think might confound that relationship and then see what that relationship is with the outcome and then squirt and so you know the other thing we get excited about is if my model actually fits the data well and so if you have you know if you offer squid is that metric that helps us figure out you know is this not even a representation of the data and if it's you know r-squared is below it's not high then you know because it's not good enough again with so we can't really do much with it and so an example would be if you've got a set of snips how well can you present predict the onset of heart disease and so there was this paper in the European Heart Journal here and so they actually looked at 50,000 snips they built a genome risk score where they you know use all of these snips to "
    },
    {
        "start": 710.9,
        "text": "kind of come up with a risk profile and they compared it to kind of Framingham risk score which is a clinical predicters and figure out that you know genomic data carries additional value beyond clinical risk factors which is not surprising but you know they really look to see which snip is important as part of the modeling process but that wasn't the key intent of this analysis the key intent was to build that model and see how can we actually predict that outcome and so we just how we measure that a supervised machine learning actually fits data well with regression 10x is actually a type of supervised machine learning model so we don't use to evaluate or test hypotheses and so "
    },
    {
        "start": 777.25,
        "text": "that's you know yes or no or it's multiple categories when you're erection is a what we call the regression algorithm and just note that you know if you've used regression multiple senses in in machine learning world people often use regression doesn't mean pretty and they use classification to mean predicting a categorical outcome or a class outcome so call me a fit and linear inflection is R squared in logistic regression it's a i CRI cocky informational criterion and supervised machine learning we did not look at these at all and we tend to look at cross-validation performance so before I get to just that and I'm always doing this because performance and I want to make sure that you guys have kind of a working understanding if you haven't dealt with it before who squared have you move I'm not going to ask to explain what it is but have you in have you dealt with R square before everyone "
    },
    {
        "start": 839.299,
        "text": "yes I know I know but with assumptions they always get broken so so thankfully you guys have not a I see you guys just a Russian okay and that's G wall things like the obviously you guys dealing with mayors p-values but say you guys have done with these things cross-validation performance how do you don't have dealt with that okay so fewer that's not even for the last two and so the question I have is why can't we use R squared or AIC for looking at machine learning algorithms and is any one of that's your kind of idea especially for familiar with cross-validation why we don't just use R squared so off squared is when we look at the residuals I mean the errors we square those we look at the total variance or bowles we square those we take the ratio we subtract from one and basically we end up with this number and if it's one that's really good if it's "
    },
    {
        "start": 900.559,
        "text": "you know the one is from one the worse it is and so why don't we just look at our square like if I built a model that had r-squared of one that was a machine learning model like why not get excited about that whereas I would get really excited if I had a regression model that would do that potential get excited not you know with some conditions you could be over sitting yeah so we know up front that linear models often rigid models so what's the vegetables and any interaction terms that you put in there have been kind of specified your work is done and so you know there's nothing else to do you can the main question you always have is do you add additional variables do you take additional variables away and do you have ammonius model that doesn't have too many things in it but those are kind of the battles that you're having in your head is your thinking about you "
    },
    {
        "start": 962.629,
        "text": "know do I have a good girl in your Russian model but machine learning algorithms have these additional things help to new parameters Jindal's you can turn that are not pre specified so oftentimes they'll have defaults that are kind of reasonable defaults but you can change those and so once you build your model you're not done yet and so you can continue to adjust it you know oftentimes hundreds of ways when you look at the kind of a grid of all the different things you could do and so when you are adjusting the model and making little changes you can get improvements r-squared that's actually definitely meaning to new data so that you are just your data better better better without necessarily a cliff application that it will fit a new data set very well so we because it's a many machine models are very flexible models they're not lying there you know they can model very complex shapes we make "
    },
    {
        "start": 1024.939,
        "text": "sure that we have a sanity check which is you know we want to see how it does on data that the model hasn't seen before and so if we just rely on these things we would over fit and so that was you know let's be our main concern when we're fitting models is we don't want to over fit so basically the steps that we're gonna walk through an R and ml are for the following so typically when we're thinking about modeling something in machine learning we set aside a popular data as our test data and what's the rule about the test data if it was like it rule number one like Fight Club what would be the rule for four for testing hmm so that is not that is controversial so this we have to be randomly selected and the reason I say that is if you're building a model with a set of like patient seating clinic you might actually want to random not do it randomly but actually keep some clinics "
    },
    {
        "start": 1086.38,
        "text": "that are set as a test set because you want to see how a real life practice variations affect care and outcomes so just have it you know so if you were to apply this to a new new clinic you wouldn't have patients leaking in with practice patterns of the clinicians in both the training and test set it's even more but it's even basic than that yeah so do not touch the test data so once you've set this aside yes so you can't leak between the two so you can't have training that goes in the test test dataset and testing that goes our training so they have to be separate but once you've separated them basically set this aside do not look at it and use it to figure out how well we think our model is going to generalize so then we select the model and many people depending on where traditionally come from or have certain ones that they're kind of attached to so you know deep "
    },
    {
        "start": 1149.02,
        "text": "neural networks is something that in computer science and I think in general people are very excited about in statistical oftentimes what I use is random force models and so I think this is kind of a a precision sometimes you absolutely can try a couple and see you know what well and I've campuses but also is this simplistic model and runs fast and then so now you take a part of your training data so this is not the test data it's your training data and you take part of that we call tres that we've treated on the training set and so now there's a part of our training that's left over that is our test set not to be computed confused with the test data so this is the test set still within the training data we measure the performance on and then we adjust our two kilometers that kind of repeat this last two steps until we come up with a model that we're happy with it "
    },
    {
        "start": 1211.929,
        "text": "is confusing so some people hold on to set my sister some people call it a validation set to separate this term there is not a clear consensus on what that should be called so I recognize it's confusing but if you look this is the most common way I've seen it is people talk about training data test data and then with the training data a training set and a test set but data yeah so this is you know we hold out data because that's another way but it's but there's not a complete cut standard across the board of how people describe that so for today I think we can call that hold on to us that you won't have to kind of go back and forth your head it's the same so training data so training a derivation cohort or a development cohort but all the same that's the cohort of data you're going to use to develop your to develop and "
    },
    {
        "start": 1274.869,
        "text": "evaluate your model before you settle on the final model the test data validation data or hold it hold out data is that data that you have locked away behind it you know in a safe that you only pull out at the very end once you've selected all the different all the different tuning parameters it that you want to use and just evaluate the besi how old did I do and the reason you lock it away is if you dog it away and you keep looking back at it you're gonna start to learn aspects of that data that will make you think you're doing really well and then if you got another new data set given to you you wouldn't do as well as you expected on that so this is to measure how you the test data hope state is kind of that Sifford how old is not actually doing and there a bunch of different ways people do this some people repeat this whole experiment with different training test splits the most people do yeah just for background there's a 2012 report from the National "
    },
    {
        "start": 1335.36,
        "text": "Academy of Sciences I was involved in called the evolution of translational omics lessons learning path forward responded to a very awkward situation where what we're fitting was extreme claims of certain treatment responses were invalid I'm a small item in that report was but distinction made between confirmation and validation I think what you just described where you take one batch of data we're bad for a reason and you take part of it for the development derivation training and the other part hell out for the test it's still quite a preliminary step yeah because you have no variables for all the other samples could be taking or similar analysis best you can say about that situation is that you confirm final training set better is reanalysis same or similar data make "
    },
    {
        "start": 1399.2,
        "text": "sure it confirms the validation should have a higher standard validation should be from different sampling time with the batch issues of timing and people so some people call this internal validation so if I call it that I would say even this final treat this kind of hold of data is internal validation and external validation is get a whole new data set from a different source where there's kind of other things in play to really see how because really the question is how well it generalizes so okay so once you select your final model and your tuning parameters go ahead and try one fight online using all the training data and now evaluate on that test data without whole data that we left out and see how well it does because that's our you know what we're gonna hang our hat on is kind of our internal validation so why split the train into two pieces why not just use all the training data "
    },
    {
        "start": 1460.549,
        "text": "and train and then measure performance on the training data it's half of my questions have the same answer a very smart model it will spit out to you the you know the correct answers for your training data but will not perform well so this is the idea that you although we've set aside a test data set or a hold-up dataset I'm still taking half the training data training on the training set and the test set is not I recommend but that's what I'm starting with and we'll get to what you know it's gonna recommend it so so since the only training on the training part of that training data the training set the performance on the test set within the training should still be some measurement of generalization if we "
    },
    {
        "start": 1521.24,
        "text": "actually split it into exactly half which one should be our training set which one should be our test set shouldn't we just you know take turns and do both so training on one test on the other train on this one test back on the first one so in fact that's but we should and if we did that's actually called two fold cross validation so if we just split in half this no that's kind of called it that's the derivation validation approach but as much as we can we typically want to do cross validation so if we cut it in half we that's called two fold cross validation but we kind of if you have so we're only using 50% of the trained Lea to learn so it's like meaning we already set aside a test data now we've cut our training a two and a half and now we're down to like very little data that it's actually being used to Train what this important relationships that we're missing because we're only learning from that's true "
    },
    {
        "start": 1585.77,
        "text": "because so because we can't touch that yet the reason we're gonna talk about your knobs and we don't want to see how the knobs affect the test set is we want to see how the knobs affect the test set within the train data because that will at least to some extent overfitting and the next is a visual slide I realized in a lot of words so let me just demonstrate this visually so if this is a completely we say this is a training data that's our test data we set that aside what people often do is they'll actually cut this training into five or ten pieces so they won't just do two pieces and in this example actually show four pieces and then what they'll do is they'll train with this three force test here train with these three forests test here and do that down here measure the Packers here or error in each of these "
    },
    {
        "start": 1647.059,
        "text": "fools hours them and that's how well you expect to do on the test data that's kind of like maybe not intuitive an extreme case of this is what you actually leave out and train with the example and just develop that one example and that's but you know you could either do kind of efficiency sake into five or ten but the extreme you can do leave one out and the idea is you're gonna turn these knobs and your cross-validation performance may improve it's possible your test data performance won't you wanna pick a model before you have a look at that so that you're not learning your test data that you're not supposed to look at so actually if you "
    },
    {
        "start": 1708.59,
        "text": "leave one out as high variance but low bias and so this bias-variance tradeoff so many people you know put subscribers have said accepted as it's a near standard because it's kind of the right it's a between bias and burns some people even repeat it 5 full cross-validation will they'll take look do this once he'll take different cuts do this again different constitute again every one of these forms because this so it so typically that's not used to decide folds but it's mainly that if each patient of each person is a row in the data all we're doing is splitting the rows into these folds Jose I mean a "
    },
    {
        "start": 1773.85,
        "text": "unless you specifically want to exclude columns or you're doing filtering on the columns based on some criteria so that's I think pretty much all the background I have to get to just actually running through code and showing you some graphs and trying to do see ourselves but any questions so far yes is the district so one thing you can do is you can evenly distribute based on a certain characteristic folds is that what you're saying or a balance classes okay so let's see we have some rare so "
    },
    {
        "start": 1838.44,
        "text": "we're trying to measure like hundreds of a rare disease and so we want to technical cross-validation only nine you know only so there's two issues at play here one is that if we just randomly allocate those 20 chances are some fools will get none so oftentimes when we have a really rare thing we're trying to model we actually will stratify to make sure that the Ripper cases and evenly kind of in each fold so that you can actually can run this otherwise you can't get an accurate measure of what accuracy means when there's no cases the other part is that if it's really a better it might not that example very well and so people often use down sampling you're up sampling or they either generate repeated example a bit sleep up sample the rare class to make it more even with the other class well they down sample the other class so that if you know there's a disease people with without "
    },
    {
        "start": 1900.07,
        "text": "disease you might cut out eighty people so it's 20 and 20 in your test set you absolutely want to make sure the distribution reflects real life in the training set there are playing that's possibly healthy you better learn class but it's not lie and then even something called smoke sampling which is essentially generating new my new kind of classes I think it's best probably beyond what we're gonna talk about today but exists and it's the thing that people worry about that so but this is a quick I think actually it's there's a less discussion here so I do want to make sure is just just that we're all the same page with this kind of stuff what kind of a task is pretty clean diabetes classification or regression just AI lab classification okay if we "
    },
    {
        "start": 1962.59,
        "text": "all using up training data it's 99% accurate on cross-validation but only 80 percent on the test or holdout data what happened oh okay what's a model is nothing a person actor on both the train data and test data is this good are we happy depends if the if only one percent of people have HIV and you're building a model that models HIV if you have 99% accurate you're no better than just saying everyone had and then no one is HIV so if you put them all that says no one is HIV that model is 99% accurate so in reality we actually often don't use accuracy as a good measure we often use things like air under the curve I won't get in that today we use accuracy for today but just realize it's always set your baseline worst accuracy based on the largest class that's there so if you know and I think for the example we're gonna do 66 percent people are diabetics so the worst accuracy emotion have a 66 percent if it's less than that that's actually worse than just saying everyone "
    },
    {
        "start": 2023.129,
        "text": "is diabetic okay and then let's say a 51% what do they call it should I call it diabetes or no thing we won't get into it which is what is your specificity at each cutoff level so for the case of today we're going to call this diabetes but just realize real life you can set threshold and that depends predict and what the purpose of your model is is a screening hmm scream or as a confirmation model and so you always won't understand you know when you pick your cutoff what it's intended to do okay so I'm gonna skip this stuff since you guys already know are and I'm let's get this stuff but just to let you know this cattle are the two packages that hundreds of other packages so carrots probably more well known but I think Apple are is actually more powerful I use both but I've switched over to MLR primarily and also because actually "
    },
    {
        "start": 2085.409,
        "text": "wraps carrot so because it can do them all or can't you can do NMR okay so this is a said we're gonna look at which is the Pima Indian data set there's actually three to this that's Pima TR Pima TR to which will ignore divided this into a training set TR is training te is test set so we're only going to use TR to train today and at the very end we'll evaluate on te these are the variables there's that not actually not that many they all pretty much final slide so number of pregnancies glucose concentration blood pressure skin fold thickness and the triceps body mass index this is a family history kind of continuous function from zero to one age and then this is the outcome where they eventually where they then diagnosed with diabetes or not when they had kind of down the road so these are all absolutely these are all numeric "
    },
    {
        "start": 2151.75,
        "text": "but you can add categoricals ordinals I mean anything you can think of obedient yes absolutely and most of my notes so most of my things aren't like concepts from notes and so I usually like 7,000 of these so I can't fit in my slide this is just to show you the code we're loading the data set it just gets blocked as a pop up so you have to hit ctrl R or f5 and then it sometimes works so let's see so this is just a few the first few rows of the train data there we go so this is basically the first you know one of 200 rows and so there's only 200 women in the train set interestingly there's actually no fluid in the test set three are 32 but usually we split the other way but that's just how this was and it may have been the way they actually collected the data the test set may have been actually collected at a later time "
    },
    {
        "start": 2212.35,
        "text": "and so this is I could be off on that is it it's it's a it's a function of 0 1 but it's it's absolutely the graph so we can see what the normal values are but it's higher means more family history and they'd it's like a function based on a variety questions that they turn into a single question so I woke up separable this is what we often do it before we ever build a model so these are these and you can you don't have to believe these if you don't believe in these four table ones but you can see that you know there's more pregnancies in the diabetics a higher glucose which is not surprising so we think that you know even just looking at the first couple of predictors this is going to be probably separable if we view this graphically we can look at this as a kind of density separation and "
    },
    {
        "start": 2274.12,
        "text": "so this is using ggplot and so I would say you know if the peaks are what most the data is and this is a yellow is yes and and green is no then you know the peak for PMI is clearly higher and this is bimodal for for non-diabetics but there's one pika so definitely lower glucose you can see that you know the peaks are clearly far apart so we think it's gonna be some you know we can't draw a perfect line because we're not completely separate but but potentially between all these variables here we may be able to separate this is almost overlapping I would say that looks very close to overlapping although it's you so ignoring p-values here you know a lot of the data is in the same boat here so probably it's not gonna be that helpful to separate way out right because you ate okay because it's the because the "
    },
    {
        "start": 2335.62,
        "text": "dotted you can have a 200 for a symptomatic it technically doesn't cause diabetes if you just ate why that's probably insulin deficiency those or not that's sure I don't know how did these ones and so I look at see quality issues okay so we need to tell i'm elora what kind of task what dataset what model what measure and whatever one i do do any cross-validation are we ready to train that final model or we ready to benchmark and i'm gonna basically skip to so we tell it we got the data set i want to use logistic regression we want to do flat fold cross validation we always said arraign a number seen so we get the same result when it does resampling and we just tell it we want accuracy airs are kind of final measure and then we ask it what happened and it tells us that on the cross validation the accuracy on average was 75% and so we know that just based "
    },
    {
        "start": 2396.68,
        "text": "on the data 66 is the blush you could get so said if it's better than that and you know how it is that we're kind of a little bit UK and so a two-by-two table of wearing my stuff you can decide based on if you have a cost in mind a cost function for what type of error is worse you could look at that and make it but this is assuming a 50% threshold and I could move that threshold if I wanted to and so this diagonal is good that diagonal is bad because that's where the predictions don't match up with the truth and there's 200 there because we need to five folds you end up getting the whole train set the training set and in machine like all the confusion matrix I know in epidemiology it's like a two-by-two table or there's like a there's other phrases for it so and then "
    },
    {
        "start": 2459.29,
        "text": "can we tune the logistic regression model and we ask MLR can we and it says you can't which we knew that we can't tune it that's why that's why when we nearest neighbors is another one that people use so it's where you trying to look for points in the data they're based on Euclidean distance or argh you know close to your points and up four times people use a kind of five or seven as a standard and so this algorithm uses seven as a default and if you do this and do the five full resampling you get 70% accuracy so water stuff is just regression and so then we ask it and then you can I'll skip over this so that we can say can we tune this model and there's actually a whole bunch of things we can tune in tune okay that this is the kernel scale and you know if you're familiar with it you try doing these on your own but one nice thing you can do is you can actually ask carat through MLR I want to try five "
    },
    {
        "start": 2520.08,
        "text": "different things what should I try what's reasonable and the author of the carrot package max Kuhn who else'll wrote a textbook on this and now actually works for our studio has a function that can figure that out and so if you want to have five different values it says focus on K and try five seven nine eleven thirteen so now we can go back and try tuning and so we make this so we redefine our learner as something that's that's a you know wrap tuning thing we basically run the same code with the only Edition being that we're going to extract the total result what you're doing is this you're not looking "
    },
    {
        "start": 2580.859,
        "text": "at the outcome whether when a cane era standard model is doing is just look at all the predictors and measuring Euclidean distance that's standardized for my nearest neighbors so I look at my five neighbors and I have them vote so as the five papers are you diabetic yes yes yes no no you're a diabetic so if my neighbors around me ever die but I must be diabetic is the idea so it's not clear how many neighbor to actually pick yes yeah that with neighbors yeah can only be one of two classes so this is like you know you can think of a red fin like you're trying to figure out how much the house cause well let me ask my neighbors and then I'll kind of extrapolate but how should I look at the whole subdivision should I look at you know two streets down so we can get this and basically what this will show you is that the question is do you get excited about these different values of the tuning parameter and if it so we knew that "
    },
    {
        "start": 2644.65,
        "text": "a 70% accuracy a 5.3 yay here we get 70.3 we're not really that excited I mean like it the yeah sniffs maybe we could time our values maybe that's what you know part of the issue but this is seemingly not getting us there and Nancy burns doesn't have waiting meaning it doesn't know what's important what's not it's trying to just find sources neighbors and sometimes in the preacher space it is making sense because one neighbor is much more one characteristic is much more important than the others and so you need to consider that one a lot more important you know more seriously than the others that's true I bet my laziness and also getting the code to fail page and if you're looking at this stuff and wondering what that is if you do if you know our hype and so you basically this is becoming the first argument of that that and a lot of "
    },
    {
        "start": 2705.549,
        "text": "people now who use are will make run type code so if this is new to you that's why it looks weird so we can do a conditional interest rate basically get a similar thing I don't see how do we do we get at sixty nine point five we're not excited we're gonna skip this let's skip this because we're not gonna get that excited so sorry no notice a pattern we haven't touched the testator yet we're still on our cross-validation and our training data I'll be ready just like the model well we only tried a couple of them maybe we actually want to try all of them and so what'll models what if you want to take advantage of parallel processing and like make sure that you know each core is running a model separately so what's so we can do is we can tell MLR here's the your task is the name of the PM adoptee our task so I can say tell me every model I could possibly run and so there's actually 70 "
    },
    {
        "start": 2769.66,
        "text": "settles that I could run if I wanted to so yesterday I decided to just run them all and so ignoring this page there's a benchmark command unless you just run them running whatever you want I said run every model that I could possibly run on this recognizing that it's not tuned and so this is accuracy here's every single model on the x-axis so obviously this is cross validation so I want a narrow band so that each fold it did well in I don't want bands that are higher this is too hard to read you can turn this into a nice table oops once again and it turns out that here yeah so so point seven eight is kind of the best we got the cross validation that was up tuned and just as importantly it's important to know that "
    },
    {
        "start": 2831.809,
        "text": "we go to the last page here you know they're just like a model here that's getting thirty four percent accuracy you might want to just flip the predictions am I actually do okay just ask it as guessing and so we're not super excited about those necessarily so how much separation in numbers there's no they're actually technically is a p-value that you can use a test that I never do and I would say it's not really taken seriously the higher the higher accuracy you're at the harder it is to get better so like put six six two point six eight should be much easier than point nine two two point nine four and people believe your end of the curve they don't use accuracy usually because this issue of class imbalance so I mean scientific papers "
    },
    {
        "start": 2892.39,
        "text": "get published around point O to better if it's already really high people get really excited about that and like an image classification you know a point know to better than what's out things like for your end of the curve is like phenomenal because people are already doing you know massive computational things or you're like 0.93 hackers or aired the curve so recognizing that we got two minutes left I'm gonna probably we're close actually but basically nowadays conditional forest and GLM net and basically said let's test them on the test set now which until now we were not touching so now I've tested them and we just pick one model we cheat it with a three but it's a Victoria three it's just done good to call them and try that again and so actually the GLM that model which is actually just a linear model that's adjusted tuned with a parameter that adjust all the coefficients downwards "
    },
    {
        "start": 2954.97,
        "text": "actually has the best performance at eighty percent and the test set even though it wasn't the best one in the original one so we can't now so we can't now say you know we think this one but we're gonna it was third best originally but in the test set actually the best you know this is not that different but this is actually this is actually a significant and if you really wanted to believe this you might want to repeat this whole thing with taking a different split of your test set doing repeated cross-validation and so what is the model Bayes model see as important this is kind of a question and we know in linear regression we have coefficients to look at here what we can do is we could randomly permute each of the variables and see how much permuting any variable messes up the bubble prediction if a variable is not important you permute it and it the breeches should say the same and so based on that concept you know naive Bayes thinks glucose is the most important beam is "
    },
    {
        "start": 3018.51,
        "text": "lower lower and interestingly I won't go through it but these dip actually looks great so here it's fourth and that might be genetic variant analysis it comes down to fasting glucose pretty much pretty much we don't have the pieces it's not a genetic you know it's a family history but yeah model you can permute and anything you know people are deep narrow Matt whatever the question that comes up is given age on the x-axis and BMI how much you split the data how does the model see that and so you know if yellow is I the officers are with green and yellow greens I apologize I went away from red green but "
    },
    {
        "start": 3080.01,
        "text": "I didn't do much better pick something like this you know or maybe even a lot right here where all the yellows are primarily here mostly greens are here and there's gonna be something I'm gonna make some mistakes regardless of how I draw the boundary and you could see naive Bayes actually draws it as a curve and the ones with white around it are the errors on either side now this is just the predictor space obviously you have all sees that data Elena sees it like this so totally different representations David you know of that data where is the same glucose and why is BMI and I picked up specifically GOM that is a line so it draws a line and deep no I'm not ignoring all this stuff going the bottom drawers and a couple curves here and so "
    },
    {
        "start": 3140.88,
        "text": "it's learning a different relationship altogether but same data yeah so and there's things you could tune which mean this is not a tune neural net this is uh you know what kind of the out of the box think it's only too deep so it's not even that deep and all that but with so few predictors you wouldn't necessarily think you would need it deep neural net to do modeling and you might actually over fit by contorting into a position that this is in general as well but bits the tampering they know well and then you know the question how much data do I need have I got all the data I need this is a hello learning curve so you take different percentages of your train data I actually see how will you generalize so you can see pretty much once you've gotten to about 50% of your train data your accuracy is cut leveled off so if this curve like this then you would go back to whoever it is you're gonna go back to and say I need more data because if my trip performance has you know I'm "
    },
    {
        "start": 3201.43,
        "text": "still getting better and better I think I'll get even better if you can give me more data but here looks like and usually only and even with twenty five twenty percent you you know you're in a reasonable accuracy range and these are the two of their models I just showed but this is something you can do to figure out figure that out so that's all I want to show you today a little bit over time but I look forward to interacting with you and feel free to reach out if you're want to just chat but what you're doing or are looking for things to do "
    }
]