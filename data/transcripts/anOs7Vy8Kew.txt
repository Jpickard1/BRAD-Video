okay if everyone can hear me i think i might uh get started now um so my name is louis joslin i'm a fifth year phd candidate in the bioinformatics program here at the university of michigan and my two advisors are dr denise kirschner and dr jennifer lenderman which in both of their labs we develop mathematical models and calibrate those models to biology and so uh while this work really wasn't the main thrust of my thesis in fact i mostly focus on modeling the immune response to tuberculosis this project which sort of started as a side project and it's now developed into a publication was really born out of some frustration in that i was spending a large part of my time day to day uh actually trying to calibrate the models to biology and biological data sets and not actually getting to answer any of the cool questions that were really even involved with my thesis work so i'm really excited to present calipro today i look forward to hearing everyone's feedback and uh i hope that potentially calipro might be able to help someone else who was in my exact situation kind of stuck in this uh loop of trying to calibrate mathematical models to biological data sets so before i begin i just want to say that we have access now to more data than we ever have before and it's really this explosion of biomedical data sets that reveals an opportunity for us to uncover mechanistic relationships um starting at just the genetic level we have genomics and then we've got proteomics metabolomics all these different types of data sets that are informing us about events happening on scales ranging from the genetic level through the molecular networks through cellular interactions and even events happening at the organ scale where we can like take images of um of lungs for example and all of these scales happen within a single host and so um these new data sets that we're starting to get access to and now the privilege of accessing um really uh give us a sense and provide a clue of how these biological agents might be interacting across these different scales leading towards emerging behavior that we can see for example we might be able to see an individual responding positively to chemotherapy but now we've got access to data sets that might lend a clue on how different cells are interacting across time and really it's important to note that these aren't static time points these data sets are now time series data sets that really uh help us get this this fundamental understanding of what's happening but as i'm stating here it really only provides a clue and we're not entirely sure what those mechanistic relationships are between these different scales and even between biological agents within the same scale and this is where mathematical modeling comes into play and this is really one of the uh benefits of mathematical modeling is that we can start to explain temporal dynamics that you see in these different data sets you can start to interrogate uh assumptions and biological theories that we have about what's happening within these different scales and then we can even provide context to better understand complex phenomena across these different scales and i think in the context of the coven 19 pandemic we've all within the last several months grown to understand the importance of math model mathematical modeling at even uh the scale of influencing public policy and deciding when to go into lockdown or when to lift lockdown but if you just google uh mathematical modeling in biology you're going to see that mathematical modeling is being applied to a bunch of different subfields within biology ranging from computational models of cell signaling within cancer cells to this pretty cool paper that's about mathematical modeling and predicting the impacts of sleep restriction in space missions uh to biophysical models of the flagellum during a salmonella movement so um absolutely mathematical modeling is uh while it's traditionally a pretty old field is now being used more and more as we have more and more access to these different biomedical data sets and so in both denise and jennifer's labs that i'm a part of we use mathematical and computational models to integrate data and make predictions across these multiple scales that i talked about so we've got whole host models within the labs we've got tissue level models we've got molecular models and even sometimes we build these models and we stack them together to make one entire kind of giant model where we've got events happening at the molecular scale which will impact events happening at the tissue level scale which in turn impacts events happening at the whole host scale so uh the lab really is experts in creating these mathematical models and integrating different data sets into the modeling framework so that way we can go on to make cool predictions using our mathematical models but what what does it take to actually build a model um first you need to start with some sort of biological theory or experimental estimates and that's going to inform your basic rules and mechanisms during the model building process so very popular starting point both within our lab and across the field of mathematical modeling is to start with an ode model or ordinary differential equation model but depending on the question at hand and the data sets that you have access to you might build a transmission model or a partial differential equation model or we sometimes use agent-based models so once you have that model in hand you might be really excited to take that model and start making those cool predictions that i talked about a couple slides ago however and this is something that's uh very key but is often missed in the literature and particularly when you read math modeling papers is that there's a calibration step and i've highlighted this in bold because this is really going to be the focus of my talk today is that this step is required required prior to using the model in any any fashion what is model calibration formally it's the process of altering model inputs such as initial conditions or parameters until the model outputs satisfy one or more of your criteria and often for us that's matching the model outputs to experimental data sets that are time series so they show different data points across time additionally depending on the the question at hand your math model and the experimental data sets that you have access to this could be really trivial and easy you might be able to calibrate your model within a day or it could be really difficult um and particularly i've spent time in this lab where i've spent actually months in what we call calibration hell where you can't quite calibrate the models of experimental data at hand and you're sort of stuck in this loop of revisiting the biological theory changing your rules and mechanisms a little bit rebuilding the model and then testing again to see if you can calibrate the data and so this begs the question of how does one actually calibrate a model to experimental data so first you absolutely need a model in hand and you need an experimental data set and so first i'd like to take us through a simple example of calibrating a classical model in mathematical modeling which is called the predator prey model this is a two equation ode model where we're tracking uh predators and prey their population levels across time according to the interactions between these two different species across time so there's several constraints to this model here are the model equations um you need to start with both species having greater than zero population and the parameter ranges are generally bounded between 0 and 1. just some quick definitions to take us through the equations i'm representing h here the variable h is the prey across time l is the predators across time we've got four different parameters which represent these different interactions between the predators and the prey so we're really sitting at this point with our model we've got it in hand and it's been built up through the literature through thousands of different papers based on some theory and some initial experimental estimates so now we've got to go find some experimental data set too as well so um using the hudson bay company pelt data from the 1900s you can actually see between 1900 and 1920 the total number of hair and links at one location in canada across time so this is the classical data set that's used with this particular modeling system and you can start to see even through these data points that we've got some temporal dynamics here both species peak twice within these 20 years and um that the peaks for the links are slightly behind maybe a year behind the peaks and the hairs so we can start to see that there's some temporal dynamics happening here and really what we want to do is calibrate our model to these data points and so the first question is can we even do that using this mathematical model and so in any traditional calibration setting you need to start with an initial point in parameter space and so this is your initial parameter selection for your model so here's the model equations on the right and here are the initial values that i pulled from the literature where we've got thirty thousand hair four thousand links and then we've got the four different parameter values that will dictate their interactions across time on the left hand side here i'm showing the experimental data again is the green and yellow dots for hair and links respectively but i've also run the model for 20 years and we can see the model simulation lines are actually pretty close to the temporal dynamics of the experimental data but importantly this initial parameter selection while good is not perfect we're missing the peak point in the experimental data for the hair as well as for the links and both populations sort of dip underneath the minimum values of the experimental data that we see right around 1910 so this leads us directly into our calibration goal which is we're going to try and minimize the differences between the observed values or the experimental values and our predicted values from our simulation and we're going to do that by creating this objective function which is the least squares error function which is quite literally the difference between the observed and predicted values we're going to try and minimize that through an optimization scheme so i used a built-in matlab optimization scheme which is a simple simplex search it starts with your initial parameter value so that's the same initial values that i showed two slides ago and then it will step through parameter space and evaluate different uh parameter inputs to the model and then it will run the model and it will compare using our objective function the least squares error function how close those model runs are to the experimental values and so it performs several hundred iterations of picking different points in parameter space and evaluating how closely that model matches to the uh experimental data until finally just shy of 400 iterations it decides we've thoroughly spaced uh and we've thoroughly searched parameter space and we can say that the final calibrated parameter set is the following four parameter values and the starting initial conditions if we just look at that the model run coming from that parameter set we can see we match to the experimental data really quite well so this is the traditional calibration scheme and this is what's been done for years and years and years in mathematical modeling is that we're fitting to a single trend line or in this case two trend lines of experimental data across time and we do that by using an optimization scheme there's tons of literature on this there's many different popular algorithms that leverage the power of optimization schemes and you could literally pick and choose your favorite here that will search through parameter space and make a smart and efficient choice in where to search next in parameter space to evaluate how close your model is to this experimental data simulated annealing is one genetic algorithms is another and gradient descent is sort of like a whole group of different uh optimization schemes for this exact type of situation but i mentioned before that we've got these uh these new technologies and this uh access to these extensive data sets that have revealed a ton of biological variation and so our lab has started to switch from this traditional sense of choosing one parameter set to represent the experimental data and to run our model with and we're now starting to really embrace this biological variation so this is changing how we think about calibration i think this is changing how the field of math modeling is thinking about calibration as well where it's not always appropriate anymore to fit to a single trend line because just in our lab alone we've got qualitative data sets now that show images of the wound that forms within the lungs following tuberculosis we've got access to blood draws and when what types of immune cells are traveling through the immune system across time and we've got access to the bacterial load within the lungs during the tuberculosis infection and there's tons of biological variation within these different data sets and it would really be a shame if we just summarized all that data down to one median trend line and tried to fit to that so what do we actually want from calibration now um our general criteria has changed where we're not looking for one single parameter set that's going to be matching to a median trend line like i mentioned if we've got these blue experimental values but we've got some outliers here that are really interesting we shouldn't just ignore those entirely by matching to a median trend line so we're going to start to see that we have some parameter variation now where instead of just having one parameter value for each parameter we've got a set of parameter values or a range of them for each parameter and the model runs with those different parameter combinations are what's going to yield the outcome variation that matches the biological variation that we're starting to see in these experimental data sets importantly though there's still some calibration that has to happen here because if you run with one parameter combination and it yields this red simulation line here that clearly doesn't fit the experimental data at hand so we still require some model calibration and really some intuition by the modeler on what's going to yield a successful model run in the sense of matching to our experimental data and so we're pivoting now from this traditional calibration scheme to what's called sampling based calibration and here is where i want to actually introduce cali pro and so this is the method that i developed um when i became a little bit frustrated with some of these previous ways of calibrating our math models to our extensive data sets so cali pro comes in right at the calibration step during uh model building process so you've already built a model using the biological theory and experimental estimates you now want to calibrate that model before you can go on and do some of the really cool fancy things that you can do with math modeling so i'll go through each of these different steps that calipro goes through in greater detail but just briefly the user is going to provide some inputs to just run calipro and then calipro itself is going to sample through parameter space and create those different parameter combinations that will then be fed to the model the model will be run for each of those different parameter combinations and then there's going to be some model evaluation step where we determine for each of those model runs how closely those model runs align with the experimental data that we're trying to match to then we will step through what's called redefining parameter space which is a method of narrowing in on those runs that actually do fit the experimental data in hand and then we're going to re-sample that parameter space that we've redefined and work through the calipro process in this iterative fashion until eventually we've got a calibrated model that can start to make predictions so let's open up calipro here and first talk about the inputs that are necessary to run calippro from the user end so necessary components first and foremost you obviously need a model and you need experimental data for calibrating that model too but additionally and i'm not sure i even do this step justice in this presentation but you need to determine initial parameter ranges so that could be really difficult depending on the model that you have at hand and what types of parameters you you've created with your model so typically what we do in our lab is we try and look through the literature we look at similar models that have been built before we try and pull multiple values from either experimental settings or from those similar models that have been built before and try to assign the widest range that's biologically feasible for that parameter so as an example of that if um you want to find a parameter value for how quickly a cell proliferates across time you might be able to find that from an in vitro study where they put a cell in a dish and they uh within the media in that dish there was uh something that made that cell grow across time and maybe there was four cells that uh after 24 hours in that dish so you could say that the growth rate there was four times greater um but then maybe you look out in the literature again and you see another in vitro study that has that exact same type of cell in a different type of dish that has a different type of media and that cell doesn't grow as quickly there but then you also see a study that's in vivo in a mouse and they can track how many cells came from this original cell within that mouse and so all of that to say you might have a parameter uh range that spans several orders of magnitude just for how quickly a cell might proliferate across time and so what we do in our lab is we actually assign for that initial parameter range the widest range that encapsulates the values that we've seen before okay so once you have those three uh things then you also need to supply some calipro specific user input and so this is where the modeler really comes in and starts to put on paper what they want to see from calibration and so the first way they do that is through defining what we call the past set definition and so quite simply this is what is a calibrated run look like uh as an example you might have several evaluation time points where you really want to see the model maybe peak and then drop through the experimental data and so you would then set up your pass at definition such that the model run that would be calibrated would actually go through the experimental data peak and then drop through the lower uh evaluation time points as well secondly you're going to specify a termination criteria which is basically what tells calipro to stop running and this is when the percentage of runs within your calibrated parameter space actually satisfy that pass set definition that you just defined so maybe you want in your calibrated uh parameter space all of the model runs from that parameter space to satisfy the past set definition so you set your termination criteria to 100 but if you're fine with having a little bit of wiggle room you might set it to 75 percent and then thirdly you're going to specify the method for redefining parameter space and i'll touch back on these here in a couple slides in more detail but just briefly their highest density region selection and alternative density subtraction selection so if we actually open up calipro and um we see the first step that kelly kelly pro will do is sampling parameter space after the user provides the initial parameter ranges so what i mean by that is that within calipro parameters are varied simultaneously using a sampling method of choice typically in our lab we use what's called latin hypercube sampling again using those reasonably broad parameter ranges this is a part of a monte carlo class of sampling methods where it's actually a stratified sampling without replacement method where you assign parameter distributions so if we look over here on the right hand side we've got three parameters a b and c where we've assigned a uniform parameter distribution for a uniform for b but a normal parameter distribution for c and those parameter distributions are then divided into n equal probability intervals which are then sampled for each parameter combination so if your sample size is n of five your first model run is going to use a sub one b sub one and c sub one for the each of those three parameters your second model rune is going to use a2 b2 and c2 for each of those parameters and along the top here i'm just showing for uh along the x-axis is parameter one along the y-axis is parameter two we can see that if you thoroughly sample that parameter space using latin hypercube sampling you pretty much sample the entire parameter space within that two-dimensional representation i would like to say briefly that um we are actually working in the lab right now on a paper that looks at the impacts of not only latin hypercube sampling but a couple other sampling methods in the context of systems biology so if you do have any more questions about sampling parameter space you can either reach out to any of the other authors of this study or you can email me after this talk happy to talk more about it a question yes yeah so your output sample space um the parameter space is this supposed to be continuous or discrete that's a great question um i'm actually going to get to that in a couple more slides so um if that's okay then i'll i'll answer it then maybe oh okay okay i can wait yep okay okay so once you've sampled parameter space um you're then going to move on within the calipro the calibration protocol to the step that is running the model and actually evaluating how well the model runs within this parameter space match up to your experimental data but prior to doing that first i'd like to talk really briefly about an important question that every modeler should be asking themselves when evaluating a model which is what is the form of my experimental data if i'm comfortable assigning a distribution to the observed experimental data at each time point then i should proceed with what's called bayesian calibration and there's a whole suite of strategies that are housed within bayesian calibration so if i'm proceeding along sort of this segway here into bayesian calibration um just briefly bayesian calibration isn't a way of iteratively updating the probability density function of parameters until the distribution of model run results matches the distribution of experimental values at each of those time points like i was talking about so essentially what you're doing here is you're maximizing the likelihood that model output statistically consistent with your measured data sets there's many techniques for this particularly in the last like 20 years or so this now has pretty vast literature there's a great review by uh menzies at all in pharmacometrics from 2017 um about different bayesian calibration techniques however i think there's um some disadvantages in making these assumptions about your experimental values at each time point and so in some contexts our lab's perfectly happy to use calibration uh using a bayesian technique but in other contexts we're not so comfortable making those assumptions about those experimental data points at each time point additionally if you choose a poor distribution for your initial parameter estimate this is going to require many many samples of your parameter space in order to match those results to the measured data and then i think this is kind of like what the last question was just asking which is that if you use bayesian calibration your calibrated parameter of space is actually going to be discrete it's not going to be continuous so if you look at i created these i used a bayesian calibration technique and an sir model using seven different parameters and you can tell that by using a bayesian calibration technique you're actually creating these sort of pockets within your calibrated parameter space where you're not sampling as heavily within those pockets so you can see right around 0.2 for parameter p you're not really sampling very highly here and anything greater than 0.25 for your mu sub l here you're really not sampling at all whereas other areas in parameter space you're actually sampling quite a lot intuitively this doesn't necessarily make sense to me personally that you can technically have a calibrated parameter space that is super rough and you only have certain points in that parameter space that you're actually truly sampling um and so that's why i turned towards calipro which actually creates a continuous parameter space that aligns with your experimental data so if there's no more questions right there about that i'll continue on with cali pro and talk about how we find that continuous parameter space so this next step now is that true model evaluation step and what we're going to do is we've already run the model we're going to look at our model outcomes compare it to the experimental data and perform what we call pass fail selection so as an example of these data sets that i'm talking about let's revisit that predator prey data set that i originally talked about from the hudson bay company what if instead of collecting from one location in canada we actually collected data points from multiple location at the same time throughout canada so now our data sets going to actually look like this where for the lynx population we've got multiple data points across time at each time point and additionally for the hair we have the exact same thing we've got these multiple data points at each time point you can still see the trends of the temporal dynamics across time here but now we're starting to ask the question if we have this data set in hand how well do our different model runs fit this data set and can we perform this pass fail selection so that's exactly what this model evaluation step is is you're identifying how well that model is fitting to the data set according to the past set definition that you specified at the beginning of calipro and so in this case and for this example i'm going to say that for both species the model run has to lie within the bounds of the experimental data for each of our time points so what that might look like is we've we look at our first run of the model using our first parameter combination and we can say that yeah this this model simulation fits pretty well with the data in that for most of those time points it fits within the bounds of the experimental data for both species so we would take the parameter values associated with that model run and place them within our past set data structure now if for the second run the peak for the links actually doesn't quite meet the peak for the experimental data that we see so our prediction is off here then we take the parameter values that were associated with run two and we place them in our fail set data structure so even though the hair outcome matched because our past set definition said we need this for both species then we're putting that into the fail set so you can see how important that pass that definition is that you made at the beginning of calipro in determining what you think is a correct model fit we can step through a couple more examples here where now the parameter values from run three are placed in the fail set uh data structure and the parameters associated with run four are placed within the past set data structure because now we've got a model run here that matches the experimental data okay so once we've got these past set data structures and these fail set data structure we can then leverage information from these uh the parameter values associated with past runs and parameter values associated with fail runs to redefine our parameter space so the first step in doing that is to build parameter density plots and so for each parameter you're going to build these parameter density plots where along the x-axis is that initial parameter range for that parameter and then the y-axis is the density plot for the parameters that come from the past runs and those that came from the failed runs so at this point we can take sort of a step back and just look at this qualitatively and we can say there's a lot of information hidden within these plots where if that parameter value for parameter alpha is quite high then we can see that it's much more like much more likely to fail than if that parameter value is lower where it's much more likely to pass and the model runs coming from parameter values that were lower for this parameter um we're actually more likely to fit our experimental data so once we've got these parameter density plots for each of our parameters that we're varying we can then redefine the parameter space by using this information that i've just outlined and so the first way of doing this might be hdr selection where we'll take the past parameter density plots and we're going to identify a sub-range within that past parameter range of values so this sub-range i'm showing in purple across three different parameter examples shows the region along the x-axis where 50 percent of the past parameter set values exist so 50 of the past model runs had parameter values that existed between 0.4 and 0.8 for this first parameter 0.1 and 0.35 for the second parameter this method of hdr selection was actually developed in 1996 by rob heinemann in order to summarize distributions and and just uh was a new way of doing some summary statistics but we've kind of taken it for our own use here within cali pro sometimes uh the smallest range will actually be disjoint particularly if you have a distribution that looks like this at this point in time we've just said we're going to take the minimum and maximum between these two disjoint sub ranges and we're going to make one very long sub-range for our next iteration of cali pro just so that way we're not too quickly narrowing in on parameter space that that might be interesting okay so the second option here instead of hdr selection is what we call ads selection so this method actually leverages information from both the past parameter sets and the failed parameter sets in order to create our new sub range so what you do is you actually normalize the density so that they now exist along the same y-axis for each parameter and you subtract the failed density values from the past density values in order to see where in along this parameter range the parameter values were more likely to result in runs that passed than runs that failed and so you pull as your new sub range all the areas that are continuous that have a positive value for after performing your ads method of refining parameter space so it's up to the user um prior to running cali pro whether they use hdr and ads typically we suggest using ads because that leverages information from both the runs that passed and the ones that failed but hdr works can work just as well depending on the situation so following the redefining of parameter parameters based step and you've done this for each parameter within cali pro you now have these sets of new parameter ranges that you can re-sample within your this parameter space re-run the model re-evaluate how well those model runs fit within this now narrower and more redefined parameter space and you can work iteratively in this fashion until the model calibration is complete according to what you decided earlier was your termination criteria and so calipro is absolutely an iterative process then um but once you actually have the calibrated uh model you can then go on and do all of the really cool things that math modeling can do and start making predictions and and explaining different biological phenomena so that's the um broad strokes of what calipro does and and we did get into some of the nitty gritty on how it redefines parameter space but let's see if it actually works so we took our predator prairie model and the synthetic data set that i had created when i talked about pulling from multiple locations in canada and we tried to calibrate our model to that experimental data set and we can see across six iterations starting with pretty wide initial parameter ranges we were able to calibrate the model to the experimental data at hand we started with 500 model runs in each iteration and had 21 time points that we were evaluating for each species whether the model runs fit or not according to our pass set definition and so altogether we had 42 evaluation uh points where we were saying is the model run actually fitting our experimental data so our past set definition was that at each time point basically the runs had to lie within the bounce of the experimental data and we gave it a little bit of wiggle room on either side of the min and max value for each of those 42 evaluation points we used hdr selection and for this uh model with a coverage of 85 percent so we were slowly moving in on our parameter space where the um where 85 percent of the uh past parameter data structure held its values along from the initial parameter range okay um so obviously if we look at this the first iteration of runs did not have a lot of runs that were even within the bounds of the experimental data in fact our model sort of blew up a couple times and really didn't fit the experimental data well at all by the second iteration we could start to see we're matching the temporal dynamics of the experimental data and as we kept iterating through we really started to hone in on that experimental data space and were able to match it with our model predictions but importantly depending on the context here a modeler might be satisfied stopping at the second third or fourth iteration and not going all the way down to having 98 of the model simulations within our calibrated parameter space fitting this experimental data depending on the question at hand your how much you trust the biological data that you're getting you might stop at the second or third iteration because you can see you're capturing the temporal dynamics and some of your runs 35 of them are lying within the bounds of the experimental data for each of those 42 evaluation time points but also some of them are peaking a little bit higher or lower and you might like that depending on the types of questions you're you're asking you might like that variance in your model outcomes so again it's up to the modeler calipro has the flexibility here because you will decide in formulating that pass that definition when you want to stop if we open up cali pro here and we look at the parameter density plots that are created during each iteration we can actually see how that calibrated parameter space changes across these different iterations so in this big graph here each iteration is shown as a different row and each column is a different parameter value and the past parameter density plot is shown as the blue density plot and the failed runs parameter values are shown as the red density plot here so if you look at just the beta parameter range for the first iteration you can see that our hdr selection with 85 selected this purple band here which uh identifies our next sub-range for the next iteration which ranges from about 0.2 to a little over 0.6 uh 0.065 additionally you can see this um yeah go ahead so uh do you when you fit a multiple models in your cases 500 models is there any way to encourage model diversity in other words to make parameters from different model as diverse as possible for example if you have all 500 models have identical parameters then they are as good as just one model right yes so that is what calipro does right now it is the same model it's just different parameter values so from each model run there's 500 model runs those are each a different parameter combination so the number of parameters doesn't change from uh model run one to model run two right now so you're asking no it's not my mask so i understand that all 500 models have the same functional form that's fine okay but if all the 500 models their exact modal parameters are identical then oh yeah no so they're not they're not no they're not right now and so that's that goes back to our sampling scheme with uh the hypercube sampling ensures that they're not going to be the same so how how do you encourage the parameter diversity so the sampling scheme itself will uh have parameter diversity you could encourage even greater parameter diversity by with those initial parameter ranges that you assign to your model being very very wide um but they're currently within cali pro there's nothing that's going to um directly encourage parameter uh diversity by stepping outside of those initial parameter ranges for example so uh it could be something that we do in the future moving forward for sure okay so in other words there's nothing to prevent all 500 models to converge onto the same final answer um well there is the you're trying to capture the spread of the uh experimental data right if you were trying to match to one single trend line then they would probably converge to the exact same parameter combinations right but if when you specify that parameter set definition as saying you really want to capture the variance there for your um experimental values across each of those time points then that would intrinsically just encourage a little bit more diversity within your parameter sets uh it's hard to say right for example in protein structure prediction is a similar problem where we have distance with and if you set a distance with strings even though every distance with string you have a minions and variance your final model will be converged if you use weight and descent so yeah yeah i i guess it depends on what kind of data set you use if exactly yeah if you don't explicitly encourage the diversity then in some cases they will converge yes yes absolutely right okay um i would love to talk with you more about that um i just have a few more slides that i want to get through so um okay uh looking at these perimeter density plots you can also see that like for the delta parameter here that there was a very narrow range of parameter space that resulted in past model runs at that first iteration if that value was greater than 0.04 there was zero percent chance that that model was going to fit your data set that you that you've given it so there's there's a lot of information embedded within these parameter density plots so that's all well and good we were able to use cali pro to calibrate this relatively simple model with a synthetic data set that we had in hand but historically within our lab calibrating these much more complex models that we've built was very frustrating and it's a much more difficult calibration process because sometimes you have a large number of parameters um sometimes those initial parameter estimates are uncertain as i alluded to with the example about cells proliferating in a dish in different conditions or even in a mouse and so in the past this has been a really labor-intensive step in our lab for developing and building models and is really keeping us from doing and answering all of those cool questions about what's happening during the immune response to tuberculosis and so that's why i came up with this calipro method and i'm hoping this is where kelly pro can jump in and help the most so for my next example i'm going to use calipro on a virtual host model of tb but quickly i'm going to take a step to the side and just talk about a little bit of the biology of the immune response to mtb just so we get a sense of where this model is coming from so it begins uh the immune response actually begins to empty the moment an individual inhales mycobacterium tuberculosis which is the bacteria that causes the tuberculosis disease and once that bacteria lands within the lung environment a type of immune cell called a macrophage will pick up this bacteria it'll grow progressively thicker it'll burst spewing even more bacteria into the lung at which point in time another type of immune cell is going to pick up some of that bacteria travel through the lymphatic system and enter the lymph node at which point it's going to uh interact with other different types of immune cells and those immune cells called t cells will travel back through the bloodstream re-entering the lung environment and entering the granuloma environment which is the the wound that forms during tuberculosis and so this is a very complex process but what i've just outlined is sort of the broad strokes of what we know biologically about the immune response to tuberculosis so using these sort of basic theories and these rules and mechanisms here i built hostsim which is a host simulation that responds to mtb infection following what we know about them in response and so it's a virtual model of tuberculosis where we've got a lung architecture here an individual will inhale mtb and a granuloma will start to form cells will travel from that granuloma in that lung environment into a lymph node which then causes different types of immune cells to prime proliferate differentiate travel through the bloodstream and re-enter the granuloma environment across time so i built this model it sort of worked sort of followed the uh in the known biology about the immune response but i needed to calibrate it to the diverse data sets that our lab has access to that i talked about before and so i used calipro for this so i calibrated hosts into these diverse data sets using calipro in each iteration we created a thousand different parameter combinations were generated according to lhs in each iteration it took nine iterations for calipro to actually calibrate this model and we redefined parameter space using ads because this was a large parameter space and we really wanted to leverage the information from both the past parameter sets and the failed parameter sets at termination um we had stated that we were totally fine if only 75 of the model runs had fit within our evaluation points that we had specified but um we actually ended up with 83 of the model runs or uh satisfied the pass that definition upon calibration so on the right here i'm showing some of those experimental data sets that we have we've got granuloma cfu macrophages and t cells those different immune cells that surround bacteria within the lung experimental data is shown as the black dots across time and then the gray min median and max lines are simulations from our calibrated parameter space additionally on the right here i'm showing some of our blood t cell data sets and the min median and max shown in red now of our simulations from our calibrated parameter space so this was really cool um we were able to actually calibrate this complex model to the different data sets that we had at hand and so then once we had calibrated hosting we took several validation steps and so first we just looked at the model itself said are there granulomas forming how many are forming compared that to data sets that we have that say how many granulomas form throughout the process of infection we can see that in the bloodstream we've got different types of t cells again that are developed that are um growing in magnitude across time and peaking and uh there's a i'm sort of glossing over a whole lot of stuff here there were even more calibration data sets there was also more validation steps that we took but in the interest of time i'm going to keep moving forward so now that we had this calibrated hosting we could make some really cool and interesting predictions so we were able to capture dissemination which is the spread of bacteria throughout the lung environment this is really bad for the host we are able to predict a role for a certain type of immune cell to prevent that from occurring we were also able to uh calibrate host into the single infection of events but then validate it against reinfection events and identify what causes protection against reinfection in certain individuals during tuberculosis which has clear implications for vaccination and we're really excited about that so moving forward i'm also going to layer in vaccination data sets that we're getting from our clinical trial collaborators in south africa when they're testing new vaccines in people and so um we're hoping to use calipro together with hosim to calibrate to those data sets and make predictions about the best vaccination strategies for against tuberculosis so we're really excited about calipro and we're really excited about hosim too um so if you're sitting out there and you're wondering if cali pro is right for you um i've built this decision decision tree here so first you should ask the question of what am i actually calibrating to and if i'm calibrating to a single trend line for several different outcomes you should absolutely use the traditional calibration scheme optimize an objective function using any of the multiple tools that are out there they've been out there forever they're really smart and efficient that's the way to do it but if you're calibrating to a range of data for each time point for your multiple outcomes and you do feel comfortable assigning a distribution to the observed data then you should use bayesian calibration techniques there's a lot out there right now that literature has been really flushed out within the last 20 years there's a lot of cool techniques there too as well but if you don't feel comfortable doing that and you want a continuous parameter space then you should really use calipro here so what are some of the benefits to calipro right now we think that it's a benefit that it's a supervised method and we think that it's good that the modeler can actually control a lot of things like the number of model runs that are created the max number of iterations that calipro could go through how you redefine parameter space and that initial input of the widest possible parameter ranges at this point in time we think is a good thing that calipro can't step outside of that and pick biologically and feasible parameter values that might allow your model to fit to the data and how the modeler does this is through that past set definition where you're really explicitly integrating what you know about the model what you know about your experimental data set into a concrete and formalized calibration goal these calibration schemes don't exist on an island you can use calipro to find robust parameter space and then use one of those optimization schemes if you think that there's like one individual who's got really cool time series trajectory in your data set and you want to personalize your model to that individual and start to make predictions about why that individual looks that particular way like why that individual responded to chemotherapy for example so future directions for calipro um as with any sort of software or coding implementation we could always generalize it more and be able to handle anyone's model implementation we've tried calippro with transmission models ode models and agent-based models we look forward to trying it with network-based models we think that right now at least calipro is definitely model agnostic um basically what you're just doing is you're empirically looking at where in parameter space your model runs fit the data that you give it so that's not necessarily contingent on how that model is run but there's some heavy coding involved there for the actual calipro software to handle any sort of model so if anyone knows an undergrad or anyone who's super interested in that type of computer science we would be happy to do something there with them so extending beyond calibration and looking at those parameter density plots i alluded to this earlier there's a lot of interesting behavior that you can look at even within your calibrated parameter space by creating those different density plots for different conditions i actually did this earlier this week with bruna one of our lab mates who i think might be listening in right now where she had a calibrated parameter space where some of the model runs fit the in vivo data set that she had but she also had an in vitro data set and some of the model runs fit that and so what we wanted to see was where in parameter space was there uh model runs where we could capture both and so looking here at the four different parameters that were being varied we can see that parameter one and parameter three were really important in determining whether that model was going to fit the in vitro data and so we could see that if you narrow those in vivo uh initial ranges parameter ranges then you might be able to fit both in in vivo and in vitro data sets additionally and kind of finally here after the final iteration of calibration you could look at those past parameter distributions as an example for how to set your prior parameter distribution for future model runs so looking at the beta parameter range here in blue the past parameter density plot you could maybe be content by giving that a uniform distribution from the range that's specified however if you look at the alpha and sigma parameter ranges here you might be more likely to uh set a parameter distribution that's a normal distribution for sampling or centered around 0.6 or 0.8 for those two parameters so i'd like to thank all the members of the lab uh current and past uh who i've spoken about this with or just lamented the process of calibration and how we got stuck in calibration hell for a little bit um i'd like to thank both of my advisors denise and jennifer for constant support as well as my committee members who were sort of the first run of this presentation last year and then our collaborators who provided the experimental data that i showed on some of those plots with the virtual host model of tuberculosis so um thank you i'd love to take questions if we have time um i've included qr codes so you can look at my paper or you can actually go to our website and actually download a zipped folder with example calipro matlab code that you could then start to do your own implementation thank you um thanks lois that was really great are there any questions i haven't seen anything come in the chat pod yet yeah um if anyone finds that they have questions after this if they download that code and i mean if it's not working that would be the worst situation but even if they download it and they want to just get a better sense of how to use their model within this framework feel free to email me i'm happy to answer any questions about this great well thanks everyone for attending thank you lewis for taking the time and uh erin whenever you're ready you can uh just turn off the recording i think thanks