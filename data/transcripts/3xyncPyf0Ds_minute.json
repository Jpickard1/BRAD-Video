[
    {
        "start": 1.199,
        "text": "thanks um uh so i was telling y'all when i first started my job michigan i actually moved in january and it's november right now but it feels like january but what i will tell you is it's 24 degrees outside but the thing i remember the most about being in michigan was like the warmth of the colleagues and i felt like you guys have been super warm with me today so i think you're warm except for what's outside so thank you so thanks students valley for me to talk with me and uh i'm not sure you cast a little pun and i was introduction she meant reproduce and we have our review stability today um so uh yeah just a little bit of back so i always you know i want to get the collaborators out first because it's really important i think you can option at the beginning so this is joint work and that's how i mentioned i've been uh since here i've moved to penn state and now used to colorado so in fact uh this actually worked that started back when i was at penn state so uh so daisy feltron was a phd student who worked with me and uh the methodology i'll be talking to you mostly about is basically former dissertation and that was joint work with uh jaffe liu who's a biopharmaceutic student at penn state "
    },
    {
        "start": 62.16,
        "text": "and uh he was my colleague in statistics there and then uh i will have a little bit time to talk a little about metabolomics um which is how al and i are connected and um that part's gonna be joined with uh several people at uh in my department including tushar gauche wenming shang and kyrie kakaris okay so uh you've probably heard about this reproducibility crisis environmental reason um and uh you know you we've seen uh studies retracted withdrawn because uh they weren't reproducible and you still see that sort of happening at a higher rate so so i think it's fair to say that we need better methods to enhance on your value reproducibility but it sort of begs the question is what does it really mean because it's one of those things where that word means different things to different people so i thought before i talk about the way i think about revisibility for for today's talk i'll give you some other examples of reproducibility so sorry for the it's really you can't see the screen here but this is actually um reproducible uh epidemiological research "
    },
    {
        "start": 123.52,
        "text": "from a paper by roger peng through the professor johns hopkins written quite about reproducibility and his um get sort of a four-step process for reproducibility again uh probably hard to read from your seating but basically the idea basically you have data methods documentation and distribution so he so he goes for the most extreme case so basically for him reproducible data basically means you actually as a reader get the same data set that the authors analyzed okay uh methods obviously there's um okay so with methods there's the analysis you run and and and potentially you should be able to reproduce that as well and then not only is uh he's arguing for "
    },
    {
        "start": 183.76,
        "text": "not just the methods but also having documentation distribution so documentation would say something about like why you made the certain choices and model assumptions and what models you fit and why you fit them and then distributions sort of let people access the software there's a second framework this is one by victoria stodden who's at the years of illinois and she describes various kinds of reproducible reproducibility requirements the first one basically is reviewable research and that's basically saying that's the one i think we most most engage in when you read a paper hopefully the authors have written a method section where they describe how they analyze the data you look through that method section you basically try to see if the results at the um if they look uh if they they're they look credible so that'd be reviewable research and again as as students as faculty we review stuff we review papers review grants so that's one we engage in a lot point two is probably more with the way roger was defining on the previous slide "
    },
    {
        "start": 244.08,
        "text": "which is replicable research basically you have all basically this would be that you have all the materials and tools made that would allow one to duplicate the results of the research and again how many of you are familiar with uh our markdown a few hands how about uh jupiter notebook more hands um those are examples of tools right that potentially allow you to do applicable research um confirmable research again you would basically the main conclusions would uh allow you could be obtained independently without the the use of software provided by the author so confirmable is kind of what a lot of us do anyway right you see you get to find it let's say you're a basic scientist you get a finding from a paper you might have the same organism or system in your lab you basically see can i can i do the experiment the way described in the paper i get the same thing that'd be sort of the basic science version of what it means to be a convertible research audible research would basically would uh basically audit it means like you leave a trail like a paper trail and this would basically be you have data and software archived so "
    },
    {
        "start": 306.32,
        "text": "that research can be defended later so how many of you uh are familiar with something called github that's good um so github is basically something that has something called version control in it and the version control basically allows you to do exactly this audible research files and then obviously then the last one would be open reproducible where you basically make it open to everybody so those are two notions of reproducibility we're not going to actually cover either of those really today we're going to do something much more much more basic and the setup that we're looking at is you have a high dimensional mix data set where you measure things where you've measured you have the same sample measured two times so that's like the most basic problem in a very basic problem so this is not a big data set the way uh allah was describing this is like two samples so obviously one way you could measure you could sort of assess uh the reproducibility is to look at the correlation right between the two samples and again there's sort of a finding by uh my "
    },
    {
        "start": 368.479,
        "text": "colleague lob where basically some genes would show high correlation and and and other genes would have almost no correlation so so she had developed something called the irreproducible discovery rate or idr and uh if you're familiar with again one common quantity we see quite a bit in bioinformatics is false is false discovery rate so how many of you ever used like a q value for doing stuff i see hands good uh how about vegemite help with procedure does that ring a bell on the other hand um so yeah so those are examples of methods and procedures that control the falsity rate and this is sort of extending that idea to looking at irreprisable false hearing rate and basically there's a modeling approach done by uh chu la and that was um what's based on a mixture of copulas to estimate the idr and that's kind of our kind of setup that we'll talk about later so here's an example from a picture from a paper about how this worked and and basically um idr so it's basically of the of the discoveries you find what what what "
    },
    {
        "start": 429.759,
        "text": "fraction of them do you expect to be uh irreproducible and irregularity is a bad thing right that means unable to reproduce so this is actually from a chip chip yeah chip chip or chipset yeah no it must be a chips uh well a chip experiment i'm not sure if a seeker or a chip but basically this is looking at different color uh peak colors uh for finding uh for for finding peaks in the chip in the chip data and basically what you see is you know some algorithms uh once you on the x-axis you basically have the number of significant peaks and then the y of the idr so again with idr the the larger the number is the worse that is right you basically see i mean by definition the more things you're calling on average your idr is getting worse so what really matters is is like how how fast you rise the algorithms that don't work as well are the ones that rise so i think quest would be an example of an "
    },
    {
        "start": 490.56,
        "text": "algorithm where we spreads it very fast a one that rises not as fast would be max which is the red line down here so so based on this you'd conclude something like you could compare uh the quest versus the math you said the max has the lower idr okay so our goal is actually much more modest uh than that we actually just want to say we have this we have this setup we just actually want to be able to say how can we estimate the fraction of reproducible markers okay and when i use this for markers i'm actually using it like fairly generically so markers could be genes markers could be proteins markers can be metabolized we'll see that later but we basically want to have a sense of like and again technical replicate means i have the same sample i've just sent it to two labs and have them run the same platform okay and basically i'm hoping right that those two things should be fairly concordant and and again so one of the cool things about this methodology is basically it's going to sort of say because i'm measuring so many things i can pull information and say some of the things "
    },
    {
        "start": 551.44,
        "text": "are concordant but some of them are not so i want to get a sense of how how many things are important okay so here's sort of the setup so so i said markers last slide i'll call the i'll call it marker a gene now so i assume i have n genes i'm assuming that i'm measuring something about two experiments so it's the same sample so uh measured two times and then i'm calling from the first first time x the second time y and then i basically convert stuff into ranks within the uh the experiments so for each gene you basically get two ranks uh this one and this one this is the rank of the gene and among the x's is the rank of the g among the y's and the x's are lab one y's or lab two okay so here's kind of the intuition so i basically plotted our two different pictures here so this is from lab one this is from lab two and each dot corresponds to let's "
    },
    {
        "start": 612.16,
        "text": "say a p-value for a gene uh so basically what you see is uh what are black dots and not quite as black dots we'll call them not quite black dots grey dots so you basically see that the gray dots are all sort of clumping near zero zero those are going to be the reproducible the black dots are basically the ones that are not reproducible okay and we want to be able to see what we basically want to do is kind of a clustering problem we want to be able to separate out the black dots from us okay so all i've done in going from this picture to this picture is now i've created that rank thing so obviously this was about so this was a thousand genes that i simulated from and actually know in this case it's simulation i know which ones are the reproducible ones which ones are not so 1350 are reproducible so basically now all i'm doing is basically converting the "
    },
    {
        "start": 672.8,
        "text": "uh converting to that rank statistic i told you about so each point you have the rank from lab one ranked from lab two so what do you guys notice about the picture in in on the right hand side what are two things to sort of notice yes exactly so so if things are ranked near the top they're highly correlated and then basically so so this point cloud is quite tight near this the bottom left and as you go that along the diagonal you go from really clumped to much less pumped okay so so that so again basically and the way we're sort of formulating the problem is we're basically thinking there's a magic transition point where you go from clumped to not clumped what we want to try and do is see can we come up with a way of estimating the transition point okay so this is basically our statistic "
    },
    {
        "start": 735.519,
        "text": "we have that ranked gene so that for g for each gene we know it's ranked within x's within y's we see the maximum of it and we call this the not surprisingly the maximum rank statistic and then what we're going to do is basically take that sort of divided by the number of genes so this number is now going to be between 0 and 1. and we do something a little bit different than what what most people do statistics in statistics we talk a lot about the cumulative distribution function of the empirical cdf and that's basically the fraction of observations less than or equal to something now if you look at the formula up there i basically what was i've actually flipped the sign around this actually was called an empirical survival function and um i'll show you hopefully later why this is sort of like a easier thing to work with than the empirical cdf here but basically we're going to be using that empirical c this empirical cdf of this statistic divided by the number of genes is kind of our our distribution function okay so here's basically the idea "
    },
    {
        "start": 796.079,
        "text": "we have a we have that empirical cdf we call that hat s i'm actually going to formulate what i'm going to call a theoretical null version of s and and then basically i'm going to use this optimization problem basically estimate this pi 1 and pi 1 is going to be the fraction of genes that are reproducible okay okay so what you look over here is you basically see this this is the that empirical c uh thing i defined earlier and subtracting from it one minus lambda this this theoretical s and i haven't given the formula for s it's in the paper but but basically you can think of it as kind of like a curve matching problem okay so have s gives me a curve one minus lambda s gives me a curve i basically want to see what value of of uh land yeah what value of lambda is going to or an eye land is going to give me the point where the curves over are basically map align "
    },
    {
        "start": 856.959,
        "text": "so how many of you guys work with a lot have used alignment algorithms this is yes i still get some hands which is good so so there's things like you know dynamic programming that's a very classic alignment program this is like way way simpler but really you'll see on the next slide basically all all you think of this is basically a curve alignment problem okay in fact here here's the curvature problem uh so i told you about that objective function on the on the previous page so again this is simulated uh this is a simulation from before we had you know 100 a thousand genes 350 were reproducible this is looking at that same picture and this is basically looking to see where exactly is there a drop um and you actually see so here the the minimum here would basically be 0.348 which is a little bit different from but not too far from 0.35 0.35 is the truth we simulated here if you look over here um "
    },
    {
        "start": 918.56,
        "text": "sorry based on this picture the you have the you have dash lines and you have this red dotted bunch of dots the red bunch of dots is that empirical as half and the dashed lines are various versions of s and again we're trying to get the curves of the s so basically the uh what's up here are basically different for different dashes correspond to different lambda values and we're basically trying to find the lambda that best matches the dots okay and that lambda value will give you up this pi 1 estimate okay so that's kind of the algorithm now the question is like why does this work so we're gonna go do a little bit of a blast to the past so full disclosure when i was here in michigan the last class i taught while i was here was bio6646 does that class or 646 of that class still exist probably not the class was on physical "
    },
    {
        "start": 980.88,
        "text": "methods for bioinformatics i had 15 students in the class 10 were from here from this department so so yeah so this table was from that class so i had to dust it off you know it was on paper or a transparency or something and convert it i'm kidding it was already a slide but but basically one thing we talked about in that class was when you have high dimensional discoveries like think about different error rates so multiple testing is a big issue and what i'll talk about now is thinking about how you can convert this this small idea to think as a multiple testing problem um so basically this was a multiple testing table that i showed in that class i thought here so uh again you have all these hypothesis view tests uh and let's say here you're you have the total of m tests but you basically do one test at a time you decide do i reject or do i not reject okay the number of rejections i make is it would be q here the number of things i don't reject by definition now when you think about multiple testing "
    },
    {
        "start": 1041.039,
        "text": "the decision to reject or not is based on the data the role here is basically the truth that we never observe it's the null so when i say null that's short for null hypothesis it's not hypothesis true or not now they're satisficing the audience they will quibble with my use of knowledge true versus null is not als it's been a big controversy but but basically the what that means is the stuff inside this capital u capital v capital t capital s these are not known because we we never know the true labels in the row for the rows okay okay so based on this table though you can define various error metrics and the one we'll talk about today is something called the marginal false discovery rate and that's given by this ratio so basically q is the expected "
    },
    {
        "start": 1102.96,
        "text": "number of nulls we reject and then v is basically of the ones we reject how many of those is the null true so you make mistakes with v you make mistakes in t so we're looking at v and v as a function of q here and again for us if we reject h0 that's the same as calling it gene reproducible conversely if we fail through ejecta zero we're we're basically saying that gene is irreproducible and then you can think of fdr as roughly the expected number of true irreproducible genes out of the genes that we call reproducible so again these are so if i call them reproducible but they're not i'm making mistakes right and we're sort of estimating a marginal fdr for each gene this way and uh the so the q is basically how many genes are reproducible in a certain critical region and then k hat is a certain critical value and then v or how many are actually irreprisable but we call them full plug again v are the mistakes "
    },
    {
        "start": 1167.039,
        "text": "so again the goal of multiple testing is to figure out what what are thresholds we need to make sure that we control the fdr and we're basically doing exactly the same thing here except with multiple testing p values we're instead calling them reproducible genes and not reproducible genes so that's kind of sort of how we can kind of combine the two problems so let's get that gets us to our mar procedure um and basically here's kind of an algorithm you get a cutoff hat k based on this uh optimization problem and then you basically say n hat is going to be the number of reproducible genes and that m has satisfies this rule and then you basically reject all genes whose maximum ranks are less legal than half the rule and again it's um so it's kind of you know so it looks complicated we're basically so if you're familiar with q values you might have heard of "
    },
    {
        "start": 1227.2,
        "text": "the name john story he's a staff agent of princeton who sort of came up with that ideas part of his phd thesis this is basically applying the q value algorithm to our problem so it's basically just so the the trick for us really was figuring out how to so he maps a ver he uses cds of p values here we use the survival function and then we kind of compute stuff here to figure out how to get the fdr but pretty much it's it's very much like most multiple tests but most so if you've ever used benchmade hook hookberg there's a rule like this for that if you ever use q values and compare it to a cutoff you're using a rule like that so in a way it's actually not any different than things many of you have done in your other projects okay so is the application so this was a rna-seq experiment as part of the cc like one of these quality control projects so uh this is obviously gonna be with "
    },
    {
        "start": 1288.08,
        "text": "rna-seq data and there were uh three platforms and i think there were 13 labs i think we only get 13 lamps but there were 13 labs in the in the photo so the platforms that were used illumina hi-c 2000 and roger 5-4 and secret inc okay so what's kind of nice about this study is actually for some of these uh genes they actually had done pcr validation and as you'll see later we'll actually use that as a benchmark to see you know dinner method help or not okay so here are um a couple labs and again um the data really come from uh chinois our my former colleague and her collaboration with people at penn state so um so this tells you a little bit about sequestering platforms and the from the different labs so here's three of them mayo psg penn state and new york university so things you notice here uh so obviously one thing is the read depth which is how many reads you get from the machine that number is like insanely variable "
    },
    {
        "start": 1349.039,
        "text": "it is super super high for mayo it is super super low for nyu does anybody know what that is mayo has better machine uh maybe i i think we're gonna we're gonna try and test that that's the hypothesis here but uh but but actually with with the sequencing machines right i mean basically the practical reality is well so one thing is the aluminum the reeds themselves the fragments that come out are actually shorter than four five four four five four fragments so that's one thing but the other thing too is cut i mean mayo might not have better machines but they might have the pi's willing to pay more to do the sequence thing and that's what's really sort of i think being reflected here is basically like you can think of you get you know you can actually think of sequencing as having a sort of a cost "
    },
    {
        "start": 1409.84,
        "text": "per read and basically maybe what's really being shown is mayo is willing to spend more to run the experiment so i think those are two reasons um yeah so anyhow so here's some other examples uh so this sort of told you how they did rep so i told you there's one sample they sent to these labs they have the option of doing you know replicas and stuff like that so you also see here so again there's some other universities um i think bgi is probably beijing genomics institute and wu is northwestern and i'm i can't say off-topic what sqw is but putting in more uh more data and sort of the on the replicas and re-depth and again the aluminum ones are definitely much much deeper reads than the 454 here okay okay so here's some results for the procedure "
    },
    {
        "start": 1470.72,
        "text": "and for the comparison we actually also looked at comparing it with this popular model that i mentioned earlier um so this was um this was uh the work of uh from the leodoll paper from 2011. so we have several comparisons here um so we'll start sort of looking at the uh let's see so this is looking at replicates uh within labs okay and and so one thing you sort of see here is basically so okay so let me go to the columns here a little bit slowly so pi 1 is the same for both methods the fraction of genes that we think are reproducible rho a hat is basically of the ones we think they're reproducible what's the correlation of those and then finally this last cup column which is fdr less than 0.01 or idris 0.01 is if i already use a cut off or estimating the idr or the f or or our new fdr measure and and say that "
    },
    {
        "start": 1532.88,
        "text": "you know how many genes pass the cut off if they pass the cut off remember we say they are reproducing okay good so anyhow if you start to look at stuff within labs uh one thing you notice uh is that the fraction of reproducible genes is comparing like these three numbers to those three numbers is higher for our mar method so we're actually estimating a higher fraction of reproducible gene then uh the copula approach if you look at the next column though you get a sort of a paradoxical thing which is again it's like you know it's in the third decimal place usually but like the correlation from the copula method of the reproducible genes is slightly almost the same but maybe in one case slightly higher "
    },
    {
        "start": 1593.6,
        "text": "and then if you look at how many again because we're calling a lot more genes reproducible we're also calling a lot more with the rule our rule tends to call more genes reproducible than their rule okay so just a couple reminders uh the other thing to notice here is that the um the pi one again this is this is the same sample right that's been run at all these different labs and you basically see the pi one changes a lot because you go from like point five six six for me for nyu up to point 0.851 for mayo so one thing we sort of concluded from this experiment and we did some other analysis we'll talk about shortly is that the read depth affects the reproducibility a lot and this probably makes sense right the more the deeper your reads the deeper your sequencing is the more reads you get the more reliable your findings are going right but like i said it comes with a catch the more reads you do the more money you have to spend right "
    },
    {
        "start": 1654.96,
        "text": "so it sort of gets to a sort of a classic kind of thing and that we think about a lot is basically what's going to be the bang for the buck in terms of uh benefit again just to remind you like this was the one go back a couple slides so 600 000 roughly for the nyu and then 150 million for the so 600 000 gives you a pi one of uh 0.566 and 150 million gives you a high one of 0.85 so again so that's uh so that's sort of one interesting finding from this analysis the next one is now we're going to compare the same platform at two different labs so that's four five and six so if you do that and look at mayo versus uh beijing genomics institute you sort of see super high concordance for the lumina and again uh with uh psu and nyu uh you get um slightly lower and then the lowest is the roche "
    },
    {
        "start": 1715.52,
        "text": "now what i found interesting about this uh analysis is that if you you're actually getting something very kind of non-intuitive which is that the between lab reproducibility is actually slightly higher than the within lab reproducibility so anyone have thoughts as to why this might be unfortunately this is a little story this paper got published "
    },
    {
        "start": 1776.96,
        "text": "and we never actually addressed it in the paper so we still we we were aware of it and had no great explanation so but yeah just something to know um this is not what you should be expecting you should be hoping that with the with within lab reasonably would be higher than the the between lab okay so this is the validation yes good question so i so i think um yeah uh so so chinois has a paper where she did this her approach and they did a similar kind of study with uh i think it was chip with either chipper chipseek same sample multi-lab thing i think they got very similar i think they got some more kinds of things so i mean i i only have my n of two here so so uh the two times we've seen this we've seen this phenomenon okay this is the validation i told you about so i told you for some of these genes we "
    },
    {
        "start": 1837.679,
        "text": "had um we had gold standard measurements for pcr so we then basically did was we basically we did the um we had different comparisons and we basically looked to see like of the comparisons of the things that were reproducible how many of those were pcr validated and what were their ranks okay and again with ranks so here what you want is like the lower your average rank which is going to be these vertical lines the the the the better your method is doing so so we have six comparisons from the previous page and we're basically looking at um so the the one on the left is our non-parametric approach mar the one on the right is the is the copula or the one by leodoll 2011. and you basically see for there's some you know for for this one is close this one's close this one gets bigger and this number four the copula method didn't find any of the pcr genes "
    },
    {
        "start": 1898.08,
        "text": "as being um reproducible and same with as same with five so i saw the six comparisons we did only in the sixth comparison did the copula method do better but the other thing you started noticing is that again we were calling more things reproducible again just because again because we find them were really reproducible they were also more likely to be sort of um pcr validated as well so so the number of points we seem to be doing better and then the average of the rank in most cases we're doing better and again this is always the challenge of mine for maths right we can make predictions all the time the question then becomes how do you validate the prediction to be able to make the claim that your algorithm is even better than other ones so the questions about any of this so far oh good question uh so "
    },
    {
        "start": 1959.6,
        "text": "we just used we computed this as kind of so again the algorithm all the algorithm really can do is do this part once you get that right you then make a decision plus what's not and then you can compute summary statistics so i mean this is sort of this is very much tied to this and um so i you're so i don't think they're because they're tied i don't think you can say one's like more important than the other because they kind of like the first one depends on the other right and basically sorry about the shiny app issues here guys um so let me put this away if i had if i called the just the first if i called i think it by chance the fewer genes you call reproducible the more your higher your row will be um because again go back to the picture "
    },
    {
        "start": 2022.72,
        "text": "the row is something about the row has something to do with this picture here right and what's the correlation and again sort of like if i take the smaller if i look at small subsets near the origin i'm going to have artificially high correlation if i keep if i make my ovals bigger right at this point i'm going to if i start to put in more and more black dots my question goes down so the correlation is going to be very much a function of what i call the cut off and what i call reproducible or not the cutoff depends on pi one so they're sort of linked i'm not sure you can decouple that you pronounce the way you're thinking about it other questions yeah good question so this is sort of a i mean so obviously this is one example i mean so i mean the basic fundamental question we're asking is like how correlated are "
    },
    {
        "start": 2084.079,
        "text": "two things right so and and this is called and look and looking at correlation setups like this where you are measuring on a sample a lot of things right so you know this is an example with g you know other things i work on i work on um things like brain imaging so brain imaging you have voxels and you have you know measurements and voxels and then you might want to know like if i've measured the same brain twice but you know two times how quarterly of the voxel it would be the same setup so it's very much so so what this will get at right is something about your instrument that you're using and how reliable is it in the measurements and that's a very general phenomenon right you guys work with all kinds of machines you work with mass spectrometers you work with sequence experiments you work with fitbits you work with monitors you want to know this is getting something at basically what's the measurement error of the "
    },
    {
        "start": 2144.56,
        "text": "and this will sort of start to tell you that ah good question uh both right i mean they're sort of again yeah they're they're those it's all protocol and uh yeah so for this one for sure the how they handle the sample maps would also affect things good other questions yeah yeah yeah yeah right right great question so um yeah so for this i mean again this is a pretty special setup and i'll get to the example later but i think in most situations unfortunately we don't have potential replicates right i mean i'm sure most of you guys work in studies where you're looking especially we're at a medical "
    },
    {
        "start": 2204.88,
        "text": "campus where you basically have people without disease you might be only able to get one one measurement from each of those people right so um so so potentially yeah it's a good question i mean i think so obviously you know i'll make my stats pitch uh which is you know having this kind of uh experiment done as part of your study can help you get a sense of what your um your this measurement error variance i was talking about is and potentially you know but if i see a pattern like this for the same sample and then let's say i get two different samples and look at this and i do the same kind of thing i can use this to calibrate what i should expect to be null in the different sample case and then deviations from that would basically be real signal so that's kind of how event image eventually use it but again the other things we can do as well right in a lot of situations we don't have this kind of uh reference sample thing and i'm sure you guys probably do lots of things where you create pseudo-reference samples by looking at medians of measurements across all your samples or something so "
    },
    {
        "start": 2265.68,
        "text": "and again that's a very standard thing people do with firefighters okay so i have a couple more examples um this is the more the metabolomic stuff um so so uh so all and i we sort of we've been working as uh we're both pis on a on separate e01s as part of this metabolism common fund consortium so so this is so basically uh you know i sort of pushed this methodology to my group and they were like we talked about what kind of examples could you could you do so we actually have a few examples we try this on on metabolomics um i don't have much slides about the data issues but but i'll tell you one catch what i showed you earlier was basically the cleanest thing possible so let me tell you what before i will show you the findings i'll just tell you something data issues we've been sort of dealing with so with metabolomics you run the same "
    },
    {
        "start": 2325.839,
        "text": "sample on twice um you might not see the metabolite both times right so you have a missing mechanism other things and again there's lots of reasons for that with uh so uh metabolomics like any instrument has the detection limit so so especially for low expressing metabolites you can imagine that the value might fall below the detection limit uh you also might get when you run the experiment you might aligning and with metabolites we're talking these are going to be mostly targeted uh annotated tablet experiments untargeted but annotated but what you basically get reachment for each from a matrix is basically going to be a mass charge ratio retention time and for sample abundance value and again there's always an issue right that different the same sample one twice might give you different mass charged mass charge and ratio retention time values so how you deal with that's another issue so so we have issues like that that we're sort of grappling with which actually makes using the smart methodology way harder on metabolomics so much more work in progress but i just want to show you "
    },
    {
        "start": 2385.839,
        "text": "some results to show you that we've tried it on on some metabolomics data so the first one is kind of like what we had earlier um with the seek with our asq experiment so we had one sample and we basically and then what they did with that sample is they they basically uh send it to so this is operator one operator operator three each operator ran it or operated for it and each operator ran it three times so that's one example and that's from this uh corey colorado uh that nicole reisdorf rex this one is a little bit more interesting we actually have a lot of subjects 131 subjects and again they have different uh some of the uh i forget what disease we're looking at but um okay good "
    },
    {
        "start": 2447.359,
        "text": "good um so this is uh so subjects are they come from different populations but for each subject we measure them three times so it's a mix of biological and technical replicates and this last one is only uh same kind of same kind of study as well so uh so basically we tried it um so here we did basically no processing and what you basically find it's hard to tell so basically the scale is from zero to 100 and we're basically looking at what a percent reproducible metabolites for the batches again run it's the same sample so it should not be changing a whole lot so this should be pretty high reproducibility and you basically see if we don't do anything to the data we're getting reproducible values reproducible these pi ones we're getting around 20 to 35 if you remember back on the cqc experiment stuff we were getting 0.6 to 0.8 or 0.9 so we're actually much lower here um and then "
    },
    {
        "start": 2507.359,
        "text": "we did it for the spike ins so this is at the top level basically of the fire with all the samples this is doing it for the uh vikings before you uh yeah so that's the layer and then these are for all 27 replicates and what i should say here so if you remember the statistic took two numbers and took their max right but you if i look at the design of the experiment at this level i have like 27 numbers up here i have three numbers so one thing we basically had to do was come up with summaries uh we the what i'm showing you on the next page are basically pairwise comparisons like for the 27 down here i'm gonna have 27 choose two high ones up here i had nine i don't have nine nine shoes uh nine shoes two pie ones so that's uh yeah that's sort of what's uh "
    },
    {
        "start": 2568.88,
        "text": "so again what you basically see though is in general at the higher layer not very good still not very good at the middle layer and then starts to slightly get better at the bottom still not good so basically one of the sort of uh conclusions is basically you have to do something if you imputation and filtering if you want to make this sort of be more reliable metabolomics that was not something i did worry about with the rna-seq okay this is the uh this is the other one uh so dss two and three so just interestingly this one um you start to see better performance and then this one we have now done some filtering and an amputation so what was between you know around 20 to 40 but before you're starting to see now more values more near 60 to 70 for the most part um and then down here with now i've mixed "
    },
    {
        "start": 2630.72,
        "text": "so this is technical samples so these are basically for the 131 subjects looking at their everyone's technical replicates so you look at stuff within the subject so now i'm doing picking uh two samples from different subjects and what was 60 60 to 70 for the technical now becomes you know closer to 40 but now potentially i'm mixing samples from different people so again you can think of like uh sort of using this to calibrate to this and then figure out what you know the differences might be actually due to biological difference and then this last one uh unfortunately has no technical really but just just a biological this is one with uh 1100 subjects and you basically see here you're getting like really really really reproducible metabolites now what i can tell you about dsf3 is basically this is one that we got from metabolon those of you metabolisms know that metabolism is probably one of the major industrial companies in this space "
    },
    {
        "start": 2692.96,
        "text": "and and basically this data set we got pretty much processed by them so it was filled in no missing values everything annotated so i forget how many uh metabolites are on this one but but the but the catch with this one basically is we never got the actual sort of raw data we have no sense of what steps they took to get to this but but but again it looks quite you know quite quite high here so yes that's a good question i've asked this to people that actually run the machines and they seem to give me the suggestion more that it is variable um there are i mean with these machines right that you have uh drift effects and things like that which would sort of impact the protection limit that's a good question um "
    },
    {
        "start": 2755.28,
        "text": "let's see uh in this one we did not throw them away and they were zeros in the dataset or we call them we call them absent correct correct so so again the work we're looking at is basically we and when this is working progress we're basically trying to figure out like what if we start to try to model the zeros and if we do that can we and again sort of i didn't show that today but basically we some preliminary stuff suggest we'll do a little bit better but again as you know as you're aware brenda like this basically gets into multiplication kind of ideas so we're thinking about if we want to do a multiple mutation here and think about how to estimate things after multiplication how do you think about combining analyses estimating imputation variance so you could do less sensory data analysis that's the other approach so we have a couple people that i didn't mention today but uh there's a postop looking at "
    },
    {
        "start": 2816.079,
        "text": "using left sensor approach to this as well good um so this is the other question this is the other part so uh this is uh yet more things um just yeah more sort of more sort of analyses here so what i showed you before were metabolites across subjects this is asking the question differently where i basically transpose a subject in a metabolite and to say like how many uh samples are reproducible across for a given metabolite so this is getting at the question is you know how reproducible are individual metabolites again we can do that here because we basically we're basically flipping the roles uh around of what's the metabolism sample so basically so when you do that you basically start to see here that photocycle replicates you know you have this very interesting pattern where basically some metabolites are super super reproducible across everybody and then some just aren't at all and again this sort of fits into that mar idea that "
    },
    {
        "start": 2876.079,
        "text": "we've been talking about right that there's going to be a fraction that are really reproducible are no fractions that are not um that is for the so d said this is for potential replicates here you see a lot less concordance but again what biological objects means are samples are coming from different populations so again you can use the idea of calibrating this distribution of that distribution to figure out what might be an important metabolite to follow up and then this is for the really really big data set and again you sort of see this is the biological replicates so you see um again not the greatest uh correlation okay so to wrap up i i you know really this goes back to a sort of uh in biostatistics just a very classical topic of assessing agreement "
    },
    {
        "start": 2936.24,
        "text": "and there's you know there's things like cohen's cap bones kappa and rose and things like that to do that and this is this i i i basically say it's a fairly flexible way of doing that it's sort of taking advantage of the fact that you have a lot of measurements on samples again uh when i went to grad school i grew up in the era of like you measured you know blood pressure height weight kind of stuff on people and now you measure stuff you know snips on a million markers or or you know rna-seq on you know thousands of genes so sort of using that using the idea that you have high-dimensional data you can sort of model agreement this way and so here's some future directions we're looking at so we sort of want to start to look at simulations uh for how well the procedure works again so the one thing that's always kind of bugged me is the statistic is a maximum of two numbers in most experiments you have way more than two numbers it turns out like uh that i mentioned an s hat and an s earlier it's really easy to find s when you have two numbers it's hard much "
    },
    {
        "start": 2996.4,
        "text": "harder to find the s when you have a lot of numbers so we've been talking a little bit how we extend the mars to handle more than pairs of subjects so wrap up uh just want to acknowledge funding uh and again this took place at two institutions over a lot of years so there's a lot of funding involved to get the work done and i also write and conclude with just a paper so describing the methodology so thank you and thank you very much for this great haul are you going to implement the calibration you are talking about the calculus yeah that's work in progress i mean uh yeah yeah uh right now we're sort of finishing up the for the metabolomics just the issues that we talked about like if you want to do the imputation how does that how can you how does it affect things and then the next step would be looking "
    },
    {
        "start": 3057.68,
        "text": "at like it basically a differential expression analysis so that that's the next step yeah okay other questions so actually so uh when you complete this sort of you know work and develop it is that going to be part of the tools that we are developing for metabolism because i think it's as you have shown from your examples and for metabolism it's a big much bigger problem than for exactly yeah yeah you know other fields but that that's the hope i mean again you know so just to sort of say a little bit when so when you know when all and i put in our proposals for this rfa one thing we had to sort of that's why was kind of a software plan and again um unfortunately this was the best example today but this is what i'm actually showing you the talk using the shiny the shiny uh or markdown command in uh in our part what i want to do is to make things a little bit more reproducible presentation wise unfortunately it was also stopping wise which is uh but yeah so so the plan basically is when we do these things we basically have a "
    },
    {
        "start": 3119.04,
        "text": "multi-level system of how we do by code release so we'll for sure we'll have at a minimum we'll have sort of our packages up on github uh at a minimum uh and then we also have as part of the grant we actually hired a computer science undergraduate major who's working on helping us sort of like do some software development so he's uh ultimately he's also going to start to build things like shiny apps based on the based on this stuff [Applause] you "
    }
]