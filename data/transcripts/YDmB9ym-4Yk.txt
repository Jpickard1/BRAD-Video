so i'm pleased to introduce to a speaker we have uh zhong ming liu who is the associate professor in biomedical engineering and electrical engineering and computer science all right and uh thank you for having me here and i i love to share some of my recent work um and hopefully it's of interest to people here just to confirm it still see my slides well yep you're good okay great um so i'm a neuroscientist i'm an engineer by training and in the same time i i study neuroscience um we study neuroscience we use a lot of different tools and those tools that matters for my lab are you know including the magnetic resonance imaging that allows us to see the structure of the brain diffusion mri allows us to see how different parts of the brain are connected with one another and we have functional mri you know we can see that how the activity goes up and down and we have uh you know eeg and meg which are recordings of the electrical field or magnetic field generated from brain activities and we actually have computational models that you know we trying to make sense all of this data with and lastly for rare cases that we also have data recorded from either macaque a monkey brain or a patient's brain using invasive electrodes for patients when they have the need for receive some implanted electrodes for their clinical needs and we sometimes leverage that opportunity for basic neuroscience studies so today i'm going to talk about functional mri and the function mi and then some some you know informatics or machine learning tools that we have developed hopefully will be used for for uh functional mri data so so moving back a little bit so this is a a nice brain slice this is a slice that we can you know of a human brain in this case this is actually my brain and this is the so-called the uh t2-weighted mri and you can see that a lot of anatomical details and these days that we can get routine images that looks so nice you can see a lot of anatomical tissue properties are using this type of scans but this takes a long time right and this takes uh you know maybe in the order of several minutes or even up to 10 minutes to get a nice looking image of this if you don't necessarily need this high resolution and you can push the same contrast but sample that image at a low resolution and this typically it can typically do this you know at a much faster speed in this case you can acquire an image that looks like this in just two seconds then you would think if we just repeatedly acquire this image then we can trace that how each one of this little in a box or a pixel how the pixel intensity would change from time to time right every two seconds and then you can get some readout of that change and that change may presumably reflect brain activity here shows the brain activity evolving uh over time here every time points you know are you know you know every snapshot is taken uh a snapshot is taken every two seconds so then you start saying wow this is a pretty complex data right and we have this activity goes on on and off and looks pretty complex how do we make sense of it one way of making sense of it is to look at the one location in the brain and any location of interest to you not just put a random location i call that location as the seed okay and this this is a seed and then i'm gonna look at all other locations in the brain how their activity are correlated with activity from this seed location where the seed is a region of interest to um that a scientist with a peak you can see that that location is correlated with this location and maybe with some other locations the most correlated locations are the ones that are connected to this particular seat location or functionally integrate in interacting with this sig location you can move your seed around you can see that a lot of patterns start to emerge and those the patterns that may reflect how the different brain regions they are connected with one another they interact with one another or in general we call them networks and those correlations become a surrogate measurements of the connectivity between different regions now you play this game now this is the case that one placed a seed along this part of the brain and this is where our somatosensory cortex you know is just for those of you who do not know the brain anatomy the somatosensory cortex manage a body map so different part of the body the sensation coming from different parts of the body are represented by different locations along this area which is called central sockets now we move the seed along the central circus that's curious and this thing is moving by itself along the central suckers you can see that the opposite hemisphere and the corresponding locations seem to show up to be mostly correlated with the seed location so you have this bilateral symmetry and then you can come up with a representation of the body map and this is this is using the scan of so-called resting state fmi so the human subject are not doing anything they're just lying in the mri and we just pick up their brain signals and we just analyze the brain signals using simple tools but this even though that we're not asking the subject to do anything and we're using very simple analysis the analysis can already inform a lot of things about how the brain organizes function and about how the different part of the brain are structurally connected but you will say maybe all right this thing this type of things from a spontaneous brain activity or this type of a spontaneous resting state networks may not mean anything but in fact this means a lot of things even though that when our brain is at the rest when we are at the rest uh the brain the brains our brains are not at the rest and they show this activity or network that's almost the same as the networks that would be activated when we do certain things for example if you see a picture and your visual cortex will be activated the activation of your visual cortex would show up as a resting state network that network that activates the visual cortex would show up even at the rest we just don't know when that will show up but it will show up the similar things that you can have a higher order visual areas you can have auditory cortex you can have some cognitive area you have cerebellum you have motor cortex somatosensory attention cognition emotion and you name it most of the functional domains were associated with some type of brain patterns and those brain patterns would show up even when the brain is in the so-called resting state this is interesting then maybe the question that we start to ask is how about we collect or use a lot of resting state fmi scans and we try to learn the patterns from those scans using some machine learning methods or tools and then we develop one tool that is we call it variational order encoder for learning of resting state fmi and we just got this paper published and uh and then the idea is now you see on the left are the you know what is called this is the left hemisphere the left part of the brain and this is the left part of the brain but the view from the middle okay so this is a lateral view this is the medial view of the left brain there's a lateral view and the medial view of the right brain now what is shown here are in a red and blue colors the red means the activity would increase relative to its mean and the the blue means the activity decrease relative to its mean and this is a map reflecting the activity surveyed for every single location on the cortex the surface of the brain now you look at this pattern and we're going to have many many many in this patterns okay the idea is that we're going to compress this very complex data a very complex pattern into a low dimensional vector and that low dimensional vector would include 256 elements in it all right and that low dimensional vector we hope it will compress and represent all the information in this map and the way of getting that a low dimensional representation of this high dimensional complex spatial pattern of the brain activity is what the goal we what we want to achieve now we know the brain activity goes as a time series of images and therefore that low dimensional vector it's gonna move that's the brain activity change from time to time and the goal is that we're interested in tracking how that representation would move in its representational space the idea is not to look at the brain activity in by its anatomical location but then it's going to look focusing looking at the trajectory of its latent representation that we want to we would like to extract using machine learning tools so that's sort of the general idea and later on i will talk about how we're going to extract that latent representation how we're going to make sense of the trajectory and what we can use those trajectories for so this is a paper that we published last year and documents all the details for the thing that we were talking about today the first thing we want to do for this tool to work is that the brain has accomplished the surface of our cortex or the brain has a very convoluted geometry right that complexity is not very friendly for machine learning methods so we hope we're going to restructure it now this one shows maybe we could you know the convoluted cortical surface we have a pattern we can try to inflate the pattern when the pattern is inflated to a certain extent basically inflate the cortex to a certain extent numerically and this is how the pattern would look like you can think that if you play the inflation to its extreme then the whole cortex would look like a sphere right you could no longer ex inflate the sphere uh while alterating uh alternating alterating the altering the the structure of the data distribution now good news for a sphere it's like our earth is like a sphere right you can define coordinates on a sphere easily and those coordinates are maybe latitude longitude something like that and we just define this as an e and a and then if we further structure the elevation in terms of not elevation itself but the sign of the elevation all right and then these this whole like this convoluted surface ever the activity pattern on the convoluted surface would show up as a just like a regular rectangular image right that rectangular image will have x-axis as a and y-axis as a sign of the c now the reason we use sine is that because the e is an angle that e that angle goes from the negative pi to pi and the sine function is a monotopic function in that range now this is a you know you know this shows that how the sampling points so now you can see that there are many many little dots those are the points on the cortical surface uh roughly evenly distributed on the cortical surface and those points are highlighted in different color and different color corresponding to different areas in the brain now when we actually restructure this 3d geometric surface into a two dimensional grid points and we can see that those two dimensional grid points has one two one roughly one two one correspondence to the points on this three-dimensional cortical surface and the density of those points would remain roughly even okay as if they were originally as they were originally distributed on a cortical surface so i would argue that this 2d two-dimensional image representation is a relatively faithful representation of the original activity patterns on a very complex geometry and we call this process as the first step of our tool that is for we call it geometric reformatting i should i should mention that uh you know if there's any questions feel free to interrupt me and i uh i can keep talking but uh if any questions if you need any clarification i'll be happy to stop any point so this is our model this model is a model that is called a variational auto encoder and the first time this model was introduced in the field of computer vision uh in the field of machine learning is 2013 and later on there's a you know variation of the model and this that variation of the model is called the so-called beta bae it is the beta vae that we are using when we build this tool so now we have the input the input is the activity pattern measured from the brain and we want to compress that input into a low dimensional vector we call that low dimensional vector as a z and each element in that z vector we call it as a latent variable and that z vector we call the latent vector and we have 256 latent variables and they assemble a latent vector and we want this latent vector to preserve the information in the input activity pattern the way that we want to ensure that is that we're going to design a model that is called decoder that turn this latent vector into a brain activity pattern we want to make sure that this brain activity pattern will look very similar to the input pattern and this is simply to make sure that the latent representation preserved information from the input and then that's defined as the loss so we're going to train this whole model including the encoder and decoder train the model such that this reconstruction of the input by the model would be as faceful as we can and we measure the loss of that reconstruction as the sum of squares and then there's another term that we want a model to do and that we want each latent variable or each element in this latent representation they represent different information so we want to minimize the redundancy in the late among all the related variables and we don't want one latent variable to encode more information than the other every later variable will be independent from an another latent variable every other latent variable so then that's we just try to minimize the so-called kl divergence between the distribution of each latent variable and a standard normal gaussian distribution okay so and then these two terms in the loss function are balanced by a value that is called a beta and that beta is a hyperparameter that we tuned while we train the model against the data and encoder and the decoder are both using the so-called convolutional neural networks and there are different layers and there are more specifics related to the model design and are listed here and more details are in our paper i will not go through the detailed design but move on to the next we need to train this data against the brain activity scans right bring fmi skins we use resting state because it's a task free therefore is not biased by any particular behavior domains of our human beings and and then we use the data from a data source that's from the so-called human connectome project and there are a lot of data in that in that in from that project we specifically use 600 data sets 600 600 650 subjects data and we use 100 subjects to trend our model we use 5 50 150 subjects to kind of verify and fine-tune our hyper parameters and then we generalize that our model to 500 other subjects that our model has never seen during the training or validation phase for each subject there will be many data points right and we're going to have we're going to have a 1200 time points each time points corresponding to a brain activity of maps and each brand activity map have tens of thousands of locations so it's pretty complex and a big amount of data we're dealing with and i mentioned there is a there was a hyperparameter which is a beta that actually balance how well the model can compress and reconstruct preserved information from the input we call the model performance and and versus how well the model is constrained that latent variables are independent gaussian variables so we know that these two terms actually compete against each other so if you want the models to be fully gauging and the model may not perform very well if you want a model to perform very well and the model may or may not may not have the type of uh you know you know the gaussian distribution that will be uh you know that we want so it turns out that when the beta equals one and the model can perform the best and when beta equals 10 the model would still perform reasonably fine but once the beta increased to a large extent okay the brightness will fail it's almost like a neural collapse right and the the the no matter what the input data is and the distribution of the latent variables would just be random samples of a normal gaussian distribution and and therefore that normal samples of of gaussian distribution will not preserve any information about input so it will start to collapse and we find out a mid-grant that is beta equals 10 so we just use this to train our model well after our model is trained we can look at how the model really performs with beta equals 10. so on the top we see there are brain activity patterns and there are snapshots coming from a time series of brain images which we just show you know five different snapshots they're coming from one two three four and five different time points and you look at the brain activity pattern this is how it looks like all right and then you take this as input goes through the trend model and you reconstruct the input you can see the reconstruction lost a lot of fine details but preserved general patterns pretty well right so the compression is not at loss but preserve the general activity pattern and you can also see that if you look at the one particular location in the brain and you just want to know that what activity pattern would look like as in the originally versus reconstructed you can see that the original time series is the uh black and the the red shows the uh the time series of that same location from the reconstructed the model reconstructed bring around you can see that it's you definitely have distortions but in general it's fine and in general that you know the brain activity patterns seems reasonable right most of the activity features are reasonably preserved and you may wonder how this tool would perform differently compared to some other tools like the tools that is much easier to think much more widely used like the principal component analysis in independent component analysis here shows that this vae tool that we built can explain more variances preserve more information in the original data than principal component analysis and independent component analysis all right so this tool arguably could use the you know have a better you know when you compress with under the same degree of compression can preserve more information about the input now another interesting things about this model is that once this model is trained this model is a generative model what do i mean by this is that you can simply sample a gaussian distribution for each latent variables and then you can use those sampled latent variables to generate maps on the brain now if we do that then this is how it will look like you can generate a map that is synthesized it's not measured it's just synthesized from those latent distributions now now though you can use those synthesized data you can do the type of experiment that showed in the very beginning though you can pick up a seed now here i pick up c in the visual cortex i can look at where in the rest of the brain are correlated with the visual cortex you find a lot of brain networks from synthesized fm activity patterns all right so what this means is that this model would have remember all the knowledge about how the brain organized its network so would have learned this the networks in the brain so this to me is very interesting and compelling and you can say that if we actually run actual experiments use the actually measured brain activity fm activity patterns and you can run this seed-based correlation analysis you will end up having the same network all right so you can compare on the top are the networks from the synthesized data in the bottom is the map that from the actually measure the data so one take-home message here is if we only care about the correlational patterns in the brain after the model has seen a lot of those snapshots of fmi scans the model would have learned about how all things regarding the brain network we can just use the model to study brain network as opposed to measure a lot of data from the brain this is not to under estimate the value of the measured data that's just to say that the brain this model has preserved information within its decoder and you can also discover new networks in the brain for example you can run you can do some clustering analysis and you can find out that how the different snapshots look similar or dissimilar to one another you can find out a lot of brain activity patterns and those activity patterns seems to be informative at least for some of them for example here is one cluster that that the model has discovered or learned and this cluster in the latent space corresponding to a network in the brain yeah you see that this is a this somatic sensory motor system there are a lot of interesting details in the sensory motor system for example you have this highlighted area that's the primary motor primary somatosensory cortex and you have some single cingulate uh what is what what is this called on the secondary uh motor cortex and you can see that this is some of the brain anatomy and and we don't have this this cluster the way this cluster is defined and visualized it's not based on any knowledge regarding the brain anatomy but it actually pulls out all this information through a data-driven method a data-driven tool and and this seems to be uh useful and there are many other examples and you can find out this examples from extracted from this this model or this tool and you can compare that with the previous literature each of these networks was discovered with a series of elegant experiments but now we just see the brain at rest and the model would have learned those structures and preserve those information now you also you may wonder well this seems to be only for basic science how this can be useful right now we start to look at different brains here we're looking at 20 human subjects right and we're gonna the data from my data measured from each one of those 20 subjects are served as the input to the vae we extract the latent representation for those brain scans and each dot represents the activity pattern represented in the latent space so each dot is one time points from one subject each subject are color coded as different for different patients so different subjects have different color and we just pull them out and we just visualize this you know 256 dimensional vector in a two-dimensional space and we use a widely used visualization tool to call the tsae to our surprise you actually see that a very nice separation from one person's brain to the other person's brain and the separation are visible in terms of how this representation show up in this tsa embedding space how this representation are positioned as well as how this representation are distributed okay because different point clouds that highlighted with the same color seems to have a little bit different shape so then we use those information the position and the shape we use that as the covariance of the latent representation for each individual subjects and we ask how can we use those latent representations of each brain to identify who is who if i scan 20 subjects and i don't know which brain scan coming from which subject can i use those brain scans to identify which subject this brain scan come from and this is just like you know taking the brain scans and their latent representation as a fingerprint of individuals and that end up being what we're looking for and we find that using this latent representation the representation geometry could allow us to uniquely identify individuals whether the individuals are five people we can identify perfectly ten people we can identify quite well even for 500 individuals we gotta end up being more than 97 percent of accuracy this is our top one accuracy so if you just use the brain activity itself you can do this when you have five patients you can still do it fine with 10 but as you actually have more and more humans you want to identify the performance drop quite notably not only that if you are saying we're gonna find out you know 500 subjects but we want to just use nine seconds of data you can still do it but the performance will be very poor whether you use this latent representation or use a cortical activity representation but as you have more you have the brain scan from each individual is longer and longer the ability to use the latent representation to identify individuals start to increase dramatically okay so you only we only need three minutes of data we can uniquely identify who is who using the latent representation with an accuracy that is about 90 so that was interesting so maybe this has implication for precision health this maybe has a lot of some other applications of using this latent representation to predict diseases classify patients population or even potentially plan treatments now another proof of concept is now we're going to look at the brain states now we are looking at the same individual and they are 15 seconds the first 15 seconds and that individual is just not doing anything and 15 seconds later that in the individual is presented with a movie and the subject just watching a movie now you see that many of the swarms or little little segments of lines and those little segments of the lines are coming from different individuals so we have about 175 73 individuals so the the image shown on the left versus the image on the right there are the same data but color coded differently in the on the right on the left is a color coded by the rest versus a movie watching and different sacraments are different subjects and on the right they're just different subjects okay now you see that when all the subjects are experienced the transition from rest to the movie you end up with the in and the young type of transition okay you know what the subject doing all right and there are individual variations but the individual variations are not as strong as this kind of brain state transition brain state transition dominate latent space representation okay and that's interesting well then the next thing we do is that we're going to average the latent representation across all the 173 subjects we're going to look at how the representation would change while all these subjects watch video clip this is the first one this is from a hollywood movie or maybe not halloween i don't know which movie is from but it's called pockets you can see that this you can you can observe how the latent representation change in the latent space while the the you know the movie is presented the brain responses in response to movie are encoded into the later representation you might find that there's a lot of jumping points and those jumping points seems to coincide with the time that the movie content changed from one to another okay this is hopefully i would argue that this wave representation isn't meaningful because it reflects what the subject was watching or in some other untested cases it may represent what the subject is attending to or what subject is doing while inside the mri scans now this is another movie and this movie is inception i guess many of you guys have watched this movie before i'm playing this movie at a much fast speed just for the sake of time you remember a lot of sensational things unrealistic it turns out that the windows aren't really things that happens that to decide to watch it there's a lot of maybe emotional responses and sensory responses responses will cause the trajectory to jump at to a great extent right jump at the over larger distance in this lightning space now this is the third example i'm going to show you this is from another movie but you can see this movie feels a lot more natural right and it's more closer to our daily life there's nothing that's particularly sensational nothing particularly arousing and the latent trajectory seems to not jumping as much and they tend to be continuous more continuous compared to the trajectory during other movies now this one shows the lighting trajectory and if we actually calculate the what we call latent gradient we can basically look at the difference between latent representation and the one-time point to the next time points you can view this as a way that we measure how much the latent representation have jumped in the latent space okay and we quantify those trajectories and we find out that oh look for one movie we presented the first time second time third time and that movie in general has a very high late in the grade and basically with one movie tend to jump more than other movies you will always tend to jump you know jump more and there's also the second time the first time you watch the movie you have more jumps and the second time you watch it you have a little bit less and then there are different movies and the latent gradient amplitude and they tend to actually have some variations you know uh for example you have the overcome inception and those actually tend to be a bit higher and this uh you know the northwest that movie that has a natural things seems to be a lot more uh is less salient in that sense so maybe you know if i was arguing one one use of this tool um is perhaps to use this tool for um you know pre-screening of hollywood movie to see that which one maybe and use that to predict how uh how much uh you know how many people would watch it and how many ratings uh how good the ratings of the movie would be all right and then we all can also discover different networks that the brand has used uh while they watch the movie okay so those are additional things that i want to say and you know i i only intend for this uh present presentation to be 40 minutes so i want to wrap up uh we do have an open source tool and this open source tool is it's currently online and um we we are uh sharing the pre-trained model and all our source codes um right now there are not a lot of things out there but we are we're going to maintain that uh repository we're going to expand it and and and disseminate that tools for others who are interested in either using fmi for basic science or for neurological diseases and uh we hope this is going to be a useful tool so what's next uh the next step we can extend the tool to also model the time for dynamics we want to extend the tool to learn to predict behavior from brain scans we're going to use this tool to inform neuromodulation and some you know behavior therapies and like for emotion regulation we want to use the tool to classify different tasks of cognitive conditions we want to use it to maybe classify different disease conditions disease populations and we further expand the tool to integrate with some other modalities like heart rate brain signals eeg and some other type of measurements from human human body and to wrap up and this uh are the work done by grad students and john hong king and egyn john both of them are right now post docs um elsewhere john who is a poster in the national children's hospital zhengchang is a postdoc right now in ucsf and some other current students involved in this project and i'm also working with my collaborators in the college of engineering and radiology and we're going to try to push this tool for uh from both technical and a translational perspective and thank for funding from ihnsf and universe michigan for supporting this work with that i will thank you for your attention i'll stop here if there's any questions i'll be happy to clarify further and answer any questions thank you very much excellent presentation uh if anyone has questions you can either put them in the chat box or unmute yourself or you can also use the zoom reactions to raise your hand and then we can also direct you to unmute yourself if no questions i just presume that my talk is extremely well received and extremely clear and and that's totally fine i'm not seeing any questions yet but we can give it a couple minutes sometimes it takes people a little time if they're typing so okay great actually it looks like a question came in uh from dana saying thank you for a great presentation i know you only touched briefly on this but are there any privacy or ethics concerns when trying to identify scans from individuals um potentially um if all the data are already identified then there will be no privacy concerns however if somebody maintain or fmi maintained fmi scans with personal personal identifiable information and there's a de-identified brain scans from the same individuals and that might become a place where people can use those scans use those tools to identify individuals that will be of concern so potentially i guess my answer is yes it would be it would be possible there is a request for the copy of your slides i don't know if you'd be willing to share those will be yeah that's okay okay yeah if you want to send those to me then patricia i can send those on to you okay all right well i'd like to thank zhang one more time um and there's a couple thank you messages coming in in the chat for you but thank you very much and i hope everyone can join us next week we'll be learning about comparative analysis and visualization for biological networks