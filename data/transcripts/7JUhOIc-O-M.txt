my cellphone but somebody's doing in Alex won't be offset this one like I can write me off cinematography offset just keeps it I can point climate change for that's right I mean come on sign in with Trump's is easy that's in the Mara Mendez I already have been fighting was the first readings Oh a stop or where my readings one that jungle resurrected Thursday another chunk I'm just queens with this we have some words with the management so if I do I'm a chocolate city is dealing myself pager and further expand the bio easy developing a program the you inspector non-computer sound genomics workers to emit in veins some of them may have an article one you who doesn't I'm surprised they only get like 40 bucks a day there was a time when my friends stole I heard it I get it he was running Grand Central Station have a clowns high-level back I'm off of the DC MVP go to the events section there's a [Music] [Music] all right welcome everyone tools and technology seminar series most of you have been here before we have a few more talks the semester and then of course we'll be starting up again next semester so most of you I think know today's speaker but our speaker is on drew John who is a grad student in DC I would be um in one point one slap I did I'll just thanks for the introduction and I just take over from here today I'm gonna present one of our on may seek quantification tools and many of you might have questions saying you know there are so many quantification tools at the moment we have top hat we have awesome we have keys so all kind of tools and all you know what's the advantage of this new tools and what's what's the new applications of these tools and now our brief introduction of all these background show you some exciting applications on some of their our new data so the major question we try to solve here is to estimate the abundance of transcripts so in human of most eukaryotic organism we have this splicing mechanism try to create a variety of transports from the same gene so in this case we see a reference genome we have multiple exons here you can generate different kinds of isoforms or transcripts from the same gene sequence by skipping some of the exons or including some of the introns which is in contention or even do something like crazy thing like a generate circular RNAs and those are completely different types of the transcripts one of the interesting part of this whole mechanism is they can have different functions for different isoforms from the same genes so if we can quantify the abundance of different isoforms from same genes we might have some clues about what functionality of this you know cell types or specific tissues may carry or maybe can be some kind of biomarkers for specific disease specific phenotypes of phenotypes and those can be very helpful for you know a biological research we have all kind of you know sequence technology the major two for this kind of research are Illumina RNA seek and impact bio so seek so for Illumina I see it sorry Illumina RNA seek this is probably the most widely used the sequencing technology we do schary sequencing and we but because as strawberries is very hard for us to recover the origin of full-length transcripts we have to do all kind of inference to guess which isoform or which transcript this tree comes from and agree to agree most analysis tool will greatly rely on existing reference transcriptome or annotations and most intra most inference process we try to assign the Ruiz back to the ISO form by distributing proportionally and maintain a like a high likelihood function this is not ideal on the other hand we have pepper I so seek surely will generates full length or you know almost the full length reads from the single transcript but they are harder to quantify especially for lowly lowly expressed because you can see wineries or Tories or not instead of having a real number there like a one point something or zero point something so to quantify those lowly expressed isoforms is a challenge and also pepper tend to have a higher sequencing errors here so in order to correct all those signals and errors most songs do you have to rely on reference genome or reference transcriptome and in order to keep a more robust quantification people now they try to combine Illumina RNA seek and power I so seek say we get the flu lengths transcript then we align short reads back to the full in stature so there's a discrepancy between this two part and again it is also an inference process to try to assign reads back so how robust this kind of a matrix are is still on the debate so for us we try to focus on the Illumina sequencing because it's the most widely used technology we have two sets of tools now they're available to most researchers once that is alignment based of quantification usually they have a aligner and a qualifier so for the line that we have so many different kind of secrets and lining tools like beat up a top hat bowtie star and they are designed some of them they're designed specific for say Arness accordion ASIC uses some of them are more general like just aligning then we have quantifiers like cufflink is one of the earth sorry I guess there's a s cufflinks is the one of the earnest eisah from quantifier that is being developed GMaps is later on widely used in PacBio I so see qualification and awesome is later on developed to incorporate all kinds of biases and like sequencing error modeling together and build a very complicated quantification model recent year we have a new set of tools called alignments free tools or some of them called pseudo element based the tools we have selfish which is probably the pioneer software in this in this area and then we have Callisto developed by the up actor who developed the same tool coupling and a similar policy analysis they are most time based on Kaymer and Callisto Callisto and Salman cooperated that a Debruyne graph try to even further optimize the whole qualification process so here we show a you know a typical workflow for like alignment based analysis we get the reefs from our CDN a fragment library and we do the sequencing get different kind of Reason map it back to the reference genome we get the fragments which are most the time axles and then we gather all these alignment and do assembly based on either the in the reads itself or most the time we will refer to a transcript on a reference transcriptome and then we calculates the overlap graph and do a log likelihood estimation on it and basically we will favor reads two shorter isoforms that can cover all John all the junctions we observed in the sample and that's the whole idea of doing this quantification sorry I our likelihood based of quantification and it's still used in now there and the same in our tool but this kind of approaches you real slow because you have to do two steps and aligning is not trivial we map to the motor baut locations are opposing a great trouble to this kind of approach especially if you start with a Jing a reference genome alignment first and then you go back to infer or the isoforms you really have to do a fairly complicated process to infer like which isoform it is and also as I said recently there are a lot of new models that try to incorporate all kinds of bias together and that make it really complicated then you have multiple parameters to tune and actually we see in recent years when people trying to do this benchmark they tend to overfit that and make the you know performance less reliable and a lot of free tools here we present the workflow of selfish mainly based on Camus that we observed from the sequencing reads and it's a smart idea that is mainly restricted by the computer capacity back to the tasing say 2000 2010 but now they we have 64-bit computers everywhere and you can do this kind of analysis on your laptop so it's making things a lot easier and to observe the camer or to collect all the chemists from your sequencing leads and align them or actually map them back to your camera index generated from the reference transcriptome make the whole thing's more straightforward and basically now you can just count how many came as I found in my sample now I can't give a estimation of like abundance of different isoforms and Obama's of a different Junction trees and do all kind of like a crazy analysis without actually doing the alignment it's much faster surely the alignment based will take days to to you know analysis on a large fault RNA seek data where this one can do is finishing say 30 minutes or even you know at the best 1 hour I would say sorry longest so but on the other hand because it's a heuristics instead of doing alignment we do exactly matching of all the cameras so likely to lose some of the information safe there's a thing there is a mutation or if there is a sequencing error this program will fail to capture that and they have to do some compromise here and later on we will show that this compromise can be devastating for some of the cases oh so we have these many tools but we still haven't solved this problem yet there's a lot of challenge first of all it's interesting it is intrinsic to all the shorter II sequencing technologies we cannot get the full length of the transcript we still have to do all the inference we don't expect this can be solved by algorithm alone and the best we can do is to make our inference process as efficient as it can be and as accurate as as it can be the other thing that we often overlooked is the lowly express the genes or isoforms where the quantification itself can be unreliable in certain cases and we don't have a way to say here we can draw cutoff here for like you know the sample because like previously people use the YF p GM v FV m is more empirical we don't have any analysis on this part and what we have but not to a level that can convince us to have a like a well-established standard the other thing is ranking so recent years when people started doing this benchmark on RNA seek wanna fication tools they started to abduct Spearman correlation coefficient which fixed on ranking and we noticed that it can be a problem so here I show a example we try to compare the ranking of different isoforms from star awesomes the quantification against the simulated truth so here this you see a solid dots here which is they are not expressed and successfully awesome what they are not expressed and we see a lot of it goes here and a lot of dots here corresponds to the isoforms where one either they are expressed but we report it or not or the other way around so you can see the correlation is not as good as we expect it from you know our common sense say if you have like a 0.99 correlation coefficients between your quantification tool and the truth they must be doing very well and actually no because for lowly expressed genes especially for the one that are expressed or not this kind of questions are hard to answer and we have problems just to just to define just to decide which ice of them are expressed which ice form are not and a similar pattern we can observe from the gene level qualification there are gene belong to the same gene family where the sequences are too similar it's hard to tell we can't really you know solve that question from the short a reasonably short to read sequencing technology so earlier this year in January 2017 grim challenge launched a new competition called smcr neighs somatic motor somatic mutation calling on a challenge the idea is to benchmark or existing tools all Oscar users to participants to develop new tools to do the quantification and we want to see which one have the most accurate quantification and this is important know especially on especially in terms of whether you can tell whether isoform is expressed or not and we can't even solve that very clearly which isoform is expressed out of all these different isoforms from the same genes and there are lowly expressed ISIL from how accurate you can quantify them and also the expression level correlation in global you know in a global overview of your quantification so I will go through a mission called Sigma as you can see say you know a trick would play on the on the words and see Camus so all too is also based on Kaymer alignment but slightly different from previous tools we'll talk about it its development unfortunately we don't have a tournament here so I can't really do all the demonstration here I have to skip it but I can tell you basically it's modeling for the holding a command-line interface is modeling similar in a similar way as selfish or crystal basically doing index and then do the quantification and it's very straight for you passing the fast queue files and that's it we will show you the development process and some of the exciting findings we get during the comparison and a benchmark will show you the evaluation of our tools against the simulated data and the real world reference samples and there are some mr. recent development we you know applied our this tools on single cell sequencing data and they show some exciting results so for development you know I myself is a more of a programmer rather than actual you know doing all the sequencing stuff in the lab although when I say programmer my P I might laugh at that but I would still call that and for me it's a new you questions you for me to tackle with its I haven't done any sequencing stuff before so when I get the data is more like a string comparison and I will show you in later on we draw those algorithms the string comparison can't helpful can be helpful in the whole process and for this challenge we were given a lot of training data simulated by awesome simulator but the actual validation data they provides our real world sparking data so that means there's a gap between the training data and the testing data and one thing we have to take into consideration is assimilated a much cleaner and they have less you know they also they might carry some artifact where real-world data are abusive real data we have to fit against the real data rather than the simulator so all tools are written in C++ using QT quota you know make the stream process a lot easier and we use eigen to accelerate the matrix calculation part so later I'll show the runtime is practical for most web application and it's comparable to Sigma oh sorry Christo and Salma but at the same time our performance is better currently I'm working on putting it back to Python in size and make it easier for third-party extension to buting and actually we have a extension project going on right now to use the same framework but detect the novel junctions and knowable fusions in the gene and the transcripts so there were embarrassed observations we found you know we found in the challenge first of all we have this conflict between super complicated likelihood models like awesome like gene map where they try to incorporate all kind sequencing bias together versus like listo they tend to stick on simplistic model we just use them most simple sorry the most of reduced model that carry the least amount of parameters to estimate the quantification and later on we notice that reducing the hyper parameters in your model actually helps to improve your performance rather than trying to incorporate those biases because it's hard to model all those biases and it's surprisingly you know for all those complicated models when they actually deliver the final software they will disable all those complicated parts by default so you know that when you when biologists act actually use all those tools they don't use that part the other part is focusing only on the hard reads so one of the challenge in this competition is we really want to accelerate the whole testing process a lot lot faster and we really want to reduce the rug time to like within hours and we don't want to waste the time lining all those reads that apparently the same as the transcripts a transcript or so instead of doing the full time alignment we only focus on the reasons they have junctions I have mutations have uncomfortable regions or positions those are the focus of our tools and this is the trick that kalisto and Salman and our tools that adopt to reduce the runtime in compared to alignments so we take the base frame work from Callisto and is I have to give credit to this marvelous work that they adopted the broom graph to reduce the wrong time the basic idea is we have observed all those k-mers here a circle represents that Kaymer's observed from your sequencing reads instead of it going through all the camus we can say okay in your transcriptome a reference transcriptome you observe all these three cameras come from the same set of the isoforms so when we align this one we don't really have to see the rest of you I know them now I just skip to the next interesting one which happens to be the first two came out after the branch and then we can say ok this came across bounced to these two not this one isoform so we can safely rule this one out in the alignment process and by skipping all those cameras we recently reduce our alignment times actually Callisto is so far doing pretty well in short time alignment even better than Salman which is published a lot later and they can this tool can finish everything in like 10 minutes or 15 minutes depends on the size of your library and the Debruyne graph is built during index process and it's varied I will say trivial but it's easy to implement because you know people been using the broom graphing signals of family for a long time but for our case as we said the reach can be does he make my carry it in my carrying theirs in my cracker a sequencing error so what's happening if you have something wrong here or wrong here then you lose the trick then you lose the quema information so for all of tools we actually do it alignment here if we find something here and later on we'll show this single trick can greatly improve the performance and the environment we use the alignment algorithm we use here we don't use traditional minimum wound or animals or smith-waterman alignment which is quadratic we use a pseudonym here scanning this acronym called algorithm called a sift from my sequel when they do the string comparison and this is based on my experiencing string comparison programming back to today's and we find it very helpful in this case greatly reduce the time you waste on you know just aligning most of time 90% identity you know when we have reserva 90% and i-10 and you don't have to do the minimum work alignment as for quantification as I said most program actually all the program nowadays or ste using this likelihood function or is faron's and this likely who functions prepares the body up actor back to days when he proved elip to the cufflinks so people still using that and it's working pretty well and to estimate the likelihood for this one basically equivalent to determine the waist for mixture model so we'd simply use expectation maximization for this I'm writing something you optimizes for this because it's a little bit slow in converging and as I said one important factors when we one of the important factors when we develop this tool is to reduce the parameter carry it up must use this parameter like the bias factors and the GC content some ratios because we find they are not helpful and easily leads to overfitting and based Corrections are not enabled at all by Miss of tools meaning we are still not confident to incorporate this kind of information or tools and actually during the challenge when we try to compare our tools against our competitors we face competitors from their practice lab and they do very well in a simulated data benchmark but later on when they adopt the real-world data are their ranks dropping immediateiy to them you know know where and why do they tolerate that why don't they change it change their scheme so the first of three rounds the you simulated that simulated it because they don't have real-world it already and before the final submission they did a real-world data just to let us let us know how where l our programs perform and where are we and we are surprised about resounding there's a huge shift in our program ranked at the top and Lea practice awesome workflow and ranking in the middle and one of the reason we suspect is during the training process in the first few rounds they actually over fit in and tuner parameters is including all those bias and GC content things against the training data and to make it worse the training data are simulated using their own tools awesome simulator so obviously it's very easily you can you know optimize the thing against your own tools but does it really work on real-world data is hard to tell and eventually the leader board approves that's all to our approach to ignore all those parameters are correct or the correct way and another thing we make sure is that the the final result is actually converge so one of the chance we had when we try to adopt the crystal program is the right short expectation-maximization and the result is not actually committed so we will show some simulated data these are the scores that we have observed from our training data they are less exciting compared to later on I showed the real word data but there are a few points I want to point out so when you compare the Pearson College everything is close to perfect they're all close to zero us or one there will be imperfect but the most exciting part is there is the medium level expression if you have genes that are not highly expressed but they are expressed say yfp km or 10 FP here however your program can perform against that because Pearson correlation can easily tracked by the large number and when they do the logarithm Pearson tries it down with all those highly president we starting to start to show there's a gap between our estimation and the final performance so here we are not comparing awesome here so you see like some of the cases are somehow actually outperform us because the they use their simulators it's fitting by themselves but Sikma actually definitely outperform Callisto in most of cases to make it even more interesting if you look at Spearman correlation which is comparing the rankings our tools is basically always good and this is small margin but to consider when you map the small margin of improvement to how many how many genes or isoforms you cracked this hell they are not expressed or they are expressed so here I'm comparing how many cracked zero call we have in our program so basic say if we found if we cracked affine this isuh form is not expressing the simulative truth and we predict the same way we count it and the secret could actually correct it called the most number of zeros while the total zero number calling compare between our tools and all the other tools are comparable we are not increasing or the zero call here to drag this one to really improve on this metric as for the wrong time Altru is significantly shorter compared to start awesome although actually when we do the quantification is dragging a little bit that's because a bug in my program and I fix it later so the quantification still is slow in this case but later on when we show the real world data I can tell it's much much shorter and is comparable to sound most still not as fast as crystal but by a few minutes you want to tell so then we later on recently we did a real world reference sample benchmark so the competition result is not out yet and they probably wants religion to you 2020 that's my expectation but for this one we got to do the benchmark ourselves against public reference samples so we have human brain reference RNA and a universal human reference colony they're widely used it to benchmark different isoform or gene quantification tools actually if you find there are a lot of paper using that and most tools paper when they publish they're using these to us at least these two samples to do the benchmark and then they do QT PCR and RNA see examples and they basically compare the result between these two metric certainly there might be some bias in qrt-pcr but that's the best we can do so here I'm showing the Sigma and the truth ecology genes do they actually test with our TPP they are jinx yeah but how how many do they test that so the problem was taking a sample like that is you yeah it's the sample what do you use to define the truth so for this one we have to like to like teach you samples I would say biological replicas no they're not replicated biological samples yeah for each sample they do qrt-pcr T PCR on 12,000 genes oh okay for each genes they can they have in that do those discriminate basic forms for some of the genes they tell the difference between acid form for some of them and they focus on genes that with only single isoforms but they do have a lot of the genes that they a lot of tests on gene family pass which is a challenge for us and this is nice sample yes and another thing about it is the the kaabah the coverage on those genes that belong to the same gene family which is technically similar to isoform quantification from the same genes there it's equally challenging and here I'm showing the correlation here which shows a lot in terms of tells a lot in terms but like currently we're still having trouble in finding the most accurate estimation order of the abundance so on the left hand these are the log Pearson which so the whole estimation protocol we follow is well-established and being used by BGI Beijing genome Institute and several other tool paper to benchmark their 2-0 and still others so we will follow the tradition and do the log Pearson correlation of estimation so mostly existing tools they have you can see so first this free tools are serial alignments base the tools and the style awesome is the actual alignment tools and we actually have a top hair cuff link estimation but that seems falling off compared to all these tools and cuff link is because of several technical reasons like the mr. Hawley Obama's and missile early Obama's genes presents they seems to be not comparable to all or the rest of the tools here x-axis is our prediction log transformed the y-axis is the qrt-pcr season so the CT value can be seen as a lot of transformative expression value and yeah yes the scale is not saying because qrt-pcr when they reported they have a shift so they have a constants that is not normalized because different samples have a different way of them are and actually in this benchmark the secret prediction is done on five on AC examples and then average versus qrt-pcr is down to samples because they use different technology and they are they use their own standard processing process for different technology so I think it's a fair comparison can see the message time when we do honestly we don't do one sample we need three or four or five technical replicas but for the scale yes that this is like expected so here you see like Sigma defers quite a lot compared to the rest of alignment based the tools and linked towards the Stars and similarly in another sample we can see there's a trans the cecum performance a more similar to the alignment based tools and rather than the other showing there's a gap in traditional alignment based tools and the traditional alignment free tools but our program successful bridged the gap in the performance and in in terms of runtime stickmen finish in 15 or 15 minutes and sama finish in 30 minutes crystal or as always twenty minutes but you know it doesn't really bother for and there's a recent application we get from the midas single cell group that we apply our tools single cell data in this case we use fluid on because it is the closest to the box sequencing there are many single cell and assets now they available and they focus on the five prime or three prime when they use different barcode as for fluid arm is more like multiple bulk sequencing using different cells and we can actually tell there are a differentially expressed that your iso forms from the same genes in different cells when they carry different biomarkers and the one nice thing about isoform sorry flu time sequencing technology is when you see the cell you can actually take the image and you can see like the ground truth so we actually collaborate with dr. Ibrahim Aziza and Max which are on this project and then they give a affirmative response on how where our performance is compared to their label so I would like to acknowledge my collaborators and even one from dr. Ramez lab so she came out with me to participate in the dream challenge developed all these tools I would like to thank dr. Ibrahim as easy and dr. max wheelchair for their support in the poster challenge analysis and a single cell tool sinks our application and the - single cell group they give a great to extend application tools I would like to thank my thesis coming seats off to him from one and a doctor Margit Burmeister Michael Mendez dr. Gilman has been supporting this research quite a lot and dr. tremely is being leading the malleus group for this work and Kevin the dhaarana Stephen Parker thing giving a lot of suggestions in the algorithm development so that's pretty much that I have all this tool and by the way I just need to show you the tool is already uploaded to github so it's publicly available and if you actually if you search Sikma in github here you can actually find the repository and then we released the first version because it's in C++ and I want to rewrite it a little bit I'm a humbug I don't like he's one this is really stable right now you can download tool and using QT and C++ and eigen library and you can compile it by yourself and do a like tests if you have an existing data and let us know the this instruction installation instruction in the readme page on the first page and you can download the tools and get all the necessary library from anakata which is very nice hopefully this is gonna be helpful to for you know above mathematician analysis that's it thanks a lot so what does the qrt-pcr corner cutie-cutie is a library the C++ library that is developed to ensure cross-platform development so this tools if I read standard libraries probably is gonna be hard to deploy on Windows it's a kitty make it a lot easier in anaconda if you insta anaconda you already have Kitty installed yeah there's our theme of the source data directly applied to this project I went a theme of all Midas said it yeah so one interesting that I actually one interesting point is that I actually showed during the development section is that in the investor samples a lot of isoforms are not expressed at all and to correct it identify the express nicer form or the other way around the correctly identify not X PRIZE die so phones this is a challenge and those are zeros so in this case how to correctly specify the matrix is a I would say computational effort in this challenge and when we divide with this algorithm we spent a lot of time trying to optimize the final quantification step make sure that we correctly zero out the isoforms that are not expressed so when you have low level expression you probably don't have that many informative reads yeah as far as isoforms culture so so do you have any kind of general reliable noisy leads thank this isn't there on your live on Boise well no you don't just from the standpoint of sampling it is low level and you don't sample things that discriminate well I only have like one counts of one or two so do you put any kind of confidence information this is something that I'm working on and I do have trouble develop this kind of confidence score and what you just said is actually more of a problem we deal with application application free sequencing and the single cell sequencing we have a lower number of a total reach because typically when we say technically km5 fpkm we intends to we use that on bulk sequencing sample that has million varies hundred millions of reads wearing single cell sequencing when you have few thousands of reasons or few hundred thousands varèse this standard should be adjusted accordingly because in that case you have a ten TPM expression that what probably doesn't mean anything so you're like oh that's a ongoing project at moment yeah how many correct zero call you Nick yeah I guess I agree with you so and on the other hand four zero call itself is not informative enough to decide like which program performs better that's why we compare in this case including Spearman correlation and log Pearson correlation to show that at different level of the expression or tools you are outperforming others I think the combinational matrix gives more info it tells model of the story all right oh thanks that's all