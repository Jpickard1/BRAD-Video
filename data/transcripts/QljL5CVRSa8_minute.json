[
    {
        "start": 3.049,
        "text": "yeah she was in we're gonna go ahead and "
    },
    {
        "start": 97.17,
        "text": "get started the technology seminar series for anyone who is unsure about that there's a sign in sheet going around these do you want to mention they come about next week so an email I sent out about this week's talk you might notice that the next week's is cancelled however it's been uncounseled the original speaker had to cancel but one phone gone will actually be giving next week's presentation so there is a top-speed don't play with our emotions like that I know sorry but you get your pizza now next week too so everyone should be happy oh they're pleased to present today's speaker Johann gongyuan barche and he is assistant professor in statistics thank you so I'm going to be talking today about the ruv package are you Stanford moving or interrogation it's tools to move batch effects and another variation this is a heads up everything that's in the "
    },
    {
        "start": 159.45,
        "text": "talk today is on docker so if you go to this URL you can you can get all the code that I use and also even these sites in fact I'm running all of this from within dr. at the moment okay so outline I'm going to start off by giving an example dataset to to motivate everything that I'm going to say I'm then going to make some high-level comments on normalization and then also make some some high-level comments about the the Reeb framework then I'll get into discussing natural re be packaged and at the end I'll do a couple examples with the shiny app ok so to motivate everything I have got an example dataset this is a microarray data set from several years ago in which the board of the study was to do differential expression analysis to find "
    },
    {
        "start": 220.41,
        "text": "genes differentially expressed between men and women in the brain so do you design there are five men five women tissues were taken from each of these ten people postmortem one tissue sample from each person from the anterior cingulate cortex went from the girth lateral prefrontal cortex and one from the cerebellum so for a total of 32 samples three from each of the ten people and then each of the 32 tissue samples one aliquot was sent to each of three labs Davis Irvine and Michigan so we end up at the end of the day with 90 microarrays three arrays for each of 30 tissue samples so the screen is a little bit off here but I don't hear that for for each of these each of these 90 they were assayed by microarray and we have data on twelve thousand citizens so I'm gonna "
    },
    {
        "start": 282.919,
        "text": "just show you the the data here I've I've loaded in this state of set in dark type LS and here's what's inside we've got a data frame with information on the genes data set with data frame with information on the on samples and then we have two two versions of the expression data I've got in the comments one is if the raw data which is to say that it's been summarized by are made but without the quantile normalization or a background correction steps and then I also have the more standard normalized version which has been background corrected in quantumwise okay so here just so you can see it there's the data I've got reliefs are the microwaves and columns are the genes and inside here's the data frame of the the sample information again rows for the for the different micro arrays "
    },
    {
        "start": 344.63,
        "text": "and then I've got information on which of the ten patients it was their gender which brain region it was which lab it was that say that and then also chip version that there are two different versions of microarray that they used Dean information I've got information on on all the genes I've got the the simple chromosome and these things I'll talk more about as the talk goes on but this is whether or not the housekeeping gene is not whether or not spike in control is whether or not it's positive control again I'll I'll come back to all of this here I load the ruv package and set a couple graphical parameters and then here yeah we can see that okay okay here I've got a PC block of of the data and I I have it colored by my brain region so the three colors are the three different "
    },
    {
        "start": 405.289,
        "text": "rain regions and then the plot symbol I've got for the laboratory and what should strike you when you look at this is that by far the the strongest signal and the data is the lab effect so you have all of the Michigan data here and all of the Irvine data here all of the Davis data appeared so again this this is the data that has already been caught on normalized so it's been pre processed in some sense but still finally the largest signal and the data is black and getting rid of this sort of stuff that's going to be the focus of this time oh one other slide I want to show you real quick so one thing that is sometimes done in situations like this is people will distribute remove the first couple of principal components when it looks like the first couple of principal components are our technical effect here I've done that I've removed I removed the first five principal "
    },
    {
        "start": 466.699,
        "text": "components you look at the different versions of chips also yes and to a large extent although not exactly it's if I remember right Michigan the other different places reason yeah so there's there's two things these lab plus the chip yeah oh yeah so sometimes you might just want to remove the first couple of principal components but you see if you have more or less removes the the lab effect but there's also no biology remaining either you don't see clustering by by brain region so this is just to to flag the point that normalization can't be too strong and you can throw out the biology that your interest in along with technical effects if you're not careful and then as a preview "
    },
    {
        "start": 531.02,
        "text": "here is data that all here's a normalized version that I'll get to later in the talk that's been generalized using one of the ruv approaches and here we see that the batch effects are gone and we also have a nice clustering by my brain region so we've got called cerebellar near and all of the of the cortex how would you ever in the batch of that that's the point doc yeah I'll show you just a preview to keep your attention ok so before I get into the details I want to make some some high-level some high-level comments on normalization the first comment I want to make is that and mind you there is no one right normalization and what I'm not saying by this is I'm not saying "
    },
    {
        "start": 591.23,
        "text": "just that there is no one right normalization algorithm although I think that's true also I'm making even stronger statement that even for a given data set there is one no right normalization and the reason I say that is because the normalization that you're going to do it's gonna depend on what variation is wanted or another words what variation is is of interest to you and what variation is unwanted and that will depend on your context it will also depend somewhat on the on the goal of your downstream analysis but I would just want to highlight this one if for no other reason the the normalization that you that you want to do will depend on what variation you're interested in in what variation so vibrating on that point a little bit I'm going to distinguish between wanted and unwanted variation and our observed data is going "
    },
    {
        "start": 655.91,
        "text": "to be composed of both variations from many sources there's going to be a bunch of variation that's unwanted that's the technical effects so things like the the lab or the batch variation due to quality of the reagents due to RNA degradation even things like composition of the atmosphere and I could go on and on with a long list here but then there's going to be sources of variation of interest also a ginger for sure in this example that was the original goal of study and then I've got a brain region with question mark next to it meaning that this is a brain region may or may not be of interest if all you're interested in is a differential expression with respect to gender you might think of brain region as a as an instance variable but in other contexts it might very well be what you're interested in so here again I'm just highlighting that what variation is once in an unwanted "
    },
    {
        "start": 715.94,
        "text": "it's going to depend on the context and the normalization will want to reflect that saying that again here should we think of brain region is wanted or what I wanted I'm going to argue that it depends and that the normalization should reflect that I should point out that this is not an entirely trivial thing that I'm saying because you could imagine a counter argument someone saying that when we normalize what we really want to do if we want to remove anything that is a technical artifact and we want to preserve all biology so brain region would be biology and we'd want to we'd want to keep it and if if I'm not interested in brain region well okay all adjust for that in the in the downstream analysis so that is reasonable but I'll say that a well I'll "
    },
    {
        "start": 779.78,
        "text": "make two points one being that some biology is nearly always all in wanted so for example any sort of changes in the gene expression because he just ate or just exercise or anything like that changes due to the circadian rhythm let's just specifically studying circadian rhythm that's probably something that you want to normalize away but even more than that and this is I think what is really critical it's a lot of these biological factors that you might want to adjust for our unknown their unknown or you don't you don't have information on them and so it won't be possible to explicitly account for them in the downstream analysis and so we do want to try and adjust for them at the at the normalization stage okay so elaborating a little bit more on this idea of unknown sources of variation "
    },
    {
        "start": 841.43,
        "text": "just highlighting it again so will very often the unwanted variation comes from sources that are unknown this is true of both technical and biological on one in variation and identifying and then then removing the Sun women variation is especially challenging if I'm gonna say just a couple words about identifiability here imagine in this this simple case we've we're just gonna say our observed variation it's just gonna lead to some wanted variation and unwanted variation and I'm going to make the point here on the next slide not at all surprising but I think we're still worth stating that different patterns of wanted unwanted variation can produce the same observed variation so I have back in the picture here I've got that our observed variation it's a sum of wanted variation and unwanted variation and I've got two different examples here "
    },
    {
        "start": 904.79,
        "text": "where where the observed variation is the same in both of these examples but the part that's watch it is different between these two and the part that is unwanted is different between these two but they add up to two the same okay if we could it's a good look and we could see within this observed data what part was wanted and what part was I wanted well then we can't we only we only see the Sun here so again nothing surprising but and when I'm feeling fine but I point this out explicitly just to highlight the fact that any normalizations are gonna do it is going to require some sort of extra information because without at least some sort of extra piece of information we're not able to distinguish between these two cases so again not nothing too surprising but still we're stating that "
    },
    {
        "start": 966.17,
        "text": "any normalization we're gonna do is going to require some sort of extra information and here's examples of things that you might use so you might use for example an assumption that the marginal distribution is the same for microwave that's something that you might justify doing the quantile normalization for example you might have information on what the unwanted factors are so you might have information on the batches for example you might know that your factor of interest is that will make it a little bit easier you might have various sparsity assumptions you might have various independence assumptions but again you need something ok one one special case I wanna highlight is when you do have a known factor of interest which is the case when we're doing a differential expression analysis even even if you have a normal known factor of interest you do still have identifiability issues "
    },
    {
        "start": 1027.48,
        "text": "the the key problem is that if if you don't know what the unwanted variation is you don't know whether the unwanted variation and the variation of interests are correlated and depending on whether or not they're correlated and to what extent um that can very much change the analysis that you do so we just show this picture one more time it's it's a little bit hard to tell from from the screen but I made these matrices in such a way that the wanted variation in both cases the factor of interest is the same there are there are six six rows in one group and six rows in another group here you've got off off on off on so you can think of these as control for example and you'll notice that the the factor of interest is and is the same in both so basically control control treated and "
    },
    {
        "start": 1087.73,
        "text": "control treated so on what different with differs between the two though is how each gene is affected so for example in the flower matrix these two genes aren't affected by the factor of interest at all whereas in this one they are so just knowing the factor of interest alone isn't isn't enough to solve the identifiability issue and again the critical issue is that the wanted variation and the unwanted variation could be correlated okay so just highlight the the challenges the the two that I've more or less focused on in these last couple of comments are that these are are related solely distinct we need to worry about defining unwanted variation so in any context we need to be some way to define what variation is because of interest to us and which variation is not and then "
    },
    {
        "start": 1148.72,
        "text": "aside from just in some sense defining what variation is wanted and I wanted we need some way statistically have identified okay so those are two of our key challenges in addition I haven't collaborated on these too much but we once we've defined and I identified our on what a variation we still need some sort of statistical method for actually separating them out and at the end of the day we also need methods to validate our analysis I'm not going to go into too much detail on those but decide flag it okay so now I'm going to just talk a little bit about in general the are you the framework which will help give us ways to deal with all of these problems put mini outline for the next couple of minutes I'm going to first present a model that is common to all of our Evie methods and then going to talk "
    },
    {
        "start": 1210.46,
        "text": "about negative controls which are one of the key tools that the arguing methods make yourself the the negative controls help us both define and identify unwanted variation sometimes or actually always in addition to the negative controls we still need at least a little bit more information to fully identify the unwanted variation and there's various different ways we can do that I refer to these as secondary identifying assumptions and the one that I'm going to to focus on quite a bit is replicants so in the rav framework two things that you you make a lot of use are make a lot of use out of our negative controls and replicates so I'll explain what these are tell that used and then I'll just conclude with the slide explaining why I think this is useful okay so here's the model um this is a model that will be "
    },
    {
        "start": 1271.33,
        "text": "used for all of the Reeb methods it's that we've got our observed expression data why it's gonna be equal to some wanted variation which is the product of a factor of interest in some regression coefficients plus unwanted variation the W alpha which again is unwanted factors and regression coefficients plus some some random error and then I've got this table here the the Y is the observed data M is the number of observations in this case it's 84 I lied a little bit at the beginning of the talk I said that there were many microwaves six of them for whatever reason are missing from GE oh so it's actually before that we have there's four thousand six hundred genes X is the factory which might be something like gender or brain region WR WR the unwanted factors and then very importantly this w "
    },
    {
        "start": 1332.2,
        "text": "typically it's it's unobserved we don't know it but it consists of things like reagent quality and just to point it out this model is originally due to leak and story and to stay go into okay so now negative controls negative controls and these are central to all of the method thing in the describe they are features by which I mean typically genes that are unaffected by the factor of interest so in our case unaffected by gender for example but are affected by the unwanted factors okay so as an example um one example might be spike in controls and a lot of micro experiments this one included a a researcher will spike in bacterial RNA and then there's some of "
    },
    {
        "start": 1394.3,
        "text": "the probes on the microarray that respond to this bacterial RNA spike in a fix little quantities and typically this is used as a as a quality check I like to be using it as the basis of a normalization basically the the logic is going to be that the spike in controls any observed variation in the spike in controls is not going to be due to gender because it's this external spike in but it will hopefully pick up a lot of these unwanted factors so that will form the basis of our adjustment to be a little bit more explicit so suppose that we have a number and sub peak of our our end columns of white or negative controls we're gonna let Y sub C be the N by n sub C sub matrix of Y that contains only the columns of the negative controls we're also going to define this beta sub C alpha sub C and epsilon sub C similarly these are all sub matrices and we then have this this "
    },
    {
        "start": 1456.4,
        "text": "equation it's the same model but just applied now to just the negative controls now what's important here if that is let's down this beta sub C that's equal to 0 by assumption that is our assumption that the negative controls are unaffected by the factor of interest so that term disappears and we're left with what looks like just an ordinary factor analysis model and so we can actually estimate W by factor analysis and that is what essentially solves our identifiability problem the the negative controls are give us a way to estimate the W even for when the unknown or when the sources of unwanted variation are unknown to us so I want to I want to stop and just talk for a second about this this negative control assumption so "
    },
    {
        "start": 1517.72,
        "text": "this is this is our key assumption it's this assumption that the negative controls are unaffected by our factor of interest and what I really want to point out is that this is this has to be some sort of assumption based on on prior knowledge the negative controls cannot be discovered novo from our data and in particular what this assumption does not or what we cannot do if we cannot just look at our data find genes that are uncorrelated with our factor of interest and call them negative controls we can't do that because it's actually possible that the the negative controls are things that are correlated with the factor of interest and that would happen if the if the unwanted factors are themselves correlated with the factor of interest then the negative controls should be all set okay so this assumption this assumption is "
    },
    {
        "start": 1579.169,
        "text": "not an assumption about correlated negatively factor of interest this is assumption about whether it might be slightly too strong a word but all it's hopeful it's it's an assumption about whether or not the the factor of interest actually causes changes in the in the negative controls this is saying that the unwanted in fact the unwind factor does not have any effect on the on the negative controls I'm talking about the negative controls they don't need to be perfect so it can it it is a seemingly strong assumption that these that we need to pick jeans or Vikings or something like that that are unaffected by the factor of interest but they don't need to be perfect and in a lot of cases "
    },
    {
        "start": 1640.72,
        "text": "having a set of jeans that at least most of them are unaffected by the factor of interest will do or in some situations depending on exactly which method we're using will even work if on average there are they're unaffected by the factor of interest okay so it's a somewhat strong assumption but also it's not one that needs to be perfect okay so that's that's a negative controls not going to talk about secondary identifying assumptions so I said that we need the negative controls but then we also need something else so we saw that the negative controls say help us get W but we still need to get also somehow or other and here are some ways that we can do that one is to have to have a known factor of interest so if we're doing a differential expression analysis and we've got negative controls we're pretty much good to go in some cases though we won't have a known "
    },
    {
        "start": 1700.76,
        "text": "factor of interest which that might sound kind of weird like what kind of analysis are we doing if we don't have a known factor of interest what I mean by that if something for example like if we're doing a clustering analysis where the the biology of interest is these clusters that we are discovering but they're not known ahead of time so in situations where we don't have a known Proctor Pinterest other things that we can do we can make you send the assumption that the the factor of interest and unlimited actors are uncorrelated that might be justified for example if we've got a randomized study where the factor of interest is treatment or control and with assigned at random another thing that we might that we can make you so this is if we have gene wide covariance this is something that I I'm not going to talk very much about in this talk fine let's skip over that the one thing that I'm going to talk about a lot though is we might have replicates and replicates are "
    },
    {
        "start": 1763.04,
        "text": "a another way of getting alpha okay so I'm going to talk about what gets them so suppose that we have two replicates y1 and y2 and so what I mean by this would be for example that say both y1 and y2 they're both micro rays from the same same sample so say they should one cerebellum but one of them was done it Irvine and one of them was done at Michigan for example so we have two relatives this is this is an expression for or from the model for the for one of them and then was about also to the second one and then critically this part here down to the bottom this is key the factor of interest in both of them is going to be the same because again both of these come from the same tissue sample so the biology is going to be the same in both it's not a replication of the same apartment since it's a "
    },
    {
        "start": 1824.42,
        "text": "different place same tissue sample and then they had different heliports and sent them off to the different labs so the the biology should be the same because it's come from the same tissues you've got you've got different people like lab techs can have an amazing effect on how these things work oh yeah train that was different chip versions between some of the sons and that can hit me but but in both cases it comes from the same same tissue sample extracted so the whatever biology that you're going to be interested in that factor is gonna be the same started was it yes the biological factor the the thing that you're interested in that it's not varying from from one to the other okay so if we usually subtract these two "
    },
    {
        "start": 1887.289,
        "text": "if we subtract these two replicates there's there's a math of it but then critically this x2 minus x1 so the whatever the biology is that's gonna that's going to cancel out this term will disappear and we'll be left with this so we're now left with just the unwanted variation so again we started this is the the same tissue sample was sent to two different labs and the what I'm saying here is that any variation that exists between what one lab says and what the other lab says is essentially by definition stuff that some wanted an idea world the two labs would say the same thing okay and so if we now have a bunch of replicates we could build up a matrix of these differences and do factor analysis and "
    },
    {
        "start": 1949.07,
        "text": "that would give us a way of identifying this alpha so just a highlight here what I said about negative controls the replicates there they're similar in a lot of ways you can think of them as has dual to each other the negative controls these are genes where we assume that the beta and zero and they allow identification of the W and then conversely replicates these are rate differences and we're in taking the difference the X is zero and this allows us identification of so between the two of those were able to get W and alpha and identify the unwanted variation and then remove it okay so just a highlight why I think all of this is useful why why do we want to try and remove unwanted variation using "
    },
    {
        "start": 2011.02,
        "text": "negative controls and replicates one reason is because this general framework is somewhat flexible in that by and by that I mean that the the negative controls in the replicates they not only help us identify the unwanted variation but when we go to choose what is a negative control and when we go to choose what is a replicate and I guess I should say for a second there are different notions of replicates there's technical replicates there's biological replicates when we choose what we mean by a replicant we are implicitly also defining what variation is wanted and what variation is unwanted and I'll make this a little bit more explicit when I give some examples in a little bit but in any case again the net by choosing negative controls and choosing their replicates we're not only identifying the unwanted variation but we're also defined at the same time so that is helpful to us it well depends a little "
    },
    {
        "start": 2081.52,
        "text": "bit on what you mean by by biological replicates you said yo I see that technically because it's by orbit but you want biological replicas days that I don't I'm not sure how often you can see them um say I'll say often enough and I'll also say that you do not very often see experiments like this one that I've showed where everything is replicated or in this case duplicated that is pretty rare but it is not that uncommon to find experiments where at least a handful of the samples are replicated and even just having a couple of them and can often be good enough so if they can't do biological "
    },
    {
        "start": 2144.97,
        "text": "replicates hmm that means they have the resource to do everything on a set with the same technical conditions then why do they bother to do the power to electric case in the first place so say they can so they have the same brain tissue they want to send two different that I don't when they do everything on the same in the same lab what can can I talk about that at the end okay sorry I fell hard a bit that I want to get to but yeah let's talk about that at the end okay so why just again why I think this is good it's it's flexible it's also transparent like I said whenever you do some sort of normalization you're gonna be right relying on something to to get the normalization going and by by doing everything in terms of negative controls of replicates the assumptions are really front and center which I think it's good "
    },
    {
        "start": 2206.47,
        "text": "because it makes it easy to to debate them people can discuss whether or not it makes sense and then as as I hope all I'll show in the next couple slides also it seems to work well okay so that is that is all the high-level comments that I'm going to make I'm not going to go through the details of actually using the Reeb package like I said all of this is is on docker so some points I might skip over fairly quickly in interest of time but quickly outline of what's included here in in the discussion of the package I'll just give a quick overview of the functions that are included in the rav package broadly speaking there is going to be mission methods for regression analysis or just to say differential expression analysis and then also methods for for global adjustments so I'll talk about "
    },
    {
        "start": 2268.72,
        "text": "those separately and then once I do I'll go we'll move on to the case studies with the show you okay so quick overview of the functions there's there's basically so here the the core are you via just ask there's there's two for global adjustments are you v1 and are you be 3 and there's four for regression or differential expression I'm going to talk about hobbies separately in a minute so those these are the the main adjustment algorithms there's some helper functions to help use these the the ones that I have highlighted here and no these are ones that you'll use on a pretty regular basis there are some some functions for making nice plots and there's also a couple functions that they're they're "
    },
    {
        "start": 2330.4,
        "text": "exposed to the user but not actually intended to the east on a regular basis there there are some cases where they might be useful but on a regular basis probably not the one that I really want to draw your attention to is this one gig get K which I've highlighted red this is a function that returns a number K which is the number of factors that you might want to adjust for I want to point out that this function should not be relied upon at best it will occasionally be useful for for an ruv for analysis but in general it's not something that you that you want to rely on and then there's also the the shiny oh okay so I'm gonna I'm gonna start off by talking about the the regression methods they they all have a fairly similar format they take its input y & x + a ctrl R u b2 + 4 also take a "
    },
    {
        "start": 2393.63,
        "text": "tuning parameter K there's an optional Z matrix an optional EDA matrix and for our UV partners there is an optional lambda parameter so I mean just explain what these are but why is the expression data that is that's going to be an M by n matrix so observations by by genes X is your factor of interest control what's an index of the negative controls the is any other covariance that you might want to include in the analysis data is Jeanne Weiss covariance that you might want to include in the analysis and then there's K which is number of factors to address 4 and lambda which is a image I'm gonna go through each of these quickly but individually so again what is the expression data its arrays "
    },
    {
        "start": 2454.47,
        "text": "by genes those least for microarray data you wanna you'll want the day to be long performed but interestingly enough other than log transforming it's often best not to do any further pre-processing so things like want to all normalization the reason for that is that a lot of a lot of cases if there's quite a bit of variation between between different biological groups the the quantum normalization can introduce more warping than then itself X is a factor of interest so things like Janene gender averages if you you do not want to include an intercept in X and just as a rule of thumb the the fewer factors that you include the better and the basic reason for that is the more stuff you put into X the less can be estimated and "
    },
    {
        "start": 2517.77,
        "text": "adjusted for in in the in are estimated W and if you do have several factors of interest in general it's gonna be best to just several times one for each factor that you're interested in Z if any additional covariance that you want to include things like if you have a known batch stuff like that if if you do have if you want to include an intercept which generally you do and that's the default V is the place to put it and then like with X just as a rule of thumb the less stuff you put in into Z the better so actually in most cases I would recommend just not having anything in Z other than the intercept again the reason is the more you could entity the less you the less or the fewer dimensions you have available to to include in your estimated and "
    },
    {
        "start": 2578.69,
        "text": "elaborating on that point slightly so even if you have for example known batches and there's variation there's yeah there's differences between the batches it's typically not the case that batch per se is causing the or it's a source of the unwanted variation it's things like differences in the reagents various technical differences that differ between the batches and so I guess what I'm saying here is that the batches are at best a proxy for the actual unwanted variation and letting the the algorithm estimate that unwanted variation rather than including it yourself often performs better there's one very notable exception to this which is a if you have a co variant that affects only a very small number of "
    },
    {
        "start": 2640.319,
        "text": "genes and specifically the the same genes that you might be that you might expect to be affected by your factor of interest then you might want to explicitly include this factor in your Z because since it affects only a small number of genes it might not get picked up in the factor analysis so just to be a little bit more explicit an example would be if your factor of interest is disease that affects only a small number of genes and then the co variant that you want to adjust for is a drug that also affects similar genes so for them you know a lot of the people that have the disease might be taking a specific drug in which case you'd want to adjust for that explicitly with your with your z minutes okay the controls these are very critical to success you want your "
    },
    {
        "start": 2700.569,
        "text": "negative controls to be unaffected by the factor of interest you want them to be affected by the unwanted factors and ideally you'd also have it that they're somewhat representative of the other genes meaning they have similar ranges of expression not affected by their own unone factors that of some extent of reference to spike in spike ins have a lot of their own noise just due to their own administration and so already these are not things that you can discover from the data it needs to be based on prior knowledge and but that said they don't need to be correct okay that is number of factors to adjust for for youths only with re B 2 & 4 and it's useful to use this specifically when you are concerned that your negative controls might contain at least some biology in that case keeping small are keeping case small will help reduce the "
    },
    {
        "start": 2762.009,
        "text": "list of over adjusting meaning that if for example you're not using spiking but you're using something like housekeeping genes yeah alright and this is partly why I say that you want to wear cheese cheese k by hand and not use this a function here's a quick comparison of the methods their their various strengths and weaknesses I'm gonna skip over this you can you can look at it on online but I'll just point out that are you the invert or excuse me are you the are inverse if a good compromise of features it's useful as a good default quick technical note so the the different methods make make use of the negative controls in slightly different ways the ruv - you actually do need the negative "
    },
    {
        "start": 2822.19,
        "text": "controls for them to be unaffected by the factor of interest whereas you you have a weaker condition for for the other methods okay so and I'm going to give an example analysis here I've got in the code I'm going to run the Reeb our inverse function I'm going to pass it my my data matrix Y gender and I'm going to say that my Mike's my negative controls are the spikes and metals that all return a fit I know then put the the fit into this re be summary function and the re be summary function it takes in addition to the fit it takes again the original data matrix the data frame with the sample information the data frame with the T and information and then what that returns is this nice summary of "
    },
    {
        "start": 2882.869,
        "text": "it's got for example F P values from an F test if this case I only have one factor of interest so the values for the individual factor there's only one and that's them you could look to the F test but when you've got more than one factor it'll give you the people who submit for them individually along with a bunch of other information forward there are a bunch of plots that you can that you can make then just by taking this this fit summary and plotting them here we've got a p-value histogram this is war that's what you want to see if in once I'm wearing variation have been adjusted for the the p-values should have more or less a uniform distribution except for the ones that are significant the ones that are truly differentially expressed and for those you get a spike right so this is looking good "
    },
    {
        "start": 2943.589,
        "text": "there's also function to make an empirical CDF if you don't want to look at a histogram you can power transform on that so you can this is basically allows us to see that there is that spike there using various ggplot things here i've got for example the different ec gifts for for the spike in controls for housekeeping so on you know skip through these clocks that well there's one other one this so i'm gonna highlight if an important way to to be able to validate your analysis to know if you're helping or not if you have some notion of positive controls so in the case of a gender study you are you don't know necessarily for sure which genes are going to be differentially expressed between men and women but you have a pretty good guess that at least a lot of them are gonna come from the XY chromosome so what I can do is I can treat genes from X and Y chromosome have positive controls once I rank my genes I can go through and I can count for "
    },
    {
        "start": 3004.619,
        "text": "example how many of the extra gene XY chromosomes show up in the top 10 once I rank them and I see that for example mind you how many in the top 50 and I see that so looking at a plot like this can help us give us a sense of whether or not we are helping or hurting and there's this there's this line here this shows us what this plot would have looked like if I had just find p-values at random to everything Thanks so we're clearly doing a lot better okay these other plots I went to oh this is just a highlight again have we have we helped this is what Anna unadjusted Libba analysis would have looked like the p-value histogram is messed up we don't find as many positive girls okay I want to I'll try to keep it to just an extra five "
    },
    {
        "start": 3065.069,
        "text": "minutes but I want to show you some of the global adjustments I'm going to skip over already one stuff speak to the rev3 stuff are UV three replicates and is going to be the mapping matrix that describes the replicate structure so it takes our observed expression data it takes the matrix that describes the replicate structure it takes our negative controls and then some some other parameters what I mean by the mapping matrix this is a matrix of dummy variables where for row I and J of this matrix it's one if observation one is in replicate set J and it's zero otherwise so basically each each column of this mapping matrix represents a set of replicates and the rows of that column "
    },
    {
        "start": 3125.579,
        "text": "that are one tell you which of the which of the micro arrays are part of that set of replicants okay and of course it's certainly the case that some what we'll refer to as replicate sets are really just a single array so in other words they can be Singleton's again the the key assumption when we're using our ub 3 into any variation within a replica set is assumed more or less by definition to be unwanted okay there's a function here that helps you create these mapping matrices I'll skip over that it's in the documentation I'll skip over that it's in the documentation but I want to at least show some results here and in this this first example I'm going to have as my negative controls I'm gonna have the spike ins and if my replicates I'm going to have the technical replicates so the the same tissue samples but at the three different levels "
    },
    {
        "start": 3186.99,
        "text": "as a reminder that is what the data look like the sir before normalizing and here is what my what my mapping matrix looks like so again these three samples are all the same these three samples are all replicates these three samples are all replicates and so on and then here is the normalized version of the data using our UV 3 ok I want to get to example 2 in example 2 I'm going to do essentially what I just did except this time when I make the the PC pot I'm going to make the PC pot only the genes from the X and Y chromosome and when I I want you to think just for a second so now I'm taking just genes from the next Y chromosome what do you expect that we will see the 4 adjustment "
    },
    {
        "start": 3247.56,
        "text": "and after adjustment before adjustment it actually looks pretty much exactly the same again these are genes just from the extra Y chromosome but still the by far the biggest signal in the data is the lab effect I've changed the graphic parameters here a little bit so that now I can look at the difference between men and women and there is no sign of the gender effect in here now I'm going to adjust interestingly after this adjustment now the biggest source of variation is the the brain region ok so again this is just the genes from X Y chromosome but even there the biggest source of variation is the brain region okay now this next example is going to to help make the the point of that I was "
    },
    {
        "start": 3310.53,
        "text": "trying to make earlier about how the negative controls and the replicates can help define which variation is wanted in which variation is unloaded so what I'm going to do now if I'm going to use of my negative controls housekeeping genes and I'm going to use as replicates all observations from a single patient so for each patient there's the the three different tissue samples from the three different brain regions and each of each of those are sent to the same there are three labs so for each patient there's there's nine microarrays I can use all of them as my notion of replicate again for the negative controls I'm using housekeeping genes so now the mapping matrix looks like this these first nine are all from patient one the second nine are all from patient two and so on now after just this way the biggest source of variation is here is the difference between men and women okay so by changing my notion of replicate and by "
    },
    {
        "start": 3370.68,
        "text": "changing my notion of negative control I have changed what implicitly I am calling wanted an unwanted variation I now get a very different result I want to pause and linger on this just for a second more I want to point out that what I just did there's more going on here than simply just in some sense progressing a brain I've done a little bit more than them so just as a comparison Here I am quite literally regressing out brain regions so I I do the adjustment that I had already done using the spike in controls and just the technical replicates and then I explicitly regress out brain region when I do that I get a get a different looking thing I I do not see the gender effect partly that's because this is driven by the 16 outlier but if I even if after I remove that outlier I still do not see gender coming up as the the main effect if I skip forward to now I'm "
    },
    {
        "start": 3432.81,
        "text": "looking at the third and fourth principal components it's well it's only by the time I get to the third and fourth principle components that I'm beginning to see this gender effect so I are using this other notion of replicates in this other notion of negative controls using the housekeeping genes and biological replicates I I've come up with a stronger adjustment I have adjusted for not not just brain region but many many other sources of biological unwanted variation and another important difference between what I just did and simply regressing out brain region that actually isn't relevant to this example because we have a nice balanced design but you can imagine that if I had a design that was not balanced yes I just naively regress our brain region but brain region was itself correlated with other biology I might be wiping out that other biology "
    },
    {
        "start": 3492.839,
        "text": "as well and with rev3 however even if you have an imbalance design you won't be wiping out biology of interest even if it's correlated with your unwanted factors one final example here I'm going to use one yet another different notion of replicates I'm going to treat all of the samples from the same brain region as a replicant so in this case my mapping matrix looks like this I've got one set of replicates is the interior single cortex one is the cerebellum and one is the berth lateral prefrontal cortex when I do that okay so not surprisingly the the brain the brain regions pop right out but actually be surprising I basically fed information about the brain regions into the algorithm what I'm gonna do here though is I'm going to do what I call bursting I'm going to burst the cerebellum and "
    },
    {
        "start": 3553.14,
        "text": "the door final prefrontal cortex meaning I'm going to treat everything that's cerebellum and anything that's or file prefrontal cortex I'm going to treat all of those as Singleton's I'm not going to treat any of them as replicates of each other so in fact what I'm going to have is a replicant structure where I'm only going to treat samples from the AC cortex as replicates so to see this visually this is what my what my replicate structure is going to look like a Nicole everything from the anterior cingulate cortex a replicant and then everything else is going be treated as when I do this oh we're good to go oh shoot okay well you can you can go back when darker and you can rerun it yourself but I'll tell you what it would "
    },
    {
        "start": 3613.68,
        "text": "have looked like what it would have looked like is that the anterior cingulate cortex samples those are the ones that I treated as replicates they have all been very very tightly clustered and that's because I basically told the algorithm but those are replicates and it removed all the variation within that replicant set but then also you would have gotten still a very nice separation between the cerebellum and the dorsal lateral prefrontal cortex another things that I did not explicitly tell the algorithm were replicant but they do posture very nicely and what I want to say about this one is that bursting is a useful for validating adjustment so for example I only tell the algorithm that the AC cortex replicates but then I see very nice clustering between the other two groups so I know that I've done a good job but even more exciting I think if "
    },
    {
        "start": 3674.279,
        "text": "what I think that this says that I might be able to do in in a particular example that I'm that I'm pretty excited about pursuing the idea would be this say that I want to do some sort of cluster analysis and I want to discover diseased subjects and so now very often when you do a cluster analysis to try and discover disease subtypes you find clusters but then very often it's kind of an open question of these clusters that I found do they actually have anything to do with the disease so there might clearly get some clusters but are they are they disease relevant clusters and what I'm hopeful that I might be able to be done using using this ruv framework is to do something like this to say I'm going to you I'm going to get myself some healthy controls and I'm going to treat all of the healthy controls as biological replicates and by treating all of these "
    },
    {
        "start": 3735.99,
        "text": "by these healthy controls of biological replicates I will be capturing all of what might be considered ordinary biological variation I can then just for that ordinary biological variation and then with any luck afterwards what will be left it's only disease related information and so when I looked at my clusters I'll have at least some extra reason to believe that these clusters that I'm looking at are actually related to disease okay so I know that I'm already over there's a shiny app and you can use it all right thanks sort of make sense thank you "
    }
]