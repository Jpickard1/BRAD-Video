[
    {
        "start": 0.0,
        "text": "they're had target was its you he was worried that he couldn't do that wasn't hard to find out I just call them on so anyway we're here I'm also proud to say he's a member of my laboratory which you know maybe you know we're going to say controversial things I don't know if I would have said that one but you know if anything I've seen that she learning is on the rise and you know it'll be hard to dislodge signal-processing from many many fields anyway with that being sat why we might as well let you state your peach alex has been working not developing an end and pipeline to process imagery from a 3d microscope in my lab that are labeled up with various things to link you know cellular function you know with the informatics elements so you know maybe you'll do something about that and he uses the "
    },
    {
        "start": 60.719,
        "text": "Loney pipeline for part of that which is you know what's developed by art and you know he bow over many many years you know it's standard image processing pipeline or radiological images brain imagery and all this kind of stuff it's world standard stuff there is it one of the bd2k centers just now at the University of Southern California which I believe has to which is housed there you know art moved from UCLA to the University of Southern California around a couple years ago on that so Alex why don't you go ahead and give us a start you know his interest really is dated science so you know let's see what you've got on your mind okay hi everybody so I'm Alex and my interested I am interested in data science but particularly in machine learning and applying it by a magical biomechanical "
    },
    {
        "start": 121.11,
        "text": "images that's what we are doing with dr. era in his lab and this particular talk is about deep learning which we are not currently using our lab but we might be in future after I successfully print present on it and so yeah the title I know right I want to say that just because of I want to emphasize the knowledge that it's a buzzword it was around for a while for a couple of years it was on the front page of the science section of New York Times another of different papers but as a as an answer I referring to that question I could give you a shorter so right now no it's not it's not the end of machine learning signal processing but thank you for coming I glad to see so many people maybe reacted to this provocative title but they but I do think that that method the collection of the earth methods there are important and they should and they do sure give good results and they do deserve some attention paid to that "
    },
    {
        "start": 181.68,
        "text": "and I would like that biomedical and bioinformatics researchers that society learn more about these methods so that's the motivation for this talk and as I just brief introduction for those of you who are not very familiar I'm in familiar with neural networks and deploring in general so deep learning is based on neural networks which are biologically inspired models for for learning from the data and the deep learning it's a actually wide set of different algorithms that are mostly based on neural networks that allows efficient learning on a data and that's the classical typical structure of neural network they were developed in in the middle of 20th century that's the model when you have entities that are called neurons in a knowledge of the brain and it's basically what it does it approximates some unknown function based "
    },
    {
        "start": 243.24,
        "text": "on them based on the input nodes towards the towards the output node and how it works it basically the model that have weights that are calculated in a in a in a way taken taken these values from from input nodes calculating using some embedded function that's called an activation function in in a middle layer which are also often called hidden layer and then there is output note that represents itself as a as an output of unknown function and what what's the difference between usual networks and and deep deep learning networks that they have multiple layers of nonlinear nonlinear part set processing unions that are in in middle layers and hidden layers and they can do the provide supervised learning but more importantly they do it did represent they do learn feature representations on "
    },
    {
        "start": 304.53,
        "text": "those layers and when you when you have many middle layers here in the middle those PT representations they start to form a hierarchy from low level to high level features and yeah there are little bit of historical perspective so an answer imited in 1950s at the same time people already tried to do multi multi layer and neural networks multi or multi-layer perceptron was was invented in 1957 and but later those techniques they were not showing great performance and it'll outperform work by Newell algorithm such as support vector machine or later random forests and the reasons why they didn't work well at that time so the process of training and the propagation was too slow and it was not enough data and I should mention the back propagation process is the specific "
    },
    {
        "start": 365.04,
        "text": "for neural networks so at the process of training from the from the input data the weights of the model are getting defined but but later when you get the output of the unknown function there is an algorithm that's called back propagation that allows to update the weights in the middle layer so the learning process can be reiterated again with the new weights and it's conversion to the to the optimal optimal weight set so I think another problem and I'd like by perception Utley the neural Matz was that they were a kind of a black box I don't think a little bit about it yeah I don't that's really an important thing because this is the computer scientists at a very negative you know kind of attitude about the neuron that's because so yeah one of the attacks against all the pronation but you know I exactly "
    },
    {
        "start": 426.9,
        "text": "so one of the objective of this of this talk is to give an update on that at in rod so so how we did it how did we get here that that that's basic everywhere it's in New York Times that's in in in papers in in science and Google is working on that and recently Microsoft said that it can recognize images better than humans do so basically since that time since the neural networks had their first Renaissance something's changed first of all new algorithms were introduced one of them is very important is Larry Weiss training algorithm by Geoffrey Hinton from the University of Toronto who who who published a couple of papers in that year including one in science now we get larger data much larger than now and dr. Eddie is going to give a talk soon I hope about the big paper in a "
    },
    {
        "start": 487.71,
        "text": "couple of weeks he's going to talk about number two in this list I'm gonna sort of I mean little bit for him now and now we got much more computing power which is a which is obviously because of the Moore's law and the GPU computing and some new algorithms because the rise of deep learning also gave boost to the not only to them hard not only to the hardware and and data but but a lot of progress was done in algorithm development and what first I'm going to talk about some results that were sure that were demonstrated it's using deep learning and then I will go a little bit into how they work so just just some examples who are deep learning algorithms for performing performing much better than any other models and beating the benchmark benchmarks by other models for example the traffic sign contest pedestrian detection that is used by different "
    },
    {
        "start": 548.88,
        "text": "companies and universities for example for self-driving cars and as you might know there is one polygon that is under construction near Ann Arbor where they're gonna test self-driving cars so these algorithms are working and getting be used in those those applications one important application in in computer vision is image classification so there is a very famous in the field of computers there is very famous data set called image net and there is a competition every year were different teams from industry and academia they compete to show the lowest error rate and error rate for some time was around 25 percent and at 2012 in this work by by students of jeffrey hilton the the error rate on this data set was dropped by ten percent to the fifteen percent "
    },
    {
        "start": 609.2,
        "text": "and those improvements that they made for deep learning algorithm they were called breakthrough in a computer vision i will mention later so they also used a machine parsing when they trying to classify the different areas of the image if it's a if it's a grass or if it's the sky or if it's people in image or sing parsing from that image including using the deep learning on on microsoft key breakthrough the Malta scale I mean was a big breakthrough here you know we've been trying to do this what is the fundamental breakthrough you know goes back to 2006 I don't know the answer to this although I could guess how models were built an algorithm that they used before that the closet they "
    },
    {
        "start": 670.56,
        "text": "algorithms could classify only 75% of those images and when after this work was done they bumped up 10% well no I know that yeah it's even better but what was the underlying technical logical breakthrough that allowed that improvement oh that's a that's how they constructed the network and they all the rhythm that algorithms that they used how many how many middle layers were put in a in a network how they were interconnected in between themselves that was the where those I will so these results and I will mention them later so he has screen parsing sim parsing some results on that and the recent result on the image seems I already mentioned that they trying to parse the different items on the image so trying to classify where "
    },
    {
        "start": 732.75,
        "text": "is grass where are trees mountains versus skies that was a pretty you know heavy-duty task in computer vision for a long time and only recently only in past parts 18 years after 20 2012 they start to actually be able to classify that with a high accuracy so that was important achievement and as it goes to image net classification just recently just literally in this month in a paper published by Microsoft researchers they surpassed human level performance on image net classification so humans took human error rate on that data set image classification was around 5% and in that work researchers reported below 5% test error on image net image classification I can't imagine a human together further "
    },
    {
        "start": 794.97,
        "text": "than say this is not the sky this is the ocean something like that no must-do maker so that it's a whole bunch of images and you know they sometimes when you go through them through the hundreds of them it's there they're not all clear like this right and for example this might be might be classified I don't know is a bug or something right so humans do bloom occurs especially on a big big amounts of data so that was a another study that I did not include in slides where the guy from from Stanford University from computer science he was trying to be a deep learning model by Google so let Google achieved something like seven percent accuracy on image net data set ELISA fication and that the the guy just said through the through the image net and tried to classify images in a row as accurate as Google and he "
    },
    {
        "start": 855.82,
        "text": "beat that but for that to classify like ten thousand images so and and right now this model is performs even better than him non-human land human level accuracy on own image set so now we go to the something more interesting related to studies at this department deeply deep learning networks they become a standard method in connectomics and studying volumetric images from the brain and basically how do how they work in images like that they go we are sections of the 3d image 7 by 7 by 7 neighborhood and classify if it's membrane or non membrane that's the tasking in connectomics that's another work that have been done in that area and there is a visualization of the multi-label classification performed by deep "
    },
    {
        "start": 916.06,
        "text": "learning method also it it was used for for biomedical image segmentation so basically this was pixel label labeling with the convolutional neural networks that i will mention later and what it does it takes the window of pixels and produces a label for those pixels over for the central pixel related to the neighbor pixels and also they use the probabilistic graphical models to clean up the images to achieve the clear classification and that is shown on the bottom row another application that it can give an example one is the breast cancer cell mitosis detection so basically they were getting the histology images to 2d images and from those images that are classifying candidates four four four cell mitosis and using the deep learning network and they seen since 2011 they achieved the "
    },
    {
        "start": 979.089,
        "text": "highest accuracy in a field and it's also become pretty remarkable result and the standard method for for for that but not only it's not only used for image processing but also for example for studying splicing isoforms deep there is a publication in science just in the beginning of beginning of this year well the team of researchers used deep learning algorithms to reveal the new insights of splicing relations to different diseases they used it to to classify curly DNA elements with splicing levels in different human tissues so and and as you as you could see the deep learning relies heavily on features and for that feature extraction itself is pretty important pretty "
    },
    {
        "start": 1040.47,
        "text": "important process and one of the good properties of the deep deploring it that i already mentioned in the beginning that it builds the hierarchical representations of features so basically how it works just to give you some understanding is it takes an image as an input for example as it's basically 2d data with in densities in different channels and then on the first layer of the frame of the deep learning framework there is a some very simple features like edges or abstract you know representations and then as it goes through levels and it learns more and more something hierarchically constructed from the from the more simple features so basically it at the at the higher levels you have something more related to the picture than on a first level so that's the "
    },
    {
        "start": 1101.34,
        "text": "another example from the voice recognition project which is actually done by Hong frankly who is now with CFS Department with here at University of Michigan and that was another pretty famous paper on the face recognition date that it described how how this feature hierarchy builds up in levels of the convolutional neural network so basically in a nutshell it works like this for image recognition this features feature hierarchy builds up as from pixel to edge to text on motif part object for text it goes from character's word world group closed sentence story and for speech through the other entities that are specific for the speech processing so basically the representations of this data is performed why discovering and distilling those independent explanatory factors another another property of the deep "
    },
    {
        "start": 1166.14,
        "text": "Learning Network is that there is an assumption about the natural data it's called manifold hypothesis and the assumption is the natural data lives in low dimensional nonlinear manifolds and then task of the classification algorithm this would be just to separate a bunch of tangled manifolds for example so many fold is a hmm it's a it's like a topological entity I would say that preserves Euclidean properties in each in each locus in each local space but I'm not sure that's that helps a lot so I I would say that's in 2d that's a plane in 3d that's like a surface that you can think about it like this that's that's "
    },
    {
        "start": 1227.049,
        "text": "that's like a something that can be represented in in a few dimensions and if you if you take a look at this example for example if you think about image of the person person oh well about this of the person as a manifold and then you want to try to estimate how many dimensions does it have you can think about it that the face has three Cartesian cartesian coordinates and three Euler Euler angles Euler angles when when it rotates to different angles and they have about 50 muscles in the place so basically the manifold that that can be continuous described the face of the person would have these six dimensions and one dimension for each muscles we better can be you know deforming the deform in sort of the face that can be considered as a different "
    },
    {
        "start": 1288.94,
        "text": "dimension so that that's the maybe intuitive idea of the manifold so basically one of one example of the difficult classification task non non linear classification tasks would look like this when you have a data that classes of which are nap cannot be separated linearly and for that some works on only neural networks show that the neural networks starting from two and higher layers can actually deform deformed in the space in such a way that it becomes those classes become linearly separable so that's the that's the insight on on how deep learning works from the topology viewpoint and if you consider something even more complicated such as when you have one class completely surrounded by another class which would basically be a pretty "
    },
    {
        "start": 1350.53,
        "text": "big problem for a lot of classification algorithms deep learning with enough hidden layers can distinct of those manifolds in a in an increased space basically there are some there are some situations when even when depth of the of the deep learning method cannot completely resolve the problem for example when you have two classes that are sort of sort of like two thoracis connect angle to each other and then the representation and higher in higher dimensional space would look something like that but it's not still not linearly separable and those those connections like this they called links in topology and but still even even when the data have topology of the data have those links in itself deep and wide enough deep learning models you can show "
    },
    {
        "start": 1412.66,
        "text": "the pretty good classification result untangling them in in a such way there still be error rate because of because of the link here because of the connectedness here but but the results that but the ability of deep learning to do the distinctive I know Falls they chose better results than most of the other models and now I just want to go briefly through a couple of very important deep learning architectures so basically how those models are built one of them is I guess the most famous convolutional neural networks and most of those results that are achieved maximum accuracy on a lot of computer vision tasks they were they were achieved is using convolutional neural networks so basically it has a in south underline to a 200 line ideas one is the sparse connectivity that is in layers neurons are nap "
    },
    {
        "start": 1473.37,
        "text": "not all the neurons in a layer in the first layer connected to each neuron in a second layer but rather they they have these forests connections and another idea is that the the waits for the waits for the next layer their share it between units so basically you have you have that the weight vector of weights that are shared between different units in a network and basically how it works it goes through the patches of images or or or data two-dimensional data for example and and basically it applies some linear filter to the to the patches of data which is basically a convolution in signal processing and and goes through to the batches of data it in a bias term and then those those batches that are collected in a in a higher "
    },
    {
        "start": 1535.77,
        "text": "higher level layer in a in a feature map and you can see that the whole the whole architecture of the network would look like this so this is a famous network from 1998 developed by young Lagoon it's called Lynette and it has so basically that demonstrates how the convolutional neural network works it takes those batches from there from the input layer and construct the feature Maps using linear convolution filters and it after that it does down sampling in a feature maps and it repeats for the few times until the at the last level it reaches full equally connected layer of multi-layer perceptron the classical neural network that used for classification so basically that's the future maps that are those levels when "
    },
    {
        "start": 1596.039,
        "text": "the hierarchical representation some features are built and after after after some layers after after some iterations of those hierarchy building up its basically classified in the last in the last layer real modern commercial neural networks can get even more complicated and this is this is the structure of deep learning method from from the paper I already mentioned from 2012 that did a breakthrough in breakthrough in a computer vision and as you can see it's much more complicated and then the conceptual example that I show you before and I should mention that it's actually computationally pretty expensive to train models that are big and complex like that but modern advances in in harbours and algorithms allow it to do in days instead of weeks "
    },
    {
        "start": 1658.059,
        "text": "before so when when this group of researchers were developing this network they spent weeks on training this network on on few GPUs but now GPUs GPU farming and CUDA programming allows it to do much much faster and another one another very important diplomatic attack sure model architecture is deep belief Network and to introduce that I first start with just to remind what's the energy based models are and basically what the energy based model are they that's the models that'll learn energy function that should take low values on a data manifold on on input data and higher values everywhere else so basically I think this is the good example of sort of get an intuition "
    },
    {
        "start": 1718.96,
        "text": "what's the input manifold and everywhere else so these these dots are our input data in two dimensional space and what you want to do too to feed the energy function to them is to sort of make their values lower the Confucian coefficients for them lower and make the values for everywhere else higher so that's the idea of the model it's a based on the energy function now notice this the states book AI research and center for data science this is a big deal - then why you be aware of this right I mean that's huge yeah you know and you know some of these centers and this is a good example really making us in higher ed rethink the whole data science movement because there's so much money such a high art partnership that's going into these centers it's not clear "
    },
    {
        "start": 1779.35,
        "text": "what you know a normal university with their paltry 50 million can do so you know it's really important that you know we've been talking to those guys but I'm sure you know a lot more than I do about this yeah but you know it's important to see where this work that's being done you know what groups are involved and helping to bond and direct this work yeah so the Facebook thing yeah young Lee kun-hee was one of the pioneers of the deep learning in general and he was the professor at computer science at New York University and after after the important work that he contributed to the field they made him the chair of the Center for data science and he also has higher was hired by Facebook artificial intelligence research which is pretty important because on using all these advances of deep learning even such companies has Facebook who are basically "
    },
    {
        "start": 1841.509,
        "text": "for doing social network in the beginning they start to invest in the artificial intelligence research and that's that's the step ahead right and I think that's very important to keep track of their works that they do in the papers that they publish because a lot of papers these days that are published not by universities but Google or Microsoft or Facebook yeah a big trend I mean you know the big trends in computer science was you know yeah before the before 2000 you know one 9/11 ok there you know yeah the big thing was the dot bomb you know so everything was building up and then there was a big financial collapse because of the dot-com calm first and then computer science third ticket and then started rising up again and you know it was quite popular and then 9/11 hit and then actually computer science became more popular but most it was classified so a lot of it moved away from the you know academic sector but "
    },
    {
        "start": 1904.49,
        "text": "wasn't the academic sector in Alaska and then you know and then these things the new wave came up Facebook more Google etc etc and then you know it's amazing but if you look at the undergraduate say at University of Michigan for example I think 60% of the undergraduates are engineering college are declaring computer science as their major I mean they all want to you know go and you know have a 240 you know and then three billion dollar you know IPO you know after two years with three was a bunch of their buddies and then couple dogs you know and that kind of thing and it's real you know we're dealing with this right now and you know the other thing I think we're dealing with so so you know there's been kind of a cycle cyclic what I'm trying to say this is a cyclic thing going on in computer science we should be really aware of that you know well you know this is a great in fact there's more article "
    },
    {
        "start": 1965.59,
        "text": "impact in society very good I mean it's huge you know there's for example in January 19 January 30th of this year which was last month there was a whole our whole journal of science devoted to what this is doing to prize so great they can you know you know the Holy Grail has been reached and we have facial recognition you know we can you know frankly pull you know pull faces right off of anywhere we want airports various security Facebook whatever I mean it's a done deal so what's what's happening with privacy so for up so I was reading last night there's a great article you know I'll I'll send you the PDF after lunch just a little thing by my my friend Dan Egorov who's now at Sloan as a VP over there just looking at you know various levels of you know "
    },
    {
        "start": 2026.97,
        "text": "identifiability of data that's supposedly de-identify okay so in 2013 the state of New York published a de-identified anonymized list of all the taxi rides in the city of New York New York City every taxi right well you know there were some little weaknesses in there and you know the thing is you have your data over here which is really protected but then you bring in other data and you know the bottom line is they were able to figure out every taxi cab drivers income they were able to figure out the average fares they were able to figure out what tips were given by probably which celebrities that actually you know we're in the caps I mean you know whatever you know so the societal impact on all of us whether it's you know hackers giving in your credit card or your health record you know the idea of privacy is just literally out the window because it is "
    },
    {
        "start": 2087.429,
        "text": "real the question is what are we gonna do about it as a society I think that's what you were asked you know and we the you know kind of the you know passengers along the ride of this you know train you know we have our issues with the health fractures and the identifiability of you know norming sequences that are merged with other things which are the ultimate on identify are much better than fingerprints these are the tools and the methods that are getting us to this point in society where privacy doesn't exist that we are our system critical systems are vulnerable due to divided tax from the outside and so on and so forth we have to something about if we don't who will okay I'll send you the little article towards reading you take a look and I'll try to figure out Marcy maybe you can help me with it try to figure out how we can get more of the articles from that important science magazine made a little bit sounds good "
    },
    {
        "start": 2150.77,
        "text": "go ahead okay so I'm just gonna finish about energy functions just to remind you what where we stopped that you're trying to push the data down and everything else up and one of the models that is probably a lot of people familiar with is my maximum likelihood estimator so basically that's the same it makes the data data points bigger and and everything else smaller and when you do actually a minimization of negative log of maximum like of likelihood you're basically doing the same but it's just mirrored in an opposite way and then for for that for optimizing this problem is usually used gradient based methods something like gradient descent or something like that and these this idea lies in a in a base of restricted Boltzmann machines which are basically "
    },
    {
        "start": 2212.89,
        "text": "artificial neural networks that can learn probability distributions over over the set of input data and they are actually presented by inspire themselves if just a particular form of Markov random fields and they look like this they have one input layer and one hidden layer and they called restricted Boltzmann machine because there is a restriction that inputs in a hidden layer they are not interconnected between themselves but only to death to the input input level and for them in energy function would look like this depending on the parameters of this model and then you can calculate the conditional probabilities of getting the values for for hidden layer based given the input and opposite given the the outputs given the hidden layer and deep belief networks is basically that's the network which "
    },
    {
        "start": 2273.599,
        "text": "consists of stacked restrict Boltzmann machines when you have multiple layers of postman machines and then the calculations of the conditional probability becomes looks like this it's basically constructed from from these probabilities layer wise all right go back to then energy function threads that come you're not ok fast but not that okay so okay so now you have a two layer so explain exactly what the Boltzmann machine is so this you can refer to you know statistical physics if you want I mean you know Boltzmann did what now what what's the what's the critical idea I mean I think it's related to this other thing your maximum likelihood yeah I'm pretty sure it came from the idea which came from the statistical physics right of course yeah "
    },
    {
        "start": 2333.989,
        "text": "so what is the idea I'm just trying to get them to to minimize the energy function they're here okay so if you wanted it so that sounds more like Gibb so but whatever so um you want to enter so when you minimize the energy function what does that mean so so you're looking at the free energy you know so you're looking for some kind of local minimum and some kind of energy landscape yeah that's deads Dan orgy landscape and you want to to see this the inputs in a local minimum so you know when you slice it like like this and in between you can actually separate the data that you have from from everything else so basically you're you're looking at you're looking at the de minimum here for the for the data points that you have I see so if you have this multi layer that every there's some kind of and then there's the assumption I guess that do you have a relatively continuous function there's "
    },
    {
        "start": 2395.279,
        "text": "some kind of landscape in every layer and then you apply this Boltzmann machine and every layer yeah it's basically constructed from Boltzmann machines and that's layer by layer so you know so you have this layer thing and on every layer there's some kind of energy landscape and then you apply a Boltzmann machine to that layer which gives you a way to actually find the various local minimum and this in the you know you know kind of a contour topology of the energy landscape layer by layer yes so that's what actually you get out of this that's what it is and then all right so now you got that you use connected layers use the data from the previous layers so you have this feature representations that I mentioned before they build up one after another one as it goes for the first layer and further and so that because of that they can be trained in unsupervised way so basically they learn the the "
    },
    {
        "start": 2458.41,
        "text": "probability the conditional probability distribution over over those in over those nodes in a network and then they basically work as a feature detectors that's the hierarchy of features that I was talking before so the inputs kind of like one layer to the next through the layer so basically you have yeah yeah yeah those lines are coming so these are labeled a just the initial so that's that's what I mentioned before you can first you train the model and then you do the back propagation to update weights in later Eric tell you until emerge to something that looks like this yes I can understand so I guess it depends on how much sampling you have so "
    },
    {
        "start": 2518.65,
        "text": "you know I mean you know this is the Nyquist I mean you know you have to have a certain degree of complexity in your in your space to pull this off and have enough otherwise it wouldn't be too sparse well you know I mean you know I'm smarter people than me but I mean it's you know what I'm saying I mean you have to have a certain amount of data yeah in the system to begin with to you know slice it up that way and have any prayer to waste various energy landscapes is that right basically all the day the first goes to the first layer and after first layer is trained it pops up to the to the second layer so it's trained player wise basically okay and then you have a bit propagation that that updates the way it's in each layer okay so this thing here then you have a probability distribution function that's keyed up on these layers and then you you convolve "
    },
    {
        "start": 2580.45,
        "text": "all these then what does that other thing actually say well it's the right-hand side of the equation what is that telling us so basically this oops these probabilities that are they are learned through the truth if they learned feature representations from the input data so basically they show I have a picture something like that so when when this the whole machine works the deep belief Network works in in here in the input layer that was just just your data first something that you put in in there as an input okay it after going through the through the iterations through the machine those features that are you know edges that intensities they they become something more complex they build up the hierarchical representation because from here you have only pixels here you already build up edges and as "
    },
    {
        "start": 2642.1,
        "text": "it goes through the layers from edges to the some feature faces features of the face and to the faces somewhere at upper level hierarchically after after it goes through the through all these layers it represents features of inputs as complex complex representations that are built up correctly if it's clear starting at the top row from left to right each samples generated after 100 steps of block Gibbs Sam so basically starting from here and and and here it was those representations were taken out of the of the deep belief Network and represented this image and you can see how do feature faces are not very clear here in the beginning and how much more "
    },
    {
        "start": 2703.9,
        "text": "like face they look in the end so basically as the as the training goes they improve through the back propagation and through the retraining they improve the representation of those faces more accurate ok so this is a training thing so now your training so that you can actually do feature recognition over all this it is unsocialized learning yes you just provide the inputs and as it goes through layers it can learn these complex representations complex features so what's kind of the compute what what's the computer overhead for something like this I mean biggest system do you need to do this I guess it all depends on how many faces and how many inputs what kind of system you set up to do this depends how much data what you need to make they say they are doing "
    },
    {
        "start": 2767.86,
        "text": "better than human and you made it nice the image 9 was originally [Laughter] yeah I don't know first oh and I think that probably when it was labeled it was some kind of verification or agreement maybe they did you know that there was not one person who was labeling the data I'm not sure yeah maybe maybe that's not exactly correct to say that the human recognition level is set at that benchmark that that was 5% maybe that's not exactly correct because there is mislabeling in a data set and maybe the "
    },
    {
        "start": 2829.77,
        "text": "model actually recognizes better than then it was labeled in the beginning but they were very patient they googled every picture not sure though and so yeah that's the that's the result of the future representation that you can't get when you're training their deep belief Network and in the end when you when you have this features represented hierarchically you also can do a supervised classification on top of those representations that you learn from data and just to finish just to finish I'm including a lot of links to different results and one of them critic points that usually address the deep learning is there is no not enough theoretical background not enough explanations how it works inside how those probabilities are built and how the what sexual is going on in between "
    },
    {
        "start": 2892.65,
        "text": "hidden layers and I should say that recently in last five seven years there have been a lot of theoretical investigation in in that area and here I just give few examples of actually books written on a deep learning by by leaders of the area such as your show Bing Joe and it's hard to pronounce the last name Schmidt Maher and there is a really really nice class for everybody who is interested in in deep learning I would I would refer to that one is the class that each teach in Stanford this semester and it's about convolutional neural networks for visual recognition basically for for computer vision it's an interest very interesting class and they publish everything online they have a code on github and slides slides on their website also there are free tutorials for that that come sort of either for implement implement in those "
    },
    {
        "start": 2952.83,
        "text": "models or some of them also investigate some theoretical aspects of deep learning for example this is really nice tutorial that they liked a lot talks about statistical view of deep learning and represents deep learning as actually recursive generalized linear models so I would I would suggest to look at that too another nice moment about deep learning in progress in few recent years is that a lot of software that dr. Adam and was proprietary at some point was used for for companies needs a lot of debt software is getting different worse now so all these tools that I show in my slides were developed at some point in a different companies start up a mile torch was two torches and they're developed with a face book but all these tools there that getting open source so you can actually go there and not just use them as a black box but get inside and see what's actually going on in the source code and I'm not sure if I have "
    },
    {
        "start": 3018.1,
        "text": "very very early on we have these deep learning well you got to to kind of goals here one is classified tell me what this thing is the other is tell me why and can you get to the whys and things out of these yeah in other words can you extract useful information from for models for human understanding of what's going on you mean going on in beta yeah basically yeah for example those those feature of hierarchy sorry so the allot of times those are weird combinations of things right so you know here I can see it and I think this is a property of the Boltzmann machine but don't shoot me about here is something called a state function and thermodynamics and you know it's you know it simply means that you know you can you know the beginning and you know the end but you don't know the path and "
    },
    {
        "start": 3079.26,
        "text": "I think I think that these work like a statement I don't think you know the you know I think the thing that's been frustrating is this might be totally wrong but I think I think it might not be and I think the frustration is you don't know what's happening along with that you only know the beginning but I won't have another way to say what you just said I just put I'm I think so this is a very useful so it's called a transformation so if this is a Boltzmann machine you go back to the thermodynamics and the statistical mechanics and you think what is it Volta my machine and then you you know dust off your thermo and your statistical mechanics or you know and ultimately what I'm saying is it's a an analogy that's actually fairly strict so you know and you mentioned Gibbs samplers and so on the same thing I mean you know they're only state functions they don't give you the path and so you know in what folks want there's something that's why I'm asking about the black box I "
    },
    {
        "start": 3141.18,
        "text": "mean so I was kind of excited to learn about these landscapes but then I was trying to understand the landscapes you know the various levels do they map anything that you can do they represent anything that your mind can comprehend well but that's the answer is no that's basically simple logistic regression it's to estimate the part when you estimate parameters with maximum likelihood it's doing the same task and I mean from the same point you can say that you don't know what's actually going on a logistic regression but you know for all the city good it might not be possible with big data and all this kind of stuff to actually what's going I mean you know we might have to be yeah and the fact that you get outputs that are working so well then you have to say doesn't really matter I mean you know then you have to really think about what we're doing here "
    },
    {
        "start": 3201.94,
        "text": "yeah if the damn thing works with such good fidelity and then you can show how it works you know and prove it to yourself but you better go back to some of what you honest talking about in terms of you know the original training you know where the origin of some of the stuff with regard to train all this where is this all coming from okay yeah no it's a good discussion I mean I'm just trying to get a discussion going I think we're talking about because you know here you know surprise surprise so what are you showing here so this is a simple demo just so this is kind of like it creators and character recognition so yeah that's that's the representations that that model learns level by level that's the input that you give and and it's actually it's not just video game or something it's that real network working in browser because it's written in JavaScript so it's possible to train not two big networks right in the "
    },
    {
        "start": 3264.009,
        "text": "browser and here is the classification result on a test set and you can see you can see that for handwritten recognition even not very big neural net deep neural network can recognize fairly well written digits Oh from the basically from the beginning just it's it's really quick it's it's a weird contract which actually led to the there's a company in town called noble dynamics you know I think third street so first they did that so that was fun I had a group way back when in the day is it Ihram yes you know so now it's basically red but they gonna I mean we did this we had a contract with the post office department years ago and my group red envelopes in a secret room boy back down in here and we figured out how to read the address top of envelopes and then somebody else got the bright idea if we could read "
    },
    {
        "start": 3324.88,
        "text": "anvil opes maybe we could read things that were written in Arabic right then we started to read Arabic pretty well and then that turned into an industry and then they spun off they ran away they tired of me they created a company which became novo dynamics and they were working on using some of these methods in the early days you know precursor it was pretty primitive but you know this is like 20 years 25 years ago yeah Alex you might remember some of us write very well okay so we don't now take a look I mean there's no arguing of success I mean it was really you know we you know it was really pretty stunning and spectacular time why because those are just handwritten maybe they're pretty ugly it's it's hard to deal with that but it looks like it deals with it very well look at those twos that's a standard "
    },
    {
        "start": 3385.17,
        "text": "yeah you know I'm saying instead of inclusion the idea would be if it is working for pedestrian detection or 3d in natural image analysis why it cannot work for medical image analysis right there obviously there are different challenges that are present in biomedical image analysis but there I think the potential of deep learning models it's not completely discovered yet in in this area in this field I was saying you know but who said that humans were that good see this is I think part of the thing with the big data in all this you know there is no way that our mind can comprehend so we like linear thinking right so we use the Fourier transform works pretty well for us I mean you can you know from beginning to end but you know this is nonlinear stuff our mind goes terrible with it the questions become so complex so if it's a "
    },
    {
        "start": 3446.02,
        "text": "radiology image or some histology image so we had some thing down and I haven't told you about it you know when I was Hopkins so we had some kind of 2d classifier on the 2d images you know of these police and graded specimens coming from the pathology lab but we had the data you know that's the patient you know get prostate cancer quickly it's all retrospective or not you know that's the question well you know with 89% fidelity by Beltre name you know was able to to be you know some things in the Gleason graded histology slides that the pathology could pathologist simply couldn't see so this model you know and you know with 89% fidelity found the guys that progress rapidly to prostate cancer versus those adult I mean that's the holy grail it's in the histology it was always there our machine learning can see it the pathology scan the point "
    },
    {
        "start": 3506.23,
        "text": "is that there's a lot of things in the data that the human will never see and that our mathematics and our models you know are you know so primitive now you're bringing you have more data more complexity the point is that there will be the whole promises there's lots of stuff in the data that we can't see we don't know how we can just see kind of the outputs that we recognize in a glyph that we know like this I'm sure it's true and so the answer is absolutely there's all kinds of stuff in medical imaging that humans and pathologists and radiologists cannot see that machine learning or now go back to your title and let's see if it hangs what's your title oh yeah go back to the first slot [Music] "
    },
    {
        "start": 3566.25,
        "text": "oh no processing well you know why don't you explain yourself on this little bit I mean I can kind of wrap my head against yeah right I have a very simple explanation I actually borrowed this title last year it was panel discussion with a machine learning experts in the in in the world world class machine learning experts and they have a panel discussion titled like this so I sort of borrowed it from there to get more attention and work but you know I think you know I have a separate question yeah well I think it's the evolution of machine learning that one legal processing is another subject I mean you know in there is a form of signal processing related image and there is a theoretical investigations relating actually deep learning "
    },
    {
        "start": 3627.24,
        "text": "explaining those mechanisms from the signal processing point of view that's I think a better title for this might be and a machine learning question signal processing is a completely different field and we all we didn't scratch the surface of it then it stopped so it's a little bit too broad Alex that's my opinion yeah I'm just curious you know sign up there where you list some open source packages have you tried any of them have you tried any of them on we tried any of them on the things that we care about in the lab that we're like tripping over like I'm making an idiot of ourselves it's it's for me personally the hardest part would be is to take the data that we have and use it for in that software because like image representations that we have that very specific those that that software potentially can be used on that but you have to have those "
    },
    {
        "start": 3688.75,
        "text": "representations how you go through the layers of the stack and I am not exactly sure how how we one start because these packages they can work it like numeric data for example some of them has some common image adapters but if you want to use any of this software on a 3d image imaging from the confocal microscope then you have to have this data pretty formatting and right representation so we should look at that in the lab meeting you know and have it you know let's actually look at these tomorrow at the lab and I think one other comment about facial recognition I mean I don't get the 50 boxes I mean you know I can see the tip of the nose the corners of the lips something about your ears it's called landmarks that's I think driving the facial recognition the muscles you can hardly discern so think about another change you have relationships between different parts yep so if you get a person if somebody's "
    },
    {
        "start": 3750.43,
        "text": "shouting they look very different I think they do know by the way so it's not really modeling the muscles but it's got to account for the fact that all these things to change well I think that's a very perceptive I agree with you on that I'm 100% and then that's really something that's very cool about all of this and it's kind of you know capturing more dynamic information it's part of the you know because you're dealing with snapshots the one is the end of the day you know this is applying to movie they're running this on movies and everything as well like YouTube videos yeah we wrote me first on a smiling if it's trying to shouting Crichton right during the video not just taking yeah right and you know I could imagine if you had surveillance cameras it's not just getting snapshots but it's watching somebody go through the line you know like when they ask you the question they "
    },
    {
        "start": 3810.79,
        "text": "say thank you mr. right you know and you're going through security and like you know I imagine if you're mr. right you have a certain facial expression if you're mr. wrong you have a different facial capture "
    }
]