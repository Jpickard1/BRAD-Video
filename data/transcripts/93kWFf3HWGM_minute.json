[
    {
        "start": 0.42,
        "text": "hi everyone so my name is this Warrior and today I'm going to give a talk on understanding bad publication in pytorch and before I begin I uh I think that there will be some people in this room who's very familiar with the topic and there will be some people here who are kind of here for the free pizza so uh I'm going to try to summarize the the main points for for uh just just some main takeaways so that even if you're not really well versed in this area you can you can still learn something from this presentation now also it's okay to be here okay so uh first of all what is python so pytorch is a python package that's used for deep learning so high storage and tensorflow are kind of the most popular packages but I in my opinion I "
    },
    {
        "start": 60.899,
        "text": "think pytorch is most popular for building customizable deep Networks and it's free and open source um yeah so today I'm going to uh first I'm going to start with kind of a set of prerequisites that I expect for the audience to kind of know about neural networks and and optimization and then we'll talk about uh back propagation and what it is and then we'll talk about how we Implement back propagation in pi torch and then we'll kind of go very lightly into how it's implemented algorithmically and take a little bit of a peek into the source code of pytorch so let's start with uh kind of the pre-requisites for kind of what I might expect the audience to know about neural networks to begin with so um first of all uh hopefully we all kind of know what a neural network looks like so it will be made up of an input layer and an output layer and then there's any "
    },
    {
        "start": 121.68,
        "text": "number of hidden layers in in the middle which can consist of any number of neurons and this is just your feed forward fully connected Network there's no uh extra bells and whistles to this network and so we expect that basically every uh value in the input layer should go to Every value and every neuron in the first hidden layer like this and in the in each neuron in the human layer we should perform some type of linear function W X plus b and yes it's in red and then we try to wrap it sometimes oftentimes into a non-linear activation like a sigmoid or a teenage or a rectified linear Union and that's really the key to neural networks because we already had linear equations for a very long time linear models and we even had non-linear models but a neural network is able to to create these really complex non-linear models which can really fit our data really well so "
    },
    {
        "start": 181.98,
        "text": "that's why we like them um so after we wrap it in the activation we send all the outputs from the first hidden layer into the as inputs into the second human layer and so on and so forth we calculate this linear equation we wrap it in a non-linear activation and then it gets sent to the output layer where some final calculation will be made and then we should get either like a regression value or we might predict a binary class or a some type of multi-class output so how do we measure the performance of our model so we want to know if our model is doing well or not doing well so we can use loss a loss function or a cost function to do this for us and you might read some computer science papers and you see that the loss functions are really complicated and confusing but at the core of uh pretty much all the loss functions in a computer science paper it "
    },
    {
        "start": 242.22,
        "text": "will be made up of one of these two loss functions either a mean squared error or a cross-entropy loss you know highlight means great error because even if you worked with linear regressions you've seen a mean square error basically you're you're taking your predicted label from the network side to shaky so your predicted label from the network and you're subtracting it from your ground truth label and then we Square it because we want to make it differentiable and so that's how we can measure the performance of our model but it turns out that loss function is important for more than that it's important for optimizing our Network and so if we have this network of a bunch of Weights in our neurons to perform this linear functions uh we want to be able to get the the exact values of the weights which can correspond to the lowest possible loss so this is uh completely an optimization problem and so in a simple example in a simple example where we have only two weights and we're using the mean squared error "
    },
    {
        "start": 302.88,
        "text": "we can actually graph out all the possible values of our loss function and we can see which values of waves out of all possible values would result in the lowest possible loss and so this is what we're doing in optimization but uh but we know that neural networks are much more complicated than that we can't just look at this graph or we can't just look at a graph and see where the lowest point is and we have we have these really highly non-linear functions where there's going to be more than one minimum and so we want to we need to do a more sophisticated process for that and so we might do something like gradient descent also known as steepest descent and there are two core uh steps in a gradient descent algorithm we want to one find the best Direction and defining the best Direction means calculating a derivative or a "
    },
    {
        "start": 363.66,
        "text": "gradient and we want basically the direction that's going to show us which which way should we lower our weights in in our neural network that will result in a lower loss and then we want to move a small step in that direction so that's that's this Alpha w i for each wave so um find the best Direction update the weights and bed propagation comes into into our talk because it turns out that using this method called back propagation or backwards propagation tends to be the best way to to do this optimization process and it means that we basically start in the back of the network and then we find the best direction we update the weights and then we work our way up so for infrastructure update the weights and do the same thing okay so that's optimization and web application so if we could actually view "
    },
    {
        "start": 425.22,
        "text": "what uh gradient descent is doing um this is this would be in a situation where we have only two weights because we could view it in three dimensions um it would look something like this it would be slowly the values of the weights that we choose would slowly inch their way along until it reached the lowest possible value for the last function a small technicality is that because the the function is so non-linear there are so many different Minima that you could fall into and we call those local Minima and the the global minimum which is the lowest lowest possible value for our loss function sometimes we never find that but we just want to get the weights to a point that uh the the model does good enough for us so so any local Minima that does good enough is really what we're looking for okay so we talked about gradient descent and we talked about how we the first most important step is to find the best Direction so finding the best Direction "
    },
    {
        "start": 486.9,
        "text": "always means you want to find the derivative of the cost function with respect to each weight in the network and I'm going to show some some confusing notation but there are two main takeaways I want you to learn from this the first one is that it's not a mystery it's not a black box we know like we know how to do this you can do this on paper you can write it all out you can calculate the derivatives yourself um and I mean you can use these equations here um but the second point is that you don't want to do that because if you did that you might be very likely to first of all mess up anytime you add a neuron into your network or you update your loss function in any way you're going to recalculate that all over again and also you can um you can imagine that in modern day neural networks you might even have billions of different weights in your network and you don't really want to be calculating this for a billion uh "
    },
    {
        "start": 547.019,
        "text": "different weights so just know that's possible but you don't really want to do it and so that's why it's good that pytorch does it for us but let's just take a quick look at edit so um basically the the main Hallmark of being able to calculate these partial derivatives is using the chain Rule and hopefully everyone here knows what the chain rule is but basically it's a special trick that you can use when you're calculating derivatives if you have a set of nested functions and if you look very shaky if you look at the the end of the neural network you can see that basically the whole network is just a set of nested functions and so that's why we can really take advantage of the change Rule and because it's a set of nested functions you might also see oh that's also why back propagation makes sense because you'll have to start at the back um and kind of work our way forward through the nested functions so uh someone was able to actually "
    },
    {
        "start": 609.36,
        "text": "summarize all of the personal derivatives you need to take with four different functions I'm not going to like go through it all in depth I just want you to see that it's possible so this is calculating the pressure derivative of the cost function with each weight all you would need is the activation of the previous layer and then you would multiply it times this Delta function which consists of Weights in layers that are closer in this direction if we call Upstream um until you get to the last layer and then you can also calculate there is an equation for calculating the the cost function in with respect to the biases as well but many takeaways you can do it by hand but you don't want to and you use the chain rule okay so how do we implement this in pytorch using some code so let's first take a look at neural networks because everyone likes that um so first thing we need to do is create a network architecture and so we "
    },
    {
        "start": 671.339,
        "text": "always create this network object um using a class and we Define all of our layers in the initializer so this is this is where we kind of uh define whether we're going to use linear layers or convolutional layers Etc and then we have a forward method and this defines the forward pass so we defined our layers here but we need to Define how the input is going to go into each of these layers and so that's what the forward forward method is for and then we have the training step so the first thing we're going to do is we're going to call up our model so we're going to create a model instance here and then we are going to Define our Optimizer and so here we're using stochastic gradient descent and we're going to optimize the parameters of the model and then we can Define the learning rate there's other bells and "
    },
    {
        "start": 731.579,
        "text": "whistles you can add to the optimizer but I don't put them here and then we have our actual training Loop and so we iterate through epochs first thing we need to do is we need to set model.train to true and uh basically what this is is we're telling uh pytorch that we are ready to train the model and this is mostly important if you have things like batch normalization or Dropout in your model because they kind of depend on knowing that your model is training if you're trying to evaluate your uh validation set your or your test set um you would set model.train to false or you would set model.eball and then we're gonna iterate through all of the batches so oftentimes we use what's called a data loader and the data loader helps us to organize our data our input data into batches for the model and then we're going to get our model predictions in this step here oh sorry "
    },
    {
        "start": 791.7,
        "text": "for people over here and then we're going to calculate the loss and so uh Pi charge has its own built-in functions like mean squared air and cross entropy loss but you can also Define your own and then the most important step uh we're going to back propagate and these in these two steps so we're going to call last stop backward and what that's going to do is it's going to calculate all of the um all the partial derivatives of the cost function with respect to all the model parameters here and then we're going to call optimizer.step and that's that's the step that's going to uh move the weights in the direction that we just calculated here and then if you do this right you should hopefully see your loss decreasing and because um in this example we use the cat the gradient scent you might see it looks a little bumpy your your loss sometimes might increase a little bit but overall the trend should decrease "
    },
    {
        "start": 852.3,
        "text": "so another method that I want to kind of talk about is a torsion tensor that requires red so uh every so the model parameters are set in a data type called torch.hinser and that data type also stores a an attribute called red and red is where it stores the the gradients basically and sometimes we don't want to store the gradients sometimes it's just a it's just using up memory but we don't want to back propagate through those values and one example of that is if I'm using a set of pre-trained weights and I don't want to update them I just want to freeze the network so then I can use this method called requires grad so if my my torch tensor is called X I just say x dot requires grad false and then it would it just won't store anything in that red attribute uh there's another a couple methods I want to kind of distinguish so we kind "
    },
    {
        "start": 913.86,
        "text": "of lost that backwards this is using torch.autobred we call it lost out backward but you can also call lost that bread and the difference uh is just that with that backward what you're doing is you are calculating the gradients and then you're storing them into that grad attribute but with red you're just going to calculate gradients and it's going to return the gradients uh to you as output so let's do one more example of using torched out autobred um you don't need to use it in a neural network you could just do it to calculate a derivative for you like let's say you you don't have Wolfram Alpha or you have access to it or you just want to be cool and use this in pytorch so you can totally do that um here we're setting we're giving a concrete value to X which is 3.6 we're setting our function to the x squared x times x and then we are applying uh our function y y dot backward and then we're "
    },
    {
        "start": 974.699,
        "text": "going to take a look at what was saved in the grad attribute and it turns out that would save this 7.2 which is equal to 2 times uh X which is equal equal to the derivative of y so this is how we use it to calculate derivatives all right so let's let's say talk about how that propagation is implemented algorithmically and before I begin I want to thank Justin Johnson who is a professor in ex he has a lot of content on pytorch and that's how I learned some of this stuff from him and I I borrowed some slide content from um okay yeah so in order to understand how we implement this in pi torch we should understand uh what computational graphs are and how to use them so a computational graph is a way that we can kind of graphically represent some equation and it it helps us to firstly "
    },
    {
        "start": 1035.12,
        "text": "structure our equations more clearly compared to writing it out on paper but it will also allow us to use that propagation to leverage gradients and it has this cool kind of trick where we can calculate gradients for all the variables using only what are called local radians so first let me explain here here's one example function it's F of these independent variables X Y and Z and our function is X Plus y times e so this is a tag or a direct a secret graph and we can see X Plus y the arrows times Z equals m so this is a computational graph representation of our function f so let's run through some example of how we would use a computational graph so I'm going to set something some example input um through our forward pass here so I'm going to set x equals to 3 y equals 4 "
    },
    {
        "start": 1096.26,
        "text": "and Z equals two so so I'm setting values into our function and I'm also going to define a kind of an intermediate variable Q which is going to equal X Plus Y and that equals seven because three plus one equals seven okay so we're gonna we're gonna back propagate through this computational graph we're going to start at the end because that's what we do um so we're going to take the partial derivative of f with respect to F so that should be equal to one right because it's do everything with respect to itself now we're going to move back one of the oops we're going to move back one of the nodes here to look at Q and we're going to take the partial derivative of f with respect to Q so f equals Q times Z because we defined Q as X Plus y so Q times e so the derivative of f with respect to Q should be Z right "
    },
    {
        "start": 1157.88,
        "text": "so Z equals negative two so the derivative here is going to be negative two we're going to go oh here's a good time to also add some um some definitions for you so we basically call so if we're if we're looking we're situating ourselves at this um part of the Brad Q we can call any gradients which are in the direction towards the the output layer as being upstream gradients and any gradients which are in the direction towards the input layer are going to be called Downstream gradients okay so now we're gonna do this note here the partial derivative of f with respect to X and you might be tempting to if you know if you're already thinking this in your head to just um take the uh partial derivative using the whole expression of f but we're not going to do that we're going to use a chain Rule and we know that partial derivative of f with respect to X is "
    },
    {
        "start": 1218.84,
        "text": "going to equal f with respect to Q times Q with respect to X that's then yeah so some cup one exercise for you um and we already calculated partial derivative of f with respect to Q um it's negative two so all we need to do is give Q with respect to X and that's going to equal one right so we just say negative 2 times 1 equals negative two and then we can do we basically use the same principle apply it to the Y um same exact chain rule and then lastly we're going to look at Z so uh f with respect to Z so we know that f equals Q times e right and uh so the derivative of f with respect to Z is going to be just Q so that's what we put here so um notice that when we were kind of back propagating through this graph we we "
    },
    {
        "start": 1281.539,
        "text": "only need to we only really uh need to think about uh nodes which are the values of nodes which are adjacent to the node that we were looking at or the node which is directly uh Upstream of our node we never actually have to go any further than that and that's what we call this that's why we call this area the local gradient so so any node which is adjacent to or directly Upstream of the node we're looking at what we call our local gradient and that's really the advantage of using computational graphs um when we're back propagating is that we don't need to worry about calculating a derivative over the overall expression we only need to look in this local area to calculate our gradient and so that's that's the main takeaway like even if you didn't follow everything about the computational graph the main takeaway is that um we can really leverage these local gradients so that we don't have to do all these calculations "
    },
    {
        "start": 1342.5,
        "text": "um on based on the the overall expression and secondly even though I showed you this simple example all of the more complicated examples I can show you work in the exact same way and so so here's another graph that we can look at but but um what I want to say now is that you now that we know how this uh works now that we've kind of taken a look at both the forward pass and the backward pass uh we can write this up in code and we can define a function which gives us our forward pass and then we can write up a function which gives us our backward test and if you become you know really good at this you don't really even have to do any math anymore all you need to do is you just kind of understand what the operator what what's the kind of trick at the operator um which which gradient are we trying to take or what calculation do we need to make and then we can just uh use that in our background pass "
    },
    {
        "start": 1404.059,
        "text": "um so I I put this box here to show okay this is this part of the graph here's here's where it's represented in the forward pass through the words represented in the back road test um yeah so so basically after a while you don't even need to use paper at all you don't need to do any type of calculations however uh once we start to scale this up to tens or hundreds or thousands or or even millions or billions of parameters you don't really want to be doing this by hand no matter how easy it is to do so thank goodness that Thai torch does that for us and they do that by since since we know that the behavior of the operator to of um calculating this this forward pass and this backward pass um are are predictable we can Define the behavior at that operator and so if we go into um this is the old high torch code for the sigmoid operator it can see code for "
    },
    {
        "start": 1465.74,
        "text": "the forward pass and we can see a section of code for the backward test and so this is kind of an algorithmic or did I say this is um some kind of a shorthand for how to calculate the derivative of a single one function so so you can just Define the behavior at each operator and this is the so I actually tried to find this code but it's it's the old code and they they rewrote it but it's more clear to see where the forward passes where the backward passes um so you can see how they Define that and you will see if you look in the code that they they have definitions for all of the operators that you can use in pi torch this is their old code and if you want which I have a link to if you're if you're interested um but I also have a link to the new code um where they Define all of the operators as well okay so a section summary computational graphs so um by George real-time gradients are made possible by leveraging these "
    },
    {
        "start": 1527.059,
        "text": "computational breaths so we had a look at what like how to use a computational graph what it is and then we also uh saw how that what the benefits are of using computational graphs they allow us to back propagate by only focusing on these local gradients rather than calculating partial derivatives over the entire loss expression and it's kind of this algorithmic trick so that we don't have to um do a bunch of math on paper we can just look at this over a short area so and the other advantage of that is that it's able to calculate gradients in real time so anytime that I'm going to modify my network like add some neurons add some layers um or change my loss function it doesn't have to do a whole lot of extra preparation to calculate the gradients because it's just always going to follow along whatever breath you've made so yeah presentation summary so we talked about back propagation and optimization so back propagation was a "
    },
    {
        "start": 1588.62,
        "text": "method for or algorithm used for finding gradients in the network we remember that gradients help us find a direction to to travel down to reduce our loss so um that propagation consists of calculating partial derivatives at the back of the network first and then moving forward we looked at code implementation of pytorch so we saw okay this is how you set up a network this is how you go through the training Loop this is lost out backwards and and Optimizer does that and then uh the algorithm implementation so we saw we looked a little carefully at um how pytorch uses computational graphs and it allows this real-time calculation based on local gradients and we looked a little bit at the source code and saw how they Define these operators which help us build back coverage and so I'm just going to end this presentation with some additional resources in case you didn't weren't able to catch all of that and I'm gonna make a Shameless plug for my blog so so "
    },
    {
        "start": 1650.179,
        "text": "one thing I I enjoy is I want to read some deep learning papers and I like to kind of just share what I've learned and so I I put this whole presentation into a two-part series there so you could take a look and and if you don't want to you there's a whole bunch of other good resources here So yeah thank you are there any questions about anything yeah if you're training a complex neural network can you use impact propagation my understanding is that you will often end up in a local minimum so does that mean we should run the training process many times to try to get to the you know the lowest local minimum we can find or how do you address that problem uh you can do that but hopefully so so I I kind of skipped over at how there's other bells and whistles to the "
    },
    {
        "start": 1710.48,
        "text": "optimizer but there are kind of these other tricks that people do to try to get out of local minimum so over here so we had things like momentum which is supposed to kind of like if you're stuck in a local minimum it's supposed to kind of push you out a little bit um uh yeah they're they're or for example we can use things like schedulers or wake Decay which can help us to reduce our learning rate so that would that would help us so that maybe in the beginning we're jumping over big local Minima and then once we kind of reach a good spot and we don't want to jump over anything we start to reduce our learning rate more and more and more and more so that we kind of reach the best minimum we can we can find so there are some tricks for that but you you can you can rerun it um but but usually I I think uh if you're finding that you you had to rerun your network to get uh like a much better performance then you probably didn't make your network very good so probably you'd have to restructure it "
    },
    {
        "start": 1774.62,
        "text": "yeah there's an online question from Hui who asked how are non-differential functions such as relu handbook yeah so um in optimization so like actually for functions like relu there is something called uh it's called like a sub differential or something or basically um even though technically there's no it you can't calculate a derivative at that point there's actually like a whole set of derivative of possible choices of derivatives that I could make if you think about the tangent that that would form at that at that elbow there's a whole bunch of lines there that won't touch the function and so that that whole set is actually a possible derivative and so usually at that point um in the code for the operator they were probably just like pick one they'll pick one of the the possible set of derivatives "
    }
]