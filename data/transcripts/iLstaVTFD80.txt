so you can't believe there'll be no seminar as it is the research computing I'm symposium next Thursday so you're all encouraged to go to that and then we'll meet up again the following at Thursday and there is a sign in to it we'll pass that around if you get please sign in it helps for the pizza and just so that we know who is attending and with that I'll go ahead and turn it over to our speaker today we're advised EXCI is gonna talk to us about Isuprel thanks Marcy for introduction and so today I'll talk about this paper that is published a while back I believe it's a useful tool for many of you in your studies so it's called izip read it's thematically differentiating function functions for all kinds of fantastic data so this is the outline of my talk so I'll first give some introduction and talk about why is it important to do functional prediction studies and methods and the data and I'll introduce multiple instance learning that we use then I'll show the results and we that we did and summary and also plus I will show the web so the accurate annotation of protein function is a is a key topic in in biology or understanding of diseases and molecular processes and experimental approaches for functional annotations such as like knockout or knockdown experiments are not feasible for genome-wide studies so that's why there is the whole bunch of computational function prediction studies but most of these studies are done on gene level so for a specific function usually the first define mind by these gene ontology terms since that are already known to have this function are all that positives and the rest as negatives and you have a machine learner you construct a model by using some kind of genomic features and you do your functional prediction but the major limitation of dia this kind of function prediction studies is that a gene is considered a single element but we know that it can be spliced into different transcripts resulting in proteins carrying maybe carrying different functions so there are multiple ways of multiple mechanisms of alternative splicing and each can produce a different protein see different mRNAs different protein sequence from a single gene so if you look at the numbers in current annotation packages so if this is from gen code there are around 57 thousand genes and close to 200,000 transcripts for those genes if you look at the protein coding genes around 20,000 genes and 80,000 transcripts so that means on average there are four transcripts theurgy so when you do your functional studies on gene gene level you're missing out this another level of our resolution so it's estimated that 95% of multi exon genes under the alternative splicing so these numbers are not the complete number so there are a lot more transcripts to be discovered and the splicing plays a role in post transcriptional gene regulation or it can alter the protein sequences and domains can modulate regulatory elements so it can have functional consequences and so here I have four examples of transcripts with from the same gym with different functions so this key rpm 3 gene which encodes a cation selectively channels in human and it has two variants targeting different ions and the Rays of b c LX gene one of the anti popped attic and the other one is products opposite functions from the same gene and same for just three L and K has three s variants also same 400s are two G so these two transcripts on the same gene can have different or even opposite functions so attempts to capture this kind of functional differences of transcripts are limited to low throughput experiments and also some computational predictions to Road domains or binding region or binding sites but one challenge facing the experimental studies are that generally isoform specific antibodies are not available so you cannot do western blood or chip shape so they don't they don't return the provide resolution at the either for now also eyes upon specific all of your nucleotides are difficult to design for qPCR or RNA interference because you have very small changes between the eyes or function so also for computational studies that are doing domains are limited in a sense that only 34% of the eyes affirm pairs of the same gene differs in terms of domains so the remaining are exactly the have same set of domains so you cannot feel a differentiate so in this study we are trying to overcome some of these difficulties so the first limitations that we face are the two limitations that we faced our first the functional genomic data are generally available for gene level at the gene level such as microarray data set most of the protein-protein interaction data sets and the other limitation is that functional annotations are centered on genes so in gene ontology or in CAG you you see the annotations based on genes so we'll try to overcome these limitations with our approach so so we use our Nasik data because it can give transcript resolution data so we use publicly available mouse or Nasik experiments from NCBI SSRI database so as of November 2012 there were on 19 top 1,900 experiments it's increasing day by day but the time we did we started with this many experiments and this includes a lot of different tissues different experimental conditions Neilsen includes some part of the encoder Nasik data and we process them using tuxedo said after trying out different tools we decided to use top hat cufflinks pipeline to get the abundance estimates for transcripts and we used ncbi's annotation package which has 20,000 genes and 30,000 transcripts so we did some quality control for the urn ASIC they ended up using 365 experiments in this study so this was the alayka first so we have we now have functional genomic data on transcript level so the second limitation is the gold standards so for that to overcome this we used our multiple instance learning so this is a kind of a semi-supervised learning approach that has been introduced in 90s I guess so in this one instead of Jeanne's as a single point in your classification we consider Jesus as a bag of transcripts so here I have this is a regular let's say a classifier you have a separating hyperplane and you are trying to maximize the distance between this point and this separating hyperplane so here each dot represents a gene for previous functional studies in our study we have individual transcripts that are labeled by their genes instead of having label for each point we have a label for each bag collection of instances so the the idea is to extract the common patterns of isoforms across positive genes then use these patterns to find which subset of this positive genes positive transcripts are actually positive and then use this to do it with classification so there are two assumptions for multiple instance learning so if a gene is positive or a bag is positive at least one of its isoform will be positive so if you know that the gene is a bag is positive here and at least one of its either function be positive and also if a gene is negative then all of it's either from combinations so there is no ambiguity about the negatives which subset of isoforms of a positive gene will be positive that is the majority we're trying to solve just a toy example you have two positive genes positive means his gene is annotated a specific function so let's say gene a and Team B are both positive and this for this one has three eyes of the three transcripts this one has two transcripts so there are 14 possible assignments of labels to these genes so multiple instance learning tries to find the optimum assignment in this community real estate so this is the kind of overall pipeline so we have an initial assignment of eyes upon function pair it's just in the beginning we assign every transcript of a positive genus positive so that's our initial assignment so then we built our classifier based on that and reclassify our input set and then we have a new set of either new set of labels and then we refine this new set to be consistent with these assumptions and then we really really classify so this iterates until there is no label change and then this will converge at at at one point and we will have the optimum labeling of transcripts after this step and then we can go ahead and build our final classifier to get the probability that either firm is annotated the function or probable that a genus annotated dysfunction and either form abundance estimates are our input features so there are different formulations of multiple instance learning the first approach is to let me show you in this figure so this is a positive gene with multiple transcript so one of the approaches just choosing one of the ER transcripts as witness and discard the others so instead of having this many transcript you will only have one positive that is representative of that gene and the other approach is to have a threshold and have the have a group of transcripts as positive and the rest as negative so you can choose different threshold resulting in different ratios of positives to negatives so we ended up choosing like a mild approach so the is the most strict version you only have one eyes upon left and this is kind of less satiric version you have a lot of so we ended up choosing some where middle approach because it was giving best performance based on cross-validation so after that we're on our predictions and we have several lines of validations including computational validations and some specific examples of genes so for all of these validations we tested around 1800 biological terms with 20 to 300 genes annotated to death so for the terms that have less than 20 they're too specific to do any prediction or for the ones that have more than 300 genes annotated them are too broad to be included in this so and also we regrouped quarter and go terms in based on their sizes based on their number of genes annotated and we reported the you see you PRC and your other matrix 2 to show the performance so the first one is cross-validation so in this one we have partitioned the genes into two groups one of one test set held out and training set we build the model based on training and test the holdout set these are the AUC values for five different groups of columns so yeah you see is around point 67 68 which is comparable to previous different gene function prediction studies that use multiple data sources so we achieve this you see just by using expression data and by using this small process learning and there's a also a trend that yeah bigger goal terms are easier to predict Andy a smaller tab we can see the same trend for a new prc2 so that was the cross-validation but in that in this figure the performance is the mix between single isoform genes and multi eyes upon Jesus so since our framework is kind of making a difference for multi the from genes we wanted to see if you have like a performance gain between multi so from genes and single Iser from genes by using this because for single Iser fermions there's no ambiguity it's just same as before this regular supervised classification so we expect to see a better performance for multi the from genes because as I said the list of transcripts are not complete so most of the single isoform genes are not really single either from yes but they're the ones that haven't been I'd the other transcripts haven't been identified yet so that will cause some performance drop for this single isoform genes if we look at the performance you CNA you PRC for all go turn groups there is a consistent performance gain in performance gain by multi so from genes compared to single Isaac objects that was another like line of validation for us and that showed us that multiple instance learning does bring some disturbing and we also tested here algorithm with respect to two with two cases so first year we compared estimates of performance of algorithm on the genes that are highly expressed versus on the genes that are lowly expressed because we know that this RNA six experiments the abundance levels estimated from our Nasik experiments will be less reliable for low expressed genes so we divided the genes into three groups of three groups based on their expression like high expression group medium expression group and low expression and show that well although the high expression group performs better there is no big difference between them so the algorithm is robust based based on the genes expression value and the second one the homologue gene pairs so when we are dividing our set gene said into training and testing we might have some examples of one gene in training set and it's envelope Union tests so that can cause some overfitting or some information leak and that may increase the AUC values so we tested in a way that none of the homologous pairs will be bought in training and testing set so if there homolog pair still both been training all or they will both be intensive so again they'll there is a little bit difference but not not that much so it is also robust to this information so so up to here I showed you some computational evaluations so here I'll show some proteomics data that are that is also another way of evaluating the results so our hypothesis that well we see that most of the cases one of the isoforms will is functional so when you look at the predictions most of the cases one either form is is doing the function where all have high probability of having the function for most of the coatrack so we want to the check are you excluding all coding traces no well but in this set that we use in the NCD i said there aren't allowed but we don't include we we don't explode any they don't get expressive yeah but as I said there first there aren't a lot of examples of long non-coding transcripts in NCBI also the ones that are that exists yet filtered are based on the expression because they're not expressed most of the experiments that we process they're not actually they are not included in the analysis muscle so the idea is that if a transcript is functional then at the protein level we should be able to see see it in the in the tissue so we first identify the isoform which is strongly expressed and also we choose among those genes we chose the ones that have an known function and also their eyes for more predictive it is different confidence level character small function so all the expressed either forms are predicted with the highest value in at least one of the known function so there is a consistency between the predicted function Eliza form and the express either form so here we have six genes and several go terms in almost in all of them the predicted either form by our algorithm is the one that is found in the proteomic sample so that was another line of validation for us so now I'll talk about these two genes two specific genes that we choose so it's not just identifying the function lies upon what you the algorithm can also predict different functions for each one of the items so we focus on analyzing these two and so this this cdkn2a agent is a the gym where alternative splicing results in different reading frames so this is an example where we have two very different proteins that are coming out of this this gene so it's the first either form second either form they share legs on but since there is a frame change the proteins that are encoded by these transcripts are really different so these are the structure prediction by eye Tesar so different domains and different shapes and we expect to have different functions for that and if you look at the predictions the first eyes affirm our eyes affirm is predicted to be functional for this function and second one is predicted to be functional for the second part for disturb and surprisingly both of them are predicted to be functional for this term which number oh so that is the all change so if we let's say if I if this term has 200 genes annotated to it II the baseline 200 over the number of genes that is the baseline probability of a gene having this function so this number is the fall change of the probability that we calculated so it's 73 times likely more likely that this isoform will perform this function over baseline probability so does your prediction agree with ITSs function so so with this 10 prediction is it from artists without or from the open know this this is from my pipeline so this structure yeah I mean I this also gives out like effective prediction go Tom so we didn't include anything from a laser in the prediction step in the functional production it's just X abundance estimates so so this is an example of a gene where two transcripts are drastically different so it's expected to be they're expected to be have different functions but so yeah this these functions all of the functions that we showed here are also mentioned in in some literature so this transcript is an inhibitor of cd34 kinase a member of this protein kinase family supporting its role in this in this phone in this function yes second function and the other Aiza form is a enhances p53 dependent transactivation and apoptosis supporting its role in apoptosis nuclear changes and although coding for the similar proteins all share a common functionality in cell cycle g1 so our second example in XA 16 this 1 to 2 transcripts are very similar the only difference is this sixth amino acids at these locations and the rest of the trans proteins that are encoded by this transcript are say-so to positions of turn in 5 35 and 75 37 in in one of the transcript compared to other make this one more likely to go phosphorylation because of the structure change in this part so the fault changes from go terms related to phosphorylation by our function prediction algorithm supported this this conclusion so for example for false the for the goal terms peptidyl serum phosphorylation this one has I'll probability of 10 mm the second one so in summary genes are gene functions are ultimately delivered through their alternative splice transcripts that encode the proteins with different functions and it's highly beneficial that the these functional studies is carried out at either firm level so however algorithmically any supervised learning algorithm developed for this new function prediction cannot be directly applied because we don't have gold standards so we chose to use this multiple instance learning strategy to predict functions that the individualized upon for the individual isoforms so we threw several lines of validation we show that it's producing promising results and we released our did you go back I just going faster than I could read okay thanks I'm sorry so you know you should go through all of them I mean you know this is important stuff so yeah these are ideal applying this multiple instance learning to this problem it's kind of a is the novel idea in this in this study so it has been used multiple instance learning has been using a lot of others different fields with success that are similar to this problem and this function function prediction problem is like a perfect match for multiple learning framework because the assumptions also for D for this problem so we release our predictions in this website I'll shows some of the examples how to use this site so you can do search with jeans or with isoform names or with go terms for example this is the one that I showed in the slides so it gives a like an information about the transcript and it gives p.m. domains informations and it gives the predictions for each quarter sorted by the maximum power change so here we see that this one is highly well this one's more likely to be functional for this turn we can we can sort based on the second Isis form to everyone or we can look at the quatrain page where we see the more likely functional genes or isoforms for this cup for this quarter I want to show another example this gene put the proteins or similar lengths but if you look at the predictions for some of the terms both of them are predicted to be functional for some the second one is functional for some the first one is functional so for this most of for most of these examples we couldn't find any evidence from literature or we don't have any means to test it out but these are really interesting predictions that are showing very distinct predictions so I don't know maybe I'm just I think a little bit more explanation about you know this looks like a fantastic and that's you know work it's really I just were worried was having trouble is just understanding you know how you news this is a discovery tool I mean you know just at the beginning of the process and if you take us through it maybe one more example it might become you know because this so there's lots of areas now in our area of psychiatric pharmacogenomics I mean alternative splicing is the name of the game in the brain absolutely no question and you know part of part of what we haven't been really looking at and not frankly is that we predict we do this reanalysis and fine structure mapping based on some of your working on you know and then we find results that are similar to what you find Steve that you know you have intergenic snips and introns nuts we haven't spent much time on the intron snips we focused more on the intergenic side to follow your regular concept but there's especially with bipolar II right disease we're seeing a lot of but you've intronic steps that most likely are driving you know different splicing niacin forms I mean that's our working hypothesis right now so you know so I guess you know is a bioinformatician that's got another tool in the toolbox yours you know if we have you know some introns that we would like to look at you know with some diseases in mind you know tissue specific you know kind of driven by Harold Brady Atlas and the NIH epigenome roadmap how would we take that data into your tool and then get predictions of you know so here I'd like to try so the predictions that we released here are only for the known transcript or the annotated transcripts that exist in the NCBI database but if you have we discover that discovered in the study and you have if you have a kind of expression or abundance data for those new isoform we can potentially run this either form transcript or other pipeline to get the predictions for that but first we need to get this this list of knowledge right so I guess part of the my question is it you know and it I had it in my mind earlier I guess part of my question is trying to and forgive me I came in like you might have had this on you know what was the knowledge base that was underlying all this because that's driving you know the space of possibilities so this whole transcript discovery is putting another level of complexity so to avoid that we started with the non non transcripts actually we used it like a conservative set of non transcripts and sümbül has a lot more transcripts and CBI but we wanted to start with NCBI to be more conservatively first really working then we can continue expanding our list of transcripts so with that I wanna acknowledge my advisor Jung from Guan and the members of corn lab also I want to thank to our collaborators give a woman Rajiv Menon and Mattias Chrysler and it looks like you've got some collaborators with the Sanwa to put the ikt oh yeah yeah I forgot thank you guys well it's good collaboration there you know it's a it shows you happen up and down the hall I mean that's kind of any other questions alright so so if I'm understanding correctly you have the list of returns that are associated with the G serve disambiguate which go transfer associative adjacent so it's using that annotation how do you what element of the model is determining which go term associates okay so major it's a major understand so each prediction each each Gautham is tested separately so it's not like you're searching for which code term is related to which either phone but you have you're taking one one go term and you're doing the predictions for that one first sure you're doing this for every one of them separately using like dummy information I know this is just expression oh I see so the idea is if - Aiza firms have similar patterns expression patterns they're more likely to be Co function this one more question I had was on the machine learning I mean you know the world isn't always black and white so is there a way to put a fuzzy boundary on some of this and some probabilities instead of making an assignment is positive for we're making positive and negative for only gold standards actually the negative set is not actually a negative set so for the positives they're really conservative we include only the ones with a with strong evidence in positives or for negative it's there are some genes in the negative set that are yet to be discovered so we're actually down weighting the negatives because also it's very unbalanced so you have a say 300 positive genes and the rest are negative so it's very imbalance so negatives are actually dominated to have less impact on the classification but for positive we really treat them as positives so the training set gold standard or really you know there's an advantage in a disadvantage yes the advantage is that you know you know things you want to find other things like it the disadvantage it would be that maybe hearing there's all classes of things that don't even fit into the bottle okay any more questions well thank you it was very nice