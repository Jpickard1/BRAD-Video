okay great so thanks the invitation to be here today so I got some new year I didn't know this tools and tech seminar existed I think it's very relevant that I'm here today I'll tell you about sort of what were what I'm building from my field of science but I think it's thing should be generally interested people here so today I'm gonna tell you about a field that's called cryo-electron microscopy so I'm gonna motivate why we care about protein structures and just in case people don't yep motivate that tell you about this technique that we use it's very powerful here at Michigan but also sort of globally in that I just want to point out that this was actually part of the Nobel Prize this year in 2017 in chemistry for three scientists who really helped in this field but we sort of think of it as sort of the field won the Nobel prizes we've sort of arrived as a technology that lets you solve protein structures and so you can read more on the Nobel Prize web play they have a lot of really nice summary material about the technology so today I want to tell you about why you want to use this technology called firing and then why if you want to use that you now are bumping into sort of high-performance computing follow next limitations and then how we address that because our user base and our field are not experts in command line and supercomputers so how do you deal with that sort of connection where you want to accelerate time to science for scientists while we're moving kind of most of the things that people probably here know how to do which is use high-performance computing you serve job submission all these different things so I'll tell you about two stories one using Amazon Web Services sort of public clouds and also a science gateway that's using Exede resources that's a new supercomputing Center so like Rio iam so I think the modern sort of part of biology research today is sort of we want understand how things go from atoms so organism and how that connects at a fundamental biology level but also how disease shows up and in the case where changes and mutations and proteins and parts of the cell can lead to dysregulation and so for us we want to know what's happening at the molecular level so they get us there we start at sort of the level of the human body when I really think about most the time ourselves so cells are really small compared to the human body these in the order of microns a cell is made of many compartments these compartments are are packed together with lipids proteins DNA RNA and these are all moving around all the time so the question is how's it all come together so if that's the cell if you were to zoom in even further you would see what in this case illustrations would look like of what the cytoplasm of the cell about the interior it looks like and these are all these different shapes proteins macromolecule complexes that have really important cellular function so down here this is sort of vesicles coming in and out of the cell this is a probably a virus being encapsulated and this is sort of the nucleus with DNA packed together in the shapes of these proteins is absolutely critical for what they do and that's the fundamental part of sort of where we are as a structural biologists which is part of it's like stamp collecting because you wanna know what they all look like there look part of it is sort of putting them all together and figuring out how they all come together to do these are amazing dynamic processes which is sort of cell biology so to give you sort of a flavor of this if you just take a subset of these and try and scale them with a cell there are these really small pieces relative to a giant cell this is a subset from the PDB and image from the PDB but these are all different shapes of proteins or macromolecules so things that you might see here that you might recognize if you're used to protein structures would be that's a virus there's a ribosome in here or somewhere I think I saw this is a proteasome that can degrade things this is DNA shown right here so these are the protein shapes that in these shapes determine protein function proteins are made of amino acids so amino acids are what you get from your food that you eat and those are get incorporated these specific shapes and that the chemistry is of amino acids that determine shapes and that's the amazing part of biology is that chemistry determines shape and so we have about 21,000 genes that maybe form you know on the order of hundred thousand different types of protein or macromolecular complexes perhaps and most of those structures are unknown oh it's anyway that's a big gap more knowledge of understanding biology so the central dogma is that your DNA encodes the sequence of the proteins and that if you want to say this in the context of Medicine it's mutations in DNA that then results in proteins that are doing the wrong tasks that then did leads to some kind of disease so cryo-em is a tool that lets us solve protein structures using a technology that's called cryo-electron microscopy and that's what we'll be talking about so why cryo-em I'll keep talking about why what the technology is but really the problem that we have is that the technology we use gives us low signal-to-noise ratio data which I feels like which I feel like is everybody's problem today which the erase data is not as high signal as you'd like and so that sort of you know have to collect a lot of it or analyze a lot of it to overcome the low signal so I'll tell you about why where our low signals ratio comes from and why it's like that so how do we use our area or cryo-electron microscopy to solve protein structures so if this really is cryo-em is transmission electron microscopy taking images of proteins so normally you think of this when you think of looking at cells or things in textbooks but it turns out you can also just take purified proteins out of cells and image them with a transmission electron microscope and so this is what the old technology the old platform used to look like this has been changing very quickly in the field and so now we have much better optics much better microscopes that are now kind of look more like a box fortunately but these this type of platform here is what we use to plot our samples in to do transition electron microscopy of proteins if you look in it you'd see there's actually probably over 50 years of physics behind them the optics of focusing electrons taking images recording images in a way that actually maintains coherent electrons for imaging but we're skipping over all of that so let's assume that you have a microscope and so Michigan has these at this equipment here that's also where it's going to talk about it here is that your work it's a great platform just for a machine in general ok so chromium is the idea you're gonna take a support sample matrix film here like a copper grid you're going to deposit onto this protein of choice so it could be a ribosome virus whatever you're interested in something but a purified protein in this case you put it here you put a very thin film of water in that in that water you actually flash freeze it so quickly that doesn't actually form crystalline water it just stops moving that's called vitreous solid it's a form of matter in that's like glass so glass is a victory of solid where it's all crystalline but solid and so cryo Liam does that so if you freeze a thin film of water really quickly just stops moving so if your protein is purified in that buffer what you now have is just a protein stuck in a in sort of some depth so if you look at the support film and zoom in you'd have your protein of choice so proteins are about a couple hundred angstroms diameter about you know plus or minus you get a film of water that's gonna be around that thickness so what you're trying to make is water that's about as deep as a hundred to 500 angstroms which is sort of amazing that we can even do this because that's like it's a fine number of water molecules deep thank you a hundred water molecules are three hundred water molecules so it's amazing we can do this we can do this you pull this thin film you get this protein embedded in the reason what's that how's the freezing dog so the freezing is done great you take your your protein coat agreed you'd wat it then you put into liquid ethane so liquid ethane is it so ethane will cool and liquid nitrogen temperatures so essentially have a liquid nitrogen bath when the connection is not very good at it has a very low specific heat it doesn't really freeze things that quickly so here to do this with liquid nitrogen usually a crystal nice because it had the water has time to reorient to crystallize but with liquid ethane the specific key is much higher and so it actually would have stopped the water so at they hoping that type of gas but in liquid form it is about you know minus minus 170 Celsius around seventy Kelvin so you so you take your protein of choice you embedded in this liquid let's say you can get there enough to take images of this with electrons and so electrons behave the same way photons do and so light your eyes but with magnets as lenses you take images of this you now get where you're slow signals ratio problem comes from so electrons are extremely damaging if you put a biological material an electron that's a higher the electron and you wait long enough it distorts melting because of ionizing damage all these different things so you can just take your sample anyone who is doing this will do at some point where you just leave your sample on the microscope too long and suddenly you sir bubbles are forming in terms melting away and so we take a really fast snapshot before it melts and so that's fast snapshot gives us the low signal problem that we have to overcome with high-performance computing it's been a contrast material so you could use contrast material but then what you lose is actually the internal details of the protein so in this case the just to finish that so what we have is we're looking at the scattering of electrons to protein into the buffer you can until the intervening space with heavy metals that scatter more then you lose all the internal details to the protein so it gives you that we can do that all the time actually but we just can't solve atomic structures of proteins that way because it's essentially a shell around it this is also worth saying the field is we've kind of arrived we have now better we can solve doctors or proteins but be honest were in the middle probably of our growth we're still there's always new sort of algorithm sample prep all these things are sort of under development I would say a key under development so this is what we do today doesn't mean we'll be doing this in ten years it'll be the same ideas but all again change so this is your low signal image that you take so it's a it's a micrograph now proteins to pixel size of your image is about an angstrom per pixel so which is about a carbon-carbon bond so very small pixel size what you really have is this so this is what it looks like but then you when you add when you actually take the images a lot of noise added it so the problem that we have in cryo a.m. is when you take low signal data compare all these different projections of a 3d object together to recover signal to actually see where all the atoms are in your protein so the idea is you're gonna extract these regions with these extracted regions you're gonna align them together to make them all in the same orientation you're really gonna average them together or classify them then you'll average them together to increase a signal and then eventually go into three dimensions to get a 3d protein structure and so this process here is we have pipelines that this is where a lot of people are thinking now is how do you do this better and faster and in more exhaust yes yes exactly so a very good point he's pointing out that so what you get here is the density map you have the model in the actual atomic coordinates and this is a model and that's it just modeling into density is still ended up itself a problem that were trying to deal with why why can't we classify them or why do we classify them so why you by is possible to classify if they are like orientation is trying to continue and you don't have this quiz days yeah I mean so the classification here in this case we're showing in two dimensions we will actually classify in three dimensions you can have arbitrarily example space but so that your question is how is even possible cuz continue it's discrete projections but a continuous 3d object and so that's where the samples I use he published way that a crime instructors are ones that can withstand this this test of you very fine sampling of imaging a route of projections around a 3d object so there are some samples where you don't get equal sampling across the whole 3d space and you won't solve a structure of it and that's actually sort of common and we just not published so we lost examples like that that are where you get a sort of an anisotropic view or distribution of views of your sample and then you're just stuck and then you have to change your buffer and start doing these things to fix it but it works for some of these and so the sampling ends up being enough job but I think we also slightly surprised that it worked as well as it does now I would say because it is true it's discrete so part of what we part of this so damaged to deal with damage we can take short movies but turns out victory these ice frozen ice isn't actually solid it saw like glass where it's actually not moving whence vitreous and being kept this cold temperature it still moves a little bit and so it turns out the big revolution in cryo am about three or four years ago was the ability to actually take a really fast movie so it's not just an image with a massive movie where you're can go up to you know 100 frames per second or 40 frames per second and so instead of an image take a movie and with that movie you can now reconstruct any kind of movements that happened so you can imagine that if you try to reconstruct where atoms are in your protein if that moved by you know an atom or two that's gonna blow out that part of the structure and you won't be able to actually solve that work and so part of what we do now is we take these movies that's sort of also have you've heard to put ourselves into closer the big data fields so if this is what you're imaging this is a same protein structure your imaging we take a really fast movie where every frame is actually very low signal and what we're gonna do is align these movies together to actually recover signal so in this case if you were to sort of go through and your different ways of doing this but you align these together you actually will recover the signal and what you get is a higher quality version of your image whereas before you probably had a snapshot that was moving now this is more crisp details and that's what lets us solve structures at high resolution and so what that means that are the movies we get from the microscope are around either 4000 pixels by 4000 pixels or 8000 because of 8,000 pixels times 40 frames and we collect that about one per minute 24 hours a day seven days a week and so each one of these movies is about one to ten gigabytes and then you do that data set that you need is about 3,000 of those just for a pile of data set so for instance yesterday we were collecting a pile of data set overnight I was about two terabytes just to see what's gonna happen and it's not even a final day I said that's just like a screening data set and so that's sort of where we're bumping in as a field into this problem of like data storage we're not deleting these movies because we can still analyze these better differently with new algorithms so there's every crime lab has about you know half a petabyte of data sitting around that usually not on cold storage and like active storage because we don't know what to do with it okay so just to kind of recap that the idea is you take your protein you're putting in this is a side-on view of a sample here so your protein suspended in some sort of you know 200px from thick water that's frozen you're taking an image with electrons you get this is now a real image from the microscope so what this is this is actually a very pure protein and this looks like you can't see anything because it's low signal and because it sort of crowded and proteins aren't perfect spheres they all kind of touch each other and stick sometimes and so in white here are shown example proteins that you can isolate all the things you see that's like a crystal nice contaminant so that's what looks like when its crystalline there's other things that can show up so to walk you through these steps this is what the raw data looks like you're gonna collect about half a million to a million of these subsets of these sort of it's not regions as individual proteins or we also call them particles you cryo-em you take this you're going to subtract them extract them out and then you're gonna start averaging them together with high-performance computing resources so that can be CPU based or we also know GPU accelerated soft code that can run on GPU workstations but you now take these and you average them together and when it's actually you're averaging to say object it's working you get things that look like this or you're increasing the signal-to-noise of your of your data of your image compared to the input data so things are working these are in two dimensions and you really you really want this in three dimensions and so what you will then will do is go into 3d analysis and so we can also do classification in 3d so the idea that you've sort of a computational purification of your data to find sub states of your of your protein of choice because people who maybe know those proteins that are usually not static usually they're changing confirmations or they're doing something so usually a lot of what time do you care about what's doing so you means how does it go from state a to state B so cryo-em can let you actually answer that from the same data set in the sense that you can come in and say well here's a global view of it and now let's sort it into sub States and then you with that you can sort of interpret what's going on so in this case let's say you serve as sort of some starting point some starting model you can start grouping into three dimensions using different types of analysis analysis algorithms you take this and you can do is sort of subsequently so if anyone who looks like writing papers today what you'll see a these sort of hierarchical trees in some mental figures because they're sort of using computers to purify the data out into sub States what you see is that when you started off there sort of its kind of blobby and lower feature anyone who's used to looking at protein structures down here you now start seeing alpha helixes and you're actually kind of getting at height you're sorting things correctly and increasing the precision of each model as far as representing a single state because you can imagine that this state here was a mixture of substates and you actually separated them out into three different states and then you can get higher higher resolution so you can go through this further interative lee in many many steps and you can get the structures where this is an atomic this one is a real data set that's harder to analyze but what you see these are all off but HeLa season you see these bumps on the Alpha helix use different features of amino acid side chains so this is filtered at a number here called five angstroms which means we can't see all the atoms but we can see sort of secondary structure elements and this gets at your question which is modeling protein density tricky if you don't know where every amino acid side chain actually is that's where software programs like your lab works on is really helpful to have sort of I Taser or rosetta or different pipelines that sort of help you kind of use chemical knowledge to model your protein besides just the density map and so we get besides having a structure of we actually can infer a sort of stable regions or flexible regions because you can imagine we're reconstructing a state these are sort of this structure here is probably from fifty thousand different images you can imagine that some of the images have a static part of the structure and some where may be flexible you actually will get that in your map where you can color up by local resolution we call which is that you know this part down here is more flexible which is low resolution whereas some parts that are more stable and averaged together constructively are higher resolution so it's similar to be factoring crystallography but the actual scale of the differences can be much more I think a B factor in crystallography is pretty narrow because it's sort of an uncertainty parameter around where one atom is or one sidechain is in this case this is saying the uncertainty here is on the order of an alpha helix almost of moving back and forth which is I think a much bigger displacement than you could probably have it be factored but analogous idea unfortunately it's not as well formulated as a be factored in though which is sort of just saying it's this is a local resolution where to be factors that you actually can go to your diffraction spots and understand just iWatch there's some other examples of raw data so you kind of see what it looks like so on the left here is an amazing data set of beta wrecked it's a sample called beta galactosidase it was published a couple years ago from a lab at the NIH what this is again a purified protein and each one of these here is a protein of interest in that on the right here actually an image where we've selected out the areas that will take to further analysis so you can see these are clumping together for instance this they're not they're not more dispersed but either way we still can get another image so again really the images that we get are noisy but we can sort of try to overcome that sorry you are picking those particles you are gonna find so some some people still do that manually most people do it automatically now or some kind of semi automated way but there are you can look in some methods and some people just will do it manually because they're sort of resistant to change but so cuz you can imagine AIT's the yet so part of our field is actually how to identify sub regions automatically reproducibly and sort of avoid sort of pitfalls where these are probably pretty good images but they're lost images where there's like a big arrogant or there's contamination sort of things you don't want to pick but a lot of algorithms actually picks they can't discern it very well so there's a lot of people are feeling on this problem it's it's like you know it's a lot of things that you see today in cryo my chart like half solve like we definitely proof of concept works but to actually make them more robust is sort of the next phase I think so if you took this data and then sort of in this case it was about a hundred and thirty thousand particles from 1500 of these sort of micrographs you start averaging them together you get things that are now higher signal-to-noise ratio where you can start seeing features that are related to the secondary structure of the protein began to show you the raw data this is different this image here these are actually aligned projections that went into this exact so you kind of can tell like you know these bottom two are about the right shape but these are it's harder to see so that's part of what the analysis is doing finally if you sort of push that all the way to the end and solve and determine this protein structure this one goes to one of the highest ones that we have in the field which is run 2.2 angstroms which means that you see densities it looks like this which is near-perfect when it comes to this efficient for this field and you can start seeing water molecules and sort of coordinating ions it's very impressive most don't go this high so that's just a caveat but this is a sort of a model sample that's been a really nice proof of concept for all of these beta galactosidase so it's also fourfold symmetric which helps a lot so it's just sort of a rock of a sample okay so now I'll kind of talk about the approaches that we took to dealing with this so let's say you can imagine the problem now so there are lots of biochemists or like I have a really pure protein I want to solve a protein structure I don't need to grow crystals I need to take it from the microscope but it gives me a structure but the problem is you're like oh well here's your 15 terabytes of data you don't have a data server probably because you're a biochemist or maybe you're x-ray crystallographer also don't not a data intensive field so that they have maybe a terabyte of storage for everything and you're like your first data set out of like probably five is gonna be 15 terabytes start analyzing it so the problem is this for us which is that people have samples that probably can be solved and they want to learn it but they don't have the data into the cyber infrastructure to do it themselves so the question is do you make everybody purchase and buy in a data storage are there other ways of sort of approaching this from up a cloud or remote resource perspective and so I'm on the side of trying to push this to remote resources but there lots of labs we just can do it in-house and just spend the money but the question if you don't the money what can you do or open other ways to do they're more cost effective so I would say state-of-the-art cryo-em is this which is that you have these amazing microscopes one of which we have here in Michigan and you just go through this sort of mass of Linux based custom scripts custom data movement things and then you get a structures so you get amazing cultures but in between there's all these homemade things that start standing the wage and the users coming into the field at the same time so number of users are going up I would say expert level of knowledge is going down and then probably cyber infrastructure available for that users also going down and so this is the problem that we're trying to face they try to deal with so how do you do this so if you think about this you could be a really wealthy Institute laboratory and just be like okay we're gonna drop you know a hundred thousand dollars per thousand dollars on really nice data storage that's backed up and then computing resources and pay for an admin to maintain it and all those things that's just very expensive and so this is possible and most clarium labs are like this right now because there's sort of the first one Thor University but then when you have so here in Michigan we've talked to probably 20 different labs and want to do this we don't have storage for everybody or computing currently and so again that's the issue so university-wide yeah this is okay like Michigan has a computing center here but it turns out we have really high memory requirements and pretty much the super cocoon the high performance computing here is not great for us it's also you cost money which is it's like maybe cheaper than Amazon or other resource but it's not so cheap that it makes you want to use it other things could be national supercomputing centers through the XE or NSF the nice thing there you can get free computing if you kind of can stay on top of the applications and get approved but there's no data stores no long-term data storage exactly through that so one of the other ways so one way is using cloud computing through Amazon or Google in today I'll talk about Amazon and so what I did as part of the postdoc and now transitioning to my independent lab here in Michigan is building software package to mostly handle Amazon Web Services for analyzing cryo-em data so moving your data up and running it in that the end I'll talk about a web-based platform that I built that's we're about to release hopefully soon that's built on NSF's supercomputers this astronomy or particle physics communities have any instincts on - or not do you know it would be nice to talk to more of them I think I'm not talking you enough for them I've talked a few physicists it seems like they mostly use NSF supercomputers they don't usually invest in their own supercomputer I think but I think you're right they're that they're really data intensive and so we can look down the fields for how to deal with this I think we're like the order of magnitude isn't quite as big as them so we're sort of somewhere between those two what's that I've seen people owns one of the regional along with Michigan State there between my brain so I think you're right though that we need to cause we know that the problem is sort of practical one yeah for Niki the problems of practical one because we all want to publish papers and not spend the time to actually think about infrastructure - yeah the intensives are also toward danger so just making on the same page I put as audience kind of noses but so the cloud does everything sort of our world around us today but for people don't know Amazon Web Services one of the longest ones and most established cloud providers for everything that relates to the website's data storage and computing and so you know we all up here what they call the application cloud when you're watching Netflix is like you're just looking at things the question is can you plug into the bottom and just go straight to the machines and do your tasks yourself the answer is yes you have to sort of do it and figure out how to do it and pay for it Michael yes that's right great ok he says it's a COPI ion the NSF computer resource for information okay great thanks okay so the reasons you might use cloud computing or you're paying per minute there's no cost it's in this case it's per minute pay as you go you're paying for everything though in some academics don't like that but essentially industry is moved to the cloud of a while ago academics are really resistant to this reliable backup storage I think there's actually this is hard to capture how reliable this is relative to your your local machines are duplicated in the same room like that's not very reliable most people in have very reliable storage this is extremely reliable because it's all over the world usually duplicated they measure how reliable it is the next thing here is flexibility so if you build a software tool into Amazon you can then distribute it all over the world and put it in data centers next to other scientists and so it makes it actually easy to distribute really complicated workflows routines whereas otherwise it's sort of always custom installation things for people don't know Amazon's all over the world they keep adding new data centers so in the US there's Ohio Virginia Oregon San Francisco and then Seattle but the things they keep opening all of it that all of the world these data centers have all different types of virtual machines in them and it just keeps changing and growing and getting cheaper so just make sure people know it's something I think kind of formative for the audience to see what they actually have there are different extremes in Amazon for the type of machines that you want there's a sort of half CPU machines that are tiny that can do like barely a task but that's there's a very cheap those are less than a penny per hour then you have very fancy ones that are you know in the order of hundred CPUs machines with a lot of RAM and these cost $13 an hour and so these prices are what you pay for yeah oh my god Brian a business agreement AWS now right so yeah I'm hopefully taking advantage of that because I am using the assistant level agreement with Amazon but yeah okay so an example here so those are two extremes there's like lots of in between computing resources here so there's more CPUs versus around there's more RAM versus CPUs they also have lots of GPUs in this case these are k80 GPUs they also have the people here GPUs there's lots of things here so there's many different types you sort of pick your type of project it's there that's what's nice for Sochi for Kylie workflows we have lots of different requirements per tasks that we're doing it's not just a single machine and so you can sort of pick and choose two flavors so the giving example the order of magnitude prices here the most expensive is about 13 to $14 an hour and the cheapest is less than a penny per hour just to point out storage they're sort of the storage on the machine that you're using there are sort of SSD type storage you can move around the pace or some number per gigabyte per month I just want to point out this name here it's called s3 or simple storage service this is one that's extremely reliable and you pay this amount per gigabyte emploice Alex I'll mention his name later but just so we know there's different types of storage details don't matter that much so we originally showed a few years ago this actually works for cryo-em we could this case we're spinning up CPU clusters on Amazon it worked people it was maybe slightly expensive but also definitely worked and so but since then we realized this old system we did was really manual and kind of cumbersome and so what we did was we wrap this up into sort of a software package so cumbersome Linux experience required so honestly if you think about the cloud no one really cares about how what's actually happening in the cloud you just care about the results that you get or what you're doing and so if you're using Dropbox you're using whatever you just want to do it from your computer and not Carrott not really know what's going on the back end so that's sort of what we built we took that as inspiration this idea is you as a user just want to send your laptop or your computer and submit your jobs and something else has all the the magic of using the cloud for you and brings the results back to you and that's what we did we sort of won't use the command line language of Amazon to write this interface here and so what we did is we took those most popular software in Crimea and it's called rely on it's very powerful software this is already set up to run on top of computing clusters and so we can kind of come in and sort of plug in to their cluster submission routine you say like instead of going to a cluster like call our code and just go to Amazon so it's actually very easy to integrate directly into the user interface so users don't have to know what's going on they see this the exact same output files everything is the same but it's actually going to Amazon step and so again there's this cluster submission feature here and now user just says yes they select our software and they hit run and then just goes Amazon so I think it's a nice paradigm also for sort of you know data intensive fields which is that I think users really like this worth it's not changing anything about their their day-to-day computing but that in this in this case it's all sinking back in real time so everything is is local but also running on Amazon so there's many steps is these are all the steps in the pipeline to solve the structure there's a gazillion steps so I took every step here and put into the software code so you depending on what you pick it it hits go it goes my software it says oh you're trying to do this task oh we want these types of machines or these types of machines sort of does I'm kind of deciding for users because again you don't know the computing on Amazon how to move dated Amazon fast all these sort of things and we can handle that for them yeah yeah yeah so it's all in the nice things you know there's a command-line language for Amazon so it's really easy to just sort of learn it but then yeah it's sort of monitoring things monitoring when the job finishes moving data around and so relate to that is that's you run the job from your computer you sort the virtual machine or what they call them instances but it turns out the fastest way to move data is through these through s3 they're sort of block storage options and this is this little bit more if you were an end-user you probably wouldn't figure out how to move data to s3 and down to your machine you'd probably just SCP it or something to your instance that's the slowest way to move data at Amazon so it's nice this sort of work works in a sort of probably the best date of movement practices you could get so limited by your your networking so bad networking means slow uploads Michigan has good networking as long as your computer's in the right backbone so kind of times we talk like 15 terabytes of data so here but usually for for me here it's about a terabyte an hour upload which is about 300 megabytes per second multi file uploads to s3 so yeah not bad it's like pretty good sulphate would take like a day or like an afternoon depending honestly if you were fully into the Amazon system every file is about a gigabyte takes about a minute to collect your part take about a minute move it up to Amazon so you probably could do it in real time if you're actually really thinking about it there yet right now but you could probably just do it on the fly it would be Zach so that's the the high level thinking guys want to walk you through sort of what actually happens in the back end to show you the idea is you're on your laptop and you're hoping that laptop hopefully not Wi-Fi hopefully like a real network connection and you have let's say 15 terabytes of data you hit run for this step called the movie alignment or the raw data alignment stuff and this like I was telling you it's about a terabyte an hour you know this type of speed of where I'm getting on good networking it goes to s3 in this case this is a GPU or sorry this one is actually a CPU accelerated step it's 1,500 files of movies from Verona and so you want to sort of do it as fast as possible so that means spin up five of these really big machines and just run it and so in this case you can run this step that normally takes you know 24 hours locally in an hour on Amazon upload it there but it does it really fast because you can get as many resources yeah what's the advantage of making this like a local client rather than web base the advantage is mostly that the user so the question is with the audience and online is the question is why why am a client versus a web interface it's because the software that everybody uses it is comfort with is a local client so you can extract it all the way that's the later part of my talk but I think you're gonna get people who like you're not changing anything about the software that people are using so it's nice that they can learn the software and not to learn my sophomore as well or it's sort of like single I think that's a web-based so yeah these diagrams are all detailed to believe the amazons there's sort of security settings all these things that I can sort of set up on the fly when you launch it so in this case it took two hours whereas normally this probably take 36 hours locally so ran really nice so I apologize for some of the cryo-em terms that are gonna be sprinkled in here you do different steps in this case you're gonna select the areas out this is a CPU based step there's other steps now that are going to be GPU accelerated where there's different types of classification alignment we can use the there's many GP machines you use those GPU machines and again all these cases are syncing it back to your computer think every 10 or 20 seconds and sync me back so it's it sort of it feels like it's running locally a few more steps or cpu base it's worth pointing out there's a couple steps here where we have to take the whole data set which is 15 terabytes downloaded to an instance and then extract all these sub regions out and they have machines that can hold up to 42 terabytes per machine so you kind of can like actually handle really data intensive steps that this actually gives a bottleneck for local machines usually is this type of status and so this is sort of the overall idea is that this software packages all the different steps my software can sort of do any of it that you need and can run it without users ever sort of seeing what's going on in the background they just see the output to give you a sense of what this is so someone had analyzed the same data set about to talk about using a GPU accelerated workstation so this is a for GPU machine with GTX 10 70s with 16 CPU cores so it's it it's they can do it 115 hours every step of this pipeline so that's that's really impressive the fact that cheap books are a it really helped a lot before this had to be in a CPU cluster it's a big step just have a GPU but everybody's buying this so the question is how does Amazon compare to this and so I took the same data set and you can do it in sort of less than half the time it's not your son it'd be nice if it was faster if it was an order of magnitude faster that'd be better half as fast is still nice but the really did take a boy here for cryo-em is that we can do it faster than you can a local machine but we also can do it sort of arbitrarily scalable whereas this machine is sort of one person one job at a time this can be as many jobs at a time and so that's sort of the take-home point not that Amazon's necessarily gonna solve everything faster because the code itself is sort of limiting in a lot of different steps five hours what's the price point at the bottom so the price point for this so I'm not even hello it has always pricing there's these sort of bidding things not using the bidding part so the price can always be cheaper but these are ticket price for this would be this was I think around seven hard ollars so it's not like it's twenty bucks a chunk of money it's also the full pipeline here and to be honest with you we'll probably use for the full pipeline but hundreds of dollars sort of per attempt at this and you could still spend thousands of dollars on Amazon and so that's sort of where the field is right now we're trying to figure like when is it worth it to do this when is it worth to buy a local resource I think there's probably the solution is like both you buy some local and then but you know wait so the the points your every step here the most expensive steps your cost maybe $13 an hour that's cheaper than all of our our lease our leads South salaries so if you're going to wait overnight for someone else's job to complete it could have finished on Amazon that time and you would have saved yourself so I think that gives me these economic arguments and academics cut hate and so it's kind of this it's in this sort of murky area right now I think industry it loves this idea because they already are on board the cloud they know to not maintain multiple resources and so so I think we are the biggest target audience here are gonna be big labs big facilities there's some cryo iam facilities in the country that are a hundred users like you how do you you buy 100 workstations and that's so won't be enough you know so they like the cloud as well I think that's the you see if you're like a one-person lab and you're like I'm in my own lab I'm gonna solve my own structure so I'm not gonna do multiple structures at once and doing more time serial processing followed by GPU workstation or something like that it's also worth pointing out that the code is running is developing really quickly and last year a new software package came up that cpu-based and so if you would invest in GPUs you'd be slower for you to use the CP ones so it's sort of this whole thing that feels moving so quickly that actually using the cloud is kind of nice because it means you have to commit to some sort of big infrastructure investment so besides the actual cryo-em structure determination people like modeling atomic density atomic models into the density maps and so we picked Rosetta because this is one of the software packages that we also had experience with so Rosetta is a very powerful modeling software actually really hard to use if anyone's try to use it's kind of awful to use and so part of the added value can give out is you can take normal input files from users in run tasks to them whereas Rosetta requires a lot of sort of dense things not very well documented that we can sort of wrap that up and make it easy to use and so part of this so this case Rosetta same idea this is arbitrarily scalable could sort of one CPU gets one task it's not as complicated as chromium analysis so means we can scale it really well I think the bigger reason to use this is honestly but this is actually maybe more affordable it gets done much faster than you doing yourself locally so go way faster because you can soar just ask for it Macy's you want but what you get is our software package doing steps for you that are easy step that or you would do anyway but it does it for you so that's comments I'm starting that's where the cloud also steps in where you can add value to pipelines and things that users probably wouldn't try and do it never there oh okay this is sort of the for me the take-home point was this that you wanted if you're running software for people who aren't experts in the field you want to do something like this where you're sort of abstraction away things that are not actually related to the science being done because people just want to do science faster I think it's the take-home point and so this lets you do that you have to pay for it and people don't like that we have to pay for it but you can do it faster and that's sort of where we are we're sort of expanding it to other software packages and thinking about how this goes in the future ok so the last thing I'll talk about is like just a few slides it's to highlight this other approach of sort of remote cloud computing based project here so this is so I started a project that we're calling cosmic 2 or cosmic squared for a long acronym that mostly the idea is it's a cryo iam website that will analyze your data on a supercomputer all without seeing command-line and so that's sort of the task that you're charged with is the users going to show up with 15 terabytes of data you need to be able to upload it ingest it and run it without with it so you can't use any normal data moving protocols it do something else you have to move it to a supercomputing Center and so this something that the kids are people have already doing the science gateway so science gateway is a term that sort of becoming popular which is it's a website that links users to super computing resources so we're serving the same same idea as I taser so the idea is users come in with their data we're going to move it in this case that there's a first case to the san diego supercomputing center and then with this will be making executing algorithms and choices ever they want to do so short term we just wanted to get people into the super queuing center to run their jobs without us coming in and telling what to do just here's a resource through website run it and that required sort of removing all these job commands and sort of centralizing software packages because it turns out if you look at cryo-em listservs right now most of the questions just like computing problems and like infant-like the workstation issues job running issue that sort of like just new users are learning how to run things so we can remove all that and this would also be free because using NSF exceed resource so you academics was like a free resource so long term you really connect people to storage and computing be a place where you could actually implement new algorithm and new pipelines so it's like sort of Clearing House cryo-em software and also probably integrate educational materials as people are coming into the field sort of teach them about the different steps so we're at the point now where we built it and we're just sort of debunking it really debugging it right now but the big thing that we implemented was through so through Exede we got developer time to really integrate the software package called Globus which probably will hear of heard of because Michigan uses them but people don't know Globus is a big data moving platform but also sharing as well but the idea is this can handle this can integrate into websites and users can interact with the data through a website and then sort of let Globus and move the data between the servers so to walk you through this idea is a user goes to a website they already have the software at an endpoint connect or installed on their local laptop data server through the website they then can interact with their data storage and say move this directory file you click it Globus then goes grabs it and move it to where it's going so in this case for our website users don't choose where they couldn't go they just say like send it to cosmic for me because then we'll grab it and just it but the idea is users come in click Globus and then we can ingest it we can adjust you know as much data as you throw at us you can upload you know there's no web us can move petabytes if they need to so it's not an issue and actually data moving the nice thing with global said it's doing all the file checking and integrity checks that you'd want in a data moving service so it's really nice you can outsource all of this so in this case we had two today but there's a globus API that we did that we edited and so I didn't do that myself we used someone at SDSU to do that but so everything we build is online and a github repo but this is very supported cuz cuz Globus is kind of a half academic app industry so it's developed at University of Chicago so the kind of like use cases and like people it says an API and they have all the documentation online you can sort of learn how to use it seems not that what's that what word Grossman perhaps I'm not sure that is but but yeah so I think it's pretty sure for to use it's actually very nice we're just using the data moving part of the data sharing there's all these different things you can do with it they also connect to cloud storage so Google box Amazon they can connect to as well so it lets you move data at different locations so one top sdsc because the Gateway is actually being hosted on a virtual machine at SDSC census where we started once you're in the exeed network your am now really amazing networking between all the supercomputers in the country so the idea is that we're just going to start putting jobs other places depending on what we need and what the type of job is so again there's high speed networking between supercomputing centers so it's to be really nice people to choose where this goes next currently just SDS II though so the idea is users are gonna land and be able to log into our website in that once they're in here they then can have a really rudimentary website for interacting with their data and so I think I'd cut out let me just sort of show you a few slides over here so I cut these out so I wasn't sure how long this gonna take I'll just show you one thing so the nice thing about Globus is that when you click log in they use the it's called the composit password sharing service for allowing you to authenticate with remote with sort of Google universities other Authenticator authentication sites so idea is if you click login you say I want that okay with University of Michigan it drops you off at your Michigan you know normal Kerberos web site here once there sort of a a handshake that says Mike authenticated okay come on in and so then let's do in our web site so it also removes all user management password management security to Globus which means moving it to the University so it's a nice thing with Louis that it removes that problem for us to deal with so the actual web interface is very bare-bones just to show you it looks like when you move your data sets and you'd there'd be coming in as sort of these kind of files analyst this is all where we're gonna be improving in the future but they do have users can come in they can execute to navigate along which is the type of rely on jobs I hit run and submits two comments and blames them back sort of output text files for now they can sync it back down so Globus is sneaking things so you sure she's syncing your home directory to our computer supercomputing gateway then it's gonna sync it all back so yeah the files are sort of changing and updating so it's very nice and so what is it available yet we're about to make it available for you great yeah go this is very powerful software so it's very nice so everything we did we were people nation then using this globus integration I think people would like that but you know we can talk more later but so now we're getting to the point now where it's going to be the user management side of this project which is that we're gonna get an excitation to the cosmic science gateway and week it's now distributed and so we currently have you know 25,000 GPU hours and so the question is going to be how do you let users you just don't want you to force them to do anything they want to go to choosers in themselves but you also probably to have some hand to make sure they don't submit things that are going to use up all of their allocations or all of yours so that's where we are right now is kind of you know setting this up so users can come in that came with Globus then we have sort of a database we're keeping track of everything and seeing how much time the user is using and so it's nice because it lets you divide up time but then we're gonna have cut off people I think but because of our next seed if the user comes to us our with an exceed allocation we can use an allocation as well so it's it's nice in that respect so we're gonna remove some administrative ala next by just giving them time for free but then if it is a big crime lab or facility is like oh we want to have this many hours on the exceed we already have it we just plug it in no big deal we're also talking about is this is so the Gateway right now is being funded through NSF sort of grant mechanism from SDSC long term that's not sustainable and so you as you turn your gateway into some kind of sustainable business consortium platform and so it's nice with exceed is that you can pay for industry can pay for time or perfect computing feeding cycle and so we're thinking about how to bring on industry partners who want to do cryo-em they can they can pay and they can we sort of can figure out a way to have them get access to our expertise and cryo-em but also sort of fund the Gateway to stay open so that's where I think we need to think next but you know where does the work in progress I think this is an exciting time I think it's gonna really move a lot of people here because of people who were trying to examine on try to do and luckily I think people would rather do like this and gets get it done and not worry about any of this so can you tell us for lated the mixture in the same sample right so the question is lay the protein structure determination what can we tell so I took an image of your sort of sake metric mixture within that phosphorylation the raw image we couldn't tell because it's so noisy but when we start analyzing it if there's any structural difference we could see it if it's honestly just a phosphate that's been added you might be able to see it so it also might get a screen it's just very very discrete yeah I know so in principle yes in practice it depends on like what signals in its place isit form so can you find spice alarms probably again it gets that sort of the analysis algorithms are using are really doing kind of template based alignment of things so the bigger the more dominant part of the protein is gonna drive alignments and so if you're looking for something that's really subtle and you don't know where it is for instance in the structure you might miss it if it's like really flexible you won't see it so I'd say in principle yes I mean if you look at papers coming out on sore spliceosomes or ribosomes where they're sort of they get a structure a part of it and they sort of say oh this parts flexible moving we can then classify that and it gets into mostly sort of the science of your project but in principle yes these are all doable it still would be hard it's all doable so gpo I was but how much strolling you'll ask for yeah so the for the storage part we're starting off with ten terabytes on comment in the reality as I'm talking about this but we're most to feed into the later stages of analysis so you just only have only have fifty two hundred gigabytes per user so it means they can't get a volume a to know yet it must give you the idea to intestine man they they must give us sort of a pre-processed data where it bends extracted in a line the goal is to do the whole thing so that's where rich in the scratch store in comment because that's how to bite scratch torrent sort of dumping data in there while it's being sort of processed and then and getting an app back out but you're right we have to sort of figure out how to do this because we pay so we actually have storage on comment that's ten terabytes we pay to use that you pay to reserve that for ourselves but we need to figure out what that should be you should be 100 terabytes shared and that's the next yeah in missionary things you're non-trivial part of it so you learn how to actually do it so you buy less yeah so it's it's on oasis we have sort of our own little corner that's ten terabytes that's ours for now own you pay you pay for it didn't seem that bad how much question I can tell you about I forget I can tell you later I'm just curious how many proteins you can count along with the current resources so with with this yeah yeah it seems like about let's say one run of your analysis I see you have your extracted data and you're gonna run it it's probably in the order of 200 to 400 GPU hours for one attempt there's an attempt not like you actually final so this is gonna be in the hopefully in the order of like structures hopefully like I mean it is like yeah a few hundred there's a pilot allocation because then we will go back to exceed be like look how popular we were give us like 10 million yeah yeah so I know this is sort of a seed funding decision you off the ground for sure and then we're gonna keep I think the ideas we're gonna keep some this aside and be able to have our own kind of supplemental application for users because again the application process the exceed is like they really try to help you do it but still certain times of the year it's kind of cumbersome and so we want to try and like remove that cumbersome aspect without but the problem is they have our own checks in place make sure people aren't wasting our time so I think that's where we will have in our pipeline there's gonna be steps there gonna be like this job looks like it failed done and then probably because we can't just let them run things forever because most people just run Java they're just like let's just see what happens or I think it failed let's let it keep going or because there's no deterministic and point for our jobs so that's gonna be a big part of this so that's it I just I say last slide fast-growing field we need computer scientists people in infrastructure and all these things are really excited open the cloud so Michigan also has a great facility here so it's a great place for anyone who is interested in services we can connect you straight to the raw data to the pipeline to the routines but also to the analysis sites so thanks thank you thank you