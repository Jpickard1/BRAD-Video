[
    {
        "start": 338.41,
        "text": "brilliant thank you thanks "
    },
    {
        "start": 461.23,
        "text": "[Laughter] you "
    },
    {
        "start": 528.18,
        "text": "okay we're ready to go look it's nice to see everybody it's an exciting seminar I'm just gonna say a couple remarks and turn it over to Santiago so he can introduce John formerly you know there's there's little brochures out there my name is Brian NPM chair of computational medicine bioinformatics I think most of you know me but I'm also a co-director of Midas which is a co-sponsor of this event I had a great pleasure to meet up with John earlier today Jun and I talked to him - IHP I you know the health analytics and medical prediction unit Mich our computational medicine biology integrative physiology which Santiago is the interim chair we're all co-sponsors of this so it must be important and it is quantitative biology and trying to understand scientific reproducibility and seeing the limits of the science you "
    },
    {
        "start": 588.37,
        "text": "know getting our students more tuned into that quantitative training these are all important issues of the day and our speaker it's going to help us frame this discussion and give us a chance for you know thinking about it and participating in the transformation of science by computation big data and statistics and modeling and things like that we've got two other talks coming up in this series there's gonna be an internal medicine Grand Rounds in NIH PPI Senate seminar the IH bi seminar will be tomorrow at 4:00 p.m. at the NCRC building 10 in the research auditorium there'll be a reception there in the internal medicine Grand Rounds mercifully will be at noon get there at 11:45 an athlete be at the Ford auditorium I'm down towards the table and center and tomorrow dr. Ian ETS will "
    },
    {
        "start": 651.01,
        "text": "be talking about precision health big data and evidence based medicine contradictions or our companions and and at the internal medicine Grand Rounds reproducible and useful clinical research mission possible so he's been known as somebody who gets some good discussions going some heated discussions and I encourage you to also attend those events and so for our introduction I'll turn it over to dr. Santiago schnell who is currently interim chair professor in Egypt professor chair of molecular and integrative physiology Santiago well thank you so I was trying to make this brief because I'm pretty sure you didn't came here to here but I or me so will the is a pleasure to introduce you and Johanna so as us you can see in this line so he's the trainer chirp a professor in disease prevention and he serves as a co-director of the meta "
    },
    {
        "start": 711.039,
        "text": "research innovation center is called matrix at a Stanford University and he's very well known now for his work on meta analysis so he holds also appointments in as professor of medicine professor of health and research and policy professor of biomedical data science professor of statistics and he's the director of the ph.d program in epidemiology and clinical research so born in New York City raised in Athens Greece he was our electorial Athens College where he received his MD and doctoring science in bio pathology from the National University of Athens trains at Harbor and top in internal medicine and infectious diseases then he held positions at NIH John Hopkins and tuff he chaired the Department of Hygiene and epidemiology at the University of Vienna Medical School from 1999 until 2010 and you know he's the recipient of numerous awards and recognitions and if I named "
    },
    {
        "start": 772.989,
        "text": "them all then you will be hitting me instead of him and you know what I'm going to do is turn the attention now to him we are incredibly excited of having him and as you know it's a very exciting time in in science and I where a speaker is at the epicenter so in fact he created probably the epicenter of what is the current crisis of we have means so half of the published results in peer-reviewed scientific journals articles are probably grown so that's what the headlines say and you know made those headlines in 2015 with his PLoS Medicine paper titled why most published research findings are foods since then researchers in many areas have caused confirming says actus ism often failing to reproduce the results of many influential journal articles and it slowly scientists are internalizing the lessons of decerebrate illa crisis to the point that now funding agencies they're taking steps to address this issue and you know we look forward to to "
    },
    {
        "start": 835.11,
        "text": "learn a few important lessons from you today and john is a pleasure to have you here and welcome to us of Michigan Thank You Santiago it's a great pleasure to be here today I had the opportunity to meet with lots of smart people this morning I look forward meeting even more people in the next couple days so the the first talk on campus is going to be about the power of bias and what to do about it and what is bias this is a very rough definition any deviation from the truth beyond chance it could be conscious it could be subconscious or it could be unconscious or it could be combinations of that we may approach it from a theory perspective create theories of bias and how it may operate in different settings in different disciplines or we may study its consequences what it means in our "
    },
    {
        "start": 896.04,
        "text": "data in the way that our data are applied interpreted used or misused theories could be robust hopefully but I think it's the consequences that eventually we can measure witness and if it comes to applications and health or disease eventually even suffer and fortunately here's one example of how bias may manifest itself this is from a project where along with Jonathan Jean field from Harvard we did cookbook systematic review that we published in the American Journal of Clinical Nutrition five years ago literally we took her cookbook we took the Boston cookbook it has been published since the 19th century and has editions that are being updated with the best evidence on recipes over time and we randomly selected with random numbers 50 ingredients then we went to the scientific literature and we asked how "
    },
    {
        "start": 958.06,
        "text": "many of those are associated with significantly increased or significantly decrease cancer risk in the scientific literature and here's what we got these ingredients have been studied in peer-reviewed studies in respectable journals so if I were to read that vo salt pepper spice floor egg but it would seem more like a an endless poem there's 40 ingredients out of 50 so some don't have a scientific literature around them supporting that the cause or protect from cancer but it's actually probably a data error on our part because for example vanillin is not listed there but we start sorry we search for vanilla but if we had searched for vanillin which is what nila has we would have found studies that report increased risk the queen's risk I don't know I mean how would I remember but probably both so "
    },
    {
        "start": 1019.8,
        "text": "most of these ingredients have scientific studies in the peer-reviewed literature suggesting that they increase risk and that they decrease risk there are some exceptions perhaps where most of the studies seem to suggest that this is something bad for you so sugar is almost always bad but there's one point that is on the good side and beef is mostly bad but again there's two points that are on the good side now if you look at the relative risks though these are relative risks for cancer per serving so a relative risk - that means with two more servings per day of whatever cancer would quadruple and with two servings less you would decrease cancer to 25% of the counter a so 75% of cancer would go away just with "
    },
    {
        "start": 1081.15,
        "text": "two servings with relative risks of five a single serving which could be like five grams per day of something would make cancer go away largely how likely are these results to be compatible even with just common sense it's it's extremely unlikely do we have some reliable studies yes we have randomized trials of interventions and most of them show absolutely no effect and we also have some long-term well done cohort studies that suggest that things that probably do matter so if I were to bet I would say that fruits and vegetables do decrease cancer risk and bacon does increase cancer is but the the magnitude of the effect would be less than one percent so a hazard ratio point 99 or closer to one or hazard ratio one point zero one or again closer to 1 so one point zero one and point 99 literally "
    },
    {
        "start": 1143.64,
        "text": "would fall on that vertical line of 1 it would be impossible to decipher so I would argue these effect sizes no matter whether they're on the left or on the right there are incompatible with logic and with common sense if the true facts are 1.01 or even if they were one point 10 the likelihood of detecting these effect sizes with the sample sizes that these studies use that are typically very small would be close to nil so even if everything that is listed here is associated with cancer is you would expect almost all of these studies to have non significant results at the nominal level of significance however it's exactly the opposite that we see we see it at the distribution of the z-scores which reflect the p-values almost all of the action the the green distribution is statistically significant if you look more carefully reading the papers you'll see some of the gray points which are statements that seem to "
    },
    {
        "start": 1206.429,
        "text": "have been primary outcomes of these studies but are hidden somehow in the fine print so you guess that that was a primary end point but it didn't end up being so nice if someone else looked at something else so some of these fine prints start filling in that middle space of non significant results but not to the point that you would get a huge concentration of quote-unquote negative results if you meta analyze these data then you start paying a little bit more of that middle ground but still most of the action in the center is missing here's another example of another field where in contrast to many of these lifestyle and nutrition studies where we have very limited sample sizes here we can have tremendous sample size so I work with colleagues from Sweden where we can pretty much work on the entire population of Sweden worth of data close to ten million people and the Swedes have wonderful registries they have a "
    },
    {
        "start": 1267.419,
        "text": "medication registry that captures practically all the prescription has written across the country a cancer registry with all the cancers being reported so we said we're going to ask whether drugs are associated with cancer risk and instead of looking at one drug at a time and publishing one paper for each one of them we will look at all drugs that are being prescribed and we categorize those there's about 2,000 of them we group them into 559 drug classes that contains similar drugs within each class Oh better blockers are packed together in that analysis here's how the results look in a plane analysis looking at the hazard of developing cancer with these 559 different types of drugs if you see a yellow or orange point it means that it is statistically significant even accounting for some false discovery rate process "
    },
    {
        "start": 1328.14,
        "text": "and if you look at some of these p-values this is a minus log 10 scale so many of those are p-values of 10 to the minus 100 you know or or less than zero point zero zero zero zero 99 zeroes and then one so it cannot look any better but if you take these results literally it means that one third of known medications may affect cancer risk and actually most of them seem to protect from cancer so we can just pick any medication we have a good chance of saving ourselves from cancer you know start getting more pills for whatever you you enjoy and if you group these medications further into larger medication classes then three out of four medication classes seem to affect cancer ease again with these p-values of 10 to the minus 100 based on the tremendous power that we have to detect any signal how likely is that to be true again I do believe that some medications do affect cancer risk but clearly here "
    },
    {
        "start": 1390.899,
        "text": "we're just taking noisy data noisy because they're confounded and we're magnifying that to a large scale and we're getting tons of reproducible results how many of these results might be publishable I mean all of them might be published but one way or another one could build narratives around them some of them might be more coherent others might be less Oh some may be more credible others would be less so this is looking at the CREM de la CREM journals one might say that well there's a hundred and eighty million papers out there in the scientific literature but we're not reading all of those were just reading nature science and PNAS and this is looking at heat values in tables and figures of nature science and PNAS in 1997 and in 2017 practically what we see is that if you have p values in figures or tables in these prestigious journals almost all of them highlight sophistical significant results some drop for papers "
    },
    {
        "start": 1455.76,
        "text": "that explicitly have multiplicity correction then the percentage drops from something like 98 percent down to 87 percent now figures and tables are one way to show all the results or kind of a comprehensive picture of the results because you cannot really list every single data point and result in the full text nevertheless this complete picture still suggests 98% success rate of significant results even in in these visual displays that are supposedly going to be more comprehensive if you look across the entire biomedical literature this is an analysis that I did with the video Valery ash from the Institute of complex systems in Paris we took the entire PubMed from 1990 close to 15 million abstracts available when we run the analysis and when we also looked at close to 1 million full-text articles both for abstracts and for full text 96% "
    },
    {
        "start": 1516.49,
        "text": "of the time when there was a p-value listed in an abstract or in a or in a full text that paper had statistically significant results to claim the vast majority of them also had language associated with it saying that there are novel if you take that literally it means that scientific discovery has become a boring nuisance because everything is novel everything is significant everything is major everything is is a great discovery and I'm not saying that medicine biomedicine does not make great progress and that we do not have discoveries I think we have hundreds and perhaps thousands of discoveries but do we really have tens of millions of discoveries as these papers would claim I would seriously doubt that this is some work that Dan Finelli did dan spent a number of years with us at Stanford in in metrics and this is one paper where he took the results of the primary hypothesis any p-value but they're trying to look at "
    },
    {
        "start": 1577.45,
        "text": "what was the statistical significance of the main hypothesis if you trust the authors that that was the main hypotheses of the paper being presented and he saw that gradient of how often that hypothesis is vindicated and supported statistically so psychiatry psychology material science pharmacology toxicology and medicine are having the highest rates of quote unquote success of ninety percent or even higher and even for the primary hypotheses and physical sciences like space science geosciences have the lowest code encode success rates of sixty percent the question here is do you want to be more successful or less successful I would argue that I feel better about the fields that are less successful I think somehow it's it makes more sense it's more realistic that not everything that they touch becomes gold and I'm talking to - so you understand what that means "
    },
    {
        "start": 1638.779,
        "text": "it's it's extremely unlikely that you know everything that we touch becomes gold even sixty percent I would argue is a rate that seems to be extremely good and even space science has evidence of irreducibility so the literature of exoplanets planets that are discovered that look like earth and therefore you know maybe our future is there most of the literature suggests that anywhere between thirty and fifty percent or even more of the claimed exoplanets are spurious discoveries and not really valid exoplanets why the research findings may not be credible basically we have these two forces we have bias and we have random error which may also be seen from the window of multiple comparisons for those who like the school of multiple comparison issues usually there's plenty of both there's plenty of bias and plenty of random error that we're fighting on a daily basis trying to work with with data and "
    },
    {
        "start": 1701.929,
        "text": "biases can take many different forms number of years ago we published that paper in general clinical epidemiology where again we mapped the entire biomedical literature and we map 235 different types of bias and this is clearly an underestimate because we use a method dad had to find bias in a two-word or three word sequence so things like Elfi worker effect that doesn't have bias but it is a bias would not be captured through that process so 2035 is an underestimate and this is a map of science focusing on communities of disciplines in their papers where specific biases tend to be more prominent so if I show you the community that we call citation bias each of these communities is called after named after the name of the most specific and least frequent most specific term that is used within that community you have the world of "
    },
    {
        "start": 1763.07,
        "text": "meta-analysis that is dealing with publications and the major biases there are publication bias and language bias and citation bias but other communities have other biases some biases affect many disciplines in many communities of investigation others are highly specific to specific knishes of scientific investigation if you look at the creme de la creme of the end product of biomedical research one could look at papers that have clinical applications and that have received the highest number of citations in the literature if you look at at those creme de la creme papers about 1/3 of them get clearly refuted either you have a better study larger study better done study that shows absolutely no effect or they're found to be grossly exaggerated and the effect is much smaller compared to what we originally thought for randomized experiments that are at that end of the "
    },
    {
        "start": 1824.279,
        "text": "spectrum of the translational pipeline even then about 25 percent of the time they get refuted for observational studies that managed to get such high visibility 5 out of 6 get refuted and there's other series like one by Stan Young that has found zero out of 52 prominent observational claims being replicated in subsequent well done randomized trials so success rate is 0% with 95% confidence interval going from 0 to 6% extremely low here's some classic examples vitamin E was thought be highly effective to prevent cardiovascular mortality and that was found in two large prospective cohorts but also one large trial of two thousand people how often do we do randomized trials of two thousand people and then large randomized trials with more than tenfold larger sample size and much better contact showed absolutely no impact on mortality or major outcomes if "
    },
    {
        "start": 1884.499,
        "text": "anything there was a trend for increased death risk with higher doses of vitamin E hormone therapy for a long time lots of specials were pushing that all women upon reaching menopause should be on hormones based on epidemiological studies from large cohorts at Harvard and elsewhere again the Women's Health Initiative a very large randomized trial showed absolutely no benefit to harm ratio that would be worthwhile actually the the harmful outcomes were far outnumbering the potential beneficial ones nitric oxide you know general medicine paper found to markedly improve outcomes in adult respiratory distress syndrome a study of nine people like the miracle cure for adult a RDS that has been very constant to treat much larger studies with a hundredfold larger sample size so absolutely no benefit in that population even increased risk of death flavonoids decrease cardiovascular mortality by 80% it's unclear whether "
    },
    {
        "start": 1946.45,
        "text": "they do anything low-fat diet romatic aliy decreases colorectal cancer heart disease stroke and breast cancer again doesn't seem to be better than low-carb diet aspirin different effects or similar effects in men and women better carotene could eliminate cancer based on large observational studies but actually it doesn't even decrease 1% cancer in six large randomized trials or fruit intake and diminish breast cancer is by up to 90% again very questionable whether it has any impact on breast cancer risk at all even with the most rigorous experimental design some time we have this type of pendulum behavior in the evidence so these are what I call recursive cumulative meta-analysis for randomized trials of my kario infarction interventions cardiology has been running the the largest trials compared to probably any other field in medicine has the best tradition of experimental designs and "
    },
    {
        "start": 2007.6,
        "text": "randomization so you would expect to see some stability of effects but in several cases you see that pendulum of major changes in the evidence as new trials appear that most prominent examples are nitrates in magnesium for myocardial infarction where multiple early studies suggested huge benefits easy treat to use treatments you can decrease mortality by 50 to 70 percent then large trial suggested modest benefits and then huge trial suggests that no benefits people still didn't want to believe it so they run even more large trials again there was no benefit and even increased harm do we at least clean the ground when we see that something doesn't seem to be what we thought to be we don't papers that get overtly refuted continue to be cited one of the most highly cited papers in medicine is the Wakefield paper in The Lancet that vaccines cause autism you know nothing can be refuted "
    },
    {
        "start": 2067.629,
        "text": "more than that but it has received about 3,000 citations here's some examples of some of the associations that had the strongest possible reputations like you know better carotene a scheme a preventive agent have had six randomized trials showing absolutely no benefit in excluding at the end of the day even a 1% difference in in beneficial outcome these papers continue to be heavily cited not only can they continue to be heavily cited but if you read how they're cited it's not that the citations say that they were all the citations either completely ignore these vast randomized trials that cost billions of dollars to run and they say we continue to measure better carotene in the tails of rats because we believe that it's a very effective he became a preventive agent or they even raise counter arguments that all these better larger studies were not actually better and larger and more accurate that they "
    },
    {
        "start": 2128.03,
        "text": "were wrong and everything was correct as it had been originally found so there is citation trajectory tends to be much better compared to the average paper published in the same journal after refutation and very often refutation even gives them a boost to citations almost any result can be obtained unless there is a preconceived analysis plan that has been agreed before looking at the data if we are in an exploratory mode in many situations almost any result can be obtained I call this the vibration of effects and it often leads to what I have coined as a term the Janus phenomenon from that Greek Roman God who could see in both opposite directions so here's one example from an Haines data the National Household Survey in the US and what I'm showing here are clouds of 1 million different ways to analyze the same simple "
    },
    {
        "start": 2189.71,
        "text": "Association of either creatinine as a risk factor for death or vitamin E as a risk factor for death the way to get 1 million different analysis is that for mortality there's many other things that could affect the risk of death so if you decide to adjust or not for gender you have two options to adjust or not adjust if you decide to adjust for age or not you have two more options if you decide to adjust for prior cardiovascular disease you have two more options for exercise for diet for whatever if you have 19 variables that you can choose you have two to the 19 options this is about a million choices and then you can plot the minus log 10 p-value on the vertical axis and the hazard ratio on the horizontal axis eventually with more model specifications you get that better that alpha tocopherol vitamin E picture where 70 percent of the results suggest that vitamin E decreases your risk of death "
    },
    {
        "start": 2249.98,
        "text": "and 30% of the results suggest that vitamin E increases your risk of death so if someone has a preconceived notion that vitamin E should be good or should be bad they can really get whatever result they enjoy the most and with enough analytical options we can almost always get the result that we like the most so how do we safeguard ourselves from that rich opportunity of running such rich analysis but at the same time being able to get results that could be all over the place one option is replication and I think that many fields have paid increasing attention to replication over the years I think this is good news and in some fields we have seen a complete transformation this is one example from genetic epidemiology one of the early papers in the prehistoric age like 1994 published in Nature found that tnf-alpha gene variation is associated with "
    },
    {
        "start": 2310.73,
        "text": "cerebral malaria that's a paper that got over a thousand citations and this is looking at a hundred of these citations half of them are papers that have no data their editorials reviews or news then you have a number of papers that are citing this work even though they look at different genes or at different diseases or both different genes and different diseases but they use that it's a nature paper so it can support whatever claim is to be made like a paper finding that some other cytokine gene not in a alpha but some interleukin gene is associated with the risk of multiple sclerosis the nature paper will be cited by the way nature published that paper that another cytokine gene is affecting another neurological condition and therefore this is Roger saying the claim for that new quad coat discovery then you have a slice of methods which "
    },
    {
        "start": 2373.64,
        "text": "is that a little bit which is I think good news and then you have replications which is the red color and it's not really an error in in the plot none of the 100 papers that built upon the nature paper actually tried to replicate this by the time the first replication happened that paper had received about 700 citations their application study was 20 times bigger and it showed absolutely no effect and then other applications also showed absolutely no effect the paper continues to be heavily cited nevertheless because you can build nice narratives and nice stories for more nature papers what is happening now well in the last 10 years genetic epidemiology has taken a completely different stance and replication is a scenic went on at some point we realize that we were not getting very far we started building large consortia joining forces teamwork sharing data very consistent analytical approaches hardcore statisticians being involved in "
    },
    {
        "start": 2435.739,
        "text": "in the process routinely and starting at about 2007 again with a very nice nature paper you have replication across multiple teams required essentially for any serious publication to have any credibility and once we do that and we change our mode of operation genetic epidemiology becomes suddenly a highly reproducible field from a point where nothing could be replicated now almost everything all the signals that we get can be replicated now whether they're useful or whatever I mean that's a different story but at least these are results that have been replicated across different teams if you look back at what is their application rate of the thousands of papers in the Canada gene era when we assess the same variants that had been published in about 40 to 50 thousand papers and top journals in the candy gene era their replication "
    },
    {
        "start": 2495.799,
        "text": "rate is 1.2 percent and is that just a problem with early candid gene genetic epidemiology I would argue probably not it's not the genetic epidemiologist did things worse compared to other scientists we're trying to isolate one factor at the time or a few factors at a time build some biological theory about it or some mechanistic explanation and then start analyzing the data until they cross the level of statistical significance so I think that these rates may affect also other scientific fields that haven't gone through the transformation of no replication shifting to routine replication some fields are taking the same route so lots of preclinical research has started focusing more on the need to reproduce results and in that case it's the industry that has taken the lead and the reason is that lots of papers appear in major journals by wonderful academic teams the industry is trying to use them to build their R&D "
    },
    {
        "start": 2558.39,
        "text": "agenda and develop new drug targets but they're getting nowhere so at some point several companies decided to take large numbers of high-profile academic papers and see how many of those can we reproduce in our hands these reproducibility series had success rates between 0 to 25% and in one of them Glenn Begley in nature in 2012 when his team could only replicate 6 out of 53 landmark studies published in top journals for oncology drug targets his claim is that the failure to win the war on cancer has been blamed on many factors but recently a new cup has emerged too many basic scientific discoveries are wrong the overall credibility of an isolated research finding depends on all of these features it depends on the pre evidence odds roughly speaking this is how much is to be discovered in the space that we explore compared to the denominator of "
    },
    {
        "start": 2620.4,
        "text": "how many analysis we can perform it depends on the data the study at hand obviously this is why we are in science it has to depend on the data it depends on bias it depends on the field because different fields have different types of pre evidence odds they have different types of data they have different types of biases and all of these may depend on each other so you can model that process with some simplified approaches and this is the most simple model that one might conceive of of a two-by-two table of research findings and through relationships alpha and beta are the type 1 and type 2 error R is the pre study odds C is a constant for the volume of analysis that we and you have the two-by-two table in this situation where you have no bias at all and there's only one team of researchers on planet earth so you have eliminated all competition and you are the only lab also you're the only editor of the only journal and you publish "
    },
    {
        "start": 2681.73,
        "text": "everything that exists it's it's all one person or one team and in that situation you can run calculations based on these assumptions to make it a little bit more realistic we can add bias meaning that some of the results that should not have been discoveries are claimed to be discoveries because bias is present and then we can add the complexity of having many teams of researchers who are trying to make these discoveries and they don't combine their data they don't work as a team they don't cross validate whatever they do but in that scenario they just separately try to make that discovery and outpace each other you can run then calculations that show you the probability of a research discovery being true as a function of the pre study odds of power of number of teams working the field and the extent of bias that operates and most of these post study probabilities are not very high in most settings and in most "
    },
    {
        "start": 2743.8,
        "text": "combinations of values that tend to take numbers and the range of what we have empirically studied in different disciplines seem to give pretty low post study probabilities of the results being correct so this is like a range of different types of designs a very large randomized control trial with little bias and starting from a very good point there's a long sequence of translational pipeline with lots of study suggesting that it's really worth it to try to see if that drug is clearly effective so we start with a 50% chance of that drug being effective if we get a p-value of slightly less than 0.05 then that means about 85 percent of that it's likely to be correct if we get at lower p-value that's even better it could be 95 or 99 percent chance of being correct a confirmatory meta-analysis where we have multiple well-done studies showing the same thing again gets up up to 85 "
    },
    {
        "start": 2804.43,
        "text": "percent or even more if we have even stronger evidence a meta-analysis of small inconclusive studies that happens to cross the boundaries of statistical significance most of those will be wrong only 40 percent of them would be correct once you go down to underpowered studies then you go down to 20% truth rates then if you go to observational epidemiological associations is even less when you go to massive discovery exploration a p-value of 0.05 means absolutely nothing and this is why many fields in these areas use far more stringent discovery thresholds like p-values of genome-wide significance in the case of genomic studies there are fields where effect sizes are the best estimators of the extent of bias in the field in some disciplines the effect sizes are a mixture of some genuine ones "
    },
    {
        "start": 2866.769,
        "text": "and some that are due to bias so fields that have few genuine effects on average they're showing you how much bias is operating the field in that vein fields that find larger effect sizes and are deemed to be more successful are probably simply more biased and others that find smaller effect sizes and within the same scientific discipline the most successful and appreciated studies may be simply those that suffer the worst net bias I'm not saying that this is the dominant situation but I have seen many situations where once the film the field transforms itself it's realized that the biggest effects that we thought were there were just the worst bias studies that were available the post study odds of a true finding becomes smaller when we have any of these factors appearing when effect sizes are small when studies are small when fields are hot and many futility competitive teams are working on them "
    },
    {
        "start": 2927.19,
        "text": "trying to outpace each other when there's strong interest the results for example financial interest from people like companies and industry to try to make money out of them where databases are large and when and outs are more flexible fortunately or unfortunately modern science has more of that and it's getting to become even more common to have most of these factors almost all of them together in many of the research endeavors that we are pursuing because large effect sizes the low-hanging fruit in many fields have already been collected so we're dealing with trying to discover smaller and smaller effects most fields are unavoidably running small studies because of limited resources and limited funding fields can be very competitive we have 20 million scientists who are offering or co-authoring scientific papers across science there's often strong interest in the results financial or other databases are becoming larger and larger and analytical options thanks to methodologies like me and you "
    },
    {
        "start": 2990.19,
        "text": "are becoming more diverse and more easy to work with different options if the second possibility is that something is not entirely wrong but it is inflated and I think that this scenario actually is as common and in many situations even more common compared to something being completely wrong and obviously depends from one field to another even when you look at the end of the evidence scale which would be meta-analysis of randomized control studies this is a snapshot of the Cochrane Database that I took about ten years ago this is how much evidence we have accumulated and this is the summary effect size and you see that decline function when we have more evidence the effect size are smaller and typically when we increase the the amount of evidence the effect sizes gravitate to smaller estimates so you have that winners curse even at the end of the spectrum this is not "
    },
    {
        "start": 3050.88,
        "text": "discovery research this is the end of applied translational research and the winners curse obviously can be more prominent in earlier phases here's a few other examples this is the effect sizes of the top sighted biomarkers studies in the biomedical literature the papers that have been cited the most and I'm showing you here their relative risk in the highly cited study typically it's early in the sequence and this is the relative risk in the largest study on the same topic almost all the action is below the diagonal it's not that it is completely now necessarily some of these effects may be different from a relative risk of one which might mean that maybe there is some effect there but for a biomarker that is to be used for predictive purposes if you have a relative risk of five or ten or fifteen that might have a good chance of being clinically useful if it's a 1.1 it's unlikely that it would be able to move the dial adjusting effects downward "
    },
    {
        "start": 3112.71,
        "text": "might be something to consider so this is a study where we looked at studies published in top journals like New England Journal of Medicine Lancet and JAMA and other studies on the same exact question exact design randomized trial published in specialty journals we found that studies in you know general medicine glanced at the Jama when they were small grossly exaggerated the effect size and this is pretty much intuitive the only way for a small study to make it a new general medicine is to find something extravagant so it has to have an exaggerated effect size if you read a paper that is a small study in these journals it's almost intuitive that you would want to downgrade the estimate of the effect size that is being presented another level where we could improve our current situation is to improve repeatability repeatability means that someone spends the time to try to repeat a study using the same data the same software the same script as the original not necessarily run a "
    },
    {
        "start": 3174.48,
        "text": "new study but just try to repeat the original one so here's some experiences that I've had in in that domain along with many other microarray analysts we got in touch with nature genetics and we asked who repeat the analysis on 18 papers on microarray gene expression data that they had published Nature Genetics already had in place a policy that all the data all the software all the script has to be available as a precondition to publication for these type of studies only two of the 18 papers that we tried to reanalyze could be reanalyzed and get the same result sometimes there were discrepancies or there were discrepancies and not even the raw data were available most often we just couldn't reproduce everything because the data were not available even though it was supposed to be the software was not available it had disappeared was homemade the methods were very unclear so different analysts "
    },
    {
        "start": 3235.62,
        "text": "read them very differently and reached different conclusions which were also different compared to the original paper or everything was clear but the result that we got was different compared to what had been published originally some fields are moving to reproducibility checks in massive scale so psychological science is one such field nature published the efforts of the open science collaboration a couple years ago where 270 investigators and their teams took a hundred papers from the three top journals in the field and try to independently reproduce according to protocols that had been vetted and had been visible and pre-registered and everybody agreed that that was the best that one could do to reproduce the same exact sequence of experimentation only about a third of the studies could be reproduced and even when they were reproduced the effect sizes were less than half of the original that had been reported 2/3 the effects were pretty much no so how do we fix the problem I "
    },
    {
        "start": 3297.21,
        "text": "think we need to go back on how most of us try to do research there's very limited funding for research I wish that the budget for scientific investigation could be 10 times bigger I think that science is the best thing that has happened to humans more investment in science would be good for our society but unfortunately budget is limited at the same time science is very competitive so each one of us is trying to become a solo silent investigator of Pi unavoidably with limited resources most of us can only run small sample size studies we to aim for getting the most exciting results so a lot of cherry-picking of hypotheses happening a lot of that is done post hoc p-value of 0.05 or slightly less with a little push with P hacking it's fine in most scientific fields no registration is required no data sharing is required to give ammunition to your competitors and no replication is required because then you waste twice the effort and you kill your original observation then nobody wants "
    },
    {
        "start": 3358.24,
        "text": "to publish that so that recipe leads to a situation where we're facing massive power failure across many disciplines in that paper that we published five years ago we looked across different domains of neuroscience ranging from basic discovery research all the way to Applied Research and clinical trials the common denominator was that the average study circulating was very small the power was about 20% even with lenient assumptions about the effects that are being chased we have seen that again and again in very different fields in your science psychology and medical science regardless of the journal that we have looked powered to detect meaningful effects is very very small when we are operating in such an environment we get tons of false negatives and we get tons of false positives and with a tiny bit of bias we may end up in a situation where the vast majority of what we get are false positives problems are different in different fields and therefore each field probably needs to "
    },
    {
        "start": 3421.119,
        "text": "realize what are the main biases and therefore work towards minimizing preemptively these specific biases this is a paper that we published a year ago in PNAS with Dan Fanelli and Rodrigo Costas we took every single meta analysis that we could find in any scientific discipline so there's 22 disciplines represented here this is how all science is divided and when there were there were many like in medicine we just took about 400 of them and we pre specified 17 types of biases 17 patterns that would be consistent with bias now when you have a meta-analysis it doesn't mean that it is correct that all the studies are very consistently showing the same thing but let me that all of them are wrong or it could be that all of them are correct what we're chasing here is patterns that are consistent with specific modes of bias operating so small study effects or small study bias is the situation where "
    },
    {
        "start": 3481.48,
        "text": "small studies give exaggerated effects compared to larger studies and we could see how common that pattern is it was very common in the social sciences common in the biomedical sciences not common in the physical sciences grey literature bias signals again differentially scattered across different fields decline effects early extreme results citation bias u.s. studies showing exaggerated extremes each bias has its footprint in specific disciplines more prominently than others these needs to be taken into account as we try to eliminate these biases from our everyday life and to do that we have the option of acting very early or acting late so ideally you want to have a pre-emptive strike at the level of the research agenda even for study start to be designed but if we cannot do that then we can hopefully try to intervene while studies are being done at the level of their analysis at the level of their reporting ideally you want to safeguard the entire process because "
    },
    {
        "start": 3542.14,
        "text": "obviously if you eliminate biases in the design phase but then they're reintroduced in the analysis or the reporting we haven't really gained much there's 12 families of bias correction methods or bias preemptive elimination methods that I have tried to summarize here large-scale collaborative research adoption of a replication culture registration of studies protocols analysis codes datasets Road data and results sharing of data protocols material software and other tools reproducibility practices containment of conflicted sponsors and authors more appropriate statistical methods standardization of definitions and analysis more stringent thresholds for claiming discoveries or successes improvement of study design standards improvement of peer review reporting and dissemination research and better training of the scientific workforce and methods in statistical literacy not all of them need to be applied in every type of design in every type of "
    },
    {
        "start": 3602.8,
        "text": "investigation so registration in principle is a good idea when there is a specific hypothesis in a specific plan there's a lot of exploratory research and in those cases I would argue the only thing that we need to do is just be honest and clear transparent that that was data exploration that was dated raging that was data fishing this is wonderful data dredging and data fishing can lead to some very interesting insights but we need to say so that this is what happened in other cases maybe we can register in data set if I register a data set I'm telling you that I possess that Dana said that has 10 million variables on 1 million people so tonight I can launch one trillion P values against you you know it's like mapping my nuclear arsenal in a way so that you know how much I can destroy the literature overnight level 2 would be registration of a protocol the protocol exists level 3 would be registration of an analysis plan that goes way beyond the protocol level 4 would be the registration of the analysis plan and the raw data and level 5 would be open "
    },
    {
        "start": 3663.73,
        "text": "live streaming where this is something that has been done in some fields alike systems biology someone is putting the protocol out there for comments and then this is vetted optimized and then the results are communicated online here's one example of register reports a hundred and seven journals have adopted that practice basically the study is peer reviewed without having any other results but the study has already been written up so it's valued and assessed based on its scientific merit and then the results come in you populate the tables and the figures so the slope might be like this or it might be like that that's okay the paper will get another second review but in principle it has been accepted based on its scientific value of the design sharing is something that lots of us want to do and if you look at the 50 top impact factor journals they do have policies in place that's the green color here of asking for sharing of data of "
    },
    {
        "start": 3724.54,
        "text": "materials of protocols even making that a prerequisite to publication but the last column has lots of zero mostly and this is how many out of ten papers in which one of these journals actually adhere to these policies so sharing is not easy to do and even though journals say yes please do share if you look at a random sample from the entire biomedical literature this is a random sample of 500 papers half of them have primary data none of them shared their entire data set and only one of them had the full explicit detailed protocol in place actually that was a separate publication that was the protocol being published reproducibility for computational methods also has plenty of room for improvement we published these guidelines about a year and a half in in science where we realize that it's impossible to get to perfection overnight so there's different levels the early level would be just to see if there's some link with software or data or script and if there "
    },
    {
        "start": 3786.11,
        "text": "is one someone please click on it and make sure that you don't get a error or something is this is a very common and frustrating experience then you have all the way to someone willing to waste or spend four years of their life like kief m\u00e4ggerli trying to show over four or five years that these Duke studies were completely irreproducible and nobody paying attention and nature sending reply letters we cannot publish this because it's not novel so it's it's something that we have to set some realistic goals but you can have different levels of trying to probe the reproducibility of results and maximize it I think I'm preaching to the converted when I'm asking for better statistics and better methods in this audience I think you're all familiar and how important it is to improve the scientific and data science literacy of the scientific workforce how important is to optimize the methods that are being used better study designs maximize the use of good practices that depend on "
    },
    {
        "start": 3846.88,
        "text": "design options and sometimes we may do the best possible but there's still some errors that get undetected for a long time that study in PNAS showed that one of the most popular statistical packages used for analysis of fMRI studies base basically had such a family-wise error rate that with the parametric methods being used when you got a p-value of 0.04 actually might have correspond to a p-value of 0.8 and about 40,000 papers were affected by that misconception it's not that they were all wrong probably as much smaller said made completely erroneous conclusions but that's something that went undetected for tens of thousands of papers before someone brought that to our attention there's also some temporizing solutions that have been proposed so I'm one of the 72 authors of that paper that proposed lowering the p-value threshold to 0.005 instead of 0.05 for fields that don't "
    },
    {
        "start": 3907.79,
        "text": "have even lower feels like genome-wide significance already and much of what we state in that paper and many of the authors who co-sign it we do not feel that this is like the last frontier and the perfect solution we see it as a temporizing measure as one way to eliminate a bulk of irreproducible false positives until we can build and we have time to go after after better solutions what might be better solutions well first of all null hypothesis testing is probably not a good idea for about 80% of research in biomedicine and probably a similar amount of research in other fields so we need to use other statistical inference tools to really get there how do we do that well we need to train scientists but we cannot train them overnight it takes time it needs integration of the curriculum it needs statistics and data science to infiltrate the everyday training and everyday experience and every science "
    },
    {
        "start": 3968.72,
        "text": "who is going to handle data how many signs are going to handle data a hundred percent if they don't they're not scientists practically so we need to train that enormous and very smart workforce to the optimal use of Statistics and we need to continue that methodological education also for investigators including senior investigators we also need to see which of the research practices that we plan to change really have to change the fact that we have some idea about change in the world does not mean that we have to do it because it be more detrimental than what we have now so we need more research on research and we also need to re-engineer the reward system to be aligned with getting reproducible results currently we're paying a lot of attention to productivity getting more patients more grants more papers but we need to find ways to value quality reproducibility sharing and for scientists who work on Applied Science the translational impact on research these probably needs a revolution so this is a manifesto for "
    },
    {
        "start": 4031.06,
        "text": "reproducible science on how we can change our incentive system and how we can prioritize research practices that are aligned with getting more credible and more useful research who is going to do that it could be scientists it has to be scientists it's not something that has to be imposed it can be journals it can be funders it can be other stakeholders including the general public who really want to see the best science communicated and affecting their lives to conclude biases are highly prevalent across different types of designs and scientific disciplines new challenges and opportunities in modern science need to consider the specific patterns of bias that may affect specific types of designs and disciplines biases erode the credibility of the evidence we need transparent participatory search practices that are effective in reducing biases increasing the credibility of literature and in many cases we don't have enough evidence even on these practices and therefore we need to do research on them to make sure that we don't make things worse I will "
    },
    {
        "start": 4092.14,
        "text": "close by special thanks to a number of my colleagues who have contributed to some of the empirical work that I showed you today I think without their smart bright ideas and work that they contributed maybe I would have been able to show you next to nothing and thank you for your patient listening to me at 5:30 I got a fine if people would like to ask questions so um I actually wrote a paper once in furnace our preventive things that you could do preventively in a certain disease area and the title the paper is when there's too much to do yeah it was all about so how do you go about trying to prioritize when there's a bunch of things you could do and there's there's a whole I can you know we each can think of senior 30 or 40 "
    },
    {
        "start": 4153.12,
        "text": "things that we could do for this but the real question is where to start I'm uneasy with starting with the p-value adjustment but it seems to me that's just playing into the weakness we have if we just set the filter lower the well more exaggerated effects than we had before there plenty of millions of scientists there'll be enough people to generate low p-value experiments it seems like maybe the first step is actually to throw out the null hypothesis testing and go to you know using so that they basically some kind of Bayesian approach where we both acknowledge AAB of formulating our priors which is where the flaw is in a lot of the you know the sample size calculations and we you know sort of reify a statistical approach that that sort of puts description more on the front burner we're describing effects and the precision of those effects you know with building on the science that's gone before so I was wondering what you thought about maybe that making that the starting point so I I'm very sympathetic "
    },
    {
        "start": 4214.55,
        "text": "with what you describe and as I mentioned 80% of the time null hypothesis significance testing is not the indicated approach to make statistical inferences you know Bayesian approaches or false discovery rate approaches would be better or just focusing on descriptives of effect sizes and and their uncertainty would be much better off the the problem is that this is a message that has been communicated for a very long time and even though we do see an increase in the use of these methods it's still a very tiny minor the literature so roughly when we did that survey we found like 1% of papers or less using Bayesian methods and even less frequently using false discovery rates as opposed to everybody reporting a value so how can you make that transformation overnight I think you need to continue sending that message about that transformation needing to happen it will take years though so the the p-value change in "
    },
    {
        "start": 4275.459,
        "text": "threshold is a little bit like the Gordian knot it's it's like a silly idea but at least you know you go there and you destroy 30% of the literature destroying some valid information by doing this but mostly throwing trash to the waste bin so it's it's like a temporizing measure that can be done in zero time I've got the mic anyway so you alluded to this in your talk briefly that maybe the current funding structure that we have in the US may not be ideal for best practices so my question is how would you change the current funding structure has that you know I mean in a way it's almost inherent that that's the route at least part of the root of the problem right right so I think that there's many possibilities and they need to be seen in the context of different disciplines and different stages of research for example for randomised "
    },
    {
        "start": 4337.019,
        "text": "trials a very simple solution to decrease publication bias is that you don't fund someone to run yet another trial unless they have published their full results without selective reporting of what they were already funded to do and this may sound well that's so easy well this is done in the UK by the the equivalent of NIH before you get funding for the next trial you need to report everything very thoroughly if you look at NIH funded trials fifty percent of them remain unpublished four years after their completion sorry two years after their completion and once you get to that point very few of them will be published subsequently and even those that are reported about 50% of them are reported with different endpoints compared to those that originally agreed and and then you have spin and misinterpretation and press releases and media taking the lead to make the complete the distortion possible if you go to earlier stages of research again you can you can find the right niche of what what really matters or you can say data sharing is a "
    },
    {
        "start": 4398.45,
        "text": "prerequisite and unless you do that we will not find the project we will not give you the last installment and will not be eligible for funding subsequently and obviously to do that you need to have infrastructure in place but this has worked in in many genomic fields it's very nice repositories are they're pretty much comprehensive so why not do it in other fields as well if we ask scientists to kind of create a repository on their own each time they publish a paper that takes enormous effort but but if the community agrees that these are the common standards and this is a common platform that is streamlined the amount of effort that is required is much less and it's becoming more efficient do we need to change the the mode of appraising scientists in discovery research I would say we do instead of trying to present protocols and in grants I would say fun the best scientists in a Howard Hughes model and allow them to explore different "
    },
    {
        "start": 4459.32,
        "text": "possibilities you know if you ask them to prespecified everything probably we'll just submit what they have already done and you're just funding work that they have already completed if you just give them some funding and you know they are documented to do high-quality work very rigorous work and reproducible work give them the option to explore because they're in the exploration space so I think our funding system needs to change but the way to change depends on the phase of research and where it stands in the translational pipeline I think you were already answering a little bit my direction I was just in a you answered it in a carrot sense what I'm thinking is part of the problem is that there is no deterrence against publishing false research so let's say it bluntly there are very famous scientists I know who are at the very high is leadership published withdrawn papers who are bad and it was considered an "
    },
    {
        "start": 4519.94,
        "text": "honest mistake or an honest situation where they were following the field so as long as you know publishing a paper and then publishing another paper saying what I published first was wrong gives you two papers whereas your approach will give you zero papers because you're your replication shows that where your first hypothesis was wrong it's a disincentive and in the moment the incentive is there to publish as much as possible and this zero disincentive there's no penalty for falsifying I mean even people who've gone through the first page of New York Times for publishing a biased review are allowed to be researchers five years later and publish continues such things so the the I feel the penalty right now is virtually zero for for biased publications you're right about that and the question is what is the right mix of "
    },
    {
        "start": 4581.56,
        "text": "rewards and penalties we have seen that both with empirical data and also with a simulation paper that I did with David Grimes where we used a system of eleven equations to model a universe of science that includes diligent scientists careless scientists and unethical scientists and we assumed that the vast majority are diligent but there are some careless in the beginning and very very few unethical because fraud I mean real fraud in intentional products is very uncommon what we see in the model is that unless you have both rewards for the diligent scientists and some penalties we're not doing good work you end up in a situation where the careless cohort takes over and eventually even the unethical corner may take over the diligent one because the diligent scientists have to put more work and more effort to get to the same point and you know others are cutting corners and publishing faster and more than their progeny is multiplying faster within the community so the right mix of "
    },
    {
        "start": 4643.989,
        "text": "penalties though is is very hard to determine and I think we need studies to see how to do it we know that whenever the replication of an original claim is debated there's a lot of pushback from the original authors and as I showed you many fields get entrenched you know the you have the strongest possible replication but they continue to exist they live parallel lives you know they they go into a different planetary system and they continue with their journals or professional societies whatever unconnected with with all the refute errs but now they have they're at their own life and they continuing funded and sometimes heavily so so some of the results that I showed you are by the teams that get the highest amount of dollars from NIH right now so you know nutrition signs that has been entirely refuted gets more funding than almost any other team that that I know of how do you have the right penalties and and how do we react as a scientific "
    },
    {
        "start": 4706.36,
        "text": "community at large I think there are open questions but the right mix is clearly urgently desirable to be identified the situation by using so it's it's an interesting possibility and I think that for many questions having multiple measurements is a plus and then the question is how much of the irreducibility is such that would be captured by the multiplicity of measurements if the problems are inherent to the measured environment then it's okay we will capture it once we measure more but if they're not you know we may have a situation where we get more data and we may end up in a situation like what I showed you with "
    },
    {
        "start": 4767.739,
        "text": "the medication associations where we have tons of there we get P values of 10 to the minus 100 power is not no longer an issue and we're we're tremendously overpowered in that situation but still we haven't solved the the basic problems which in that case is confounding in other cases maybe other biases that are not captured by the multiplicity of the repeated measurements so it's it's a it's useful but I don't I cannot say that it's a panacea that all the time does getting repeated measurements resolve every problem so a great talk most of the examples you gave of clinical studies were of diseases involving large populations studies where there might be hundreds of thousands of patients but the reality is in clinical medicine you know depending on how many diseases you acknowledge there might be most of them "
    },
    {
        "start": 4829.179,
        "text": "turn out to be rare diseases or orphan diseases so the you know must have criteria of hundreds if not thousands of subjects really turns into a would be nice to have and and so where were not only faced with smaller sample sizes but also if you're developing therapeutics the fact that regulatory agencies demand testing a null hypothesis so maybe you could could you comment on that and you're right about that sample size is not always easily available I would argue that in most of the situations where small studies predominate large sample size is possible and actually it would be more efficient to design these large studies rather than have these zillions of small studies do to give you one example for antidepressant trials "
    },
    {
        "start": 4890.469,
        "text": "the average trial has a sample size of about 60 participants the average licensing package has four or five of these trials but this is a disease that affects 5% of the population so you know we're talking about hundreds of millions of people potentially affect being affected and that's being possible to study if you look at all the antidepressant trials there's more than 120,000 participants that have been included but they have been fragmented across five hundred and twenty trials so instead of running all these micro trials I would argue maybe we could have joined forces and run some more definitive ones with with better chance of addressing the questions that we want there are diseases that are indeed rare and then you cannot get a hundred thousand obviously and you cannot even get a thousand in in these cases we have to work with the most efficient design and minimization of errors and biases I mean here the "
    },
    {
        "start": 4952.35,
        "text": "necessity to minimize bias and Aaron to use the optimal design is at its at its highest you know if you can allow a little bit of noise and error with a hundred thousand when you just have so limited samples I mean you really need to take the best shot so I think that having knowledgeable methodology statisticians data scientists and optimizing the design and getting to the highest level of certainty which may not be a hundred percent but it will be hopefully at a level where you can make decisions is even more important yeah so thanks very much for the talk so when you're comparing observational studies and clinical trials a gold standard and it's true that it does deal with found in the way of observations but if there's effect modification or interaction then there you know there "
    },
    {
        "start": 5014.61,
        "text": "may be discrepancies due to that as well and clinical trials very work very rarely worried about sort of the population sampling design and so forth observational studies may be someone less rarely so I just wonder some of the issues may be revolving around that and how you absolutely and randomized trials are not a gold standard they're a silver standard or bronze standard and in some aspects they have major problems if you look at some of the bread-and-butter randomized trials they have very serious problems so they may be vastly unreliable on their own but I tried to use examples where really these were extremely well-designed pragmatic careful efforts we had the best methodology involved protocols that were widely vetted and reviewed and so it's like the best that you can have effect modification and interactions may be "
    },
    {
        "start": 5074.79,
        "text": "there but if you look at the reproducibility of the literature on effect modification subgroup analysis interactions we've done several empirical projects in my team and many other method dollars have looked at that the reproducibility is literally horrible and I think there's an easy explanation if the the main effects have such major problems once you go to second grade or second level effects which is subgroups interactions effect modifications it's even more difficult you know you know all the problems that I mentioned become accentuated when you try to go after these more subtle but still very interesting effects so if we want to get reliable evidence on interactions effect modification subgroup differences and not get the situation like Richard Peto has shown that the zodiac sign is the strongest "
    },
    {
        "start": 5135.3,
        "text": "predictor in subgroup analysis for effect modification of and darker ectomy you know we need to really bolster our research on on all of these fronts to start having some genuine chance of hitting the right target thank you [Applause] "
    }
]