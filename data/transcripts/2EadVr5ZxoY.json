[
    {
        "text": "okay",
        "start": 0.9,
        "duration": 2.12
    },
    {
        "text": "we'll get started in just a couple",
        "start": 16.379,
        "duration": 2.581
    },
    {
        "text": "minutes",
        "start": 17.94,
        "duration": 3.56
    },
    {
        "text": "sure",
        "start": 18.96,
        "duration": 2.54
    },
    {
        "text": "management",
        "start": 60.899,
        "duration": 3.0
    },
    {
        "text": "that disease",
        "start": 66.72,
        "duration": 4.609
    },
    {
        "text": "[Music]",
        "start": 68.25,
        "duration": 3.079
    },
    {
        "text": "[Music]",
        "start": 73.56,
        "duration": 3.09
    },
    {
        "text": "thank you",
        "start": 82.04,
        "duration": 3.0
    },
    {
        "text": "very much",
        "start": 87.979,
        "duration": 3.601
    },
    {
        "text": "all right I think we're right at the",
        "start": 105.799,
        "duration": 3.82
    },
    {
        "text": "hour so I think we can go ahead and get",
        "start": 107.939,
        "duration": 4.021
    },
    {
        "text": "started uh this is Marcy I'm actually on",
        "start": 109.619,
        "duration": 3.96
    },
    {
        "text": "Zoom today so you might be hearing a",
        "start": 111.96,
        "duration": 3.839
    },
    {
        "text": "little voice in the sky for me but I",
        "start": 113.579,
        "duration": 4.08
    },
    {
        "text": "think you all for coming to today's",
        "start": 115.799,
        "duration": 4.801
    },
    {
        "text": "tools and Technology seminar series our",
        "start": 117.659,
        "duration": 5.941
    },
    {
        "text": "speaker is also on Zoom today",
        "start": 120.6,
        "duration": 4.68
    },
    {
        "text": "um and remote so if you're in the room",
        "start": 123.6,
        "duration": 3.0
    },
    {
        "text": "and you have questions you'll probably",
        "start": 125.28,
        "duration": 3.539
    },
    {
        "text": "need to speak up to get everyone's",
        "start": 126.6,
        "duration": 4.379
    },
    {
        "text": "attention if you are watching online",
        "start": 128.819,
        "duration": 4.261
    },
    {
        "text": "today you can put your questions in the",
        "start": 130.979,
        "duration": 4.62
    },
    {
        "text": "chat box I'll be monitoring that and can",
        "start": 133.08,
        "duration": 3.72
    },
    {
        "text": "let our speaker know if there are any",
        "start": 135.599,
        "duration": 3.481
    },
    {
        "text": "questions that come in or you can raise",
        "start": 136.8,
        "duration": 4.26
    },
    {
        "text": "your Zoom hand using the reactions at",
        "start": 139.08,
        "duration": 3.78
    },
    {
        "text": "the bottom right of your screen and we",
        "start": 141.06,
        "duration": 4.56
    },
    {
        "text": "can call on you to unmute",
        "start": 142.86,
        "duration": 4.379
    },
    {
        "text": "um so I don't think I have any other",
        "start": 145.62,
        "duration": 4.38
    },
    {
        "text": "announcements for today with that um I'd",
        "start": 147.239,
        "duration": 5.281
    },
    {
        "text": "like to introduce our speaker today uh",
        "start": 150.0,
        "duration": 4.5
    },
    {
        "text": "we have salar fatahi who is assistant",
        "start": 152.52,
        "duration": 3.96
    },
    {
        "text": "professor and Department of industrial",
        "start": 154.5,
        "duration": 5.04
    },
    {
        "text": "and operations engineering",
        "start": 156.48,
        "duration": 3.72
    },
    {
        "text": "um",
        "start": 159.54,
        "duration": 3.479
    },
    {
        "text": "thanks Marcy uh hello everyone uh",
        "start": 160.2,
        "duration": 4.74
    },
    {
        "text": "welcome to my presentation",
        "start": 163.019,
        "duration": 4.5
    },
    {
        "text": "um uh thank you Marcy for the kind",
        "start": 164.94,
        "duration": 5.579
    },
    {
        "text": "introduction and also for the invitation",
        "start": 167.519,
        "duration": 5.401
    },
    {
        "text": "um a little bit of background about me",
        "start": 170.519,
        "duration": 4.681
    },
    {
        "text": "I'm an assistant professor here at the",
        "start": 172.92,
        "duration": 4.2
    },
    {
        "text": "University of Michigan in the industrial",
        "start": 175.2,
        "duration": 4.819
    },
    {
        "text": "and operation engineering department",
        "start": 177.12,
        "duration": 6.479
    },
    {
        "text": "and my research focus is on developing",
        "start": 180.019,
        "duration": 5.681
    },
    {
        "text": "efficient algorithms and computational",
        "start": 183.599,
        "duration": 4.86
    },
    {
        "text": "methods for different",
        "start": 185.7,
        "duration": 3.38
    },
    {
        "text": "um",
        "start": 188.459,
        "duration": 4.441
    },
    {
        "text": "practical data-driven problems uh that",
        "start": 189.08,
        "duration": 6.34
    },
    {
        "text": "uh first of all are scalable meaning",
        "start": 192.9,
        "duration": 4.44
    },
    {
        "text": "that if we Implement them in meaningful",
        "start": 195.42,
        "duration": 6.179
    },
    {
        "text": "scale uh we that we can run it and at",
        "start": 197.34,
        "duration": 5.94
    },
    {
        "text": "the same time they should come with some",
        "start": 201.599,
        "duration": 3.42
    },
    {
        "text": "sort of approval guarantee meaning that",
        "start": 203.28,
        "duration": 4.26
    },
    {
        "text": "if I use them I know for sure that they",
        "start": 205.019,
        "duration": 3.561
    },
    {
        "text": "will",
        "start": 207.54,
        "duration": 3.96
    },
    {
        "text": "converge or they will recover a",
        "start": 208.58,
        "duration": 4.9
    },
    {
        "text": "desirable solution",
        "start": 211.5,
        "duration": 4.62
    },
    {
        "text": "so you can probably guess uh that my",
        "start": 213.48,
        "duration": 4.5
    },
    {
        "text": "research is sitting on the methodology",
        "start": 216.12,
        "duration": 5.16
    },
    {
        "text": "side but at the same time I'm uh craving",
        "start": 217.98,
        "duration": 5.88
    },
    {
        "text": "for some meaningful applications and in",
        "start": 221.28,
        "duration": 4.379
    },
    {
        "text": "fact this combination methodology",
        "start": 223.86,
        "duration": 4.379
    },
    {
        "text": "combined with uh meaningful application",
        "start": 225.659,
        "duration": 4.561
    },
    {
        "text": "led to This research that I'm going to",
        "start": 228.239,
        "duration": 4.56
    },
    {
        "text": "be talking about today so the title of",
        "start": 230.22,
        "duration": 5.64
    },
    {
        "text": "my talk is scalable learning of dynamic",
        "start": 232.799,
        "duration": 6.061
    },
    {
        "text": "graphical models Beyond maximum",
        "start": 235.86,
        "duration": 6.12
    },
    {
        "text": "likelihood estimation now",
        "start": 238.86,
        "duration": 5.54
    },
    {
        "text": "um before we get started I'd like to",
        "start": 241.98,
        "duration": 5.399
    },
    {
        "text": "acknowledge my collaborators this is a",
        "start": 244.4,
        "duration": 5.44
    },
    {
        "text": "joint workbit Professor Andres Gomez",
        "start": 247.379,
        "duration": 5.94
    },
    {
        "text": "from USC and our very own Professor",
        "start": 249.84,
        "duration": 6.6
    },
    {
        "text": "Arden Rao from biostatistics and",
        "start": 253.319,
        "duration": 6.901
    },
    {
        "text": "computational Medicine together with two",
        "start": 256.44,
        "duration": 6.479
    },
    {
        "text": "very strong PhD students uh vishwesh",
        "start": 260.22,
        "duration": 6.24
    },
    {
        "text": "Ravi Kumar and tongshu who used to be a",
        "start": 262.919,
        "duration": 5.641
    },
    {
        "text": "master's student working with me and now",
        "start": 266.46,
        "duration": 5.28
    },
    {
        "text": "he's a PhD student at Northwestern also",
        "start": 268.56,
        "duration": 6.8
    },
    {
        "text": "the research is sponsored by NSF ornr",
        "start": 271.74,
        "duration": 7.019
    },
    {
        "text": "mikti and Midas now",
        "start": 275.36,
        "duration": 4.059
    },
    {
        "text": "um",
        "start": 278.759,
        "duration": 2.701
    },
    {
        "text": "before we get started I just want to",
        "start": 279.419,
        "duration": 3.84
    },
    {
        "text": "give you a brief overview of the",
        "start": 281.46,
        "duration": 4.739
    },
    {
        "text": "structure of this talk first I will",
        "start": 283.259,
        "duration": 6.121
    },
    {
        "text": "briefly talk about the general problem",
        "start": 286.199,
        "duration": 6.121
    },
    {
        "text": "of dynamic Network inference especially",
        "start": 289.38,
        "duration": 5.7
    },
    {
        "text": "in the context of biological processes",
        "start": 292.32,
        "duration": 6.24
    },
    {
        "text": "then I will talk about perhaps the most",
        "start": 295.08,
        "duration": 5.82
    },
    {
        "text": "popular method to solve this problem",
        "start": 298.56,
        "duration": 4.62
    },
    {
        "text": "which is based on maximum likelihood",
        "start": 300.9,
        "duration": 5.22
    },
    {
        "text": "estimation or mle and I will also try to",
        "start": 303.18,
        "duration": 5.76
    },
    {
        "text": "shed some light on its shortcomings and",
        "start": 306.12,
        "duration": 4.859
    },
    {
        "text": "then I will talk about our proposed",
        "start": 308.94,
        "duration": 4.14
    },
    {
        "text": "method and finally we will see some",
        "start": 310.979,
        "duration": 4.561
    },
    {
        "text": "experimental results on gene expression",
        "start": 313.08,
        "duration": 6.119
    },
    {
        "text": "data now as you probably guessed I am by",
        "start": 315.54,
        "duration": 5.659
    },
    {
        "text": "no means an expert in biology or",
        "start": 319.199,
        "duration": 5.94
    },
    {
        "text": "biostatistics so throughout my talk if",
        "start": 321.199,
        "duration": 5.381
    },
    {
        "text": "you have any questions related to the",
        "start": 325.139,
        "duration": 3.601
    },
    {
        "text": "methodology or the algorithm I would be",
        "start": 326.58,
        "duration": 4.32
    },
    {
        "text": "more than happy to answer I could also",
        "start": 328.74,
        "duration": 4.019
    },
    {
        "text": "try to answer your questions on the",
        "start": 330.9,
        "duration": 4.68
    },
    {
        "text": "application side but I would not trust",
        "start": 332.759,
        "duration": 4.461
    },
    {
        "text": "my own answers",
        "start": 335.58,
        "duration": 5.22
    },
    {
        "text": "on that front but we'll see Team all",
        "start": 337.22,
        "duration": 5.979
    },
    {
        "text": "right first uh",
        "start": 340.8,
        "duration": 4.679
    },
    {
        "text": "I'm going to start with some motivating",
        "start": 343.199,
        "duration": 3.601
    },
    {
        "text": "applications",
        "start": 345.479,
        "duration": 3.961
    },
    {
        "text": "um so as we all know many Real World",
        "start": 346.8,
        "duration": 3.839
    },
    {
        "text": "Systems",
        "start": 349.44,
        "duration": 2.94
    },
    {
        "text": "um have some underlying structure that",
        "start": 350.639,
        "duration": 4.921
    },
    {
        "text": "changes over time or space in some of",
        "start": 352.38,
        "duration": 4.92
    },
    {
        "text": "these problems the underlying structure",
        "start": 355.56,
        "duration": 3.78
    },
    {
        "text": "of the system is actually unknown for us",
        "start": 357.3,
        "duration": 4.32
    },
    {
        "text": "and all we have is a limited number of",
        "start": 359.34,
        "duration": 4.199
    },
    {
        "text": "potentially High dimensional and",
        "start": 361.62,
        "duration": 3.96
    },
    {
        "text": "randomized or noisy samples or",
        "start": 363.539,
        "duration": 4.321
    },
    {
        "text": "measurements for example it is well",
        "start": 365.58,
        "duration": 4.02
    },
    {
        "text": "known that different brain regions",
        "start": 367.86,
        "duration": 3.839
    },
    {
        "text": "interact with one another in response to",
        "start": 369.6,
        "duration": 4.68
    },
    {
        "text": "different physical or or mental",
        "start": 371.699,
        "duration": 5.78
    },
    {
        "text": "activities but all we have is the fmri",
        "start": 374.28,
        "duration": 5.96
    },
    {
        "text": "data that is collected from the brain",
        "start": 377.479,
        "duration": 5.44
    },
    {
        "text": "now understanding this underlying brain",
        "start": 380.24,
        "duration": 4.42
    },
    {
        "text": "connectivity network is very important",
        "start": 382.919,
        "duration": 4.381
    },
    {
        "text": "specifically for the early discovery of",
        "start": 384.66,
        "duration": 4.62
    },
    {
        "text": "different brain pathologies such as",
        "start": 387.3,
        "duration": 4.619
    },
    {
        "text": "Alzheimer but this problem is very large",
        "start": 389.28,
        "duration": 5.28
    },
    {
        "text": "scale it is a huge scale problem in fact",
        "start": 391.919,
        "duration": 5.941
    },
    {
        "text": "the full brain image I believe has more",
        "start": 394.56,
        "duration": 6.12
    },
    {
        "text": "than 200 000 voxels which implies",
        "start": 397.86,
        "duration": 5.64
    },
    {
        "text": "billions of potential links uh between",
        "start": 400.68,
        "duration": 5.84
    },
    {
        "text": "these water Waters or or",
        "start": 403.5,
        "duration": 5.96
    },
    {
        "text": "brain Networks",
        "start": 406.52,
        "duration": 7.019
    },
    {
        "text": "and this is also well known",
        "start": 409.46,
        "duration": 4.079
    },
    {
        "text": "that the the underlying brain",
        "start": 413.72,
        "duration": 5.979
    },
    {
        "text": "connectivity Network changes with age",
        "start": 417.479,
        "duration": 5.641
    },
    {
        "text": "and maturity uh for example uh here you",
        "start": 419.699,
        "duration": 6.541
    },
    {
        "text": "can see uh that the level of gray matter",
        "start": 423.12,
        "duration": 5.519
    },
    {
        "text": "in the brain changes with age so instead",
        "start": 426.24,
        "duration": 4.14
    },
    {
        "text": "of learning one static connectivity",
        "start": 428.639,
        "duration": 3.961
    },
    {
        "text": "Network we essentially need to learn a",
        "start": 430.38,
        "duration": 4.259
    },
    {
        "text": "sequence of dynamic connectivity",
        "start": 432.6,
        "duration": 4.379
    },
    {
        "text": "networks and that potentially changes",
        "start": 434.639,
        "duration": 4.5
    },
    {
        "text": "over time to better understand the",
        "start": 436.979,
        "duration": 5.421
    },
    {
        "text": "behavior of the brain with age",
        "start": 439.139,
        "duration": 5.881
    },
    {
        "text": "and another application is in the gene",
        "start": 442.4,
        "duration": 5.139
    },
    {
        "text": "regulatory networks here given the gene",
        "start": 445.02,
        "duration": 5.16
    },
    {
        "text": "expression data the goal is to infer a",
        "start": 447.539,
        "duration": 4.38
    },
    {
        "text": "network that describes the correlation",
        "start": 450.18,
        "duration": 4.019
    },
    {
        "text": "between different between the activity",
        "start": 451.919,
        "duration": 4.521
    },
    {
        "text": "of different uh",
        "start": 454.199,
        "duration": 6.181
    },
    {
        "text": "genes within each cell but similar to",
        "start": 456.44,
        "duration": 6.28
    },
    {
        "text": "the brain connectivity networks Gene",
        "start": 460.38,
        "duration": 4.439
    },
    {
        "text": "regulatory networks are also larger",
        "start": 462.72,
        "duration": 4.379
    },
    {
        "text": "scale and they change over time or space",
        "start": 464.819,
        "duration": 4.32
    },
    {
        "text": "in response to different environment",
        "start": 467.099,
        "duration": 5.16
    },
    {
        "text": "mental or physiological disorders",
        "start": 469.139,
        "duration": 6.18
    },
    {
        "text": "or different diseases like cancer for",
        "start": 472.259,
        "duration": 7.321
    },
    {
        "text": "example here on the left you can see the",
        "start": 475.319,
        "duration": 5.88
    },
    {
        "text": "changes in the gene regulatory Network",
        "start": 479.58,
        "duration": 4.2
    },
    {
        "text": "for a specific cell as the patient",
        "start": 481.199,
        "duration": 5.581
    },
    {
        "text": "develops cancer here the first figure",
        "start": 483.78,
        "duration": 5.639
    },
    {
        "text": "figure a shows the regulatory Network",
        "start": 486.78,
        "duration": 4.979
    },
    {
        "text": "for a normal cell and the last figure",
        "start": 489.419,
        "duration": 4.68
    },
    {
        "text": "figure D shows the regulatory Network",
        "start": 491.759,
        "duration": 4.741
    },
    {
        "text": "for the same cell that has developed",
        "start": 494.099,
        "duration": 7.081
    },
    {
        "text": "breast cancer so you can uh you can see",
        "start": 496.5,
        "duration": 6.539
    },
    {
        "text": "that we can have both of these networks",
        "start": 501.18,
        "duration": 3.78
    },
    {
        "text": "at different times in a single patient",
        "start": 503.039,
        "duration": 4.201
    },
    {
        "text": "or even at the same time but in",
        "start": 504.96,
        "duration": 5.1
    },
    {
        "text": "different cells in fact we'll see an",
        "start": 507.24,
        "duration": 5.4
    },
    {
        "text": "application of this later in this talk",
        "start": 510.06,
        "duration": 5.64
    },
    {
        "text": "so in all of these applications our goal",
        "start": 512.64,
        "duration": 5.1
    },
    {
        "text": "is to go from the collected data or",
        "start": 515.7,
        "duration": 4.92
    },
    {
        "text": "measurements to a sequence of dynamic",
        "start": 517.74,
        "duration": 5.039
    },
    {
        "text": "graphical models that describe this",
        "start": 520.62,
        "duration": 4.56
    },
    {
        "text": "dependency structure in the data now one",
        "start": 522.779,
        "duration": 4.141
    },
    {
        "text": "popular approach to do that is to use",
        "start": 525.18,
        "duration": 3.96
    },
    {
        "text": "the undirected graphical models or",
        "start": 526.92,
        "duration": 5.099
    },
    {
        "text": "Markov random fuels where each variable",
        "start": 529.14,
        "duration": 4.92
    },
    {
        "text": "is defined as a node in the graph and",
        "start": 532.019,
        "duration": 4.141
    },
    {
        "text": "the correlation or dependencies between",
        "start": 534.06,
        "duration": 4.56
    },
    {
        "text": "different nodes are captured by weighted",
        "start": 536.16,
        "duration": 6.179
    },
    {
        "text": "edges in the graphical model so for",
        "start": 538.62,
        "duration": 5.88
    },
    {
        "text": "example if you have three nodes or three",
        "start": 542.339,
        "duration": 3.841
    },
    {
        "text": "variables their model has three nodes in",
        "start": 544.5,
        "duration": 3.72
    },
    {
        "text": "the graph and if there is a non-zero",
        "start": 546.18,
        "duration": 4.38
    },
    {
        "text": "correlation between these variables that",
        "start": 548.22,
        "duration": 4.739
    },
    {
        "text": "are captured by weighted edges in the",
        "start": 550.56,
        "duration": 4.38
    },
    {
        "text": "dynamical setting understanding this",
        "start": 552.959,
        "duration": 4.741
    },
    {
        "text": "dependency structure essentially boils",
        "start": 554.94,
        "duration": 6.12
    },
    {
        "text": "down to estimating a sequence of dynamic",
        "start": 557.7,
        "duration": 5.759
    },
    {
        "text": "Markov random fields for example these",
        "start": 561.06,
        "duration": 3.839
    },
    {
        "text": "Markov random Fields can change",
        "start": 563.459,
        "duration": 3.961
    },
    {
        "text": "spatially over space here you can you",
        "start": 564.899,
        "duration": 5.341
    },
    {
        "text": "can see one example and for example",
        "start": 567.42,
        "duration": 6.78
    },
    {
        "text": "across different cells or over time with",
        "start": 570.24,
        "duration": 6.0
    },
    {
        "text": "age with maturity or in response to",
        "start": 574.2,
        "duration": 6.36
    },
    {
        "text": "different uh exogenous events",
        "start": 576.24,
        "duration": 7.32
    },
    {
        "text": "now uh one popular method to do that is",
        "start": 580.56,
        "duration": 4.56
    },
    {
        "text": "based on the so-called maximum",
        "start": 583.56,
        "duration": 4.2
    },
    {
        "text": "likelihood estimation or MLB where the",
        "start": 585.12,
        "duration": 4.8
    },
    {
        "text": "goal is to essentially find a graphical",
        "start": 587.76,
        "duration": 5.1
    },
    {
        "text": "model based on which the observed data",
        "start": 589.92,
        "duration": 5.039
    },
    {
        "text": "or the measurement is most probable to",
        "start": 592.86,
        "duration": 4.62
    },
    {
        "text": "occur now if you want to also",
        "start": 594.959,
        "duration": 4.201
    },
    {
        "text": "incorporate some prior or site",
        "start": 597.48,
        "duration": 3.539
    },
    {
        "text": "information about the problems such as",
        "start": 599.16,
        "duration": 3.72
    },
    {
        "text": "sparseity smoothness or any other",
        "start": 601.019,
        "duration": 5.88
    },
    {
        "text": "structure such as locality we can add",
        "start": 602.88,
        "duration": 7.019
    },
    {
        "text": "different regularizers to the mle method",
        "start": 606.899,
        "duration": 4.801
    },
    {
        "text": "to promote these types of structures in",
        "start": 609.899,
        "duration": 5.041
    },
    {
        "text": "the estimated model now the regularized",
        "start": 611.7,
        "duration": 6.3
    },
    {
        "text": "version of the mle approach is probably",
        "start": 614.94,
        "duration": 5.579
    },
    {
        "text": "one of the most commonly used inference",
        "start": 618.0,
        "duration": 4.26
    },
    {
        "text": "methods for graphical models and really",
        "start": 620.519,
        "duration": 5.161
    },
    {
        "text": "for for General inference problems so",
        "start": 622.26,
        "duration": 5.699
    },
    {
        "text": "we're going to start with that in fact",
        "start": 625.68,
        "duration": 4.8
    },
    {
        "text": "to better explain this let's consider a",
        "start": 627.959,
        "duration": 4.261
    },
    {
        "text": "very well-known instance of the problem",
        "start": 630.48,
        "duration": 4.08
    },
    {
        "text": "which is the gaussian Markov random",
        "start": 632.22,
        "duration": 4.26
    },
    {
        "text": "Fields now here we assume that the",
        "start": 634.56,
        "duration": 3.42
    },
    {
        "text": "collected data is coming from a",
        "start": 636.48,
        "duration": 3.66
    },
    {
        "text": "multivariate distribution that changes",
        "start": 637.98,
        "duration": 5.28
    },
    {
        "text": "over time and space and now in this",
        "start": 640.14,
        "duration": 5.939
    },
    {
        "text": "context the regularized mle boils down",
        "start": 643.26,
        "duration": 4.139
    },
    {
        "text": "to the to solving the following",
        "start": 646.079,
        "duration": 3.601
    },
    {
        "text": "optimization problem given a bunch of",
        "start": 647.399,
        "duration": 4.56
    },
    {
        "text": "samples that are collected over time and",
        "start": 649.68,
        "duration": 4.14
    },
    {
        "text": "space first we find the sample",
        "start": 651.959,
        "duration": 4.681
    },
    {
        "text": "covariance Matrix uh we showed that with",
        "start": 653.82,
        "duration": 6.24
    },
    {
        "text": "Sigma s and t and then find the fault",
        "start": 656.64,
        "duration": 4.62
    },
    {
        "text": "and then solve the following",
        "start": 660.06,
        "duration": 3.48
    },
    {
        "text": "optimization problem right it is a",
        "start": 661.26,
        "duration": 4.5
    },
    {
        "text": "minimization over a sequence of Matrix",
        "start": 663.54,
        "duration": 4.56
    },
    {
        "text": "variables and the objective has four",
        "start": 665.76,
        "duration": 3.54
    },
    {
        "text": "terms",
        "start": 668.1,
        "duration": 3.9
    },
    {
        "text": "the first term here corresponds to the",
        "start": 669.3,
        "duration": 4.8
    },
    {
        "text": "maximum likelihood estimation of the",
        "start": 672.0,
        "duration": 4.019
    },
    {
        "text": "inverse covariance Matrix when the",
        "start": 674.1,
        "duration": 3.96
    },
    {
        "text": "samples are coming from zero me gaussian",
        "start": 676.019,
        "duration": 4.56
    },
    {
        "text": "distribution now the other three terms",
        "start": 678.06,
        "duration": 5.16
    },
    {
        "text": "are regularizers that are controlled by",
        "start": 680.579,
        "duration": 4.621
    },
    {
        "text": "the regularization coefficients beta 1",
        "start": 683.22,
        "duration": 5.52
    },
    {
        "text": "beta 2 and better three and they have uh",
        "start": 685.2,
        "duration": 5.4
    },
    {
        "text": "the role of promoting special structures",
        "start": 688.74,
        "duration": 3.599
    },
    {
        "text": "in the estimated inverse covariance",
        "start": 690.6,
        "duration": 4.22
    },
    {
        "text": "matrices for example in individual",
        "start": 692.339,
        "duration": 5.041
    },
    {
        "text": "parameters or individual elements or",
        "start": 694.82,
        "duration": 5.98
    },
    {
        "text": "their temporal or spatial changes now",
        "start": 697.38,
        "duration": 5.399
    },
    {
        "text": "why do we care about the inversible",
        "start": 700.8,
        "duration": 3.719
    },
    {
        "text": "range Matrix well in the gaussian",
        "start": 702.779,
        "duration": 2.881
    },
    {
        "text": "setting",
        "start": 704.519,
        "duration": 2.94
    },
    {
        "text": "um they have a very nice interpretation",
        "start": 705.66,
        "duration": 4.679
    },
    {
        "text": "the zero non-zero element of this",
        "start": 707.459,
        "duration": 5.161
    },
    {
        "text": "inverse covariance Matrix or Precision",
        "start": 710.339,
        "duration": 7.041
    },
    {
        "text": "Matrix at any time or or uh location",
        "start": 712.62,
        "duration": 7.86
    },
    {
        "text": "reveals the support City pattern of the",
        "start": 717.38,
        "duration": 6.34
    },
    {
        "text": "graphical model and the regularization",
        "start": 720.48,
        "duration": 4.62
    },
    {
        "text": "on the inverse covariance Matrix",
        "start": 723.72,
        "duration": 4.26
    },
    {
        "text": "actually enables us to promote different",
        "start": 725.1,
        "duration": 4.26
    },
    {
        "text": "structures",
        "start": 727.98,
        "duration": 4.32
    },
    {
        "text": "in the corresponding graphical model",
        "start": 729.36,
        "duration": 5.88
    },
    {
        "text": "for example we know many graphical",
        "start": 732.3,
        "duration": 5.219
    },
    {
        "text": "models have localized or Sports",
        "start": 735.24,
        "duration": 5.82
    },
    {
        "text": "structures this can be captured with an",
        "start": 737.519,
        "duration": 6.06
    },
    {
        "text": "l0 regularizer in the objective what is",
        "start": 741.06,
        "duration": 5.58
    },
    {
        "text": "l0 l0 basically penalizes the number of",
        "start": 743.579,
        "duration": 5.94
    },
    {
        "text": "non-zero elements in this Theta or the",
        "start": 746.64,
        "duration": 5.819
    },
    {
        "text": "Precision Matrix so if you have a lot of",
        "start": 749.519,
        "duration": 4.801
    },
    {
        "text": "non-zero elements that penalty is going",
        "start": 752.459,
        "duration": 4.38
    },
    {
        "text": "to be large we can also add different",
        "start": 754.32,
        "duration": 5.639
    },
    {
        "text": "temporal or spatial regularizations in",
        "start": 756.839,
        "duration": 5.341
    },
    {
        "text": "the mle formulation for example some",
        "start": 759.959,
        "duration": 5.041
    },
    {
        "text": "graphical models have a sparsely",
        "start": 762.18,
        "duration": 5.279
    },
    {
        "text": "changing structure meaning that only a",
        "start": 765.0,
        "duration": 5.76
    },
    {
        "text": "few edges change over time or space in",
        "start": 767.459,
        "duration": 5.341
    },
    {
        "text": "that case we can also promote sparsity",
        "start": 770.76,
        "duration": 5.28
    },
    {
        "text": "with an l0 regularizer on the temporal",
        "start": 772.8,
        "duration": 6.08
    },
    {
        "text": "or or spatial differences of the",
        "start": 776.04,
        "duration": 6.599
    },
    {
        "text": "parameters some other graphical models",
        "start": 778.88,
        "duration": 6.28
    },
    {
        "text": "such as brain networks for example they",
        "start": 782.639,
        "duration": 4.26
    },
    {
        "text": "change smoothly over time we don't have",
        "start": 785.16,
        "duration": 4.619
    },
    {
        "text": "abrupt changes in the graphical model in",
        "start": 786.899,
        "duration": 4.861
    },
    {
        "text": "that case for example we can use L2",
        "start": 789.779,
        "duration": 4.5
    },
    {
        "text": "penalty on the differences and there are",
        "start": 791.76,
        "duration": 4.56
    },
    {
        "text": "some other penalty structures that you",
        "start": 794.279,
        "duration": 3.201
    },
    {
        "text": "can see here",
        "start": 796.32,
        "duration": 4.5
    },
    {
        "text": "now let's just consider the special case",
        "start": 797.48,
        "duration": 5.44
    },
    {
        "text": "of sparsely changing gaussian Markov",
        "start": 800.82,
        "duration": 4.62
    },
    {
        "text": "random Jewels uh where each inverse",
        "start": 802.92,
        "duration": 4.5
    },
    {
        "text": "covariance Matrix we assume it is sparse",
        "start": 805.44,
        "duration": 5.519
    },
    {
        "text": "and it also changes sparsely over time",
        "start": 807.42,
        "duration": 7.32
    },
    {
        "text": "we don't have a space component here",
        "start": 810.959,
        "duration": 6.481
    },
    {
        "text": "just for Simplicity in this case the",
        "start": 814.74,
        "duration": 4.8
    },
    {
        "text": "regularized mle corresponds to this",
        "start": 817.44,
        "duration": 4.579
    },
    {
        "text": "highly non-linear and in fact",
        "start": 819.54,
        "duration": 5.76
    },
    {
        "text": "combinatorial optimization problem it is",
        "start": 822.019,
        "duration": 5.861
    },
    {
        "text": "non-linear because of this strange log",
        "start": 825.3,
        "duration": 5.7
    },
    {
        "text": "debt term and it is combinatorial",
        "start": 827.88,
        "duration": 4.8
    },
    {
        "text": "because you're essentially optimizing",
        "start": 831.0,
        "duration": 4.38
    },
    {
        "text": "over zero non-zero pattern so if you're",
        "start": 832.68,
        "duration": 4.86
    },
    {
        "text": "suddenly faced with this cursive",
        "start": 835.38,
        "duration": 3.959
    },
    {
        "text": "dimensionality because a search space",
        "start": 837.54,
        "duration": 4.62
    },
    {
        "text": "will grow exponentially right now",
        "start": 839.339,
        "duration": 4.5
    },
    {
        "text": "although these regularized mle",
        "start": 842.16,
        "duration": 4.32
    },
    {
        "text": "approaches enjoy very strong statistical",
        "start": 843.839,
        "duration": 7.041
    },
    {
        "text": "guarantees it is classically conjectured",
        "start": 846.48,
        "duration": 7.56
    },
    {
        "text": "that they are very hard to solve they're",
        "start": 850.88,
        "duration": 6.34
    },
    {
        "text": "intractable to solve now to get rid of",
        "start": 854.04,
        "duration": 5.76
    },
    {
        "text": "this combinatorial nature the the common",
        "start": 857.22,
        "duration": 4.559
    },
    {
        "text": "approach is to basically replace l0",
        "start": 859.8,
        "duration": 5.219
    },
    {
        "text": "penalty with its L1 relaxation which is",
        "start": 861.779,
        "duration": 5.761
    },
    {
        "text": "continuous and Comics this is the",
        "start": 865.019,
        "duration": 4.201
    },
    {
        "text": "underlying idea behind several",
        "start": 867.54,
        "duration": 4.14
    },
    {
        "text": "well-known methods like lasso or",
        "start": 869.22,
        "duration": 5.359
    },
    {
        "text": "graphical lasso we can handle the",
        "start": 871.68,
        "duration": 6.12
    },
    {
        "text": "combinatorial nature of l0 so we just",
        "start": 874.579,
        "duration": 5.2
    },
    {
        "text": "relax it to L1 and just hope for the",
        "start": 877.8,
        "duration": 4.92
    },
    {
        "text": "best right uh now uh with this",
        "start": 879.779,
        "duration": 4.74
    },
    {
        "text": "relaxation the the problem becomes",
        "start": 882.72,
        "duration": 4.619
    },
    {
        "text": "convex but the issue is that it will",
        "start": 884.519,
        "duration": 4.741
    },
    {
        "text": "have inferior statistical guarantees",
        "start": 887.339,
        "duration": 4.5
    },
    {
        "text": "especially in the dynamical setting",
        "start": 889.26,
        "duration": 4.68
    },
    {
        "text": "now let's see the performance of this",
        "start": 891.839,
        "duration": 5.221
    },
    {
        "text": "relaxed regularized mle approach on a",
        "start": 893.94,
        "duration": 5.339
    },
    {
        "text": "very simple case study suppose that here",
        "start": 897.06,
        "duration": 5.519
    },
    {
        "text": "we have only four time steps so T is",
        "start": 899.279,
        "duration": 5.461
    },
    {
        "text": "equal to four also each inverse",
        "start": 902.579,
        "duration": 4.56
    },
    {
        "text": "covariance Matrix is 25 by 25 so it's",
        "start": 904.74,
        "duration": 4.92
    },
    {
        "text": "relatively small instance the number of",
        "start": 907.139,
        "duration": 4.14
    },
    {
        "text": "non-zero elements in the inverse",
        "start": 909.66,
        "duration": 4.32
    },
    {
        "text": "covariance Matrix at any given time is",
        "start": 911.279,
        "duration": 5.101
    },
    {
        "text": "is we assume that it's a hundred that's",
        "start": 913.98,
        "duration": 4.5
    },
    {
        "text": "how we design it so they're relatively",
        "start": 916.38,
        "duration": 5.579
    },
    {
        "text": "sparse and uh the number of changes in",
        "start": 918.48,
        "duration": 6.06
    },
    {
        "text": "the inverse covariance Matrix is limited",
        "start": 921.959,
        "duration": 5.281
    },
    {
        "text": "to 10 so it is sparsely changing this is",
        "start": 924.54,
        "duration": 4.799
    },
    {
        "text": "an example of a sparsely changing",
        "start": 927.24,
        "duration": 5.82
    },
    {
        "text": "gaussian Markov random field so um here",
        "start": 929.339,
        "duration": 7.381
    },
    {
        "text": "on the left uh um you what you you see",
        "start": 933.06,
        "duration": 5.82
    },
    {
        "text": "is the mismatch error or the number of",
        "start": 936.72,
        "duration": 4.32
    },
    {
        "text": "mistakes in the estimated parameters",
        "start": 938.88,
        "duration": 4.8
    },
    {
        "text": "using the relaxed regularized ml mle",
        "start": 941.04,
        "duration": 5.039
    },
    {
        "text": "that I just mentioned now you can see",
        "start": 943.68,
        "duration": 4.98
    },
    {
        "text": "that even with an exhaustive surge over",
        "start": 946.079,
        "duration": 5.101
    },
    {
        "text": "the regularization coefficients beta one",
        "start": 948.66,
        "duration": 5.28
    },
    {
        "text": "and better two the best mismatch error",
        "start": 951.18,
        "duration": 5.399
    },
    {
        "text": "we can get a 70 meaning that the",
        "start": 953.94,
        "duration": 5.339
    },
    {
        "text": "estimated Matrix will have 70 mistakes",
        "start": 956.579,
        "duration": 6.0
    },
    {
        "text": "in its sparsity pattern uh the right",
        "start": 959.279,
        "duration": 5.161
    },
    {
        "text": "figure here shows the value of the",
        "start": 962.579,
        "duration": 4.801
    },
    {
        "text": "non-zero elements or the true Precision",
        "start": 964.44,
        "duration": 5.459
    },
    {
        "text": "Matrix and the estimated one at any",
        "start": 967.38,
        "duration": 5.22
    },
    {
        "text": "given time you can see that this this",
        "start": 969.899,
        "duration": 5.581
    },
    {
        "text": "red curve is the true values that we",
        "start": 972.6,
        "duration": 5.34
    },
    {
        "text": "want to estimate uh the the blue curve",
        "start": 975.48,
        "duration": 4.38
    },
    {
        "text": "is the is the value that we estimate",
        "start": 977.94,
        "duration": 4.019
    },
    {
        "text": "using regularized mle you can see that",
        "start": 979.86,
        "duration": 4.38
    },
    {
        "text": "there's a huge bias in our estimation",
        "start": 981.959,
        "duration": 4.32
    },
    {
        "text": "this is because of the shrinking effect",
        "start": 984.24,
        "duration": 4.74
    },
    {
        "text": "of the L1 regularizer",
        "start": 986.279,
        "duration": 5.761
    },
    {
        "text": "okay now let me give you an overview of",
        "start": 988.98,
        "duration": 7.919
    },
    {
        "text": "uh what we have seen so far uh uh as uh",
        "start": 992.04,
        "duration": 7.02
    },
    {
        "text": "we mentioned uh if you want to have",
        "start": 996.899,
        "duration": 4.5
    },
    {
        "text": "statistical efficiency we typically need",
        "start": 999.06,
        "duration": 5.76
    },
    {
        "text": "to solve a combinatorial regularized",
        "start": 1001.399,
        "duration": 5.041
    },
    {
        "text": "maximum likelihood estimation for",
        "start": 1004.82,
        "duration": 3.84
    },
    {
        "text": "example if you want to impose Sports in",
        "start": 1006.44,
        "duration": 3.839
    },
    {
        "text": "our model the best choice would be",
        "start": 1008.66,
        "duration": 5.76
    },
    {
        "text": "regularization uh with L1 l0 penalty uh",
        "start": 1010.279,
        "duration": 7.381
    },
    {
        "text": "but uh the issue is most of these uh",
        "start": 1014.42,
        "duration": 4.979
    },
    {
        "text": "problems are intractable because of",
        "start": 1017.66,
        "duration": 4.979
    },
    {
        "text": "their search space the search space",
        "start": 1019.399,
        "duration": 5.461
    },
    {
        "text": "grows exponentially so in order to get",
        "start": 1022.639,
        "duration": 4.2
    },
    {
        "text": "computational efficiency we relaxed",
        "start": 1024.86,
        "duration": 5.339
    },
    {
        "text": "regularization to arrive at a convex or",
        "start": 1026.839,
        "duration": 6.36
    },
    {
        "text": "continuous uh regularized mle for",
        "start": 1030.199,
        "duration": 6.541
    },
    {
        "text": "example we relax l0 to L1 but we showed",
        "start": 1033.199,
        "duration": 5.281
    },
    {
        "text": "that this these methods often do not",
        "start": 1036.74,
        "duration": 4.559
    },
    {
        "text": "come with a strong statistical guarantee",
        "start": 1038.48,
        "duration": 6.959
    },
    {
        "text": "now our goal is to introduce a method",
        "start": 1041.299,
        "duration": 6.481
    },
    {
        "text": "that achieves The Best of Both Worlds it",
        "start": 1045.439,
        "duration": 4.561
    },
    {
        "text": "can efficiently handle the combinatorial",
        "start": 1047.78,
        "duration": 4.019
    },
    {
        "text": "structure and or the combinatorial",
        "start": 1050.0,
        "duration": 3.72
    },
    {
        "text": "nature of the problem without any",
        "start": 1051.799,
        "duration": 5.221
    },
    {
        "text": "relaxation and at the same time we want",
        "start": 1053.72,
        "duration": 5.4
    },
    {
        "text": "it to come with a strong statistical",
        "start": 1057.02,
        "duration": 5.1
    },
    {
        "text": "guarantees now the key idea behind our",
        "start": 1059.12,
        "duration": 6.0
    },
    {
        "text": "method is to replace this mle with a",
        "start": 1062.12,
        "duration": 5.88
    },
    {
        "text": "more tractable optimization problem now",
        "start": 1065.12,
        "duration": 5.46
    },
    {
        "text": "before talking about our key results I",
        "start": 1068.0,
        "duration": 4.5
    },
    {
        "text": "just want to say a few words about the",
        "start": 1070.58,
        "duration": 4.74
    },
    {
        "text": "tractability of any optimization problem",
        "start": 1072.5,
        "duration": 6.0
    },
    {
        "text": "and it's computational efficiency",
        "start": 1075.32,
        "duration": 5.82
    },
    {
        "text": "now it is a conventional wisdom in",
        "start": 1078.5,
        "duration": 4.679
    },
    {
        "text": "optimization theory that computational",
        "start": 1081.14,
        "duration": 4.5
    },
    {
        "text": "methods with exponential dependency in",
        "start": 1083.179,
        "duration": 5.761
    },
    {
        "text": "the dimension time memory or data are",
        "start": 1085.64,
        "duration": 5.7
    },
    {
        "text": "both theoretically and practically",
        "start": 1088.94,
        "duration": 5.099
    },
    {
        "text": "inefficient to use and Implement on the",
        "start": 1091.34,
        "duration": 5.579
    },
    {
        "text": "other hand uh there is this class of",
        "start": 1094.039,
        "duration": 5.64
    },
    {
        "text": "methods with polynomial scaling that are",
        "start": 1096.919,
        "duration": 4.14
    },
    {
        "text": "known to be at least theoretically",
        "start": 1099.679,
        "duration": 4.74
    },
    {
        "text": "efficient to use but even these",
        "start": 1101.059,
        "duration": 5.281
    },
    {
        "text": "so-called polynomial methods can quickly",
        "start": 1104.419,
        "duration": 3.901
    },
    {
        "text": "become intractable with increasing skill",
        "start": 1106.34,
        "duration": 3.959
    },
    {
        "text": "of the problem for example regularized",
        "start": 1108.32,
        "duration": 5.099
    },
    {
        "text": "mle belongs to this class so within the",
        "start": 1110.299,
        "duration": 5.521
    },
    {
        "text": "realm of binomial methods really the",
        "start": 1113.419,
        "duration": 4.561
    },
    {
        "text": "only class of algorithms that are truly",
        "start": 1115.82,
        "duration": 4.38
    },
    {
        "text": "scalable to massive problems in genomics",
        "start": 1117.98,
        "duration": 4.38
    },
    {
        "text": "and brain networks with millions or",
        "start": 1120.2,
        "duration": 4.44
    },
    {
        "text": "billions of parameters are those that",
        "start": 1122.36,
        "duration": 4.5
    },
    {
        "text": "scale near linearly with respect to the",
        "start": 1124.64,
        "duration": 3.96
    },
    {
        "text": "dimension of the problem so the",
        "start": 1126.86,
        "duration": 3.54
    },
    {
        "text": "fundamental question is how can we",
        "start": 1128.6,
        "duration": 3.6
    },
    {
        "text": "design computational methods for the",
        "start": 1130.4,
        "duration": 3.6
    },
    {
        "text": "inference specifically for the inference",
        "start": 1132.2,
        "duration": 4.859
    },
    {
        "text": "of dynamic Markov random fields that are",
        "start": 1134.0,
        "duration": 6.36
    },
    {
        "text": "almost linear they scale linearly with",
        "start": 1137.059,
        "duration": 5.941
    },
    {
        "text": "time memory and data",
        "start": 1140.36,
        "duration": 5.22
    },
    {
        "text": "now I'm going to try to summarize our",
        "start": 1143.0,
        "duration": 5.64
    },
    {
        "text": "key results in in only one slide this is",
        "start": 1145.58,
        "duration": 4.74
    },
    {
        "text": "probably the most important slide of my",
        "start": 1148.64,
        "duration": 3.6
    },
    {
        "text": "talk so in case you need to stop",
        "start": 1150.32,
        "duration": 3.479
    },
    {
        "text": "listening this would be the ideal place",
        "start": 1152.24,
        "duration": 4.14
    },
    {
        "text": "for that the rest of my presentation",
        "start": 1153.799,
        "duration": 5.76
    },
    {
        "text": "will be to uh to clarify some of the",
        "start": 1156.38,
        "duration": 5.52
    },
    {
        "text": "points I'm going to make next okay so",
        "start": 1159.559,
        "duration": 4.921
    },
    {
        "text": "our first key result is that unlike the",
        "start": 1161.9,
        "duration": 5.399
    },
    {
        "text": "conventional wisdom uh we can in fact",
        "start": 1164.48,
        "duration": 5.579
    },
    {
        "text": "infer Dynamic markup random Fields with",
        "start": 1167.299,
        "duration": 5.721
    },
    {
        "text": "exact and combinatorial l0",
        "start": 1170.059,
        "duration": 6.601
    },
    {
        "text": "regularization not only that we can",
        "start": 1173.02,
        "duration": 5.68
    },
    {
        "text": "solve this problem in near linear time",
        "start": 1176.66,
        "duration": 4.379
    },
    {
        "text": "and complexity and now this is very",
        "start": 1178.7,
        "duration": 4.08
    },
    {
        "text": "important because it basically implies",
        "start": 1181.039,
        "duration": 4.26
    },
    {
        "text": "that not only can we handle the cursive",
        "start": 1182.78,
        "duration": 4.32
    },
    {
        "text": "dimensionality or the combinatorial",
        "start": 1185.299,
        "duration": 5.101
    },
    {
        "text": "nature of the problem efficiently but we",
        "start": 1187.1,
        "duration": 6.36
    },
    {
        "text": "also can do that even more efficiently",
        "start": 1190.4,
        "duration": 6.36
    },
    {
        "text": "than the relaxed regularized mle uh",
        "start": 1193.46,
        "duration": 5.579
    },
    {
        "text": "which we already know is statistically",
        "start": 1196.76,
        "duration": 5.4
    },
    {
        "text": "inferior okay now the second key key",
        "start": 1199.039,
        "duration": 5.88
    },
    {
        "text": "result is that we can find the solution",
        "start": 1202.16,
        "duration": 6.3
    },
    {
        "text": "not only for a single sparsity level but",
        "start": 1204.919,
        "duration": 6.0
    },
    {
        "text": "for all sparsity levels this means that",
        "start": 1208.46,
        "duration": 4.74
    },
    {
        "text": "we can recover the entire solution path",
        "start": 1210.919,
        "duration": 4.441
    },
    {
        "text": "for all values of regularization",
        "start": 1213.2,
        "duration": 4.14
    },
    {
        "text": "coefficient this is important in many",
        "start": 1215.36,
        "duration": 5.58
    },
    {
        "text": "cases uh because uh we don't know what",
        "start": 1217.34,
        "duration": 5.36
    },
    {
        "text": "sparsity level we need",
        "start": 1220.94,
        "duration": 4.08
    },
    {
        "text": "so we need to do some sort of cross",
        "start": 1222.7,
        "duration": 5.44
    },
    {
        "text": "validation for example or Bic to find",
        "start": 1225.02,
        "duration": 5.519
    },
    {
        "text": "the correct sparsity level our result",
        "start": 1228.14,
        "duration": 5.039
    },
    {
        "text": "says that we can actually do this much",
        "start": 1230.539,
        "duration": 4.921
    },
    {
        "text": "more efficiently than just solving the",
        "start": 1233.179,
        "duration": 4.441
    },
    {
        "text": "problem over and over again okay because",
        "start": 1235.46,
        "duration": 4.92
    },
    {
        "text": "we can recover the solution path",
        "start": 1237.62,
        "duration": 5.82
    },
    {
        "text": "the third key result is on the",
        "start": 1240.38,
        "duration": 5.28
    },
    {
        "text": "statistical side we show that we can",
        "start": 1243.44,
        "duration": 4.56
    },
    {
        "text": "actually learn Dynamic Markov random",
        "start": 1245.66,
        "duration": 5.04
    },
    {
        "text": "field consistently meaning with a small",
        "start": 1248.0,
        "duration": 5.64
    },
    {
        "text": "statistical error even if you have as",
        "start": 1250.7,
        "duration": 6.3
    },
    {
        "text": "few as one sample per timer location now",
        "start": 1253.64,
        "duration": 5.82
    },
    {
        "text": "this is something that even mle some of",
        "start": 1257.0,
        "duration": 5.6
    },
    {
        "text": "the mle based methods cannot achieve",
        "start": 1259.46,
        "duration": 5.579
    },
    {
        "text": "now um",
        "start": 1262.6,
        "duration": 4.9
    },
    {
        "text": "how can we achieve this result uh",
        "start": 1265.039,
        "duration": 4.02
    },
    {
        "text": "through the following optimization",
        "start": 1267.5,
        "duration": 4.039
    },
    {
        "text": "problem it's a constraint optimization",
        "start": 1269.059,
        "duration": 6.201
    },
    {
        "text": "our goal is to recover the parameter",
        "start": 1271.539,
        "duration": 6.52
    },
    {
        "text": "Theta ST which is the so-called",
        "start": 1275.26,
        "duration": 5.56
    },
    {
        "text": "canonical parameter of the graphical",
        "start": 1278.059,
        "duration": 6.301
    },
    {
        "text": "model from which the graph structure can",
        "start": 1280.82,
        "duration": 5.46
    },
    {
        "text": "be recovered for example for gaussian",
        "start": 1284.36,
        "duration": 3.799
    },
    {
        "text": "Markov and field this canonical",
        "start": 1286.28,
        "duration": 4.8
    },
    {
        "text": "parameter is the Precision Matrix the",
        "start": 1288.159,
        "duration": 5.5
    },
    {
        "text": "objective is purely based on the",
        "start": 1291.08,
        "duration": 4.38
    },
    {
        "text": "regularization that comes from the prior",
        "start": 1293.659,
        "duration": 4.14
    },
    {
        "text": "or side information such as a smart city",
        "start": 1295.46,
        "duration": 4.74
    },
    {
        "text": "or some specific temporal or special",
        "start": 1297.799,
        "duration": 4.941
    },
    {
        "text": "spatial structure",
        "start": 1300.2,
        "duration": 5.88
    },
    {
        "text": "the constraint here basically bounds the",
        "start": 1302.74,
        "duration": 5.26
    },
    {
        "text": "distance between the unknown parameter",
        "start": 1306.08,
        "duration": 3.56
    },
    {
        "text": "that we want to recover the unknown",
        "start": 1308.0,
        "duration": 4.32
    },
    {
        "text": "canonical parameter and the so-called",
        "start": 1309.64,
        "duration": 5.26
    },
    {
        "text": "approximate backward mapping of the",
        "start": 1312.32,
        "duration": 4.56
    },
    {
        "text": "Markov random field now I'm not going to",
        "start": 1314.9,
        "duration": 3.86
    },
    {
        "text": "talk about the exact definition of",
        "start": 1316.88,
        "duration": 4.5
    },
    {
        "text": "approximate backward mapping but roughly",
        "start": 1318.76,
        "duration": 5.98
    },
    {
        "text": "speaking it gives us a crude estimate of",
        "start": 1321.38,
        "duration": 5.1
    },
    {
        "text": "the parameters of the graphical model",
        "start": 1324.74,
        "duration": 6.26
    },
    {
        "text": "that you're trying to estimate okay",
        "start": 1326.48,
        "duration": 4.52
    },
    {
        "text": "um turns out that this this uh backward",
        "start": 1331.039,
        "duration": 5.941
    },
    {
        "text": "mapping can be efficiently obtained",
        "start": 1334.82,
        "duration": 4.08
    },
    {
        "text": "directly from the data for different",
        "start": 1336.98,
        "duration": 3.66
    },
    {
        "text": "classes of this distribution such as",
        "start": 1338.9,
        "duration": 3.96
    },
    {
        "text": "gaussian Markov random Fields I'm going",
        "start": 1340.64,
        "duration": 4.56
    },
    {
        "text": "to talk about that later now let me give",
        "start": 1342.86,
        "duration": 5.16
    },
    {
        "text": "you two instances of this problem now if",
        "start": 1345.2,
        "duration": 5.58
    },
    {
        "text": "we know our Markov random field changes",
        "start": 1348.02,
        "duration": 5.1
    },
    {
        "text": "sparsely over time and there's for",
        "start": 1350.78,
        "duration": 4.259
    },
    {
        "text": "Simplicity again there's no spatial",
        "start": 1353.12,
        "duration": 5.22
    },
    {
        "text": "component then we can pick l0 penalty",
        "start": 1355.039,
        "duration": 5.221
    },
    {
        "text": "for the functions f and g in the",
        "start": 1358.34,
        "duration": 4.74
    },
    {
        "text": "objective and if we pick l0 L Infinity",
        "start": 1360.26,
        "duration": 5.76
    },
    {
        "text": "Norm as a dense distance measure for the",
        "start": 1363.08,
        "duration": 4.68
    },
    {
        "text": "constraint we end up with this problem",
        "start": 1366.02,
        "duration": 5.34
    },
    {
        "text": "right similarly if we know that the",
        "start": 1367.76,
        "duration": 5.46
    },
    {
        "text": "Markov random field changes smoothly",
        "start": 1371.36,
        "duration": 4.559
    },
    {
        "text": "over time and then we can use L2 Norm",
        "start": 1373.22,
        "duration": 5.04
    },
    {
        "text": "for the function G now the important",
        "start": 1375.919,
        "duration": 4.801
    },
    {
        "text": "question is what do we gain by going",
        "start": 1378.26,
        "duration": 4.799
    },
    {
        "text": "from this regularized mle to this",
        "start": 1380.72,
        "duration": 4.8
    },
    {
        "text": "constraint optimization problem it seems",
        "start": 1383.059,
        "duration": 5.341
    },
    {
        "text": "that we still have the combinatorial",
        "start": 1385.52,
        "duration": 4.68
    },
    {
        "text": "nature of the problem because you have",
        "start": 1388.4,
        "duration": 5.46
    },
    {
        "text": "zero Norm everywhere next I'm gonna show",
        "start": 1390.2,
        "duration": 6.12
    },
    {
        "text": "you that in fact this new optimization",
        "start": 1393.86,
        "duration": 5.34
    },
    {
        "text": "framework can outperform regularized mle",
        "start": 1396.32,
        "duration": 5.28
    },
    {
        "text": "both in terms of the computation and",
        "start": 1399.2,
        "duration": 5.04
    },
    {
        "text": "statistical guarantees okay",
        "start": 1401.6,
        "duration": 4.559
    },
    {
        "text": "now before talking about these results",
        "start": 1404.24,
        "duration": 4.38
    },
    {
        "text": "or these guarantees well let's consider",
        "start": 1406.159,
        "duration": 4.981
    },
    {
        "text": "the same small case study that I showed",
        "start": 1408.62,
        "duration": 6.36
    },
    {
        "text": "before just as a recap we generated 25",
        "start": 1411.14,
        "duration": 6.3
    },
    {
        "text": "by 25 inverse covariance matrices that",
        "start": 1414.98,
        "duration": 5.88
    },
    {
        "text": "are changing sparsely over time now our",
        "start": 1417.44,
        "duration": 5.28
    },
    {
        "text": "goal is to recover these inverse",
        "start": 1420.86,
        "duration": 3.9
    },
    {
        "text": "covariance matrices with our proposed",
        "start": 1422.72,
        "duration": 2.9
    },
    {
        "text": "method",
        "start": 1424.76,
        "duration": 4.2
    },
    {
        "text": "here on the left you can see the",
        "start": 1425.62,
        "duration": 5.32
    },
    {
        "text": "mismatcher for our proposed method",
        "start": 1428.96,
        "duration": 4.68
    },
    {
        "text": "compared to the regularized mle for",
        "start": 1430.94,
        "duration": 4.68
    },
    {
        "text": "different levels of approximation error",
        "start": 1433.64,
        "duration": 4.5
    },
    {
        "text": "in the backward mapping you can see that",
        "start": 1435.62,
        "duration": 4.98
    },
    {
        "text": "our method in fact leads to zero",
        "start": 1438.14,
        "duration": 3.96
    },
    {
        "text": "mismatch error it does not make any",
        "start": 1440.6,
        "duration": 2.819
    },
    {
        "text": "mistake for a wide range of",
        "start": 1442.1,
        "duration": 4.439
    },
    {
        "text": "approximation levels in the sample",
        "start": 1443.419,
        "duration": 4.74
    },
    {
        "text": "covariance Matrix whereas the",
        "start": 1446.539,
        "duration": 4.321
    },
    {
        "text": "regularized mle always has a non-zero",
        "start": 1448.159,
        "duration": 5.88
    },
    {
        "text": "mismatcher even if there is no error in",
        "start": 1450.86,
        "duration": 6.12
    },
    {
        "text": "the available sample covariance now the",
        "start": 1454.039,
        "duration": 5.341
    },
    {
        "text": "second figure is more interesting here",
        "start": 1456.98,
        "duration": 5.22
    },
    {
        "text": "it shows the non-zero elements of the",
        "start": 1459.38,
        "duration": 5.34
    },
    {
        "text": "true and estimated signals the blue",
        "start": 1462.2,
        "duration": 4.38
    },
    {
        "text": "curve as I mentioned is the estimated",
        "start": 1464.72,
        "duration": 4.439
    },
    {
        "text": "signal using the the regularized mle",
        "start": 1466.58,
        "duration": 5.219
    },
    {
        "text": "which we have seen before the estimated",
        "start": 1469.159,
        "duration": 4.921
    },
    {
        "text": "signal using our method is shown in",
        "start": 1471.799,
        "duration": 4.561
    },
    {
        "text": "green and as you can see it is very",
        "start": 1474.08,
        "duration": 6.36
    },
    {
        "text": "close to the true uh solution or the",
        "start": 1476.36,
        "duration": 6.54
    },
    {
        "text": "true non-zero values that you're trying",
        "start": 1480.44,
        "duration": 4.739
    },
    {
        "text": "to recover now based on this case study",
        "start": 1482.9,
        "duration": 3.96
    },
    {
        "text": "I hope I could at least convince you",
        "start": 1485.179,
        "duration": 5.521
    },
    {
        "text": "that our proposed method can potentially",
        "start": 1486.86,
        "duration": 6.299
    },
    {
        "text": "be a better choice for the inference of",
        "start": 1490.7,
        "duration": 4.92
    },
    {
        "text": "dynamic graphical models",
        "start": 1493.159,
        "duration": 6.0
    },
    {
        "text": "now to streamline my presentation in the",
        "start": 1495.62,
        "duration": 5.939
    },
    {
        "text": "rest of my talk I'm just gonna focus on",
        "start": 1499.159,
        "duration": 4.201
    },
    {
        "text": "the sparsely changing gaussian Markov",
        "start": 1501.559,
        "duration": 3.661
    },
    {
        "text": "random field so we assume that the",
        "start": 1503.36,
        "duration": 4.14
    },
    {
        "text": "Markov random field changes sparsely",
        "start": 1505.22,
        "duration": 4.26
    },
    {
        "text": "over time so there's no spatial",
        "start": 1507.5,
        "duration": 4.2
    },
    {
        "text": "component for now and the underlying",
        "start": 1509.48,
        "duration": 4.199
    },
    {
        "text": "distribution is gaussian I should also",
        "start": 1511.7,
        "duration": 4.14
    },
    {
        "text": "mention that our results",
        "start": 1513.679,
        "duration": 5.821
    },
    {
        "text": "um can be extended to more General uh",
        "start": 1515.84,
        "duration": 6.36
    },
    {
        "text": "setting when we have both changes in",
        "start": 1519.5,
        "duration": 6.6
    },
    {
        "text": "both space and time and it can also be",
        "start": 1522.2,
        "duration": 5.459
    },
    {
        "text": "extended to more General Distribution",
        "start": 1526.1,
        "duration": 4.02
    },
    {
        "text": "Beyond gaussian Markov random views for",
        "start": 1527.659,
        "duration": 4.14
    },
    {
        "text": "example discrete Minecraft random Fields",
        "start": 1530.12,
        "duration": 3.72
    },
    {
        "text": "but we we're not going to talk about",
        "start": 1531.799,
        "duration": 4.801
    },
    {
        "text": "those cases here uh I would be more than",
        "start": 1533.84,
        "duration": 4.98
    },
    {
        "text": "happy to discuss them offline and",
        "start": 1536.6,
        "duration": 3.42
    },
    {
        "text": "towards the end I'm going to give you",
        "start": 1538.82,
        "duration": 4.2
    },
    {
        "text": "some references that have these",
        "start": 1540.02,
        "duration": 4.92
    },
    {
        "text": "extensions okay",
        "start": 1543.02,
        "duration": 6.38
    },
    {
        "text": "so as I uh is there any question",
        "start": 1544.94,
        "duration": 4.46
    },
    {
        "text": "all right uh so",
        "start": 1550.46,
        "duration": 5.28
    },
    {
        "text": "um as I mentioned here we consider this",
        "start": 1553.039,
        "duration": 5.041
    },
    {
        "text": "uh sparsely changing Markov random",
        "start": 1555.74,
        "duration": 4.98
    },
    {
        "text": "Fields with uh gaussian distribution so",
        "start": 1558.08,
        "duration": 5.219
    },
    {
        "text": "our canonical parameter is the Precision",
        "start": 1560.72,
        "duration": 5.76
    },
    {
        "text": "Matrix and the functions f and g are l0",
        "start": 1563.299,
        "duration": 5.701
    },
    {
        "text": "penalties to promote sparsity into",
        "start": 1566.48,
        "duration": 4.14
    },
    {
        "text": "individual parameters and their",
        "start": 1569.0,
        "duration": 5.039
    },
    {
        "text": "differences and the distance uh from the",
        "start": 1570.62,
        "duration": 5.4
    },
    {
        "text": "approximate backward mapping is measured",
        "start": 1574.039,
        "duration": 4.62
    },
    {
        "text": "with respect to the L Infinity Norm okay",
        "start": 1576.02,
        "duration": 5.1
    },
    {
        "text": "now as you can see our entire",
        "start": 1578.659,
        "duration": 5.161
    },
    {
        "text": "optimization framework relies on the",
        "start": 1581.12,
        "duration": 5.039
    },
    {
        "text": "availability of this approximate",
        "start": 1583.82,
        "duration": 4.5
    },
    {
        "text": "backward mapping now the question is how",
        "start": 1586.159,
        "duration": 4.02
    },
    {
        "text": "can we find this approximate backward",
        "start": 1588.32,
        "duration": 5.219
    },
    {
        "text": "mapping in the gaussian setting a recent",
        "start": 1590.179,
        "duration": 5.641
    },
    {
        "text": "work actually proposed to use the",
        "start": 1593.539,
        "duration": 5.76
    },
    {
        "text": "inverse of a soft thresholded version of",
        "start": 1595.82,
        "duration": 5.64
    },
    {
        "text": "the sample covariance Matrix as the",
        "start": 1599.299,
        "duration": 5.421
    },
    {
        "text": "backward mapping okay so the idea here",
        "start": 1601.46,
        "duration": 7.56
    },
    {
        "text": "is that given the samples first we find",
        "start": 1604.72,
        "duration": 6.22
    },
    {
        "text": "the sample covariance Matrix this this",
        "start": 1609.02,
        "duration": 3.84
    },
    {
        "text": "is this is going to be a dense and",
        "start": 1610.94,
        "duration": 4.56
    },
    {
        "text": "potentially low rank Matrix if the",
        "start": 1612.86,
        "duration": 3.9
    },
    {
        "text": "number of measurements is not",
        "start": 1615.5,
        "duration": 3.6
    },
    {
        "text": "sufficiently large then the soft",
        "start": 1616.76,
        "duration": 4.799
    },
    {
        "text": "threshold the off diagonal entries at",
        "start": 1619.1,
        "duration": 5.04
    },
    {
        "text": "some level okay the resultant Matrix is",
        "start": 1621.559,
        "duration": 6.0
    },
    {
        "text": "going to be sparse uh but probably full",
        "start": 1624.14,
        "duration": 6.24
    },
    {
        "text": "Rank and finally take the inverse of",
        "start": 1627.559,
        "duration": 4.86
    },
    {
        "text": "this thresholded Matrix this is going to",
        "start": 1630.38,
        "duration": 4.679
    },
    {
        "text": "be our approximate backward mapping now",
        "start": 1632.419,
        "duration": 5.221
    },
    {
        "text": "the interesting thing here is that we",
        "start": 1635.059,
        "duration": 4.681
    },
    {
        "text": "can actually drive very strong",
        "start": 1637.64,
        "duration": 4.38
    },
    {
        "text": "statistical guarantees for our proposed",
        "start": 1639.74,
        "duration": 4.98
    },
    {
        "text": "estimation method assuming that we can",
        "start": 1642.02,
        "duration": 4.74
    },
    {
        "text": "solve it efficiently using this simple",
        "start": 1644.72,
        "duration": 4.559
    },
    {
        "text": "backward mapping okay in particular",
        "start": 1646.76,
        "duration": 4.08
    },
    {
        "text": "suppose that the number of available",
        "start": 1649.279,
        "duration": 5.28
    },
    {
        "text": "samples at each time which we show it",
        "start": 1650.84,
        "duration": 7.62
    },
    {
        "text": "with NT skills logarithmically where DD",
        "start": 1654.559,
        "duration": 6.36
    },
    {
        "text": "is the size of the Matrix the Precision",
        "start": 1658.46,
        "duration": 4.8
    },
    {
        "text": "matrix it's D by D and also",
        "start": 1660.919,
        "duration": 4.76
    },
    {
        "text": "logarithmically with t then",
        "start": 1663.26,
        "duration": 4.919
    },
    {
        "text": "if the the other parameters of the",
        "start": 1665.679,
        "duration": 4.421
    },
    {
        "text": "problem satisfy certain properties then",
        "start": 1668.179,
        "duration": 4.041
    },
    {
        "text": "we can ensure that with high probability",
        "start": 1670.1,
        "duration": 5.1
    },
    {
        "text": "we will recovered the correct sporting",
        "start": 1672.22,
        "duration": 4.959
    },
    {
        "text": "pattern of the inverse covariance Matrix",
        "start": 1675.2,
        "duration": 4.2
    },
    {
        "text": "and their differences and we're going to",
        "start": 1677.179,
        "duration": 4.38
    },
    {
        "text": "have small estimation error",
        "start": 1679.4,
        "duration": 4.98
    },
    {
        "text": "and the estimation errors decays at the",
        "start": 1681.559,
        "duration": 5.701
    },
    {
        "text": "rate of 1 over a square root of N and T",
        "start": 1684.38,
        "duration": 6.0
    },
    {
        "text": "okay so roughly speaking this theorem",
        "start": 1687.26,
        "duration": 5.1
    },
    {
        "text": "basically implies that the estimated",
        "start": 1690.38,
        "duration": 4.62
    },
    {
        "text": "inverse covariance Matrix that we get by",
        "start": 1692.36,
        "duration": 5.4
    },
    {
        "text": "solving this optimization problem it's",
        "start": 1695.0,
        "duration": 4.14
    },
    {
        "text": "going to have correct the sparsity",
        "start": 1697.76,
        "duration": 4.44
    },
    {
        "text": "pattern and a small estimation error",
        "start": 1699.14,
        "duration": 5.88
    },
    {
        "text": "provided that the number of samples at",
        "start": 1702.2,
        "duration": 4.979
    },
    {
        "text": "any given time scales logarithmically",
        "start": 1705.02,
        "duration": 4.139
    },
    {
        "text": "with the dimension of the problem now",
        "start": 1707.179,
        "duration": 4.021
    },
    {
        "text": "this implies that even if the dimension",
        "start": 1709.159,
        "duration": 4.62
    },
    {
        "text": "of the unknown parameters in this case",
        "start": 1711.2,
        "duration": 4.44
    },
    {
        "text": "is the dimension of the Precision Matrix",
        "start": 1713.779,
        "duration": 3.721
    },
    {
        "text": "is significantly larger than the number",
        "start": 1715.64,
        "duration": 4.32
    },
    {
        "text": "of samples we can still recover the true",
        "start": 1717.5,
        "duration": 4.74
    },
    {
        "text": "solution with high probability and with",
        "start": 1719.96,
        "duration": 5.28
    },
    {
        "text": "strong statistical guarantees now this",
        "start": 1722.24,
        "duration": 6.66
    },
    {
        "text": "is a strong result to some extent but it",
        "start": 1725.24,
        "duration": 5.46
    },
    {
        "text": "has an important limitation",
        "start": 1728.9,
        "duration": 5.519
    },
    {
        "text": "it requires multiple samples at each",
        "start": 1730.7,
        "duration": 6.54
    },
    {
        "text": "time step right the theory says that we",
        "start": 1734.419,
        "duration": 4.5
    },
    {
        "text": "need to have at least logarithmic number",
        "start": 1737.24,
        "duration": 4.799
    },
    {
        "text": "of samples at any given time but uh in",
        "start": 1738.919,
        "duration": 5.101
    },
    {
        "text": "many cases in many applications we may",
        "start": 1742.039,
        "duration": 4.681
    },
    {
        "text": "only have one sample per time right uh",
        "start": 1744.02,
        "duration": 5.159
    },
    {
        "text": "while the underlying graph continues to",
        "start": 1746.72,
        "duration": 6.0
    },
    {
        "text": "change with the incoming data so we may",
        "start": 1749.179,
        "duration": 7.561
    },
    {
        "text": "have asked you as one sample per time",
        "start": 1752.72,
        "duration": 6.54
    },
    {
        "text": "for for the underlying Markov random",
        "start": 1756.74,
        "duration": 5.52
    },
    {
        "text": "field so if we ask this somewhat",
        "start": 1759.26,
        "duration": 3.72
    },
    {
        "text": "um",
        "start": 1762.26,
        "duration": 3.48
    },
    {
        "text": "ambitious question can we learn the time",
        "start": 1762.98,
        "duration": 5.34
    },
    {
        "text": "very Markov random field with as few as",
        "start": 1765.74,
        "duration": 6.6
    },
    {
        "text": "uh one sample per time and uh turns out",
        "start": 1768.32,
        "duration": 6.239
    },
    {
        "text": "that the answer to this question is yes",
        "start": 1772.34,
        "duration": 5.219
    },
    {
        "text": "and the key idea is to use kernel",
        "start": 1774.559,
        "duration": 6.181
    },
    {
        "text": "averaging okay the main intuition here",
        "start": 1777.559,
        "duration": 5.641
    },
    {
        "text": "is that if the if the underlying Markov",
        "start": 1780.74,
        "duration": 4.86
    },
    {
        "text": "random field changes slowly over time",
        "start": 1783.2,
        "duration": 6.0
    },
    {
        "text": "then the past observed samples can still",
        "start": 1785.6,
        "duration": 6.54
    },
    {
        "text": "probably reveal some information about",
        "start": 1789.2,
        "duration": 5.28
    },
    {
        "text": "the the structure of the Markov random",
        "start": 1792.14,
        "duration": 4.8
    },
    {
        "text": "field at the present time because they",
        "start": 1794.48,
        "duration": 4.14
    },
    {
        "text": "are probably collected from different",
        "start": 1796.94,
        "duration": 4.979
    },
    {
        "text": "but very similar distributions okay now",
        "start": 1798.62,
        "duration": 4.86
    },
    {
        "text": "I'm going to explain this intuition with",
        "start": 1801.919,
        "duration": 3.48
    },
    {
        "text": "some graphs our previous theorem",
        "start": 1803.48,
        "duration": 4.62
    },
    {
        "text": "basically said that if if the data",
        "start": 1805.399,
        "duration": 7.201
    },
    {
        "text": "arrives in batches okay uh we can we can",
        "start": 1808.1,
        "duration": 8.34
    },
    {
        "text": "recover uh the true solution if you have",
        "start": 1812.6,
        "duration": 7.679
    },
    {
        "text": "multiple samples at any given time and",
        "start": 1816.44,
        "duration": 6.479
    },
    {
        "text": "we can recover the the the the",
        "start": 1820.279,
        "duration": 5.581
    },
    {
        "text": "uh Precision Matrix based on these",
        "start": 1822.919,
        "duration": 4.14
    },
    {
        "text": "samples",
        "start": 1825.86,
        "duration": 4.22
    },
    {
        "text": "on the other hand in different settings",
        "start": 1827.059,
        "duration": 6.84
    },
    {
        "text": "the data may arise sequentially or one",
        "start": 1830.08,
        "duration": 6.28
    },
    {
        "text": "at a time and each sample May correspond",
        "start": 1833.899,
        "duration": 4.801
    },
    {
        "text": "to for example different distributions",
        "start": 1836.36,
        "duration": 5.1
    },
    {
        "text": "right here the idea is to consider a",
        "start": 1838.7,
        "duration": 5.9
    },
    {
        "text": "weighted average of of these samples",
        "start": 1841.46,
        "duration": 5.52
    },
    {
        "text": "where the weights are coming from a",
        "start": 1844.6,
        "duration": 4.36
    },
    {
        "text": "kernel that essentially assigns",
        "start": 1846.98,
        "duration": 4.439
    },
    {
        "text": "different weights to different samples",
        "start": 1848.96,
        "duration": 6.06
    },
    {
        "text": "so if you're at time T for example uh",
        "start": 1851.419,
        "duration": 6.181
    },
    {
        "text": "the the samples closer to time T will",
        "start": 1855.02,
        "duration": 5.039
    },
    {
        "text": "have a higher weight than the ones that",
        "start": 1857.6,
        "duration": 6.059
    },
    {
        "text": "are far away from time t okay now using",
        "start": 1860.059,
        "duration": 5.881
    },
    {
        "text": "this approach the the sample covariance",
        "start": 1863.659,
        "duration": 4.981
    },
    {
        "text": "Matrix corresponds to this uh weighted",
        "start": 1865.94,
        "duration": 5.04
    },
    {
        "text": "average of the samples where the weights",
        "start": 1868.64,
        "duration": 5.1
    },
    {
        "text": "are coming from a kernel by just uh I",
        "start": 1870.98,
        "duration": 5.1
    },
    {
        "text": "just described that kernel to your H the",
        "start": 1873.74,
        "duration": 4.439
    },
    {
        "text": "parameter H is",
        "start": 1876.08,
        "duration": 5.04
    },
    {
        "text": "um a parameter that controls the",
        "start": 1878.179,
        "duration": 5.701
    },
    {
        "text": "bandwidth of the kernel it is chosen",
        "start": 1881.12,
        "duration": 4.32
    },
    {
        "text": "based on the rate of change in the",
        "start": 1883.88,
        "duration": 4.2
    },
    {
        "text": "Markov random field for example if uh we",
        "start": 1885.44,
        "duration": 4.26
    },
    {
        "text": "know that it is that the Markov random",
        "start": 1888.08,
        "duration": 4.979
    },
    {
        "text": "field changes slowly then we can pick a",
        "start": 1889.7,
        "duration": 6.24
    },
    {
        "text": "large value for for the span width and",
        "start": 1893.059,
        "duration": 4.74
    },
    {
        "text": "if you pick a large value for the span",
        "start": 1895.94,
        "duration": 5.58
    },
    {
        "text": "width the weights will be more evenly",
        "start": 1897.799,
        "duration": 6.561
    },
    {
        "text": "spread out across time now it turns out",
        "start": 1901.52,
        "duration": 6.0
    },
    {
        "text": "that here we can use the same backward",
        "start": 1904.36,
        "duration": 6.64
    },
    {
        "text": "mapping uh that we saw before except",
        "start": 1907.52,
        "duration": 5.6
    },
    {
        "text": "that here we assume",
        "start": 1911.0,
        "duration": 5.7
    },
    {
        "text": "we use this thresholding uh instead of",
        "start": 1913.12,
        "duration": 6.4
    },
    {
        "text": "uh the sample covariance Matrix on the",
        "start": 1916.7,
        "duration": 6.719
    },
    {
        "text": "weighted average that I mentioned above",
        "start": 1919.52,
        "duration": 7.08
    },
    {
        "text": "with this modification we show that even",
        "start": 1923.419,
        "duration": 6.841
    },
    {
        "text": "if we have as few as one sample per time",
        "start": 1926.6,
        "duration": 5.819
    },
    {
        "text": "we can still correctly recover the",
        "start": 1930.26,
        "duration": 3.84
    },
    {
        "text": "sparsity pattern of the inverse",
        "start": 1932.419,
        "duration": 3.301
    },
    {
        "text": "covariance matrices and their",
        "start": 1934.1,
        "duration": 3.9
    },
    {
        "text": "differences under some conditions",
        "start": 1935.72,
        "duration": 6.24
    },
    {
        "text": "okay now uh we're done with the",
        "start": 1938.0,
        "duration": 6.419
    },
    {
        "text": "statistical guarantees uh for the",
        "start": 1941.96,
        "duration": 4.98
    },
    {
        "text": "proposed inference method but we still",
        "start": 1944.419,
        "duration": 5.221
    },
    {
        "text": "haven't explained how we can solve this",
        "start": 1946.94,
        "duration": 3.839
    },
    {
        "text": "problem",
        "start": 1949.64,
        "duration": 4.1
    },
    {
        "text": "efficiently in practice right",
        "start": 1950.779,
        "duration": 5.28
    },
    {
        "text": "before moving on I just want to make",
        "start": 1953.74,
        "duration": 6.179
    },
    {
        "text": "sure that there is there's no question",
        "start": 1956.059,
        "duration": 3.86
    },
    {
        "text": "foreign",
        "start": 1960.02,
        "duration": 2.96
    },
    {
        "text": "so let's move on now let's go back to",
        "start": 1963.279,
        "duration": 4.961
    },
    {
        "text": "the the original problem that we",
        "start": 1966.44,
        "duration": 3.959
    },
    {
        "text": "introduced uh for the sparsely changing",
        "start": 1968.24,
        "duration": 5.58
    },
    {
        "text": "uh Markov random fuels uh now the first",
        "start": 1970.399,
        "duration": 6.241
    },
    {
        "text": "and perhaps the most uh important thing",
        "start": 1973.82,
        "duration": 4.8
    },
    {
        "text": "to note here in this optimization",
        "start": 1976.64,
        "duration": 4.38
    },
    {
        "text": "problem is that in fact this this",
        "start": 1978.62,
        "duration": 4.86
    },
    {
        "text": "problem can be fully decomposed over",
        "start": 1981.02,
        "duration": 4.98
    },
    {
        "text": "different coordinates of the unknown",
        "start": 1983.48,
        "duration": 5.28
    },
    {
        "text": "parameters now our constraint here is",
        "start": 1986.0,
        "duration": 5.1
    },
    {
        "text": "the element wise maximum it's it's the L",
        "start": 1988.76,
        "duration": 4.08
    },
    {
        "text": "Infinity Norm which is the element by",
        "start": 1991.1,
        "duration": 4.14
    },
    {
        "text": "its maximum of the differences which is",
        "start": 1992.84,
        "duration": 5.579
    },
    {
        "text": "essentially equivalent to D Squared",
        "start": 1995.24,
        "duration": 5.939
    },
    {
        "text": "number of linear inequalities over",
        "start": 1998.419,
        "duration": 4.681
    },
    {
        "text": "different elements of the Precision",
        "start": 2001.179,
        "duration": 4.5
    },
    {
        "text": "Matrix you can reformulate this maximum",
        "start": 2003.1,
        "duration": 5.579
    },
    {
        "text": "into individual linear inequalities now",
        "start": 2005.679,
        "duration": 5.401
    },
    {
        "text": "why is it important because it leads to",
        "start": 2008.679,
        "duration": 5.281
    },
    {
        "text": "a fully decomposed optimization problem",
        "start": 2011.08,
        "duration": 4.74
    },
    {
        "text": "over different coordinates of the",
        "start": 2013.96,
        "duration": 3.839
    },
    {
        "text": "Precision Matrix for example suppose",
        "start": 2015.82,
        "duration": 3.959
    },
    {
        "text": "that we need to estimate this Precision",
        "start": 2017.799,
        "duration": 5.281
    },
    {
        "text": "Matrix uh corresponding to three",
        "start": 2019.779,
        "duration": 6.061
    },
    {
        "text": "different times if you try to estimate",
        "start": 2023.08,
        "duration": 5.839
    },
    {
        "text": "it using mle for example",
        "start": 2025.84,
        "duration": 6.42
    },
    {
        "text": "all these uh unknown elements are going",
        "start": 2028.919,
        "duration": 6.64
    },
    {
        "text": "to be coupled together because the mle",
        "start": 2032.26,
        "duration": 6.24
    },
    {
        "text": "is not really decomposable our method",
        "start": 2035.559,
        "duration": 5.941
    },
    {
        "text": "naturally decomposes into different sub",
        "start": 2038.5,
        "duration": 6.539
    },
    {
        "text": "problems each Define on an individual",
        "start": 2041.5,
        "duration": 5.399
    },
    {
        "text": "coordinate of the Precision Matrix for",
        "start": 2045.039,
        "duration": 3.901
    },
    {
        "text": "example here to estimate the first",
        "start": 2046.899,
        "duration": 4.74
    },
    {
        "text": "coordinate in the first row of the",
        "start": 2048.94,
        "duration": 4.56
    },
    {
        "text": "Precision Matrix the only need to solve",
        "start": 2051.639,
        "duration": 2.96
    },
    {
        "text": "an",
        "start": 2053.5,
        "duration": 3.72
    },
    {
        "text": "optimization problem or an optimization",
        "start": 2054.599,
        "duration": 5.08
    },
    {
        "text": "sub problem defined over the blue",
        "start": 2057.22,
        "duration": 3.8
    },
    {
        "text": "entries",
        "start": 2059.679,
        "duration": 4.44
    },
    {
        "text": "uh now more rigorously each of these sub",
        "start": 2061.02,
        "duration": 7.0
    },
    {
        "text": "problems uh look like this for every",
        "start": 2064.119,
        "duration": 6.3
    },
    {
        "text": "component I and J we need to solve an",
        "start": 2068.02,
        "duration": 4.92
    },
    {
        "text": "optimization problem in this form that",
        "start": 2070.419,
        "duration": 5.341
    },
    {
        "text": "is coupled only in time it's not coupled",
        "start": 2072.94,
        "duration": 4.76
    },
    {
        "text": "over different coordinates it's only",
        "start": 2075.76,
        "duration": 6.0
    },
    {
        "text": "coupled in time here the function I is",
        "start": 2077.7,
        "duration": 5.86
    },
    {
        "text": "just the indicator function it takes the",
        "start": 2081.76,
        "duration": 6.54
    },
    {
        "text": "value 1 if the i j element of theta T is",
        "start": 2083.56,
        "duration": 7.2
    },
    {
        "text": "non-zero and it takes zero otherwise and",
        "start": 2088.3,
        "duration": 4.92
    },
    {
        "text": "the constraint is just a simple lower",
        "start": 2090.76,
        "duration": 5.94
    },
    {
        "text": "and upper bound on the parameter",
        "start": 2093.22,
        "duration": 6.0
    },
    {
        "text": "um now to recover the full Precision",
        "start": 2096.7,
        "duration": 5.94
    },
    {
        "text": "Matrix we need to solve D Squared number",
        "start": 2099.22,
        "duration": 6.0
    },
    {
        "text": "of sub problems in this form now what",
        "start": 2102.64,
        "duration": 3.719
    },
    {
        "text": "are the implications of this",
        "start": 2105.22,
        "duration": 3.18
    },
    {
        "text": "decomposition well the first implication",
        "start": 2106.359,
        "duration": 5.281
    },
    {
        "text": "is that we need to solve significantly",
        "start": 2108.4,
        "duration": 5.88
    },
    {
        "text": "smaller sub problems each sub problem is",
        "start": 2111.64,
        "duration": 4.62
    },
    {
        "text": "going to be defined only over T",
        "start": 2114.28,
        "duration": 4.16
    },
    {
        "text": "variables T is the length of the time",
        "start": 2116.26,
        "duration": 4.62
    },
    {
        "text": "the other implication is that the",
        "start": 2118.44,
        "duration": 5.74
    },
    {
        "text": "overall complexity becomes linear in the",
        "start": 2120.88,
        "duration": 5.64
    },
    {
        "text": "size of the Precision Matrix why is it",
        "start": 2124.18,
        "duration": 4.38
    },
    {
        "text": "linear remember the size of the Matrix",
        "start": 2126.52,
        "duration": 4.559
    },
    {
        "text": "size of the Precision Matrix is 2 D",
        "start": 2128.56,
        "duration": 6.299
    },
    {
        "text": "Squared right d by D Matrix and we have",
        "start": 2131.079,
        "duration": 5.941
    },
    {
        "text": "in this case we have D Squared number of",
        "start": 2134.859,
        "duration": 4.921
    },
    {
        "text": "variables this is very useful since in",
        "start": 2137.02,
        "duration": 4.2
    },
    {
        "text": "many cases the size of the Precision",
        "start": 2139.78,
        "duration": 4.559
    },
    {
        "text": "Matrix is very large the other very",
        "start": 2141.22,
        "duration": 4.8
    },
    {
        "text": "important implication is on the",
        "start": 2144.339,
        "duration": 3.721
    },
    {
        "text": "implementation side based on this",
        "start": 2146.02,
        "duration": 4.559
    },
    {
        "text": "decomposition our proposed optimization",
        "start": 2148.06,
        "duration": 4.62
    },
    {
        "text": "can be easily parallelized this is very",
        "start": 2150.579,
        "duration": 3.721
    },
    {
        "text": "important because most of the",
        "start": 2152.68,
        "duration": 3.36
    },
    {
        "text": "regularized mle approaches are not",
        "start": 2154.3,
        "duration": 4.799
    },
    {
        "text": "amenable uh to uh to parallelization",
        "start": 2156.04,
        "duration": 5.88
    },
    {
        "text": "we're going to see the benefits of this",
        "start": 2159.099,
        "duration": 4.441
    },
    {
        "text": "parallelization later in our numerical",
        "start": 2161.92,
        "duration": 3.78
    },
    {
        "text": "study now um",
        "start": 2163.54,
        "duration": 4.22
    },
    {
        "text": "we still haven't answered one question",
        "start": 2165.7,
        "duration": 5.28
    },
    {
        "text": "how can we solve each sub-problem",
        "start": 2167.76,
        "duration": 5.62
    },
    {
        "text": "efficiently now each subproblem as you",
        "start": 2170.98,
        "duration": 4.44
    },
    {
        "text": "can see here is still combinatorial",
        "start": 2173.38,
        "duration": 4.56
    },
    {
        "text": "right they have indicator functions so",
        "start": 2175.42,
        "duration": 4.56
    },
    {
        "text": "in principle we may still face with this",
        "start": 2177.94,
        "duration": 4.44
    },
    {
        "text": "curse of dimensionality in each of these",
        "start": 2179.98,
        "duration": 4.379
    },
    {
        "text": "sub problems but it turns out that we",
        "start": 2182.38,
        "duration": 3.42
    },
    {
        "text": "can actually do much better than that",
        "start": 2184.359,
        "duration": 3.661
    },
    {
        "text": "now I'm going to try to explain this",
        "start": 2185.8,
        "duration": 3.84
    },
    {
        "text": "this algorithm that we came up with",
        "start": 2188.02,
        "duration": 2.599
    },
    {
        "text": "through",
        "start": 2189.64,
        "duration": 3.9
    },
    {
        "text": "different steps let's start with a",
        "start": 2190.619,
        "duration": 5.74
    },
    {
        "text": "special case where this beta coefficient",
        "start": 2193.54,
        "duration": 4.86
    },
    {
        "text": "is equal to one this means that the only",
        "start": 2196.359,
        "duration": 4.201
    },
    {
        "text": "care about the sparsity in the parameter",
        "start": 2198.4,
        "duration": 5.4
    },
    {
        "text": "differences so we do not penalize the",
        "start": 2200.56,
        "duration": 5.4
    },
    {
        "text": "non-zero elements in individual",
        "start": 2203.8,
        "duration": 5.46
    },
    {
        "text": "parameters in this case it turns out",
        "start": 2205.96,
        "duration": 5.76
    },
    {
        "text": "that a simple greedy algorithm actually",
        "start": 2209.26,
        "duration": 5.46
    },
    {
        "text": "can give us an optimal solution the idea",
        "start": 2211.72,
        "duration": 5.639
    },
    {
        "text": "is the following at time t equal to zero",
        "start": 2214.72,
        "duration": 4.98
    },
    {
        "text": "at the initial time we look into the",
        "start": 2217.359,
        "duration": 5.901
    },
    {
        "text": "future and find the longest sequence of",
        "start": 2219.7,
        "duration": 5.94
    },
    {
        "text": "non-empty overlapping feasible interval",
        "start": 2223.26,
        "duration": 5.079
    },
    {
        "text": "models so each of these problems have a",
        "start": 2225.64,
        "duration": 4.62
    },
    {
        "text": "lower bound and upper bound they Define",
        "start": 2228.339,
        "duration": 4.26
    },
    {
        "text": "a feasible interval we look at the",
        "start": 2230.26,
        "duration": 4.38
    },
    {
        "text": "non-empty overlapping feasible interval",
        "start": 2232.599,
        "duration": 5.52
    },
    {
        "text": "then we set the variables uh within that",
        "start": 2234.64,
        "duration": 7.08
    },
    {
        "text": "range to an arbitrarily value from that",
        "start": 2238.119,
        "duration": 5.881
    },
    {
        "text": "non-non empty overlapping interval and",
        "start": 2241.72,
        "duration": 4.44
    },
    {
        "text": "we do this same process for the rest of",
        "start": 2244.0,
        "duration": 4.98
    },
    {
        "text": "the variables now here's a toy example",
        "start": 2246.16,
        "duration": 5.88
    },
    {
        "text": "suppose that these are segments uh that",
        "start": 2248.98,
        "duration": 4.619
    },
    {
        "text": "show the upper bound and lower bounds",
        "start": 2252.04,
        "duration": 3.9
    },
    {
        "text": "for different feasible intervals at",
        "start": 2253.599,
        "duration": 4.341
    },
    {
        "text": "different times x axis at the time",
        "start": 2255.94,
        "duration": 4.139
    },
    {
        "text": "y-axis is the value of these feasible",
        "start": 2257.94,
        "duration": 3.52
    },
    {
        "text": "intervals",
        "start": 2260.079,
        "duration": 4.26
    },
    {
        "text": "using our greedy algorithm first we find",
        "start": 2261.46,
        "duration": 4.68
    },
    {
        "text": "the longest non-empty overlapping",
        "start": 2264.339,
        "duration": 4.561
    },
    {
        "text": "interval which is this blue box uh in",
        "start": 2266.14,
        "duration": 5.28
    },
    {
        "text": "this figure then we set the variable",
        "start": 2268.9,
        "duration": 4.98
    },
    {
        "text": "from time 0 to time five to any",
        "start": 2271.42,
        "duration": 5.28
    },
    {
        "text": "arbitrary value from this interval after",
        "start": 2273.88,
        "duration": 6.06
    },
    {
        "text": "this we inevitably have a change in our",
        "start": 2276.7,
        "duration": 6.24
    },
    {
        "text": "variables because we we don't have a non",
        "start": 2279.94,
        "duration": 5.639
    },
    {
        "text": "uh uh we don't have an overlapping",
        "start": 2282.94,
        "duration": 5.46
    },
    {
        "text": "interval and we incur a cost for that",
        "start": 2285.579,
        "duration": 5.701
    },
    {
        "text": "but then we do the same process we just",
        "start": 2288.4,
        "duration": 4.92
    },
    {
        "text": "find a non empty overlapping interval",
        "start": 2291.28,
        "duration": 5.4
    },
    {
        "text": "and set the variables accordingly now we",
        "start": 2293.32,
        "duration": 5.58
    },
    {
        "text": "can show that this very simple and very",
        "start": 2296.68,
        "duration": 4.86
    },
    {
        "text": "greedy algorithm can in fact give us the",
        "start": 2298.9,
        "duration": 4.199
    },
    {
        "text": "optimal solution for this special case",
        "start": 2301.54,
        "duration": 5.22
    },
    {
        "text": "where beta is equal to one okay now the",
        "start": 2303.099,
        "duration": 5.581
    },
    {
        "text": "situation is very different in the",
        "start": 2306.76,
        "duration": 3.66
    },
    {
        "text": "general case where beta is strictly",
        "start": 2308.68,
        "duration": 4.56
    },
    {
        "text": "between zero and one now in this case",
        "start": 2310.42,
        "duration": 5.88
    },
    {
        "text": "um if you want to promote sparsity in",
        "start": 2313.24,
        "duration": 5.46
    },
    {
        "text": "both the parameters the Precision Matrix",
        "start": 2316.3,
        "duration": 4.74
    },
    {
        "text": "and their temporal differences not only",
        "start": 2318.7,
        "duration": 5.58
    },
    {
        "text": "do we want to have as few changes as",
        "start": 2321.04,
        "duration": 5.539
    },
    {
        "text": "possible but also",
        "start": 2324.28,
        "duration": 6.48
    },
    {
        "text": "we want to to set the variables to zero",
        "start": 2326.579,
        "duration": 8.701
    },
    {
        "text": "as much as possible now clearly if we uh",
        "start": 2330.76,
        "duration": 8.579
    },
    {
        "text": "have a non-z if if zero is not a",
        "start": 2335.28,
        "duration": 6.76
    },
    {
        "text": "feasible solution we can just use a",
        "start": 2339.339,
        "duration": 4.321
    },
    {
        "text": "greedy algorithm right because we don't",
        "start": 2342.04,
        "duration": 4.079
    },
    {
        "text": "have a zero feasible solution as soon as",
        "start": 2343.66,
        "duration": 6.179
    },
    {
        "text": "we see a zero uh a feasible zero in",
        "start": 2346.119,
        "duration": 5.641
    },
    {
        "text": "these intervals we need to choose",
        "start": 2349.839,
        "duration": 3.481
    },
    {
        "text": "between two options",
        "start": 2351.76,
        "duration": 3.66
    },
    {
        "text": "first we can just ignore this feasible",
        "start": 2353.32,
        "duration": 3.9
    },
    {
        "text": "zero and just stick with our greedy",
        "start": 2355.42,
        "duration": 4.86
    },
    {
        "text": "algorithm just to avoid paying a penalty",
        "start": 2357.22,
        "duration": 6.119
    },
    {
        "text": "uh for changing the value of the",
        "start": 2360.28,
        "duration": 6.12
    },
    {
        "text": "variable but note that here we incur a",
        "start": 2363.339,
        "duration": 5.461
    },
    {
        "text": "cost for the non-zero elements the other",
        "start": 2366.4,
        "duration": 5.16
    },
    {
        "text": "option is to switch to zero incur the",
        "start": 2368.8,
        "duration": 5.7
    },
    {
        "text": "cost of switching but instead avoid",
        "start": 2371.56,
        "duration": 4.68
    },
    {
        "text": "paying the penalty for the non-zero",
        "start": 2374.5,
        "duration": 4.619
    },
    {
        "text": "elements now if you take the first",
        "start": 2376.24,
        "duration": 4.8
    },
    {
        "text": "approach or first option in this case",
        "start": 2379.119,
        "duration": 4.441
    },
    {
        "text": "the total cost is going to be 10 times 1",
        "start": 2381.04,
        "duration": 4.5
    },
    {
        "text": "minus better if we take the second",
        "start": 2383.56,
        "duration": 4.92
    },
    {
        "text": "approach the cost will be 6 times 1",
        "start": 2385.54,
        "duration": 5.039
    },
    {
        "text": "minus beta plus beta",
        "start": 2388.48,
        "duration": 5.4
    },
    {
        "text": "now a simple calculation would show that",
        "start": 2390.579,
        "duration": 5.461
    },
    {
        "text": "the optimal solution is to switch to",
        "start": 2393.88,
        "duration": 3.66
    },
    {
        "text": "zero",
        "start": 2396.04,
        "duration": 3.84
    },
    {
        "text": "um only if the the beta is less than",
        "start": 2397.54,
        "duration": 5.4
    },
    {
        "text": "450. now this intuitively makes sense uh",
        "start": 2399.88,
        "duration": 5.34
    },
    {
        "text": "because if beta is small that means that",
        "start": 2402.94,
        "duration": 4.56
    },
    {
        "text": "we put more weight according to your",
        "start": 2405.22,
        "duration": 4.26
    },
    {
        "text": "objective we put more weight on the",
        "start": 2407.5,
        "duration": 4.5
    },
    {
        "text": "non-zero elements so having a non-zero",
        "start": 2409.48,
        "duration": 6.24
    },
    {
        "text": "element is more costly than a change in",
        "start": 2412.0,
        "duration": 7.44
    },
    {
        "text": "our variables okay now uh this intuition",
        "start": 2415.72,
        "duration": 5.22
    },
    {
        "text": "would imply the following possible",
        "start": 2419.44,
        "duration": 3.78
    },
    {
        "text": "algorithm right we we use greedy",
        "start": 2420.94,
        "duration": 3.659
    },
    {
        "text": "algorithm as long as zero is not",
        "start": 2423.22,
        "duration": 3.84
    },
    {
        "text": "feasible as soon as we see a feasible",
        "start": 2424.599,
        "duration": 5.341
    },
    {
        "text": "zero we take one of these options if",
        "start": 2427.06,
        "duration": 5.64
    },
    {
        "text": "beta is large meaning that we have small",
        "start": 2429.94,
        "duration": 5.58
    },
    {
        "text": "penalty for non-zero elements we stick",
        "start": 2432.7,
        "duration": 4.26
    },
    {
        "text": "with our greedy algorithm we don't",
        "start": 2435.52,
        "duration": 3.66
    },
    {
        "text": "switch to zero because the cost of",
        "start": 2436.96,
        "duration": 3.96
    },
    {
        "text": "switching is too high",
        "start": 2439.18,
        "duration": 5.04
    },
    {
        "text": "if beta is small then the cost of having",
        "start": 2440.92,
        "duration": 7.56
    },
    {
        "text": "non-zero elements is very high so we",
        "start": 2444.22,
        "duration": 6.18
    },
    {
        "text": "would want to switch to zero now the",
        "start": 2448.48,
        "duration": 4.379
    },
    {
        "text": "question is uh does this algorithm give",
        "start": 2450.4,
        "duration": 4.56
    },
    {
        "text": "us the optimal solution perhaps",
        "start": 2452.859,
        "duration": 4.26
    },
    {
        "text": "surprisingly the answer is no",
        "start": 2454.96,
        "duration": 4.68
    },
    {
        "text": "now there's a contra example for this",
        "start": 2457.119,
        "duration": 3.921
    },
    {
        "text": "intuitive",
        "start": 2459.64,
        "duration": 3.54
    },
    {
        "text": "algorithm which I'm not going to discuss",
        "start": 2461.04,
        "duration": 4.72
    },
    {
        "text": "because of the time instead I'm going to",
        "start": 2463.18,
        "duration": 5.399
    },
    {
        "text": "talk about the the optimal algorithm",
        "start": 2465.76,
        "duration": 4.8
    },
    {
        "text": "okay for a general case where beta is",
        "start": 2468.579,
        "duration": 4.201
    },
    {
        "text": "between zero and one turns out the",
        "start": 2470.56,
        "duration": 4.98
    },
    {
        "text": "correct way to solve this optim uh to",
        "start": 2472.78,
        "duration": 4.5
    },
    {
        "text": "solve this optimization problem is to",
        "start": 2475.54,
        "duration": 4.079
    },
    {
        "text": "cast it as a shortest path problem over",
        "start": 2477.28,
        "duration": 6.24
    },
    {
        "text": "a directed acyclic graph or uh or a dag",
        "start": 2479.619,
        "duration": 5.881
    },
    {
        "text": "let me explain this algorithm with a",
        "start": 2483.52,
        "duration": 3.72
    },
    {
        "text": "simple example suppose that the feasible",
        "start": 2485.5,
        "duration": 4.74
    },
    {
        "text": "intervals are in this form first we find",
        "start": 2487.24,
        "duration": 5.58
    },
    {
        "text": "the maximal zero sequences in these",
        "start": 2490.24,
        "duration": 4.5
    },
    {
        "text": "intervals so for example here we have",
        "start": 2492.82,
        "duration": 4.62
    },
    {
        "text": "three zero sequences or zeros that are",
        "start": 2494.74,
        "duration": 4.379
    },
    {
        "text": "feasible",
        "start": 2497.44,
        "duration": 4.02
    },
    {
        "text": "we map them to three different nodes in",
        "start": 2499.119,
        "duration": 5.581
    },
    {
        "text": "the graph so uh each feasible uh zero",
        "start": 2501.46,
        "duration": 5.879
    },
    {
        "text": "interval is going to be a node in our",
        "start": 2504.7,
        "duration": 5.7
    },
    {
        "text": "graph we also add a source and sync node",
        "start": 2507.339,
        "duration": 4.561
    },
    {
        "text": "these are going to be the nodes in our",
        "start": 2510.4,
        "duration": 4.56
    },
    {
        "text": "graph next we connect the source to the",
        "start": 2511.9,
        "duration": 5.16
    },
    {
        "text": "first node corresponding the first zero",
        "start": 2514.96,
        "duration": 4.92
    },
    {
        "text": "sequence and we assign a weight to this",
        "start": 2517.06,
        "duration": 6.12
    },
    {
        "text": "Edge then this this red this Edge uh",
        "start": 2519.88,
        "duration": 5.82
    },
    {
        "text": "comes from the previous greedy algorithm",
        "start": 2523.18,
        "duration": 5.22
    },
    {
        "text": "that I mentioned uh and I'm not going to",
        "start": 2525.7,
        "duration": 5.22
    },
    {
        "text": "talk about details of this construction",
        "start": 2528.4,
        "duration": 5.4
    },
    {
        "text": "but turns out",
        "start": 2530.92,
        "duration": 5.939
    },
    {
        "text": "uh the the resulted crap is going to be",
        "start": 2533.8,
        "duration": 5.4
    },
    {
        "text": "a weighted complete uh directed",
        "start": 2536.859,
        "duration": 4.381
    },
    {
        "text": "acilogram now the reason why we",
        "start": 2539.2,
        "duration": 4.68
    },
    {
        "text": "construct this synthetically generated",
        "start": 2541.24,
        "duration": 5.82
    },
    {
        "text": "graph is because it turns out that the",
        "start": 2543.88,
        "duration": 4.92
    },
    {
        "text": "optimal cost for the problem that I",
        "start": 2547.06,
        "duration": 4.26
    },
    {
        "text": "mentioned for General beta between zero",
        "start": 2548.8,
        "duration": 5.46
    },
    {
        "text": "zero and one corresponds to the shortest",
        "start": 2551.32,
        "duration": 5.519
    },
    {
        "text": "path between the source and the sink in",
        "start": 2554.26,
        "duration": 4.74
    },
    {
        "text": "this new graph and which can be solved",
        "start": 2556.839,
        "duration": 4.081
    },
    {
        "text": "very efficiently using a dynamic",
        "start": 2559.0,
        "duration": 3.3
    },
    {
        "text": "programming",
        "start": 2560.92,
        "duration": 2.58
    },
    {
        "text": "okay",
        "start": 2562.3,
        "duration": 4.34
    },
    {
        "text": "now uh",
        "start": 2563.5,
        "duration": 5.64
    },
    {
        "text": "finally we can put all these pieces",
        "start": 2566.64,
        "duration": 6.16
    },
    {
        "text": "together and we can we can uh solve the",
        "start": 2569.14,
        "duration": 5.88
    },
    {
        "text": "original optimization problem that I",
        "start": 2572.8,
        "duration": 3.84
    },
    {
        "text": "mentioned and we can recover the",
        "start": 2575.02,
        "duration": 3.599
    },
    {
        "text": "Precision Matrix now I'm not going to",
        "start": 2576.64,
        "duration": 4.199
    },
    {
        "text": "talk about different steps together but",
        "start": 2578.619,
        "duration": 3.841
    },
    {
        "text": "I'm going to mention the last result",
        "start": 2580.839,
        "duration": 3.601
    },
    {
        "text": "which is the computational guarantee",
        "start": 2582.46,
        "duration": 5.28
    },
    {
        "text": "turns out that the proposed method or",
        "start": 2584.44,
        "duration": 7.02
    },
    {
        "text": "all these steps can be done uh in um",
        "start": 2587.74,
        "duration": 6.599
    },
    {
        "text": "time and memory complexity that scales",
        "start": 2591.46,
        "duration": 6.659
    },
    {
        "text": "with d squared times T which is exactly",
        "start": 2594.339,
        "duration": 6.24
    },
    {
        "text": "the number of uh variables in the",
        "start": 2598.119,
        "duration": 5.46
    },
    {
        "text": "problem remember e at each time the",
        "start": 2600.579,
        "duration": 4.981
    },
    {
        "text": "Precision Matrix is d by D so we have D",
        "start": 2603.579,
        "duration": 3.601
    },
    {
        "text": "Squared number of variables and we have",
        "start": 2605.56,
        "duration": 3.66
    },
    {
        "text": "two different times so the total number",
        "start": 2607.18,
        "duration": 4.139
    },
    {
        "text": "of variables is going to be D squared",
        "start": 2609.22,
        "duration": 4.68
    },
    {
        "text": "times T which means that the complexity",
        "start": 2611.319,
        "duration": 5.28
    },
    {
        "text": "of solving this problem scales linearly",
        "start": 2613.9,
        "duration": 5.699
    },
    {
        "text": "with a number of unknown uh variables or",
        "start": 2616.599,
        "duration": 5.161
    },
    {
        "text": "unknown parameters this is exactly what",
        "start": 2619.599,
        "duration": 4.201
    },
    {
        "text": "we wanted all right",
        "start": 2621.76,
        "duration": 3.96
    },
    {
        "text": "now for the rest of my talk I'm just",
        "start": 2623.8,
        "duration": 3.66
    },
    {
        "text": "going to give you some some uh",
        "start": 2625.72,
        "duration": 4.74
    },
    {
        "text": "experimental results now in our first",
        "start": 2627.46,
        "duration": 5.1
    },
    {
        "text": "case study we consider a randomly",
        "start": 2630.46,
        "duration": 4.74
    },
    {
        "text": "generated massive data set here the data",
        "start": 2632.56,
        "duration": 3.9
    },
    {
        "text": "is collected from a gaussian",
        "start": 2635.2,
        "duration": 3.36
    },
    {
        "text": "distribution with a sparsity changing",
        "start": 2636.46,
        "duration": 5.04
    },
    {
        "text": "inverse covariance Matrix now here on",
        "start": 2638.56,
        "duration": 5.519
    },
    {
        "text": "the left figure you saw you you you you",
        "start": 2641.5,
        "duration": 6.78
    },
    {
        "text": "see the runtime of our algorithm as a",
        "start": 2644.079,
        "duration": 5.401
    },
    {
        "text": "function of the total number of",
        "start": 2648.28,
        "duration": 3.72
    },
    {
        "text": "variables which is equal to D squared",
        "start": 2649.48,
        "duration": 4.98
    },
    {
        "text": "times T you can see that the run time is",
        "start": 2652.0,
        "duration": 4.38
    },
    {
        "text": "almost linear with respect to a number",
        "start": 2654.46,
        "duration": 3.78
    },
    {
        "text": "of variables which verifies our theorem",
        "start": 2656.38,
        "duration": 4.26
    },
    {
        "text": "using our method we could solve",
        "start": 2658.24,
        "duration": 4.619
    },
    {
        "text": "instances of the problem with more than",
        "start": 2660.64,
        "duration": 4.38
    },
    {
        "text": "5 million variables in less than one",
        "start": 2662.859,
        "duration": 4.381
    },
    {
        "text": "hour on a normal laptop computer the",
        "start": 2665.02,
        "duration": 4.38
    },
    {
        "text": "regularized mle in this scale cannot",
        "start": 2667.24,
        "duration": 4.8
    },
    {
        "text": "even start running running the algorithm",
        "start": 2669.4,
        "duration": 5.4
    },
    {
        "text": "okay and the right figure shows a",
        "start": 2672.04,
        "duration": 5.88
    },
    {
        "text": "statistical consistency and we get as we",
        "start": 2674.8,
        "duration": 5.4
    },
    {
        "text": "increase the number of variables we get",
        "start": 2677.92,
        "duration": 5.52
    },
    {
        "text": "close to 100 percent sparsity accuracy",
        "start": 2680.2,
        "duration": 5.639
    },
    {
        "text": "okay okay now the next slide shows the",
        "start": 2683.44,
        "duration": 4.02
    },
    {
        "text": "performance of an algorithm after",
        "start": 2685.839,
        "duration": 4.02
    },
    {
        "text": "parallelization remember our algorithm",
        "start": 2687.46,
        "duration": 5.76
    },
    {
        "text": "is amenable to uh parallelization uh",
        "start": 2689.859,
        "duration": 4.681
    },
    {
        "text": "because of this element-wise",
        "start": 2693.22,
        "duration": 4.44
    },
    {
        "text": "decomposability using five cores we",
        "start": 2694.54,
        "duration": 4.5
    },
    {
        "text": "could reduce the runtime of our",
        "start": 2697.66,
        "duration": 4.86
    },
    {
        "text": "algorithm by 40 percent on average but",
        "start": 2699.04,
        "duration": 5.34
    },
    {
        "text": "beyond that increasing the number of",
        "start": 2702.52,
        "duration": 3.78
    },
    {
        "text": "cores does not really significantly",
        "start": 2704.38,
        "duration": 4.08
    },
    {
        "text": "reduce the runtime in fact it may lead",
        "start": 2706.3,
        "duration": 3.92
    },
    {
        "text": "to some memory issues",
        "start": 2708.46,
        "duration": 5.52
    },
    {
        "text": "which I'm not going to discuss here now",
        "start": 2710.22,
        "duration": 6.099
    },
    {
        "text": "our next case study will be on gene",
        "start": 2713.98,
        "duration": 4.44
    },
    {
        "text": "expression data I mentioned at the",
        "start": 2716.319,
        "duration": 5.28
    },
    {
        "text": "beginning of my talk uh that here our",
        "start": 2718.42,
        "duration": 6.0
    },
    {
        "text": "goal is to understand the underlying uh",
        "start": 2721.599,
        "duration": 5.701
    },
    {
        "text": "Gene regulatory Network given the gene",
        "start": 2724.42,
        "duration": 5.28
    },
    {
        "text": "expression data and uh the gene",
        "start": 2727.3,
        "duration": 4.26
    },
    {
        "text": "regulatory Network can be used to",
        "start": 2729.7,
        "duration": 3.72
    },
    {
        "text": "identify different regulatory",
        "start": 2731.56,
        "duration": 4.44
    },
    {
        "text": "interactions between genes which can",
        "start": 2733.42,
        "duration": 4.56
    },
    {
        "text": "probably help us better understand",
        "start": 2736.0,
        "duration": 5.22
    },
    {
        "text": "different disease processes like cancer",
        "start": 2737.98,
        "duration": 5.879
    },
    {
        "text": "in a macro level rather than individual",
        "start": 2741.22,
        "duration": 6.0
    },
    {
        "text": "Gene levels uh here we collect the data",
        "start": 2743.859,
        "duration": 7.621
    },
    {
        "text": "from a glioblastoma tissue of a patient",
        "start": 2747.22,
        "duration": 7.32
    },
    {
        "text": "here in Michigan medicine the data is",
        "start": 2751.48,
        "duration": 5.639
    },
    {
        "text": "collected uh in Michigan medicine and",
        "start": 2754.54,
        "duration": 4.5
    },
    {
        "text": "the data is pulled from two adjacent",
        "start": 2757.119,
        "duration": 5.281
    },
    {
        "text": "tissues and expression data is obtained",
        "start": 2759.04,
        "duration": 4.22
    },
    {
        "text": "from",
        "start": 2762.4,
        "duration": 3.9
    },
    {
        "text": "2500 most variable genes and then",
        "start": 2763.26,
        "duration": 6.099
    },
    {
        "text": "through a 3D fancy 3D embedding of the",
        "start": 2766.3,
        "duration": 6.059
    },
    {
        "text": "data which I'm not an expert in we could",
        "start": 2769.359,
        "duration": 5.641
    },
    {
        "text": "cluster the data in five different parts",
        "start": 2772.359,
        "duration": 5.581
    },
    {
        "text": "or regions you can see these clusters",
        "start": 2775.0,
        "duration": 5.579
    },
    {
        "text": "here with different colors each cluster",
        "start": 2777.94,
        "duration": 4.5
    },
    {
        "text": "will be modeled as an individual Markov",
        "start": 2780.579,
        "duration": 3.901
    },
    {
        "text": "random Guild now the important thing",
        "start": 2782.44,
        "duration": 5.399
    },
    {
        "text": "that I need to mention here is that um",
        "start": 2784.48,
        "duration": 5.879
    },
    {
        "text": "unlike the previous case study here the",
        "start": 2787.839,
        "duration": 4.861
    },
    {
        "text": "data does not have a temporal component",
        "start": 2790.359,
        "duration": 5.701
    },
    {
        "text": "and instead it is spatially varying so",
        "start": 2792.7,
        "duration": 5.28
    },
    {
        "text": "these five different marker of random",
        "start": 2796.06,
        "duration": 3.72
    },
    {
        "text": "fields are specially correlated to each",
        "start": 2797.98,
        "duration": 3.48
    },
    {
        "text": "other rather than temporal correlation",
        "start": 2799.78,
        "duration": 4.44
    },
    {
        "text": "but our method can be extended to this",
        "start": 2801.46,
        "duration": 4.8
    },
    {
        "text": "case as well now",
        "start": 2804.22,
        "duration": 2.639
    },
    {
        "text": "um",
        "start": 2806.26,
        "duration": 2.28
    },
    {
        "text": "next thing I want to mention is the",
        "start": 2806.859,
        "duration": 3.381
    },
    {
        "text": "parameter tuning",
        "start": 2808.54,
        "duration": 4.68
    },
    {
        "text": "now there are different parameters that",
        "start": 2810.24,
        "duration": 4.78
    },
    {
        "text": "we need to find you and obviously based",
        "start": 2813.22,
        "duration": 4.379
    },
    {
        "text": "on the available data for example",
        "start": 2815.02,
        "duration": 4.799
    },
    {
        "text": "how do we pick the regularization",
        "start": 2817.599,
        "duration": 4.381
    },
    {
        "text": "coefficient right in the objective or",
        "start": 2819.819,
        "duration": 5.04
    },
    {
        "text": "really other parameters in the in the",
        "start": 2821.98,
        "duration": 5.04
    },
    {
        "text": "proposed method now to fine tune the",
        "start": 2824.859,
        "duration": 4.201
    },
    {
        "text": "regular regularization coefficient the",
        "start": 2827.02,
        "duration": 4.799
    },
    {
        "text": "objective we make them proportional to",
        "start": 2829.06,
        "duration": 5.519
    },
    {
        "text": "their spatial and expression distance so",
        "start": 2831.819,
        "duration": 5.401
    },
    {
        "text": "if for example two clusters are closed",
        "start": 2834.579,
        "duration": 4.921
    },
    {
        "text": "spatially the regularization coefficient",
        "start": 2837.22,
        "duration": 4.44
    },
    {
        "text": "between these two clusters will be large",
        "start": 2839.5,
        "duration": 4.5
    },
    {
        "text": "because we want to encourage more",
        "start": 2841.66,
        "duration": 5.0
    },
    {
        "text": "similarity in their learned networks",
        "start": 2844.0,
        "duration": 5.28
    },
    {
        "text": "there are also other parameters that you",
        "start": 2846.66,
        "duration": 5.679
    },
    {
        "text": "need to fine-tune for example we need to",
        "start": 2849.28,
        "duration": 5.16
    },
    {
        "text": "set the upper bound and lower bound on",
        "start": 2852.339,
        "duration": 5.701
    },
    {
        "text": "the constraint of our problem uh and we",
        "start": 2854.44,
        "duration": 5.399
    },
    {
        "text": "set these parameters based on pic",
        "start": 2858.04,
        "duration": 3.84
    },
    {
        "text": "Criterion we just",
        "start": 2859.839,
        "duration": 4.321
    },
    {
        "text": "try different values estimate the",
        "start": 2861.88,
        "duration": 4.5
    },
    {
        "text": "network and measure their likelihood and",
        "start": 2864.16,
        "duration": 3.54
    },
    {
        "text": "then pick the one that gives us the",
        "start": 2866.38,
        "duration": 3.8
    },
    {
        "text": "highest likelihood okay",
        "start": 2867.7,
        "duration": 6.119
    },
    {
        "text": "this was a brief overview of the of how",
        "start": 2870.18,
        "duration": 5.38
    },
    {
        "text": "we set the parameters now I should also",
        "start": 2873.819,
        "duration": 4.5
    },
    {
        "text": "mention that the gene expression data is",
        "start": 2875.56,
        "duration": 5.759
    },
    {
        "text": "not gaussian it is known to have a",
        "start": 2878.319,
        "duration": 5.221
    },
    {
        "text": "negative binomial distribution now to",
        "start": 2881.319,
        "duration": 4.861
    },
    {
        "text": "use this gaussian Markov a random field",
        "start": 2883.54,
        "duration": 4.68
    },
    {
        "text": "we need to use this non-paranormal",
        "start": 2886.18,
        "duration": 3.96
    },
    {
        "text": "transformation to change the",
        "start": 2888.22,
        "duration": 4.139
    },
    {
        "text": "distribution of data to gaussian now",
        "start": 2890.14,
        "duration": 3.66
    },
    {
        "text": "next I'm going to talk about very",
        "start": 2892.359,
        "duration": 3.781
    },
    {
        "text": "briefly talk about the the results now",
        "start": 2893.8,
        "duration": 3.9
    },
    {
        "text": "unfortunately I can't really show the",
        "start": 2896.14,
        "duration": 3.24
    },
    {
        "text": "infrared networks in each cluster",
        "start": 2897.7,
        "duration": 3.84
    },
    {
        "text": "because there are big networks what I",
        "start": 2899.38,
        "duration": 5.16
    },
    {
        "text": "show instead is the 15 strongest edges",
        "start": 2901.54,
        "duration": 5.819
    },
    {
        "text": "in each cluster for the infrared Network",
        "start": 2904.54,
        "duration": 7.02
    },
    {
        "text": "and the node sizes here are scaled with",
        "start": 2907.359,
        "duration": 6.121
    },
    {
        "text": "their degree so larger nodes means that",
        "start": 2911.56,
        "duration": 4.32
    },
    {
        "text": "they have a higher degree based on these",
        "start": 2913.48,
        "duration": 4.02
    },
    {
        "text": "graphs you can easily see that the",
        "start": 2915.88,
        "duration": 4.14
    },
    {
        "text": "different clusters have uh really",
        "start": 2917.5,
        "duration": 4.7
    },
    {
        "text": "distinct with regulatory interactions",
        "start": 2920.02,
        "duration": 5.46
    },
    {
        "text": "that are really different from uh from",
        "start": 2922.2,
        "duration": 5.8
    },
    {
        "text": "each other in fact if you look at the",
        "start": 2925.48,
        "duration": 5.4
    },
    {
        "text": "number of edges in different clusters uh",
        "start": 2928.0,
        "duration": 4.92
    },
    {
        "text": "you can see that they are drastically",
        "start": 2930.88,
        "duration": 3.66
    },
    {
        "text": "different some of the Clusters are super",
        "start": 2932.92,
        "duration": 4.34
    },
    {
        "text": "sparse for example cluster 3 has only",
        "start": 2934.54,
        "duration": 7.799
    },
    {
        "text": "446 edges recall that our uh the each",
        "start": 2937.26,
        "duration": 8.2
    },
    {
        "text": "cluster has has 2500 nodes another",
        "start": 2942.339,
        "duration": 4.441
    },
    {
        "text": "cluster",
        "start": 2945.46,
        "duration": 4.68
    },
    {
        "text": "um has close to 14 000 edges this shows",
        "start": 2946.78,
        "duration": 6.539
    },
    {
        "text": "that uh uh uh the the the infrared",
        "start": 2950.14,
        "duration": 6.54
    },
    {
        "text": "networks are heterogeneous but you also",
        "start": 2953.319,
        "duration": 5.28
    },
    {
        "text": "have some level of sparsity that are",
        "start": 2956.68,
        "duration": 3.6
    },
    {
        "text": "coming from the spatial regularization",
        "start": 2958.599,
        "duration": 5.101
    },
    {
        "text": "uh here you can see the heat map of the",
        "start": 2960.28,
        "duration": 5.579
    },
    {
        "text": "similarity between different inferred",
        "start": 2963.7,
        "duration": 4.56
    },
    {
        "text": "networks in different clusters uh you",
        "start": 2965.859,
        "duration": 4.021
    },
    {
        "text": "can see that some of these clusters have",
        "start": 2968.26,
        "duration": 3.48
    },
    {
        "text": "some level of similarities for example",
        "start": 2969.88,
        "duration": 3.78
    },
    {
        "text": "here cluster one and three have",
        "start": 2971.74,
        "duration": 4.56
    },
    {
        "text": "similarity degree of 0.3",
        "start": 2973.66,
        "duration": 4.56
    },
    {
        "text": "a similarity degree of one basically",
        "start": 2976.3,
        "duration": 4.019
    },
    {
        "text": "means that they're exactly identical",
        "start": 2978.22,
        "duration": 6.06
    },
    {
        "text": "uh now I uh I also want to mention uh",
        "start": 2980.319,
        "duration": 6.54
    },
    {
        "text": "that our results are biologically",
        "start": 2984.28,
        "duration": 4.68
    },
    {
        "text": "meaningful here you can see the",
        "start": 2986.859,
        "duration": 3.841
    },
    {
        "text": "interactions between the transcription",
        "start": 2988.96,
        "duration": 5.159
    },
    {
        "text": "factors in in our infer network if I",
        "start": 2990.7,
        "duration": 6.119
    },
    {
        "text": "understand correctly uh these uh",
        "start": 2994.119,
        "duration": 4.681
    },
    {
        "text": "transcription factors are proteins that",
        "start": 2996.819,
        "duration": 4.26
    },
    {
        "text": "play an important role in the evolution",
        "start": 2998.8,
        "duration": 4.92
    },
    {
        "text": "of the cancer uh here are the",
        "start": 3001.079,
        "duration": 5.101
    },
    {
        "text": "interactions among the most uh important",
        "start": 3003.72,
        "duration": 4.26
    },
    {
        "text": "transcription factors in different",
        "start": 3006.18,
        "duration": 3.54
    },
    {
        "text": "clusters based on their infrared",
        "start": 3007.98,
        "duration": 4.08
    },
    {
        "text": "networks some of these factors clearly",
        "start": 3009.72,
        "duration": 5.52
    },
    {
        "text": "stand out right since they are they have",
        "start": 3012.06,
        "duration": 5.7
    },
    {
        "text": "large degree interestingly these are",
        "start": 3015.24,
        "duration": 4.619
    },
    {
        "text": "exactly the transcription factors that",
        "start": 3017.76,
        "duration": 4.92
    },
    {
        "text": "are that are known to play key role in",
        "start": 3019.859,
        "duration": 5.401
    },
    {
        "text": "the glioblastoma cancer which was the",
        "start": 3022.68,
        "duration": 5.939
    },
    {
        "text": "subject of our study all right now uh I",
        "start": 3025.26,
        "duration": 4.74
    },
    {
        "text": "think I'm running out of time so I'm",
        "start": 3028.619,
        "duration": 3.361
    },
    {
        "text": "just going to conclude my talk uh as I",
        "start": 3030.0,
        "duration": 4.26
    },
    {
        "text": "mentioned the main goal of this talk was",
        "start": 3031.98,
        "duration": 6.24
    },
    {
        "text": "to infer large scale uh Dynamic Markov",
        "start": 3034.26,
        "duration": 6.0
    },
    {
        "text": "random Fields with strong statistical",
        "start": 3038.22,
        "duration": 3.78
    },
    {
        "text": "and computational identities that was",
        "start": 3040.26,
        "duration": 4.26
    },
    {
        "text": "our goal we developed a scalable",
        "start": 3042.0,
        "duration": 5.94
    },
    {
        "text": "inference method for dynamic Markov",
        "start": 3044.52,
        "duration": 5.16
    },
    {
        "text": "random fields for the inference of",
        "start": 3047.94,
        "duration": 3.419
    },
    {
        "text": "dynamic markup random fields are under",
        "start": 3049.68,
        "duration": 3.54
    },
    {
        "text": "different site information such as",
        "start": 3051.359,
        "duration": 3.361
    },
    {
        "text": "Sports City",
        "start": 3053.22,
        "duration": 4.139
    },
    {
        "text": "and other temporal or spatial structures",
        "start": 3054.72,
        "duration": 4.859
    },
    {
        "text": "and in practice we showed that they they",
        "start": 3057.359,
        "duration": 6.181
    },
    {
        "text": "can we can solve different instances of",
        "start": 3059.579,
        "duration": 6.961
    },
    {
        "text": "the problem in fact instances with more",
        "start": 3063.54,
        "duration": 5.1
    },
    {
        "text": "than 500 million variables in less than",
        "start": 3066.54,
        "duration": 4.019
    },
    {
        "text": "one hour and we also talked about an",
        "start": 3068.64,
        "duration": 4.26
    },
    {
        "text": "important application of this problem in",
        "start": 3070.559,
        "duration": 4.381
    },
    {
        "text": "gene regulatory networks now here are",
        "start": 3072.9,
        "duration": 4.62
    },
    {
        "text": "the references uh this talk was mainly",
        "start": 3074.94,
        "duration": 4.26
    },
    {
        "text": "based on the first two papers that I",
        "start": 3077.52,
        "duration": 3.66
    },
    {
        "text": "mentioned the part that I didn't talk",
        "start": 3079.2,
        "duration": 5.659
    },
    {
        "text": "about was this the spatial component",
        "start": 3081.18,
        "duration": 6.84
    },
    {
        "text": "and and other distributions those are",
        "start": 3084.859,
        "duration": 5.921
    },
    {
        "text": "based on the the last the the bottom two",
        "start": 3088.02,
        "duration": 4.74
    },
    {
        "text": "papers one of them is published the",
        "start": 3090.78,
        "duration": 5.64
    },
    {
        "text": "other one is underper preparation now",
        "start": 3092.76,
        "duration": 5.16
    },
    {
        "text": "I'm going to stop here and I would be",
        "start": 3096.42,
        "duration": 5.06
    },
    {
        "text": "happy to answer any questions",
        "start": 3097.92,
        "duration": 3.56
    },
    {
        "text": "thank you sir we'll give you a round of",
        "start": 3105.24,
        "duration": 5.06
    },
    {
        "text": "applause thank you",
        "start": 3107.339,
        "duration": 2.961
    },
    {
        "text": "um so does anyone have any questions if",
        "start": 3110.339,
        "duration": 4.381
    },
    {
        "text": "you're in the room uh you can ask so far",
        "start": 3112.02,
        "duration": 4.44
    },
    {
        "text": "there have been no questions uh online",
        "start": 3114.72,
        "duration": 3.24
    },
    {
        "text": "but if you're online you've got",
        "start": 3116.46,
        "duration": 3.06
    },
    {
        "text": "questions you can put them in the chat",
        "start": 3117.96,
        "duration": 2.399
    },
    {
        "text": "box",
        "start": 3119.52,
        "duration": 2.46
    },
    {
        "text": "um we can read them out or raise your",
        "start": 3120.359,
        "duration": 3.361
    },
    {
        "text": "Zoom hand and we can call to unmute you",
        "start": 3121.98,
        "duration": 3.54
    },
    {
        "text": "and again if you guys if there are any",
        "start": 3123.72,
        "duration": 3.54
    },
    {
        "text": "questions in the room uh feel free to",
        "start": 3125.52,
        "duration": 3.86
    },
    {
        "text": "ask",
        "start": 3127.26,
        "duration": 2.12
    },
    {
        "text": "so we'll just give it a couple of",
        "start": 3142.26,
        "duration": 2.88
    },
    {
        "text": "minutes just in case anyone online",
        "start": 3143.76,
        "duration": 4.38
    },
    {
        "text": "typing",
        "start": 3145.14,
        "duration": 3.0
    },
    {
        "text": "foreign",
        "start": 3164.339,
        "duration": 2.78
    },
    {
        "text": "or hearing any questions and solar I",
        "start": 3179.9,
        "duration": 5.08
    },
    {
        "text": "know that you have another meeting that",
        "start": 3183.059,
        "duration": 3.241
    },
    {
        "text": "you need to get to so I don't want to",
        "start": 3184.98,
        "duration": 4.639
    },
    {
        "text": "yes keep you uh extra long or anything",
        "start": 3186.3,
        "duration": 6.48
    },
    {
        "text": "thanks again thanks uh everyone for",
        "start": 3189.619,
        "duration": 5.681
    },
    {
        "text": "attending to talk uh thanks Marcy for",
        "start": 3192.78,
        "duration": 4.44
    },
    {
        "text": "the invitation",
        "start": 3195.3,
        "duration": 4.08
    },
    {
        "text": "um and yeah",
        "start": 3197.22,
        "duration": 3.72
    },
    {
        "text": "thanks for taking the time thanks",
        "start": 3199.38,
        "duration": 3.3
    },
    {
        "text": "everyone for coming and hopefully I'll",
        "start": 3200.94,
        "duration": 4.619
    },
    {
        "text": "see everyone at next week's presentation",
        "start": 3202.68,
        "duration": 5.54
    },
    {
        "text": "bye",
        "start": 3205.559,
        "duration": 2.661
    }
]