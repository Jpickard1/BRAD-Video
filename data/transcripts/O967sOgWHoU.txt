Brett your ear used to run the DARPA defense Sciences office you know George post it was one of our big speakers and a luminary and they all want to know what you can do so let us know the Sabbath great all right stop so first let me introduce myself my name is Jo I was a postdoc working with Professor jibing year for the last of three and a half years recently I was promoted to a research assistant professor so archery and very odd and honored to present our work in the past Assyrian half years so the topic of today's talk is a smart screening for a pig actually when I are prepared this talk it really took me a long time to decide if I should include this keyword or not because our method is not limited to medical data our method is actually alert you know we do not we do not make any special assumptions on the data so essentially it can be applied to any type of deal ok so right now there are many projects carried out by our group which is a list we have fulfilled our embroideries this major depression disorder and imaging genetics the combination of all of those projects with that the unusual huge scale data in many medical applications the data is a quite wide and fat basics which mean that the number of features can be much larger than the number of samples so for example for the edema and she was thinner many said has a few hundreds or some of patients but it may have millions or tens of millions of the coherence so that pretty matrix such as recommender systems of image classification is not those kind of applications the data is many real-world applications we have a common observation which is from the data with with a meaning the variables of complicated structures it can actually be well interpreted a very small subset of the features to select important features and powerful methods is a so-called feature learning models in detail what if what's the meaning of stuff feature [Music] which means the number is much larger than the number of features studying have shown that we can frequently find a small subset of most representative Houston days this set of data is representative distances can be very helpful for us to understand the underlying structure for those kind of applications and the k-means contouring okay there are many many like I mentioned before there are many many evidence proposed to select the most important features of the most representative instance the so called the heuristic based on data reduction method sure screening random projection something like that this actually works pretty well in real applications by a leading figure but the chief who is a professor in Princeton University this is actually is actually a approximation of the will sometimes results with specific there are many existing works analyze the quality of the approximate solution we have devoted research efforts to atomic named the screening this is actually a new framework for for exact they are in action to imply that this method is exact so what does it mean if the model learnt from the reveals that data it's guaranteed to be identical to the mono learnt from the floor data set this is probably one of the most appealing feature of our method so using stock focus on two models the 5y beta which is a feature reduction method second why is support vector machine for Paul data which is somehow with actually method actually pretty flexible they can be extended to much complicating one of them these introduced what is let's give a brief review of linear regression pretty simple linear regression model is probably still one of the most popular methods for linear regression model we usually assume that the outcome why it's linear regarding to the regarding regarding to the variable we run into the covariance and as if you assume this noise term obey the normal distribution so the good for linear regression more is how can we estimate the the coefficient the coefficient the coefficient vector which regression is actually used the technique to estimate the coefficient matrix ax if the coefficient matrix and this is a it's represent the divulging of rich Russian consists of two parts the surfaces last function which measures the fairness of model we're going to the observed data at the second one is the horizon they say is actually here and they limit the complexity of the model because we know that complex math is usually prone to overfitting in case you don't know feeding is computable or feeding in high school general evasion performance for mass spec so now specifically didn't mean that you may have very small training error which means your model feet we absorb the data pretty well it may give you better generation performance which means this may result in huge errors actually to estimate the coefficient the coefficient the coefficient vector the only difference between lasso and the ridge regression is you see we have the same last function we also use make use of the lift will ask to we know as a regular atom so the major property of this is that we are researching many components in the solution vectors so according to matrix and vector multiplication let's assume that this component of the coefficient the coefficient vector is 0 it means the corresponding seizure has nothing to do with the outcome so the in this case even if we remove this feature from the deficit we are essentially the same model as a final stage and in the applications including genomics genetics singular and audio processing image processing neuroscience for instance Keys is first proposed by this guy Robert Ronnie kind of the price which is it's kind of like the Nobel Prize in Physics maybe two years ago fifteen years after the publication of this paper actually a few of people not always measured the reason is probably because is probably due to the this river island because this really rather is not differentiable are not able to solve this kind of problem this is not very practical because we do not have fish in the fourth to solve another problem moderate dataset with maybe a few hundred thousand features even some features it may need takes all combinations so this is probably the most important reason that limit the application of in first two years at least you can see that this fever is founded by 14 solvent UI researcher working in machinery and statistics probably the most models we have the of managing technique original to smooth functions family of the state of the include continent descent sub gradient descent documentary the Lagrangian method where it is gradient descent absolutely required indecent and our group has contributed significantly in this in this we have proposed to many fishing to evidence to solve the last type evidence package name the slap slavery implements many instead of the state-of-the-art evidence and right now we have forty forty five hundred users from 25 countries this package receives two hundred about two hundred and thirty citations and this package is developed acting around 2010 it's not a seed of the art anymore [Music] Universal texels are often we have developed a much much more efficient to solve our benefit ok if you are interested in interested in this we can we can talk about this that the scale of the data increases exclusively in the past few years and it's still keep growing but really traditional that is extremely huge with meanings even in real applications - action parameter tuning right so this envious even quicker challenge to all the existing arrogance few years we actually trying to answer two questions the first way is can we make existing lots of our much more efficient this is the first question can we and second it's yes with we spent last three and a half years to answer those two questions that we can pretty good result in our kitchens so let's go to today's topic our screening we have original data we may have one meaning basically sparsity assumption we know that a very large portion of fishers have nothing to do with the outcome if we can pre identify the features the highly relevant to the output we can remove we can throw a little fever we obtained significantly reduce that error that we to be to the our this is least square loss this is why is beta is what we need to estimate all right this is a figure are you the only the only difference is that we induce feature tree model to the regular rider we will eventually have in the in the coefficient vector and we know that 0 components implies the creatures have nothing to do with which coming through away from all my vision or originates from the do formalization of Lhasa this is actually a projection problem this is a this is a to optimum is to what do you feel well set according to these geometric intuition and the automatic needy so be free all these this term the Keynesian vision is the first order optimal condition you just said the gradient equal to 0 or we mention it before as a precondition we have finally arrived at this simple rule this is this is the linear right so it's actually very simple to solve this on a vision problem if this maximum value is strictly less than 1 then we can conclude that the corresponding component of the solution has to be there remove the eye feature from the data matrix if there was a mass techniques pretty suffocated to make you know the now expensive needs of the to optimal solution finally can be divided into three steps the first is we estimate the do variable as I mean it's the admin smooches of the do problem eternal that we can we can use the all the very small pole to count the to optimum is to some reason or another aging problem because the objective function is linear and the contrary the ball so it's not very easy to solve the problem and family is just determining if X is strictly less than 1 or not if this if s is strictly less than 1 then we can conclude that the cratering component in the coefficient vector must be there as a creek running features can be removed from the origin now let's see some results this one derived from the Arcadian using the first all there Ultimo conditions here may not be the easy to answer with a condition based on this condition we can derive this equation is X inner product is Y or minus 1 the quiz bonding components must be it is these calls that you know then it must be here so according to these two rules we can conclude that it was the absolute value of these in your product is strictly less than 1 [Music] we have a sequence of different this is the first mode much better sequence alumnus each you fix you yes yes yes yes one two three four yeah hundred and it is to actually I forget to the denim it is actually have 550 thousand two features yes yes 54 the features is about seven yeah these shoes the solver with a screen to solve the problem of water dance Ernie is about more than 40 minutes to fill them all 100 this is a preliminary version of our method it needs about five minutes fine how minutes and this is the improved version of our method it only needs a bit more ten seconds so compared to the solo without screening we can see that this trick you can see these teeth actually but as you are 230 times speed is strong this matter is proposed by relative Johnny yeah to find out why the method heuristic based actually been a long time to prove their methods are accurate because in your applications the method is the method will in the in the paper the to mention a counter example the method can make mistakes so they call the American matter to prove to fix the method so this is one of reason we publish our work usually actually email our group and he is very glad to see that we have solve this problem who have proposed exact winning method is comparable to raro images images this is a handy I'm talking about if I think to truly determine by the party of like how many how many parameters can you push this to before II like used to start Randall you some that work they can a mere mortal can yeah so just of these method but maybe this is our major contribution this is rejection rigid respect that quickly almost all of their components the visually is approximated to one 100 percent is 7 4 7 and the it has about half meaning the response is the volume of the brain region and without results winning the solar actually needs more than 10 hours to fill the model our winner tricks with with a screening method the time it reduced to about 3 minutes means we actually evaluate our method are even larger Tecna measure phase the x-axis represents the number of features the unit is our meaning we can see that if the is roughly the same the speed-up gained by method we are in visual dimension in critics these features are sleep data yeah that's a snip data yeah this is a pretty new book published this May written by our three leading researchers yeah field trip 50 robot ship journey and man human right and this is the guy who developed the lasso this one is his favors working in statistics and the other one in statics we believe I will eventually be a member of American Academy of Science they spend a whole section to introduce our method this is the preliminary version of our method the week a very high compliment home our our Amazon in fact our method is the one of the best method in our screening he said the message is actually dominant with earliest maybe our accepted by the Journal of machine learning research and published this week due to the publication of this book is a pretty flexible it can be extended to pretty complex models for example the tree lasso in the tree we assume that the tree structure in the features and this way is actually very complicated to solve finally sure that we can extend our method to these complicated problems oops I created a website to release our code and resourced you can take a look if you are interested in that you can take a look at this website download the code and put we may we may have a Python version and a version the next few months talk about the animation machine is a high value of the technique to select the most representative data instance if the maximum margin classifier it actually will give you a higher plan this will separate the positive samples and the negative samples they tries to maximize the margin represented by these are yellow region fear me not vn leads are determined by the so called support vectors compared to the benefit the number of support vectors might be very small for example in this only three the Clintons are the small vectors so question is can we make use of these observation because really it can be determined completely so-called small vectors this in high that the non small vendors are irrelevant to this classifier so our question is can we make use of this observation thus winning the idea that our idea of screening for SVM is this is the problem and we first apply the testing rule to identify if in an instance is a small vector or not if we can see that the factor the data uses is a non-polar actor today we can remove from the region existing solvers to solve the small vector machine problem all these reduced at in a set free milk is almost the same with so first we estimate the two ordinal solution second we saw two simple organization problem so we just if F is strictly larger than 100 G is strictly less than 1 we can identify a large portion of the non small vectors example on three synthetic data we can see this this repair this figure represents the rejection issue the relation between the number of identified non support vectors vector in truth so this figures show that our method can correctly identify almost all of the non support vectors even if the process that overlaps largely overlaps this for this for this example yes we add a noise to the data uses this is our experiments are several reinvent the to the speedup cleaned by our method and we can say that combining with our Aksum they can actually give you up to eighty times beyond this is another way to use a winery all tomorrow we should call the IOA d it's pretty similar to to SVM so I will just skip skip this one and for this one the performance is even better more than it can give you all that 100 times laughs you know I work a lot screaming in the past three years Kennesaw can be summarized as we have developed a suit of novel techniques the first type is for feature screening and the second hobbies for our sample screening the most appealing feature of our technique is that the model based on the reduce the data it's identical to the construction from the for data experiment issue that our method works pretty well in your applications is sometimes a bit of our future work we still have work to do for example we need to activate we may need to the Swinney method for more complicated smart small off the network net actually in recent years many many studies on real applications in private not correct France models usually perform so this is the one of our future direction ok I think that's it thank you for your attention how about some questions we got some time for a good discussion here which is a knowledge okay but I'm already I guess mine fur to begin with it's a little bit more on understanding this is related I think the Jeff's questions what kind of you know more about what kind of data you know sets are you working on now what kind of data you think this these techniques work especially in our heterogeneous shine in the context of specific requirements on the data so basically it can be applied to any type of data but speaking of the real applications the best was the number of samples it's much less features because because theoretical you can see that the normal features selected by NASA is bounded by the number of samples so if the number of features is much larger than samples this imply that the solution will be very fast their method works the best if your solution is not fast if if the ground truth is not fast the measure may not be that useful right now we have many data for example any data that you ask data of data when now patiently you are a not not that large but you only have a few hundred patients for example the geodata we may have millions of the covalence images we usually use the volume of the brain region as a response yeah yeah learner across the brain what kind of intervention do you do ahead of time to kind of guide your machine against the so right now we have is a process of Leda brain regions I know these things creep on into which region yes yes mm-hmm so you're really proud you're learning you're learning against the okay so you have in the nd case you've had G wise and you know there was a number of different studies that were brought together people severely here you know you have age there may be at Nissa t then you know images the cognitive tests and the sniff so this is related to dr. Romans question and how do you bring all that together and process that together questions you answer and I mean is this is this data set of an appropriate want to think about in terms of thus far disorder we we slack to the bottom of the regions which is known most related to a response vector G oscillator we put them together we train the model to see if the last correctly identify the most irrelevant to see the most irrelevant of names so power results show that niacin can also we need to combine some other techniques like stability in action combining with these those kind of techniques of experiment issue that can correctly identify those names Chris Manning to the genes which are known to be most relevant to Emily the lagina the method why you were working on that is that that can do regression and the future selection at the same time array so and it was slow and then you like spend 12 years to trying to speed it up you try to compare that performance to for example if I take the same data and I throw like in a distributed run tourists that can be like paralyzed very easily and train like trees on the multiple you know computers and multiple nodes and then it also provides the future importance that allows you to do a feature selection yes so okay did you like compare if it's worth it to do speed ups the last so versus like just you know using the red to distribute to reduce to be to turn the poorest I think the reason why we do these kind of work is right now it's still one of the most popular are masses reflection because it's simple so if you have too many large extremely large dataset that sometime you might not be able to use some fancy models random forests if you have many of millions or even billions of variables but for this might be your only choice if you need to decide Chi then maybe you can apply some other models like the actually one of the approaches that you can to prefilter features they apply your favorite models to the reduce that is that this is the one way to to fit them all yeah common features yes didn't consider bias or just assumed the body there this is a potential potential way we can improve our method yeah this this is a very good idea but we have an aside yeah this maybe we can do something like that it's wonderful to have you here with us in the department to have this expertise down that you represent with the lab it's a big plus for us and it's gonna make a big difference you know one thing that we learned the other day from Patrick Harrington who gave a nice talk is explained here was a bit of song of stick some great so it was really kind of fun and Patrick was one of our former students and he you know made it crystal clear the importance of defensive techniques the future out of the job market how we needed this right dead center and I bought from a store he was very clear about what we had going for us and what we've been used to complete ourselves and if there's no question that you know the expertise that I could add to our curriculum that could make make us even more cutting-edge than we are today I want to make one other announcement Monday night Marsha exceed the stellar award president University and the provost for outstanding service and leadership librarian function Jane Blumenthal not together but separately also received this award her accepted role she's the head of the higher was really wonderful wife initiative for you Marcy and all very proud of doing everybody should realize [Applause]