[
    {
        "start": 0.179,
        "text": "thank you so much for having me introduction it's a pleasure to be here and please feel free to ask questions throughout the talk today I'm actually going to deviate a little bit from my cancer biology work to talk about some of the single-cell analysis techniques that we've been developing in my group and thinking about so focusing on applications to neuroscience that were rapidly taking back to clinical translational research questions so I'll be talking today about how to use matrix factorization to interpret single cell data and my lab focuses largely these days on computational methods for single cell analysis and these have taken on sort of two primary branches one is through matrix factorization being led by Genevieve Stein O'Brien and Tom Sherman in my group and the other is problems in how do you study transcriptional heterogeneity from "
    },
    {
        "start": 60.66,
        "text": "single-cell data being led by Emily Davis Marcy sac and BOM and F sorry in my group these are sort of two branches of our single-cell analysis pipelines and I'm going to be focusing today largely on the matrix factorization approaches so what is going on when we have single when we have transcriptional data be a bulk viet single cell right if we think about what's going on in the cell there's a number of different concurrent processes that are going on within a cell at any given time there's a phenotype that that cell has which might be a continuous state right age like I don't know like cell transitions things like that there's a discrete cell type right like mi an immune cell a Maya t-cell and Maya B cell there's the spatial position of where that cell is located and then there's some sort of transitional progression and then "
    },
    {
        "start": 123.119,
        "text": "there's state changes like cell cycle or metabolism and all of these come together within transcriptional data and what's so powerful about transcriptional D is it's really a combination of all of these features and if you have bulk data they're all convolve together if you have single cell these are somewhat resolved at a cellular level but still some prophecies like the state the spatial position and continuous phenotypes are still going to be convolve together in your data so if we think about the central dogma of molecular biology and what we're trying to do right and all of these things is we're really trying to relate these sort of the high dimensional data we get from DNA to RNA to protein to what's the underlying biological process in the system right at the end of the day we want to know what are the drivers of a given biological process and in order to "
    },
    {
        "start": 183.37,
        "text": "do that you know historically in bulk data we were in a situation where you know we had tens of thousands of molecular measurements and maybe order hundred samples and so a lot of the work there has been focused on differential expression techniques and statistical techniques where you really were able to query the data with known phenotypes and ask very specific questions I've always made the argument and it's becoming even more apparent with single cell data that by doing that you're losing some information in the system and that the underlying data itself can tell you something about the system that your prior knowledge might not have given and now that we're at a point where our datasets are containing upwards of a million cells with large consortio like human cell atlas we're really getting to a point where we need to now let the data drive some of the questions that we're asking and the data can inform new hypotheses about the underlying biology "
    },
    {
        "start": 243.609,
        "text": "of a system that we might not have been able to get in a supervised type analysis so I'm gonna focus on transcription because like I talked about it's convolving all these signals and that's been the focus of my group but really a lot of these apply to any of the other high dimensional single cell types that are getting out if you have continuous phenotypes that you're getting so if you think about these technologies you know be it both be it single cell you get some sequence reads that you align and quantify write to specific genes and at the end of the day when you go to interpret the data what you end up is with is some matrix of genes by conditions right and your goal is to say from that matrix of genes by cells by samples whatever your conditions what's the underlying structure in that data that's giving you information about what's going on and we're fortunate that "
    },
    {
        "start": 304.18,
        "text": "in mathematics this problem has been well solved for a number of years in math and computer science where there's low dimensional representations there's different forms of matrix factorization that what you find low dimensional representations of the data that give you an easier mode of interpretation of the underlying structure than higher dimensional data that you're given of the whole genome RNA seek and single cell data and one of the things that a lot of people and a lot of the literature has really been focused on is picking a method and really doubling down on it and talking about wit my methods the best right and I have my bias you know I wrote Co gaps is a non-negative matrix factorization but I think that it's really set the field back in terms of data interpretation because everybody comes up with their own algorithm they named it something slightly different they have a different fancy name for their outputs and it "
    },
    {
        "start": 366.01,
        "text": "makes it very hard as a field to compare what the outputs are and to then convey to collaborators hey here's sort of the commonality here are the different features that we're learning so broadly matrix factorization I'm going to focus on linear techniques cuz that's what my group does but you can apply this to nonlinear techniques like auto-encoders as well so these factorization approaches largely are divided into three major classes there's principal component analysis independent component analysis and non-negative matrix factorization so principle component analysis is aiming to find the maximum separation and orthogonal conditions between your data independent component analysis was looking for statistically independent features and non-negative matrix factorization is looking for dependent features where the matrix elements are all non-negative at the end of the day regardless of which approach you use you "
    },
    {
        "start": 427.98,
        "text": "get out two matrices and one of the things that we want to argue for is coming up with standards of how you visualize the data coming out of these two matrices and how you quantify performance so that way you can really start to think about can we compare across methods how are we learning what are we learning from the data in ways that you can't do if you're doubling down on one single method so there's two matrices one is going to be genes by some lower dimensional representation we call it patterns in our case other people use the term factors there's a number of different components this matrix we call the amplitude matrix again there's a number of different names for it the weight matrix the loadings what you know call it what you may we refer to it as the amplitude and this gives you a set of how are the genes correlated or how are the genes being Co used in different low dimensional representations so this is typically "
    },
    {
        "start": 489.78,
        "text": "what you would use for gene set discovery pathway analysis or biomarker discovery where you're really trying to find what are the relationships between the genes that's as opposed to the second matrix which is the pattern matrix that gives you the relationships between the samples and you can use this for clustering for subtype or subclone discovery and timecourse analysis there's a broad list of applications but again all of these are going to come out of these different techniques and you can think about using them similarly we wrote a review sort of trying to start bringing together this more consensus in the matrix factorization field to think about what are the ways that this you know is uncovering different levels of information the data to start developing a standardized notation and really start thinking about what our standard visualizations that are needed for these low dimensional approaches in order to interpret data so I'm gonna focus on an "
    },
    {
        "start": 550.259,
        "text": "example of non-negative matrix factorization and just think about well what does it mean to have a low dimensional representation of the data okay like what are we actually getting out so I'm gonna be in this cartoon world this nice simple world where I have only four genes right so imagine I'm in this thought experiment I have four genes and there's two scripts of data like one first half and one second half and then you have some maybe time variants in both of them so you can very clearly visually see by eye that there's two dominant patterns in this data right one that's going up and down in the second set of samples and one that's going up and down in the first set right then the third one is a combination of those two and the fourth one is a different combination of those two that's using the second one so if you think about what a matrix factorization approach is going to do is it's going to want to represent this data in terms of "
    },
    {
        "start": 611.129,
        "text": "those two factors so what does that look like you have one again that's going up and down in the first set of samples and one that's going up and down in the second set of samples and then you go back to each gene and query and just say which one is it being used in so this first gene right it's not at all going up in the first set of the patterns samples so get the zero here and it's entirely going up in the second set of samples so it gets a blonde here right and this is just basic matrix multiplication right the second one flips the third one you get a one in a one and the fourth one you get a one in it too because it represents the relative usage of each pattern so that gives you a lower dimensional way of saying okay overall these are the patterns these are the processes that are going on and then these are the genes that are associated with each one of those so we can start thinking about how to blink specific genes back to the process you can scale this up right so this was my world of four genes you can just "
    },
    {
        "start": 673.0,
        "text": "scale this up right and all these techniques work scaled up to larger datasets so this is going to be our cartoon example that I'm going to switch to where yellow is a heat map of higher expression and blue is lower and I imagine they're sort of these classes of genes and these classes of samples that are indicated by color so then how what are the different techniques giving you how do you interpret the data based on different techniques and so if we want to figure out the prophecies if you look at different types of factorizations you're going to get different solutions right so PCA for example you can visually I'm gonna go back you can visually see in this data there's three patterns right one going up in this third set of samples one going up in the second set and one going up in the first and everything else is a linear combination of those so you can sort of visually by I see there are those three groups if you look at PCA it gets do things going "
    },
    {
        "start": 735.67,
        "text": "up right and then it gets in the first two groups then it gets one going up in the third and then it gets another one that goes negative right that goes way down to compensate for the fact that it's getting to things that are highly expressed here so it's it's recapitulating the data but this is not a feature that we ever see in our actual data matrix why is PCA so useful well these two features are really what separates most of the variation in the data so if I plot one versus the other I've captured most of the variation in the data and lo and behold I separate out the blue group of samples from the yellow and the gray the PCA is phenomenal for clustering right because I've done I've separated most of the samples but it's terrible at learning the patterns that are actually reflected in the data so if my goal is to cluster i 100% want to use pca if my "
    },
    {
        "start": 798.279,
        "text": "goal is to figure out what are the process in the data I'm gonna want to go to another technique like independent component analysis or non-negative matrix factorization and I'll admit my bias up front for non-negative matrix factorization and you know of course happy to get into discussions with people about independent component analysis or other you know nonlinear techniques but that's my bias and I'll be upfront about it at the start so in non-negative matrix factorization what do you get out you get three patterns one that's higher in the first set one the tire and the third and one that's higher in the second set of samples and that's exactly what you observe visually within this data so we think that's more reflective typically of what are the underlying processes in the data and then the way you can visualize this so whereas you in a PCA plot you're going to tend to plot the first PC versus the second so you can see the separation of the data in non-negative matrix "
    },
    {
        "start": 858.82,
        "text": "factorization each of the factors is equally important right so I can't just go down to two and visualize the data because the goal of it is fundamentally for each one to represent a different piece of the data so here instead we'll do for example a box plot by the groupings to see okay this is higher in this first set than the second this is higher in the third cetera so there's different ways that you're going to want to visualize the data based on the goal of the factorization approach and I think that's something that's underappreciated in the field so we applied this this different approaches to gtex data so I don't know if people are familiar with gtex this is a consortium data set that collected postmortem tissue from hundreds of individuals and it collected it from different tissue sites from the same individual and so we this was an earlier "
    },
    {
        "start": 920.86,
        "text": "iteration of gtex where there were eight patients and we focused on just the brain regions so what we did is we ran PCA here independent component analysis and negative matrix factorization on this data and what do we see to sort of illustrate what the different approaches are the PCA remember the factors are going to explain each an increasing amount of variation in the data that's exactly what you see and you see two factors explain almost all of the variation in the data do you plot the two one versus the other and you see this yellow region of the brain is separated out from the blue and I apologize we colored by tissue type I didn't indicate it on the legend do you can see a really strong separation of tissue but no separation by individual donor right so it's doing a great job of clustering this tissue is different from this one but it has no way of saying this is the individual that it came from "
    },
    {
        "start": 983.67,
        "text": "independent component analysis what does it do so here we've changed the plot instead of plotting PC one by PC two we now plot the patterns separately because again they're all supposed to be equally informative and we plotted them by brain region and color them by individual so what you see is one pattern learn from independent component analysis there's a really good job of separating out the cerebellum which I promise you is one of these two I forget if it's the blue or the yellow dots but it's one of the two there so you're getting the same separation of tissue and at the same time another one is separating out two individual donors right so you get one signal that's positive for one donor and negative for another so that's suggesting that the gene weights are just flipping directions between those two individuals if you look at non-negative matrix factorization you "
    },
    {
        "start": 1044.97,
        "text": "again get the separation of the same brain region but now whereas you have one pattern that separates the individual because of the non negativity constraint you get out two separate patterns that separate those individuals so whereas the gene weights are going positive and negative here the gene weights are all positive here so what's the advantage one versus the other this probably is giving you a simplified structure right because you only have to you only have two patterns and you're seeing that in a relationship this lets you account for the fact that there might be different genes that are being used between those individuals where you have some genes that are shared and some genes that are different that's lost in this so again we would argue it's not that one approach is right it's that all of these are giving you different insights into the data and we think when you're doing unsupervised learning you really need to think carefully about what the underlying assumptions of your model are and maybe pluck try multiple methods because "
    },
    {
        "start": 1107.169,
        "text": "they're all going to give you different pieces of information about the data right we see separation of two brain regions and PCA and we only see separation of one in these other approaches it's not that one is right and one is wrong it's that they're all giving you different aspects of the data so I'm going to jump now to single-cell RNA seek right so there was bulk and the goal in bulk is can we deconvolve what are the different signals that are going on and how are they occurring the goal one single cell is now can we find cell types that are in common and shared right so it's almost the same problem it's just focused from a different direction right instead of being focused on D convolving we're now focused on putting things back together a little bit so again we want to figure out what are all of these features from the single cell data that were learning and we were specific this was a collaboration with a Seth Blackshaw and "
    },
    {
        "start": 1168.73,
        "text": "loyal gothic Johns Hopkins and they were very interested in retinal development because the development of the retina is a simplified model for the whole development of the central nervous system so and in the cell types and the progression of cell types in the development of the retina are very well defined so it gives you a really nice system in order to really baseline some algorithm development learn you know some new cell types that might not be in this progression and also serve as a model of central nervous development so they collected 125,000 single cells of with in developing mouse retina at several embryonic and post embryonic developmental time points and then did clustering analysis and hand annotated the different cell types in the retina from this data set so we have a sort of reference cell types based on "
    },
    {
        "start": 1230.29,
        "text": "standard gene markers and then we have the gages are known a priori so we wanted to see what else can we learn from these data because what's really critical are the transitions between these cell types that are totally lost in this clustering analysis right we don't we don't see the relationships we don't see the biological processes in this data so I've developed a Bayesian my groups developed a Bayesian non-negative matrix factorization algorithm called Co gaps so I'm gonna be biased and you know I'm gonna put my non-negative matrix factorization hat on now and say of course this is the best approach to use on these data in spite of my real beliefs and what this algorithm does is we have a normal prior on the data the likelihood follows a normal distribution which again for single cell we need to do a little work there what's unique about this model is every element in the matrix for the "
    },
    {
        "start": 1291.22,
        "text": "factorization takes on a gamma prior where the shape of that gamma is set by a Poisson distribution so why do we do it that way because that is a super complicated model right so why do we do this what we found is that if you think about a gamma distribution right so when this parameter is small your data is going to tend to be exit your your parameter is going to ten it's going to look like an exponential so if you think about an exponential distribution it's really constrained towards zero do you get a sparsity constraint when this parameter gets larger what happens is the data becomes more smooth out your gamma sort of starts to look like a normal distribution with wider and wider variants and so you're actually constrained away from sparsity so as opposed to a standard sparse non-negative matrix factorization which is going to typically have a uniform "
    },
    {
        "start": 1352.25,
        "text": "sparsity pram constraint on it we think that this is giving an adaptive sparsity constraint that lets you learn different levels of sparsity for different genes to model the fact that some might be really expressed in all conditions and some are not going to be and it's learned directly from the data so we applied it to this retinal development now I've gone from the fancy you know thinning picture to a two-dimensional map so that it's a little bit easier to see so we applied this approach to this data and lo and behold we found one pattern that distinguishes one group of cells turned out those were the rod cells we find patterns for all the different cell types as well I'm just highlighting rods because it's clear we find another one that shows up across several different early cell types and when we look at it it turns out that that's actually an enrichment in the cell cycle genes and so what's going on "
    },
    {
        "start": 1414.74,
        "text": "is the cell cycle is very tightly synchronized early on in development and as the cells develop different fates each of them goes on their own cell cycle projection progression and it gets more heterogeneous so we're actually able to apply our earlier heterogeneity tool to quantify that as well but it was nice that it showed that the cell cycle was coming up in early stages and then falling off later we see other transitions for example during neurogenic States and cell cycle and we find patterns that are associated with all the different cell types some that are associated with age the other thing that was cool though in the projection I showed you before we removed this group of cells because they were all doublets right so it was found that they were all doublet Sammy there's two cells accidentally there instead of one for single-cell data and what was cool with every single doublet that we found it turned out was a "
    },
    {
        "start": 1474.75,
        "text": "combination of two of the patterns so we think that the matrix factorization approach actually lets you resolve some of the signal in doublets without having to throw them out which is very nice they don't actually is we're showing the deconvolution directly in the doublet cells so that's great so we have all these cool patterns and gene weights how do we know they have any meaning whatsoever right like how do we know that this isn't just some artifact learned in this data how do we know that the you know I told you because it's Dogma that the retina is a model for central nervous system development how do we actually go and test that now so what we did is take this idea sorry I guess I went back you slowed alone all right jumping back and forth a little bit with the pointer I can go back here "
    },
    {
        "start": 1539.179,
        "text": "the joys of these plots with all these data points they end up being a little bit slow to display so what I'll sort of talk through it while it comes up there we go so the idea was okay we learned all these patterns in one context right but we really care about okay how real are these right how preserved are these this is a mouse development how much does it relate to human how much is the biology of the gene weights that we're learning technology dependent how much does it relate to central nervous system development we have all these millions of questions that we want to ask now that we found these patterns so the idea is that we wanted to do a transfer learning approach to see how much does what we've learned in one context relate to what we see in another context and our hypothesis is that the bio "
    },
    {
        "start": 1599.309,
        "text": "logically relevant patterns are going to be preserved across multiple paths multiple data sets whereas the technical patterns won't be right they'll fall out so things that are related to read depth there other technical biases and the data will fall out and the other thing is that if we see something in one data set that's really well annotated that we found in our original data set that isn't can we learn something and borrow information from a new data set the query what's going on in the patterns that we weren't able to interpret in our original data so that's one of the concept so what we do is we get all these different patterns of gene weights and we'd like to think of them as lenses right so we're gonna get these different patterns of gene weights each pattern forms a plan we've learned this in one data which will be our retinal data set and then we ask okay if I have a new data set and I shine it through the lenses if this pattern that we've "
    },
    {
        "start": 1659.34,
        "text": "learned and I see how it's used in different contexts and then figure out is it preserved where is it used what are the annotations and try to learn more between the systems so this is um an approach that's on bio archive till a fellow by Genevieve Steiner Brian and Brian Clark and it's available it was just released on bioconductor today so if folks want the software's out there and I'll give you some examples now so how did we use this how did we benchmark it and how do we sort of start using this to explore the data so one of the things that we did is we took our developing marryin retina dataset and there was a mature retina dataset that was available ours was using 10x this dataset was using drop seek and we asked okay so we have these developing patterns this is a mouse retina but it's all "
    },
    {
        "start": 1720.509,
        "text": "mature mice right so it's a characterization of the mouse retina in adults so what's our hypothesis in this case the cell types right the patterns that we learn that are associated with cell types and mature cell types we should still see in the adult whereas the early developmental phenotypes we shouldn't write because this is an adult so if we take this one pattern that's focused on the retina and project it onto this new data set it shows up in the top half of the data and lo and behold those are all it's blue if you look at the clustering those are all the rod cells so what we found is I'm a pattern associated with mature rods in the developing mouse is also preserved in the adult that was cool let's see if this shows up sorry these points are a little slow because there's a lot of data points so next if you remember that "
    },
    {
        "start": 1786.899,
        "text": "cell cycle pattern that was associated with early development we projected that into the data it didn't come out at all right no cells were enriched for that which is really reassuring that we're not seeing that highly synchronized cell cycle that's associated with early developmental faith in an adult all right so that's reassuring that we've really segregated out a phenotype that's there versus one that's not and that this transfer learning approach is going to let us see features that are preserved versus ones that aren't we applied it to all the different patterns in our dataset and we found that all the patterns that were associated with mature cell types in our dataset projected nicely onto mature cell types in the mouse so what we learned in one context we were able to transfer to this other one as a benchmark even though it was cross-platform different lab different Mouse type all of the sort of usual suspects of where things are going to go wrong in "
    },
    {
        "start": 1847.739,
        "text": "a bioinformatics analysis so then we also wanted to see how does this relate to other tissues right because again we're thinking of the retina as a model for the central nerve system so we projected the same signatures into the tabula Mira's data set and sure enough we find different patterns that are enriched in different regions where clustering has defined different cell types do we find that what we learned about different cell types in the retina are preserved in different cell types throughout the mouse and if you look at the genes the ones that are coming up makes sense and there is some preservation of those cell types one thing that was cool there was one pattern that we couldn't explain in our original data and it showed up in exactly half of the cell in "
    },
    {
        "start": 1909.15,
        "text": "our original projection space so we went to the tabula Mira's data and looked at the annotations and saw what was it enriched in derp enough it was enriched in Saks right so whether it was female or male and so we in our system the mice were too young to sex so we didn't have the information about that so we were able by going into the tabula Meera's data and seeing this separation right we were able to say and go back and say okay we had this annotated we could use the pattern to figure out what was seperated and then go back to our original source data to say oh of course that was sex and when you look at the gene marker it comes up with a gene marker for sex differentiation which is reassuring but you have to kind of you know it gives you a little bit of a way of exploring your original source data through a new data set that you've projected it into if this comes up so "
    },
    {
        "start": 1971.04,
        "text": "that's great we went mouth to mouth but we also want to know how much is development preserved across species right because it's well known that a lot of these processes in development are very highly conserved across species so we went to a human data set of the developing retina and we saw the early cell cycle changes or served in the human data and the mature rod pattern or no sorry terminal retinal RG sees I'm sorry this is where my biology on the neuroscience is weaker than it is in cancer but we also saw that the mature cell types were preserved mouse to human so these gene weights that we learned in the mouse were preserved in the human and some of the transitional patterns as well they also had human bulk RNA sequencing data from the developmental time points as "
    },
    {
        "start": 2031.73,
        "text": "well as the single cell and we found that the projection of the developmental patterns that we learned in the mouse were also preserved in bulk and human so this gives us a way that we can become geek involved and go from single cell to deconvolve within both what's going on and again transfer across different measurement technologies to learn different relationships between systems and building onto that right you want to learn mechanisms of regulation within your system right so you don't just want to learn these or the patterns but what's causing them and in development a lot of fates are solidified by epigenetic modifications and chromatin modifications so our collaborator also collected this was bulk a tack seek they're doing single cell now do they collected bulk a taxi data of each of the different developmental time points and there are clearly differences in the "
    },
    {
        "start": 2092.12,
        "text": "developmental processes if we look at different patterns they show different relationships when they're projected into the a taxied data so we're working on now interpreting this a little bit further to figure out what's the relationship between the developmental datasets and their breath where are the cases where this is preserved versus where the case is where it's really not to figure out what's the rules of epigenetic regulation in the data so to summarize matrix factorization is an incredibly robust set of tools for biological pattern discovery we think our gaps approach you know as much as I said we should look across methods we think our Co gaps approach does a really great job of learning different biological patterns and the data you're able to use projection techniques to transfer information learn from you know "
    },
    {
        "start": 2153.0,
        "text": "one low dimensional representation to interpret another data set and the project our package is general it's not just tailored around Co gaps it works for any of our men tional representation where you have continuous gene weights it also has functions that let you transfer clustering to clustering and other things we're going to we're in the process of writing up a full app note of all the different functionality of that algorithm so the patterns can also be used as a classifier for discrete subtypes and transfer learning of annotations really lets you do an integrative analysis so obviously this is work that I couldn't have done on my own you know highlighted a lot of the work of Genevieve Tom and Brian and thanks to the funders from NIH and Chan Zuckerberg and I'm happy to take questions so thank you wants to start I "
    },
    {
        "start": 2220.41,
        "text": "will start you want to comment more on the integration of different dynamic types you can catenate yeah and I don't show that here but one thing that we've done in the past is to sort of let me see if I can skip over the bigger slide so it goes faster is if you have different types of cell types and this is very similar to the work that I think Josh is doing as well so if you have different data types one thing you can do is you can concatenate one on top of the other and that lets you look for common patterns between the data sets that give you a sense of where they're integrated where they're not so we developed an approach to do that for subtype analysis in head and neck cancers of looking at epigenetically driven subtypes in transcription and one "
    },
    {
        "start": 2280.41,
        "text": "of the things you can do that's cool with that approach is within the Bayesian context and within our model you can change the error distribution to model different types of relationships between data so when we did that model we were at the time using bulk DNA methylation and gene expression and we modified this term in the model too to model the fact that when a gene is fully unmethylated you would imagine you have a high variance in gene expression because methylation conveys the potential for expression but doesn't guarantee it whereas if it's fully methylated you would expect the gene to be silenced and so you would have a lower uncertainty do you can model the relationship between the data modalities through this uncertainty matrix and we apply that to head and neck cancer work beautifully we got segregation of tumors and normals and then you know different known subtypes of head and neck it was gorgeous and then my lab developed a we "
    },
    {
        "start": 2342.869,
        "text": "the wet lab portion of my group is focused on acquired therapeutic resistance and what we did is we took a head and neck cell line model exposed it to a drug that it was initially sensitive to developed resistance by long-term exposure and what we did that was a little bit different is collected cells every single week as they acquired resistance we found that integrated approach completely bombed and when we looked at it what we did is we did a separate analysis of the unsupervised learning patterns on the DNA methylation and gene expression data separately and what we found is there were fundamentally different patterns in them so what was going on is in the gene expression data you see an immediate change due to the fact that you've exposed the cells drug and then you see a slower change that's due to resistance in DNA methylation you only see the drift that's due to resistance because there's no immediate response to therapy and it back delay is actually that that drift is actually delayed relative to gene "
    },
    {
        "start": 2406.14,
        "text": "expression changes so it turns out that the factorization can't possibly work by just putting one on top of the other because there the data are occurring it the patterns are occurring at different time scales and there's fundamentally different patterns between the data sets so that's something that we really want to explore and it's something we think that the transfer learning approach gives you a little bit more flexibility to do because you can take what are the patterns that you learned in one context and really see where they align temporally but there's a lot of work I think to do and semi-supervised learning approaches and other approaches to think about that I just to follow up I think what you talked about the two are mixed layers give somewhat conflicting views we've seen that in TCGA data as well you know with one data type ask for five top-class is another for four they don't align that it's not something computationally you can resolve it's a real conflict in your hand yeah and we "
    },
    {
        "start": 2466.589,
        "text": "don't know if that's because they're taken from different regions of the tumor or there's fundamentally different biology in them I think we don't know what we don't know there yeah I think the multi-omics is a real challenge Thanks I have this scratchy voice right now sorry definitely need the microphone that was a really great talk you do a really great job of intuitively explaining how your methods work I like how I use the gamma distribution to more flexibly model different levels of sparsity and the data and I was wondering when you when you went on to project are does that also take into account potentially different patterns of sparsity and the different data set or is it simply using the linear combinations from the patterns that's a great question it's it's simply using the linear combinations of the patterns and that's you know one thing we want to "
    },
    {
        "start": 2526.769,
        "text": "do is think about if you if you use those as the basis and different semi-supervised learning models right like do we put these back in you know as fix patterns in our matrix factorization approach how does it compare I think that's absolutely something that we need to be doing the advantage of the linear combination is it's a whole lot faster than the Bayesian sampling um so it gives us a quicker way of looking but you're a hundred percent right that's an area that is ripe for exploration thank you for the talk it was very clear so you mentioned that each of these matrix factorization approaches have their own advantages and disadvantages so do you think it's computationally feasible to potentially develop some sort of meta "
    },
    {
        "start": 2587.97,
        "text": "method approach that does every single factorization and then weights them based off of how reliable one approaches over another for certain inferences absolutely I think the trick in that is how do you quantify reliability right so if I were to ask every single person in this room I bet you every single person in this room would have a different metric of reliability all right so some people would say is it the best fit to the data some people would say is it getting the cell types the best some people would say is it getting the early transitions the best so I don't know if it's feasible to develop a meta method of the right answer but I think it's feasible to develop a set of unique features that are learned from different factorizations that together will give you a different piece of the data and even within one factorization if you run it at different numbers of patterns which is always the question everybody asks how do you figure out the right number of patterns right if you run it "
    },
    {
        "start": 2649.26,
        "text": "at different numbers of patterns you're gonna resolve different hierarchies in the data so we think even within one approach you need a meta method that really is going to compare not only across algorithms but across dimensionalities as well so I think that that's an that's ripe for future research no more questions let's Thank You Lane again [Applause] "
    }
]