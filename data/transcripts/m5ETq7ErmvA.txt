since I'm books three and five everyone I know we have a few new faces here so I'll just give my brief introduction to the seminar series this is the tools and technology seminar series for the department of computational medicine bioinformatics and it's basically just a venue to talk about and learn about tools technologies sometimes they're newly developed sometimes they're in development they don't have to be a nice formalized polished product yet we're just you know what is out there tools and technologies they're out there that researchers have available to them and can use um so today I'm happy to introduce Brock Palin he is I believe assistant director of advanced research computing the technology services yeah I can do that so I'm Brock Balan I am the assistant director for advanced research computing technology services we work with a number of people in the room of course number of the students and faculty we've worked with but people who have a direct affiliation with RTS we have Jeremy hollom he's one of the managers at RTS but specifically leading up a relationship with Life Sciences and things such as the Medical Center can is many of you know also as a direct relationship and supports users on our system he's what we call one of our liaisons and so he's he's like an our joint support group our work support working group in a number of things like that that helps mold the services and what they are and gives feedback back and forth from the users so this is a public presentation this is going to be an overview we put together of all the different services that are Casillas has some of these are forward-facing comments some of them are currently available I'll try to make that clearer if I don't feel free to inject a question at any time and I'll address it it is also a public presentation that we aim to keep updated as status of things change and new products come online and there's a short URL here feel free this is accessible to anybody I was just at Google presentation you can pick this up this is our web address you can follow me on Twitter if you want but most importantly remember this email address hpc - support you midi you that that does go into our ticket system because this is also where users send questions and things like that but it really is if you drop into there and say like we're trying to set up a project on Amazon Cloud and it's for doing large-scale sequencing runs or something like that and we'd like to have some help or what do you know or the performance is low can you help just drops up there if you're curious about literally anything having to do with research technologies I'm in the IT space so this is um basically a quick overview any pictures of computers or data centers in here are of those here on campus so this is actually a photo of the data center that flux and Armas resides in the lights are off but as you see we buy LED stock I mean seriously look at the number of LEDs we've got in there so cool so many LEDs now it's an unmanned facility nobody's in there unless one was actively working on something because it's it's a very uncomfortable environment to be in it can be as much as 90 degrees in this aisle right here just hot aisle and it's very very loud from all the fans and coin but what do we actually try to enable we only have infrastructure to build support scientific work so we're looking on we provide systems infrastructure and consulting services we try to deliver services at scale so we try to leverage just buying power administrative overhead and all those sorts of things we try to do and you know keep things as accessible but also his low-cost as possible to the institution um but at the same time as much as we have production research resources we also try to do forward-looking things so that's the respond to cutting-edge research needs so we are very very different than the enterprise organizations on campus if anybody here is familiar with the Gartner Magic Quadrant of hype and stuff like we're in the hype column a lot while our peers and the enterprise group would be like oh that's in the hype column we shouldn't go there and we're like why you even listen to Gartner I want to know what Facebook is doing I wonder what Pinterest is doing because their infrastructures and how they manage environments are more similar to what we actually do now so we do implement a lot of things in the hype column we do partner with faculty researchers students administrators centrality and experts but this last line is very important so like Ken is an example of this the University of Michigan has 101 top 10 graduate programs it's really unique in the world in that respect that also means that we can not have domain level specific knowledge or relationships with all the different faculty and what they're doing on campus inside one single central group is just impossible there's a classic problem in economics called the knowledge problem how you disseminate information and get it there quickly as possible the way we do this is is we scale where it makes sense that assists of administration infrastructure but we don't scale and we dispersed where information is dispersed and an example of that is the unit IT and the direct research or engagement for what do you doing what are your needs you know if you talk to us and we get feedback so if you feed information to Ken he feeds it to us if you want to go deeper and then have a direct conversation with that we're happy to do that too but you know really he's your point contact for a lot of this stuff now RTS is only one of four different units in arc so we're the only unit that has anything to do with IT in the traditional sense if you've heard we're part of IPs technically all the RTS employees RIT s employees but we're funded by Office of Research we're under faculty leadership dr. Erica Nielsen so we're their organizational to get like HR support and Finance afford all that overhead stuff that we don't want to have s into one of us into technology and science so we reside here we really focus on infrastructure the other service group is cease car center for statistics computing in analytics research if you're familiar with cease car you might recognize it that that acronym is different than it was in the past it's because we've expanded their scope as they got reorganized under arc especially like things like the data science initiative we now have people here like we have Alexander guy anko who is a application specialist so he is able to paralyze focus on performance tuning and he's he's a PhD computational chemist I mean he developed a scientific code for a long time support here is both there's this there is a fee for using consultants here but it's only after some modest amount so they have workshops and open houses where you can come in with your code and they'll do a quick overview or come in with your science problem like if you're doing statistical study they're very strong on statistics given their history and it's like oh this is this will be faster for you or this will give you more statistical power or this is this is the appropriate way to do this a lot of their customers are social scientists and things like that in a statistics base where they're using statistical methods to really make sure that what they're doing is giving them you know useful results so those are the two service groups brawn brain that's the easiest way to think about it we bring the muscle they do a smarter there's a good example of this we were working with a political scientist who had a eight hundred and eighty gigabyte data set he was trying to use it in Stata which is what their tool of choice was problem is 880 gigabytes with a tool like Stata or SAS or SPSS or MATLAB that tries to read the whole thing in memory even though we have machines with a terabyte plus of memory in them as soon as he tried to modify it the thing ran out of memory and it fell over so what we did is is so his thing wasn't working at all there was a 1.4 billion rows CSV file what he wanted was actually very simple with the big was cumbersome we put him on our Hadoop cluster which is free and I'll cover that later and we put it in the hive 14 lines of code later his Coria was now running in under two minutes great right Kirby dr. Shedden who's a leader of this group over here he's also has appointment in statistics took his need wrote a python code using advanced methods and the thing runs on your desktop in like 10 minutes now so you know we we work this thing up and you've got it going significantly faster they can do before so there's a nice synergy here and there's a nice example here so the other two over here are faculty led Institute's we have the computational science institute em I've seen EE there's a graduate program here there's affiliated faculty there's a data science institute as you know the you know Midas Michigan Institute for data science so Dave science initiative alright it was a set of investments in two faculty positions research infrastructure and consulting compared to most schools when they did this they invested into one of those that create a bunch of faculty positions but they don't fund infrastructure or consulting or they've put a bunch of infrastructure but they don't have faculty lined up that do that sort of work so Michigan actually did a very nice set of investments here so the consultants reside here socials Saturday they have specialists in social science social media big data database design infrastructure resides here I do software-defined databases high performance computing in memory analytics and then you have graduate work faculty organization around proposals something that will stresses is uh just because you're in one of these Institute's doesn't mean you get preferential access to any of these resources these are available to everybody on campus so you don't have to be an affiliated faculty here to write a grant with us or to get help from us or use our resources that's not required at all okay so specific resources is what the bulk of the presentation is gonna be on and I start off with the one that you've probably heard of before so this will were best known for flux the high performance computing cluster here at the University of Michigan is the largest cluster at the university is currently over 20,000 CPU of course it has nodes at this time so I mentioned earlier the political scientists we had knows with up to it termites Ram now we have a terabyte and a half so if you want 1500 gigabytes of main memory under a single operating system my laptop has team a beefy desktop we'll have like 128 so we're 10x bigger and that's really something you should keep in mind here everything we do we do 10x bigger than most people so our performance will be 10 X or memory footprint will be 10 X or met work bandwidth me 10x everything will be connects so we have up to one and a half terabytes of shared memory up to 54 cores these are real quarters are not hyper threaded so we actually have 54 core machines ago something that paralyzes or uses threads in a single machine we have that and we also have other technologies like NVIDIA GPUs so you might have seen of general-purpose processing on graphics processing units we have those and support those with the full development tool chain debugging capability we even have machines that have up to 8 GPUs in a single node which is kind of ridiculous become in most people's minds but to give you an idea we actually even though the GPUs here are like a generational we actually have the fastest benchmark on the blender umm 3d rendering benchmark because we have machines that had 8 GPUs in them um so you can do some interesting things there so that's like what the infrastructure looks like now I have some key features all these machines are connected by a 40 or 100 Giga Network to give you an idea the university backbone is only 100 Gig the last 136 machines we add it to the cluster all have 100 Giga our total network capacities measured in two terabytes this is a special network called InfiniBand it's used for storage we can technically run IP traffic over and I'll touch on that a little bit later which is important for large data type flows but it's mostly used for low latency so if you have ever done parallel programming before distributed memory parallel there's a common method called MPI message passing interface is a standard that's ratified by a number of academic and commercial organizations they have a form where to define the API it runs over that network to give you an idea our latency here is sub 2 microsecond good Ethernet not doing anything funky with our DMA or iWork and all these other things that are really not mature and all we've tested them trust me we've tested them they're not quite ready for primetime that we're excited for market competition future is like 40 microsecond and that's good if you're like a gamer right you're normally happy with like 30 milliseconds so look at how many orders of magnitude lower latency we are you're then super high bandwidth to so like when you are sending data on the network it's getting to the other computer sort of clear as possible so that your CPU is not stalled out now that's handy for parallel computing it's also handy for data intensive even if you're not doing parallel because our scratch file system runs over that so every machine gets about one gigabyte per second of i/o bandwidth capable over this network so that's greater than 10 gig Ethernet speed over that it has a shared one-and-a-half petabytes scratch file system this is the fastest non flashed file system on campus in terms of actual delivered workload it sustains several gigabytes a second read/write speed continuously all day long all the time and it's completely shared so when you get an allocation on the cluster it comes with all the scratch space you can eat but it is a scratch space so we purge data off of it because people tend to write a lot of restart files checkpoint intermediate data and then they leave it there so we blow it away it's very very fast and we can grow this pretty much as fast and as big as it needs to be we do have HIPAA and pH I on compliance and alignment so we have a new cluster called armas you might have not heard of artemis is identical to flux and pretty much every respect scratch is a bit different some of the access methods are a little bit different for tighter security control so you lose a little bit of usability to you know a little bit stronger security but now the HIPAA control group for the university ever resides in the hospital I said you can put patient data so it's also nice little cos going through this little exercise we had to document how you add or remove users how do you do updates how do you do physical access controls we have a lot of that documented already but now we have it all documented and so if you have a data use agreement like you want to get data say from an insurance provider it's technically not HIPAA but they have restrictions on prove to us that you will protect this because our entire business right or location data so you're getting like you know like Fitbit tracker data or something like that and they really don't want that stuff to get into the wrong hands because of ruins faith in their business partners sometimes fits a need for that because we have all this documentation of controls on how we control access and updates and security 160 gig upon connecting so the InfiniBand network can emulate an Ethernet network and we have these boxes that make Ethernet to InfiniBand run over top of each other why is it 160 gig when the backbone is only 100 it's because of the way the technology works its 448 connections and they're hooking up a family over pair so there's pairs of everything what's neat about this is we let you bring your own storage so on campus a lot of groups that have large data needs have their own storage built up staging the data in a scratch takes time especially if their data needs aren't that great but they have a lot of it and I have it on storage as long as you meet certain requirements professionally managed use university UI these two relatively easy to answer questions we will directly mount your storage onto cluster over this gateway and to give you an idea SPH has 16 petabytes of human genetic data sitting down at the Mac on the other side of town we mount their storage over this and a normal production run day we will pull 20 gigabit a second which is 20% of the entire backbones capacity that most we've ever done between this and turbo coming a cluster once it's been 40 so and we've tested synthetically much higher than that but that 40 was real-world workload so we can pull data into this system at ridiculously high speeds to give you an idea we have broken is a bad word demonstrated weaknesses in existing IT infrastructure given the speed at which we can ingest data you know a network connection that's normally fine at a gigabit plug the whole thing with data flows there's also a program where we give extra cycles for free to undergrad so if you have an under that has her own project there's a very light vetting process just to make sure you're not picking up a UROP student to do your normal research but if undergrad so this is like student teams work done here an example of the Michigan data science team pull together all the public data sources on the lead in the water in Flint and generate some of the first high-quality maps of where the water actually is in their predictive way of we would expect your water have this much life all that work was done here on the cluster on flux under that program they'll talk about this Friday ok but was interesting about that is is that work got a lot of traction a number of companies want to sponsor what they were doing we then help them out because Nvidia donated to them to professional grade graphics cards for doing computational on so we bought the matching server and added it to our GPU service and they now have a two GPUs that are theirs on the cluster they can use wherever they want yeah their family class sir they were Cave K forties Pascal I'll talk about Pascal later we have a bunch of those coming with envy link the other thing about it is is the cluster comes with a complete software environment so we have unlimited license of MATLAB along with the most popular toolboxes as well as 600 tokens of the MATLAB disturb recomputing server all of those you get to use for free with your cluster subscription so you don't have to pay extra so it's funny because we have some faculty that I know used to cluster because it's cheaper to rent cores here and use a software than to buy the software and load it on their own machine yeah but it's neat because we lump all of these together in the cost across everybody is like a dollar so it's just um provided that way and it's nice so we have I mentioned that lab at Mathematica we have common EFI a CAE tool so Lucas Nast ran an sis we have people who do of course you know the classic structures model but they'll also do prosthetics for like dental implants and do an EFI a model of that for struck here it's not gonna wear out after 30 years stuff like that ancestor all the cfd and CAE tools SAS Stata Stata MP this is something that is normally prohibitively expensive to an individual researcher but we have Stata MP supported up to 16 cores we have multiple copies of that so that's a parallel version of statum then of course we have our which of course is open source but we maintain a whole set of our libraries and parallel are multiple versions of it there's over 400 different packages that are on the cluster and we install them and update them basically on faculty request so just because you don't see something there doesn't mean we don't have the ability to install it both commercial or free commercial if we don't have it is a little bit more of a hoop and you can always bring your own software but check with us first before you you know bother spending the money on it to make sure what you haven't already did and then we have a full development compilers debuggers math library so your linear algebra packages your and our number generators your Fourier transforms your SPARC solvers all those sorts of things are optimized for the processor architecture role there and I highly encourage you to use other people's code one has probably been way more tested than what you can develop into is probably much faster especially the CPU manufacturers make their own math libraries and tells us called mkl the math kernal library I highly recommend you look into it because normally it's a good order of magnitude faster than reading C or Fortran on the same hardware flex operating environment so I'm not getting into too much about the different ways you actually pay for and consume for the resources but this is an important one because a lot of times people think they can't do this so the novel cluster is a subscription you either pay for it as you use it or you pay for it monthly sometimes you have granting agencies that really want you to spend all the money for a three-year project in one fiscal year you have one year you have to spend it all but get your infrastructure you need to support your work so we have the service called the flux operating environment think of it as the flux or Armas cluster - the computer so we provide data center power cooling admin the software the shared storage the login node the queueing software everything except for the actual processors in memory then what we do is we leverage our buying power we work with you get your requirements we leverage our buying power because we spend a few million dollars a year on equipment and we go to our vendors and we get you the lowest cost machine that meets your needs and then we add it and effectively it's yours for its life which is between four and five years normally it does end up looking exactly like the cluster so you don't get as much flexibility in terms of configuration it's one of the ways we scale the staffing it's not everything is completely customized but you can get customized hardware so you need a little bit more memory per core or you'd want to consumer-grade GPU which is terrible for FAA codes because it doesn't support double precision but great for machine learning in terms of cost per performance and we've actually done that so we support Titan cards on here too this is something new this came out probably only like a month ago in terms of going live our Kinect is an easier way to use the cluster there's no cost to using this it's just a feature of the cluster now you go through this web address connect RTS des umich.edu and you get this webpage you log in through cosign you have to use your M token just like log into the cluster to secure a profile it is the same but from here you pick your allocation you want to run under you pick VNC for remote desktop which is what I'm showing over here Jupiter notebooks formerly ipython so literate programming our studio you pick your desktop size number of processors memory software licenses GPUs all the usual things you can't do advanced scheduling in here but for the most common use case this works great and it does all the work of submitting the job to the batch system for you so there's no Linux command line back script learning every option literally fill in the boxes hit start when it starts you get a terminal on a cluster node here it's fifty seven eighty eight all in your web browser strong encryption not virtualized this is bare metal performance you should see the same performance as you get when you run into batch queue with the InfiniBand with the software with the shared storage so this is a way that you can visualize so you can see here I'm starting on MATLAB the full GUI will start it will get full GUI you're not gonna have the same performance as a local machine because there's no accelerated GL here there is the ability to do that the newest GPUs we just added do you have the capability to do that we've just not implemented it yet that is something in the future to have full accelerated graphics but otherwise 3d does work just don't expect it to be super snappy responsive like your thousand dollar graphics card on your desktop but on the other hand you're on a computer with 24 cores in 128 gigabyte RAM so this might be more to your liking than your desktop with 4 cores and 16 gig this nice thing is if you're teaching a course you can have every undergrad come in sit down they log into this they all get their own two cores and eight gigs of ram or their own 16 cores and you know 64 gigs of ram or whatever is they need and not have to have that turtle of learn the place command-line before you can do anything that's of course everybody you know we've had people in the past where they'd hack into jupiter server into the cluster it's not all completely automatic you just click that radio button say number of processors you want and you get the jupiter server running in your browser but it's actually running on a cluster node dedicated processors dedicated memory so like something like my desktop where your virtualizing all these different desktops and piling 20 of them on a single piece of hardware but you're trying to do something that requires performance here you have that bare metal performance the processors are dedicated to you the memory are dedicated to you so it's a neat feature about that it's one final interesting feature which is after this thing starts you click on one of these tabs there will be a unique link in there you can give that link to a collaborator there's a read-only ante readwrite and it creates a little session password you share that with them in a secure way so text them don't don't send it by email you can send the link by email with them send the password by email and you give them that password any bus of you from across the world if you're using the University VPN can see and manipulate the same data some so you could pull up a plot here see something weird cause somebody said like hey I had to say yeah link click on it tell them the password over the phone they can pull up and be like oh that does look weird and you can both work on it and control the same computer simultaneously when you log out a second those unique passwords and stuff are all scrubbed so it's only vulnerable for the life lifetime of this session but that is there for collaboration and interaction and hopefully higher productivity this is something really excited about there's no cost to using this because it's just another way of using the cluster so if you are you if you're using flux today you can use connect you don't even have that's because you just do it so we have a lot of people interested in training sessions class work as well as on ramping you know like junior PhD students who maybe have not touched when it's as much big data so Hadoop so we have a new platform today what we call a technology preview it's modest in size it's only about 300 terabytes of raw disk about a hundred terabytes total useable disk in the normal world it is still very fast at doing certain things it's a great way to get your feet wet it's free come and use it we have spark on there we have high Pig all those different tools also if you go to a website you can find links to training I give a high-level introduction to the big data tools hive Map Reduce I do spark so that you can kind of do your you know get familiar with the environment you can start messing with it we do have in the works to be planned for a significantly larger investment into Hadoop something on discord or of three petabytes of raw disk one petabyte usable ten or 40 gig networking something very much on par with the you know decent size analytics clusters around the country if you're interested in using that about it it's free just at send an email to HP support ask for an account will suck you up on it also you can sign up for that course and give you some information also you know research cloths you might have heard about my BRC as we call it Yoda by research cloud this was a donation to the University from a company in Detroit of about five and a half million dollars in IT equipment it's going to be built out over three years the first build-out is going on right now but we do have some capability to provide these sets of resources as soon as the rest of that's built out which we expect by the end of the month maybe it'll spill over into November is that right okay it's his project he owns the project so if it fails blame him but we do have the ability to do this today so the HPC cluster is a bachelor Mary you submitted job you get access to resources a vehicle owner through something like our connect it's not good for setting up a database server where you have multiple people accessing data simultaneously or transactional data like you know where you're updating data on a regular basis from multiple entry points or for data ingest this would be something like creating something that remote sensors in the field collect data and are syncing to a single location for any analytics or running work on this is what this is for this is a persistent set of resources so if you need a sequel database so Maria DB or Postgres this is just a representative list what we will support will be faculty driven this is all a docker container base that we can spin these up very very fast and we can scale them up and down based on needs so we have sequel databases Redis and Kafka these are reliable data bus systems it's like the way to push notifications work on your phone you have a data publisher and you have a data subscriber this is the best that data gets published - and your analytics on the back is this subscriber so this deals with all the reliability and things like that so if I am getting push notifications about that weather from The Weather Channel app The Weather Channel is a publisher they are publishing - in my case Apple message bus system my phone is a subscriber so when it reconnects likes a shut off my phone because my airplane it reconnects it picks up all the messages for it it's neat about those is you can have multiple subscribers to multiple sets of data so you can have data coming in and have maybe a real-time analytics pipeline an archiving pipeline and until all of them have picked up the data Custer and Redis hold on to that so it's really kind of a neat thing when you can think about the whole lifecycle of data generation data ingest data storage and then analytics we have a project here where we're working with a professor in engineering where they are working with a industrial controller manufacturer so they have you know when you have a big assembly line you have all these different boxes controlling lathes and mills and lines and they have certain data how much power what's the temperature how many hours have I ran and what they want to do is they want to be predictive failure analysis these boxes effectively look like patient monitors there's also a project going on the hospitals using patient monitors for the exact same thing so they're doing a real-time predictive failure bleed out in the hospital here we're trying to do real-time predictive failure of industrial components so if you've got a billion-dollar factory with 300 people on that line and something breaks unexpectedly the whole thing goes idle for 12 hours that's a lot of money on the other hand if you're doing too many preventive maintenance 'iz shutting the thing down on a schedule to check everything and replace bearings before they're actually worn out well that's also so we want to reduce the amount of maintenance but we also want to know when things are going to fail unexpectedly so they're sinking off the data in real time through Kafka and we're storing data and we're running a Hadoop pipeline for doing analytics on elasticsearch is a document database so this is something you put tips documents into and you can search them kind of like a search engine we actually use this internally for all of our logs we consume internally about a half a terabyte of logs a day 500 gigabytes and we can search them in real time I mean can tell you how this works um here's a web page he goes to you if you get a problem with a job he can go there search for your job ID search for your username and find all the information all the logs related to your jobs but those things really do come in in real time sinking in from 1,200 different sources it's really quite impressive and you can scale it out it uses the same indexing engine on the back side as the Google book scanning project does Lucy so this can scale to multiple petabytes of data and you can search it so if you have text dating you wanna be able to search it real fast this is a really powerful engine for that influx DBA Agrippina this is a time series database we also use this internally we pull about 30,000 metrics a minute off of our cluster a time series database is literally if you've ever heard of round-robin database this is basically what this is but think of it as I have a metric say number of observations all right I have any value 26 and I have a timestamp so when the last interval I saw 26 observations okay but here's a fun thing though what if you could collect all of the data on a very high find resolution in flux TVs ridiculously fast at ingesting those like I said we're doing 30,000 metrics a minute today on one hard drive and this in clusters and skills out so you want to store a million data points a minute on different metrics and then graph Ana's a nice web front-end if you walk into our office there's a graph on a dashboard on the wall that shows current file system performance current cluster load it's all updated in real time we run that off of a raspberry pi that's pretty easy but we can create all of that for you because it's just a web page but is pulling data off of in flux DB in real life Apache storms a real-time analytics pipeline MongoDB is an example of a no SQL database so if you're Harrison Impala's a large-scale scale-out sequel like data warehousing thing so it's think of it gives you sequel so you can write normal structured query language but it doesn't give you transactions in the way Oracle would but on the other hand this will give you the ability to have a petabyte database and maintain performance so it's a train off HBase isn't the same space cassandra is not sequel but it's a similar kind of idea as a cluster Abell large-scale database you know I can want to build a petabyte database you know there is the integrated rule-oriented data system this is a server you put in front of storage that has we call it policy enforcement points in their language but think of it as an event new data comes in okay kick off this pipeline pull out this metadata run it through this thing create a thumbnail of that large image that just came off the microscope and store it in two different locations or this is sensitive data log every access from every user ID to any piece of data ever you can have these different pipelines whenever an event happens new data touch data update data you can have different things happen yeah this real real world real time data coming in Kinesis is much more fleshed out than ours because we basically customized ours to whatever your use cases the other thing is this is all on-site and when I talk about secure enclaves later something known as glovebox of the whole to use HIPAA data on a third-party provider you need something called a be a business associates agreement which is basically an agreement between two entities saying that in the event sensitive data is like who makes the announcement who pays the fines who's accepting the risk if we keep it all on campus we don't have any dues issue so we can iterate very fast to give me not even work down to be a a with Amazon for like three years now yeah so if you have a different data use agreement that's sensitive but not HIPAA okay budget three years plus to get that yeah so this is why we still do a lot of things on site that said a lot of these things I hope to we were we will provide the service where we can help you get your data connect it up to it and do all that piece and become more of a consulting group than having to worry about running a gear because they Google is just keep enabling science don't actually care about the gear that much we just have it because we have to like flux we measures about three times faster than Amazon because it's not virtualized it's bare metal it's not hyper threaded and it's about one-third the cost so okay well there's that so there's a factor of six difference on the other hand Amazon gives you a lot more flexibility for up and down so pick your poison that said we do support and we've actually recently published a paper with clasp used to be called a OSS on doing climate simulations on AWS so we do help with that sort of work so said this is just a representative list of thinking what's possible um we can host pretty much any server you need are free to a point so because it is a donation we do have the ability for high value projects institution where we could provide more resources for the average faculty we can provide a modest set of resources equivalent to a server for projects at no cost beyond that they're ready to add on and if it's something like what we really need one server most the time but we have this symposia where we have a bunch of people working on the same set of data can you scale it up for one week yeah that's no problem [Music] yeah so this is all pretty new so a lot of people who have existing infrastructure are we can see it's people who have needs that hadn't been addressed that we're working with right now because they're like we got to get this going that said this is a decent-sized thing as much as I said it's spread out over three years in the first year we will have a petabyte of spinning disk 300 terabytes of flash well actually nvme it uses a new interface that's faster and like 1400 cores a little over a thousand cores real cores every like twenty eight cores has a half a terabyte of RAM and it's all on a 40 gig Ethernet network so the data movement and everything is super fast but also a neat thing about the storage here is especially when we get to the enclaves is it's globally deduplicated that means if you upload a reference is to say you have about a thousand genomes okay the first copy we store the 70 terabytes you uploaded the genomes because you don't realize where we have a copy on there it doesn't actually store the 70 again it recognizes that all the blocks hash to the same value and it just increments a counter saying like hey there's two copies here's your copy here's your copy if you make a modification to it it stores just a difference so if you push when we get to the sensitive enclaves where we need to have people really isolated from each other the data are still globally deduplicated so if we have data set like your Enclave that can't see into this other Enclave has the same reference data we don't actually end up storing it twice drives a cost way down now so really for the Yoda byte research cloud and we're really grateful to Yoda byte and Paul Hodges and company for donating this institution and working with us on this just like AWS has the ec2 service the s3 service - RDS service you will see Yoda budget research cloud twelve box dusty Enclave Yoda buddy research cloud DSi infrastructure tested databases and persistent storage here it will eventually be a research desktop also pretty much everything we make for the Enclave to support them will be available on a I can say unsecured but a more accessible you know risk acceptance type way to remember is say let's say you wanted to do something you could do it on flux we're not going to set you up on Yoda bite unless you've got an absolute you know reason to be ano we're gonna need Forks or armlets yeah yeah if you just want to run compute jobs and you're not like falling into the Enclave use case where just be really isolated for everybody else just because we got it donated and we got some capacity we're not just gonna you know give it to does anybody because it's a large campus a lot of faculty so we will fill it up with the use cases that it is for high-speed storage this is also something that's available today so um in the past the only storage offerings were for research were msis or my storage we have now created something called turbo it is a pay by the terabyte service so if you need a terabyte of storage you just fill in the form say what machines you want exported to and here you go what's neat about turbo is a couple of features is good for pH I so it's good for patient data and restricted data again it's very fast it's 40 gigabit a second to give you an idea my storages entire network connection was only 10 so we got 4 times faster than the entire network connection for all of my storage that's another example where we found weaknesses in the infrastructure on campus because we had a researcher fill it up doing genomics work and broke everybody else using my storage so you know like is it in this case we went ten times bigger were four times bigger sorry and here's anything though they watched what we did here and the new generation of my storage is actually based on this technology so this is the other difference we do we push technology forward faster because we don't do a five nines level of support we're next business day for cost and performance reasons and new technology we take a little bit more risk in terms of availability now I'm not saying we're completely going home we reboot things every day but you know where you don't have on-call staff and things like that so that's a difference between like us and my stores you know they have 24 by 7 support and here's a neat thing though because they're moving that same direction they actually support our physical infrastructure so we actually do have 24 by 7 support for the physical equipment so if it drops off the network and completely dies yeah there's somebody who's going to fix it it's your user question of can I get more space isn't going to get acted on till the next business day so it's very fast so another feature we did that we rolled out was mixed NFS and SIF so a lot of people think you can only use turbo with the cluster not true at all we have lots of people who use this without using flux at all you can mount it on your own personal desktop laptop supports roaming clients so you can log in from a hotel room but independence you used to have to pick between Sif's which is the way you mount storage on Windows or NFS which is the way you do it on Mac or Linux actually Macke and kind of do either but Mac was in the middle but if you were Windows you had to stick with one if you were Linux you had to stick with the other we actually support multi protocol that is a single set of data on here you can mount both onto cluster which is Linux and on your desktop which is Windows so this is a new feature to be able to have all your data working in one spot and support both Windows Linux and Mac clients access into the same set of data so this works today is available today if you want to use it and it's a future difference between us and my storage and MSI o stores again it's good for oh there's data publishing and sharing with Globus I'll touch on a little bit more later so you can share data with others Oh some schools subsidize the cost most of you here are not affiliated where they'll say basically LSA is the main one right now okay and you have multiple options here you can get replicated or snapshots or unreplicated or without snapshots they're basically options when you set it up this look like we have a group that has data they really don't want to lose so they buy one volume that's replicated replicated costs twice as much because you have two copies of the data in two geographic locations but then they have a set of data that they can download back from the data provider so if it went away or they corrupted it it wasn't a big deal so they have a second volume that is unreplicated that's quite a bit larger but then it cuts their cost for storyi data in half that said the cost here is about the same as my storage it's like a hundred and fifteen dollars per terabyte per year unreplicated replicate is like two hundred thirty twice that we are working on two projects one I will touch in here but another one is farther out that I don't touch down here where we are aiming to provide significantly lower cost storage offerings to campus that don't match you know they don't look as flexible as nice as this but especially for those of you who have very large data volumes it makes sense to build it a different way and we will probably have a service around that but we're probably three years away from that network connection improvement program this is a set of funding we have to get data out of instruments so what this is is we will pay to get a fatter network connection into your office so like if you have a CT scan or MRI scan or a flow cytometry box you know some sort of device generates a lot of data and getting it out so instead you've got a stack of USB drives that could die on you or something like that we can basically get a larger network connection and we pay for it into your lab to be able to get that data off the instrument and out into the world to other researchers or to you know replicated stores or professionally managed stores then stacking it up on USB drives in your office so we work with pretty much all the network providers on campus a unit i.t i.t SMS is MCIT or now h i TS so pretty much whoever owns your network they do the build-out we pay for it in collaboration so this is basically us saying if you're doing a grant that has infrastructure involved with it engages or earlier during the grant cycle you can use examples of some very large infrastructure grants so like conflux is a dedicated cluster we built it was three and a half million dollars of equipment this is the machine that has 40 p 100 pascals so the fastest GPU that exists there's 40 of them on this thing each with envy link it's an IBM power8 system we wrote that grant together with them because when you do a large piece in the structure like this the provider want sounds like how you gonna secure it what are you gonna put it I mean that thing kicks out a lot of heat and requires a lot of power who's gonna manage it you know also these questions and we can answer all those questions we haven't helped answering the policy questions because a faculty were like well 20% of this machine supposed to be available for non pis it's like well we looked at the national providers we work with and their policies for dishing out cycles to different people and their a refund process and all thing we're like how about we adopt the policies of the NSF provider the national provider and they're like okay and so we had a well-understood set of things and we because we have almost relationships we did that so yes engage us earlier on you know talk to Ken talked to Jeremy talk to me and we can we can contribute language we can contribute letters of support um see scar with the consultants we sometimes split staff so say you need a software developer but you can rule may justify half their salary for say like two years or three years we're like okay we'll pick up the rest of it and find another faculty to use the rest of their time and when we do that they actually sit in your office for the time they're working for you I mean they they work for you but we provide bridge funding to keep them on campus so we can attract better talent because they're not really they're being paid off the soft money but at the same time they have a backstop so they don't have to go look for the next job right away we do work external providers are mentioned AWS Globus is a file transfer service we have on a vein of uses this allows for data sharing and other things and a lot of our testing is two and a half to three and a half times faster than SCP it also deals with machines rebooting network connections coming in going away it deals with firewalls automatically it's very nice it uses the same protocol the Large Hadron Collider uses for moving around a couple of petabytes of data a day great FTP it's very nice and it's all in your web browser so it's very easy to use and understandable and if you're doing a large transfer just emails you what it's done and then we also support other providers in ourselves so like the do-e leadership computing this is the largest supercomputers in the country and the world we will support you on things like Titan and Mira and Aurora you know those machines we will help you with that we tend to have accounts on all these machines exceed is the one that's more accessible um way I'm the exceed campus champion that means that there's actually a formal arrangement between NSF and the University for my time to build bridges between our faculty and the NSF built resources so exceeds a place where you can get CPU time storage consulting up for free you just have to submit a proposal to them and to give you an idea it's way easier to request time here than the equivalent amount of dollars to build or buy so I highly recommend that if you need a lot of compute so here [Music] this will have some unique machines this is a machine here with I think 800 terabytes a flash attached to it there's a machine here with 12 terabytes of shared memory um there's there's some interesting things going on here so these are things that are forward-looking so glovebox already mentioned Yoda byte research cloud glovebox uses that software-defined infrastructure to create a private data center or virtual data center VDC now what is this picture if we had the ability to give you a virtualized desktop a virtualized file server virtualization database we can give you all these different things we have docker containers or profiles for every one of these things and we can spend them all up because of data use agreements this thing needs to be really locked down so here's what we do we take all those things we put them on a private virtual network inside the VM environment so if you're familiar with AWS which I get the impression at least one of you and here is this is like a virtual private cluster of the PC then we have one of these very tightly controlled Bastian hose it's normally a Windows desktop it can also be a Linux desktop that you can connect to with remote desktop it's accessible from the outside world but it's the only thing that can see into this private Enclave behind it by locking everything down at the border we don't have to actually tightly lock down everything inside that border so like today if you're a hardening infrastructure on campus you'd have to take your file service and make it you know pass FISMA requirements you'd have to take your you know desktop and your physical location is like here we have it all done so if you want to do FISMA we have all the designs we go to the auditor which is like we're reaching this exact same decision okay and we just spin it up but the nice thing is because it's all Software Defined private cloud you need them you need another virtual desktop because you have a new student ok it's already been signed off we can clone it instantly you need more storage attached to your server ok we hook up more storage to it it's very flexible yeah defining Dockers okay so for the restricted enclaves it's pretty much pre-selected but that's obviously with effort flexible actually on the HPC cluster a batch system right now can actually launch docker containers as a job we've not tested it it needs to be done we're looking stronger at something called singularity singularity is a container project that's focused for scientific computing so it's not designed to run it like a precision service like a web server but it's really kind of neat so like there's a MRI analysis software that has not been updated since 2012 or something like that it's not currently run on Red Hat 7 it doesn't about there's no compatibility libraries what we were actually able to do is we were able to create a singularity image container which you can import docker containers into singularity which basically has all the Red Hat for runtime which is what this thing was designed on and the user can actually submit that as in their job so they they have their batch script and they run singularity image name executable that's inside the image we map in our scratch directory with map in home it squashes root so you can't see anyone else's data but so it's really kind of neat so containers will be a regular thing going forward oh yeah most definitely for the unrestricted one but today with a little bit effort we just have to the project's not quite roll out general yet you'll be able to take singularity containers and run those onto HPC cluster all you want singularity comes from the guy who started CentOS project and it supports MPI and all that stuff too so it's really quite neat for bringing your own environment and then making that reproducible and we're very interested in containers for reproducible science you can basically freeze that run in time with everything inside the container this is another thing so this is a different storage offering than what I mentioned earlier so turbo is fast reliable not cheap right it's very fast very reliable but it's not the cheapest thing there is this on the other hand is extremely cheap reliable but not fast this would be similar to Amazon's glacier it's an HSM system it's a tape robot it's got cache disk in front of it you put data into it when that catch gets full it writes it off the tape the business model here is as important as the technical features that is this is designed to take one-time funding to store data for 10 years so the way this is designed is if I get everything I'm asking for this is a little bit forward facing the price per terabyte here is going to range between $4 and like $35 per terabyte per year Wow I'm going for the foot dollars now what is it it is you're at the end of your project you want to archive a bunch of data you want to keep it around so you can retrieve it to do future work with but right now you don't have funding to support it but you've got two hundred and thirty dollars so magic number $230 buys you two tapes one that goes in each replica site that gives you six terabytes of uncompressed storage for ten years one-time costs $230 that's what we expect to be able to do we're working on this with the library now this is different blue data deep blue data is when you publish something and you freeze the data and you give it to the world so it's accessible in everything this is when you still want to control it now why are we working with a library on this they're interested for deep blue data for large data sets possibly back ending deep blue data with this service for the larger data sets so they can cost-effectively store data for a long period of time now I say it's not performant it's not performant in the sense that you can't compute directly against it so turbos fasten if you mount it and you can run right on this in our hand you can't do that but a tape drive can hit 300 megabytes per second plus so if you've got large genomes a big zip file of a ton of data and it's large after it's in the drive it will spin off of there actually very very fast so that's what this is this this serves that need of storing data for the long term so this is not meant to take business away from turbo serves different use case what this is is this is USB drives you stack on the shelf in the corner of your office which are susceptible to bit rot susceptible to damage susceptible to theft susceptible to not getting funded again if it's sensitive data and they find out you're leaving in the corner unlock office so this is meant to address all of those needs we do hope also that would be able to connect globus or some sort of data publication thing to this also so if you have a large data set on here and you want to get a copy of it to somebody at Berkeley you type in their email address and they get a magic link and it can just download to them with glaciar I know the cost of putting data on to glacier is relatively cheap yeah it's nothing and but getting it off yes whole different ball of wax yes so de de Graaff side and was about 10 cents per gigabyte right which technically right now the institution has a waiver for for research cases MOOCs and enterprise stuff no but you technically have a data address waiver for research glacier has some number of problems one it's three times as expensive as this based on my business - it's at a cloud provider so baa restricted data problems again I did not want to actually build this I wanted to use glacier I took the money numbered every way I could glacier also charges you a surcharge if you delete the data before 90 days or retrieve the data before 90 days on the other hand I see the archive use case being one that the most recently placed data is probably the most likely dated to be accessed again so the exact opposite of the glacier use case so in this case because we control it we had control the cache disk in front we can actually there's no cost to that literally here you might pay for with time but glacier already has horror stories of wait for hours to get your data right so it's really the exact same system but we're gonna put cache disk in front we're gonna manage it differently but long-term I really hope that we don't have to pick up floor space with this thing it is neat though technically we can expand this out to was a three point six exabytes cost you a pretty penny but we can technically do it and actually even at the so this is based on the design of five and a half petabytes replicated to 11 petabytes of raw space we did a model where we did it on ten and a half petabytes 20 petabytes Ross pays the cost of the institution still goes down so this thing actually still scales out at very high volume um also tapes our design curve shelf-life of 25 to 35 years as opposed to disk you're lucky if you can read your data back after seven years so this really is a more secure way of not losing your data it also is going to be encrypted to run through so the tapes we might even have a service worth the library where you can pick a subset of files encrypt it with a different encryption key write it to a tape and eject it and you can pick it up at the circulation counter just like you're picking up a book from the from the interlibrary loan and then because sometimes a station wagon full of tape is faster than the internet if you can put a hundred terabytes in a suitcase and get on a plane it's a faster way to move a lot of you have to ask why you're so reticent about giving them some floor space to discus given the nature of science you need to keep a lot of data for very long amounts a long period of time that's just a cost effective to stored on Amazon right right so it's actually not floorspace it's mostly just the effort and maintaining and feeding the robot itself that takes labor when you when you give it to a third party they deal with it so we can focus on the higher level service of turning into something that faculty and graduate students can consume that's the important part we don't care where the bits land we just care that it meets all regulatory needs it the faculty are getting funding sources and datasets from that it meets technical requirements and that is actually accessible and usable and then cost-effective so yeah I mean if we shipped it off site we really don't care and that's why I like we support AWS we support the exceed clusters the old mantra was we care about computing our computers storage obviously change the words around a little bit but the same idea so if we can free up labor to directly work with scientists I think that's better for everybody but it has to be cost effective right now the cheapest way is to throw a laborer and buy gear at the large scale but if you need to run a web server it's way to keep it around on Amazon for 25 bucks a month [Music] so that does everything this is our contact information again HPC support is the magic email address that's the arc Tia's Twitter mostly tells you like when two clusters getting updates and stuff like that and you can email me there so that's everything I had so a little bit over on time so any questions on anything they're basically on their way right now so the conflux cluster which was an NSF MRI proposal that was co-written between Marc staff and MI CDE faculty is a three and a half million dollar data-driven computational physics cluster it's got a bunch of IBM power aides and a bunch of IBM Minsky servers with four Pascal's with envy link all connected up together so it's really the flagship way the way Nvidia envision how these GPS will work so they've got the env linked directly to the processor main memory as opposed to going through a PCI Express if you don't know intel will not implement an v link because they have their knights landing they don't want people to use GPUs um we have mics also Xeon Phi's because we will support what the faculty find useful with that project we got early access and so basically they're on their way right now so 20% of the closer is supposed to be made available to non p i-- users that probably won't happen until next year but that said if you have a small project ask the faculty working on this are very mountable to other people utilizing the resource the pascal's are not on there today there's a rack marked off and they're on their way we have 2k 80s in there right now instead kind of do development work the bottleneck is not I mean the first one like the GPU but then you company if you have a water GPU that gets better then you come but like a CPU but then CPUs on the same machine that's it's better but only becomes storage and that's why a lot of this architectures are built in a way so you have for example hard drive or as a temporary storage Danika one this is deep or OS and they will have super fast SSD but actually serve data to some complex because it's a machine learning platform every node also has like an 800 gig flasher and vm e drive local on the node and mention an OS drive we don't even have an OS drive on those machines they boot from the network yes so we have an 800 gig thing sitting there that is flashed directly inside the note and then Conflux it's on a hundred gig InfiniBand network connected to IBM a petabyte of IBM GPFS file system that what they just tuned it the other day where they get seven gigabytes a second out of it or something like that yeah so we have a seven gigabyte per second file system that's a slow thing you have shared across the entire cluster um so I mean if you want that skills I mean we can just add them to the cluster actually dr. Michelsen and myself Nvidia create a thing called the DG x1 it's their machine learning appliance but we're looking at dozen behind one of those for campus Eric did not realize how expensive they actually work it's a hundred and thirteen thousand dollars but you're required to buy at least one year of their maina's which is like another thirty so you really got to have a hundred fifty grand before you even talk about one of these things yeah because we are probably gonna be worth it yeah so there's multiple ways to slice it if you're not exactly sure that this is a direction you want to go you don't want to invest into it yet get a GPU from us for a hundred bucks for a month and just test it first now the fastest thing we have is a k40 right so it's not gonna be Pascal class but it'll still give you an idea how does that compare to a CPU because I can tell you oh you did that already okay okay so what you can do is is if you want specifically a Pascal architecture or a Maxwell architecture because those are better for the machine learning I get the half precision and single precision and the higher memory bandwidth um we have a faculty it has like 64 of these on the cluster right now um we can actually just under fo e add on to the system machines with very specific cards no okay f OE sounds great because you can get exactly the thing you want here's the deal though compared to just renting stuff from us we're constantly refreshing things like we just added Cave 40s but we added more K forties does a bunch of K 20 X's and K for days um if you buy a machine and you run it for five years when it's four and five years old the technology on its old on the other hand if you're renting it from us we've already refreshed or added at least where you can test the new technology so it's a little bit of a trade-off so the the brand-new machine may be the fastest thing but it's gonna end up being the slowest thing on their hand if you're getting from you're getting a mix of something that's a couple years old something that's brand-new and we kind of always have this nice average that's across its whole life yeah we have all of that too so you look in our environment we have something called modules your module load CUDA / 8.0 you get today because I just came out no no don't just defaults rapidly because we're worried about reproducibility and changing stuff from underneath people so we normally only change defaults for high-profile things at a defined boundary normally one of our OS updates because now we're also updating G Lib C and everything else on the cluster so we tend to move everything for then that said we will install whenever we just added a bunch of Titan Pascal's to the cluster for this faculty for machine learning those required could eight which was funny because we literally installed those like four days ago so it's the CUDA eight release candidate and then they release the RIPTA stable one yesterday but we missed about like 48 hours to have a stable one but the point is that hard to required it they did have a newer version of the driver but the toolkit was available wherever so so yeah we tend to have many many versions and permutations of versions of software on the system obviously driver level stuffs a little bit different because it has some hardware you can't change that as rapidly but yeah it's also the same mirrors mapped on the hardware drivers version to match in the machines yeah if you want to use Dockers but I'm a master with yeah it's it's getting harder than it was in the past because people want more than just CPUs and everything is a little bit different we even have faculty talking to us about FPGAs so around machine learning so you've probably have seen the Google tensor processor the processor is optimized for machine learning because it's like a fuzzy processor it doesn't give you an exact answer but you got way more silicon back to make it more cores what if we could do that in FPGA so you could have other flexibility but also implement that sort of thing Hardware on the system and have it attached to a processor so it's like this was an interesting idea and it might be a proposal going out the door on that sort of work