[
    {
        "start": 0.08,
        "text": "uh welcome everybody to this week's seminar um this is supposed to be hosted by dr anika rajapaksa i'm a student stephen linsley in his lab um but something came up and he's running a bit late so i'll have the pleasure of introducing our guest this week uh duncan ralph uh so dr ralph attended the university of santa cruz for his undergraduate studies in physics completing his thesis on energy transport and condensed matter theory in 2005. he completed his phd at mit in 2014 working on the large hadron collider at cern and his thesis described the observation of the higgs boson decays to four leptons since 2014 he's worked in frederick matson's lab at the fred hutchinson cancer research center first as a postdoctoral post-doctoral researcher and more recently as a staff scientist writing new computational methods for the analysis of b-cell receptor deep sequencing data so it's my pleasure to turn over the presentation to dr ralph okay thanks a lot and thanks everyone for having me um let me know if there's any problem with my audio or the background or anything like that "
    },
    {
        "start": 60.559,
        "text": "and i think questions is just interrupt and unmute if that works for everyone and yell loudly if i don't hear you okay start sharing my screen okay you should be able to see everything there okay so just need to start my timer so i'll be talking about a variety of different inference methods that we've been writing over the last few years that that you infer a lot of interesting things on b-cell receptor deep sequencing data sets they use pretty heavy use of hidden markov models in a lot of cases so first i'll describe a little bit of the structure of how the immune system works and how antibodies fit into that structure and then i'll talk a very little bit about how we measure this with sequencing data but mostly i'll be talking about three of the main inference methods that that the software package that all this is in called partis uses um so first i'll talk about uh "
    },
    {
        "start": 121.68,
        "text": "grouping sequences in a clonally related families that stem from each of which stems from a single naive rearrangement event and then i'll talk about uh so in newer data um we now have like pairing information about the heavy and light pairs that make up an antibody and i'll talk about how we can use this this information and newer data to improve the clinical family clustering from the first step there the final major piece i'll talk about is how we can predict antibody affinity just from the sequence of the bcr receptor and then for all these i'll be talking about how we've applied them to the real data sets that you know we actually care about so one way to think about the main goal of what your immune system is trying to do is that it's trying to kill things that aren't you and there's two main parts to your immune system there's the innate immune system and the adaptive part the innate immune system is much older we share even with fungi and plants have an innate immune system it's generally fast and not very specific to "
    },
    {
        "start": 183.28,
        "text": "particular pathogens typical typically might respond in minutes to hours and one example of it responding is just you swelling up when you get a cut or a bruise or something like that the adaptive immune system on the other hand is generally very specific to particular pathogens and at least in the first exposure is very slow so antibodies for instance you start to build up significant quantities after maybe a couple of weeks on subsequent exposures the adaptive system responds quite a bit faster and this is the foundation of immune memory which is the reason in many or most cases you don't get disease from a single pathogen from the same pathogen twice and also of course this is how vaccines work so antibodies are part of the one of the main parts of the adaptive immune system and i'll be talking about them in the rest of this talk so two ways to kind of i'm showing here two ways to visualize just the general structure of what antibodies look like it's kind of shared with all antibodies on the right this is just a space filling diagram and on the left is "
    },
    {
        "start": 244.319,
        "text": "the same thing but showing a little bit more a few more labels here so it's a dimer and each half of that is composed of the heavy chain and the light chain and both the heavy and light chain contribute to the main goal here which is binding to a particular antigen and the heavy and light have a lot of differences but they're basically analogous in everything they're doing so one way the way i like to think about the main goal of antibodies is that they're identifying or tagging things that we don't want to be there that we want to get rid of and once they're done those things are identified or tagged maybe the antibody actually does the destroying but neutralize correctly neutralizing for instance or maybe the antibody signals to something else in the immune system to get rid of the thing but in either case we generally want antibodies to be extremely precise so they bind to a specific piece of a specific pathogen like a specific viral variant and this precision is really important because i mean in a lot of cases if you have antibodies that aren't very specific that bind to several different things that that's this that can be the "
    },
    {
        "start": 304.639,
        "text": "source of a lot of diseases so we want antibodies to be able to very specifically bind to a particular pathogen but we also need antibodies that can cover the entire space of potential pathogenic threats that we might encounter um and this uh this space is kind of it's it's basically like the space of all possible peptides that are like 15 or 20 amino acids long so those is of course incredibly large and incredibly high dimensional but one way i like to visualize this problem is in two dimensions here where i'm imagining that these three red squares or pathogens that we might be exposed to and the green crosses scattered around here are dcrs or antibodies that we have available to respond to them so so you can imagine that your immune system would want to kind of evenly populate the space with the green crosses and this because this hypothetical space of potential antigens is so impossibly large and high dimensional um the amount of information necessary to code for that kind of diversity directly is just many many many orders of magnitude greater than you have in your genome "
    },
    {
        "start": 366.08,
        "text": "so this diversity generation um happens somatically after after germline and in the so the first step of b cell development that generates this diversity is called bdj rearrangement those three letters are just arbitrary labels for gene families and so the this bottom sequence here is the complete bcr sequence that codes for the antibody and the way your body constructs that during b cell development is that you have so you have a bunch of different b genes like around 50 different kinds of different v genes and one of these v genes will be chosen uh here and you also choose one d gene and one j gene and then you you join the three um b and d and j genes together and as you join them you delete some random number of bases from the boundaries and you also insert some random number of non-templated nucleotides here in blue so the end result of this is a naive bcr sequence which means like one minute so "
    },
    {
        "start": 427.44,
        "text": "this is a b cell that is not yet encountered it's called antigen and this picture so this is for the heavy chain the light chain is basically the same but there's no d gene so this diverse reason is is much smaller and tying together the last few slides uh so this is the diverse region and that's called the cdr3 and as you might perhaps guess like the cdr3 is the diverse part and it ends up being the bit that's um going up against the antigen so it's determining complementarity so bdj rearrangement is kind of the initial diversification step that gives you kind of an initial rough draft affinity maturation is the next step that kind of refines that diversity and affinity maturation is a selective process with this mutation going on you start out with if you're a naive b cell at the top here so these colors are representing your bcr sequence so like the green is the particular b gene you have the orange is the d gene and then we're "
    },
    {
        "start": 487.68,
        "text": "imagining that this bcr codes for an antibody that's represented here or a bcr that's represented here on the surface where the shape of this is in cartoon form um representing the specificities we're imagining that some antigen comes in and it binds well enough to this naive b cell that it's activated so when that when a b-cell encounters its cognitive antigen it's given permission to migrate to a germinal center and then start reproducing to create offspring and when b cells reproduce in germinal centers they purposefully introduce a huge amount of mutation right of the bcr locus it's about 10 to the 5 times the background rate in your germline because there's a selective process where offspring that have higher affinity for the antigen have more offspring the way we represent this here is so this naive cell has two offspring this one on the left has a red mutation in the blue bit here and this other one has a red mutation in the purple bit and we're imagining it so we're imagining that this mutation changes the antibody structure here "
    },
    {
        "start": 547.76,
        "text": "to fit the energy a little bit worse whereas this one changes it to fit the anagen a little bit better so since this one fits a little better it goes on to have another offspring that has another mutation here that changes the antibody or the bcr to fit the antigen even better and if we decartiunify this picture there's of course many more b cells and a much longer process over weeks or months but uh still so at the end of this process you end up with antibodies that are typically or at least a lot of the time several orders of magnitude higher affinity than the original antibodies so tying together the last few the pictures in the last few slides on the left here we have vdj rearrangement at the top it's kind of doing the rough draft of diversity and affinity maturation and the bottom is kind of refining that and the way i tie this into the two-dimensional representation is that vdg rearrangement is choosing these green crosses so where these green crosses go and like trying to evenly populate space "
    },
    {
        "start": 609.68,
        "text": "and then affinity maturation is represented as we're imagining that this um red square which is a pathogen that we're actually exposed to that and then when we're exposed to it the nearby and the nearby uh b cells the nearby green crosses uh reproduce and go through affinity maturation which is represented as this this tree that's kind of like their offspring evolving to be closer to the red square so we've all been at higher infinity for the red square so before we talk about measuring these processes i just have a couple of examples on a slide about things that were generally interested in learning from these processes so so first um we can imagine that this might be an antibody that we know is really good against a particular pathogen and we might want to know like is this an antibody that most people can make or not so so one of the main contributors to that is the bee gene it uses so the vg and locus is incredibly incredibly diverse between people it's similar to the mhc locus which you might "
    },
    {
        "start": 670.24,
        "text": "have heard about more so there's about 50 v genes in each person and you you probably share between a quarter and a half of those with a person with a person sitting next to you if you're unrelated um so in terms of this if an ana if we know that an antibody or an antibody class requires the use of a specific v gene um some v genes you know everyone has so that would be a great animal body to try and target with a vaccine but there's lots of other antibodies that like these v genes that only a couple like 10 of the population or maybe even only in particular some population in a particular part of the world have another thing that we might be interested in uh learning is so learning about is so so the um the way you respond to a particular flu strain is kind of unique in that it's it's very heavily dependent on your exposure history to different flu varieties in the past this is called antigenic seniority and this relates to the antibodies they're doing that that are responding to that in that "
    },
    {
        "start": 732.32,
        "text": "um if we're interested in finding out why a particular antibody in a person uh that developed that was highly effective against flu developed would it be very important to know what the flu strains they were exposed to in the past and the final example here is something i'll actually be talking about a lot in this talk uh where this is where we're imagining this we know this antibody here is highly effective against neutralizing hiv and maybe and most um hiv neutralizing antibodies have really high mutation levels just because uh it's just hiv is a really effective virus that usually takes a lot of mutation to find an antibody that's broadly neutralizing so that means that this is a really long path and one way that i'll talk about a little more later that we might be able to create an hiv vaccine is by recapitulating all these steps with sequential vaccination and that's the kind of thing in order to do that we need to have a really good understanding of this mutational pathway here okay so when we want to observe the "
    },
    {
        "start": 793.76,
        "text": "processes i've just been describing the main tool that at least is relevant for this talk is deep sequencing and we've been doing i mean not as personally but just as a field we've been doing lots of deep sequencing bcr repertoires for the last 10 years maybe and this gives you a really large subset maybe 100 000 of the sequences in the body and it is large but it's important to keep in mind that there's only maybe one out of ten to the fifth of the b cells alive in your body at any one point in time and probably only maybe like one in 10 million or 100 million of these cells that are alive in your body over the course of months or years and so the way uh i'm kind of representing this data in this cartoon here is that so in the data the sequence here uh the bcr sequence is just gray now so so that we don't know showing that we don't know what v and d and j genes were used we don't know the deletion boundaries or the insertions and also all of the sequences are just messed on top of each other into a morass because we don't know which ones are clonally related and we also don't know "
    },
    {
        "start": 854.88,
        "text": "the phylogenetic structure here so what we need what we want to do and what i'll be talking about in the rest of this talk is going back here to find some to infer some representation of the reality that's that's occurring so the first step in this is to group together the sequences that are clonally related so i'll call clonal families and what i mean by that is sequences that stem from a single naive rearrangement event so by definition cleanly related sequences have identical v and d and j genes and exactly the same deletion boundaries and also inserted bases so given that um you might imagine a naive way to cluster these together and indeed what's basically the standard approach the vast majority of practical papers still use is to group together sequences that have identical v and j genes um you don't use the d because it's just much harder to infer that accurately because it's so short and there's so many deletions so you have require the same v and j gene and then you do single linkage "
    },
    {
        "start": 915.6,
        "text": "clustering with a nucleotide similarity threshold within the cdr3 which is the most diverse region of the antibody the problem with this approach is if there's any kind of significant shm or any significant mutation the accuracy is just absolutely terrible the first problem is that if your gene infers is wrong which is a lot of time or at least there's a lot of uncertainty in it and the clusters will be wrong the other problem is that it conflates on the one hand somatic hyper mutation and then the other hand naive sequence inference and accuracy um and that this figure here i think it may have had a little bit too much fat in it adding a lot of detail to it but let's focus on one aspect this is kind of explaining the last thing i said and that so we have this true family here with a bunch of black dots which are the observed sequences and it's showing that if we cluster with these red dashed circles on here just based on the observed mature black dots we're going to spuriously erroneously split apart this family "
    },
    {
        "start": 975.68,
        "text": "whereas if we were if we were just doing this like simple-minded single linkage clustering if we use the inferred ancestral sequences here even though these are not perfectly inferred we would do much better so what i mean by conflating is that we're conflating this distance with this distance and if we're doing clustering we really don't need to spread them apart by this distance so an approach that we think makes a lot more sense for this is to use hidden markov models one reason is that these let you incorporate all the details of ddj rearrangement somatic hypermutation and also equally importantly it lets you integrate out your annotation uncertainty so your uncertainty about what particular b and d and j genes and deletions were used so just in general there's a lot of uncertainty in a lot of cases about what the annotation was and if you care about clustering you don't want to tie that cluster into a particular annotation one way to visualize what an hmm is doing is by analogy to a normal distribution so on the left "
    },
    {
        "start": 1037.6,
        "text": "you can think of a normal distribution in this box as something where you put in an x value and turn the crank and it spits out this y value which is more or less the probability of x given that the normal distribution is the right model and given that these two parameters the mean and width have the correct values so looking at the right um in hmm it has is way more complicated and it has many many more parameters but it's still doing basically the same thing and that you put in a bcr sequence and what we get out of it is the probability of a particular annotation or a particular clustering or something like that zooming way in on the hmm to see what it looks like so what an hmm consists of is a series of states so in here each each of these open circles is a state um so an hmm consists of a bunch of states together with rules for transitions between them where each transition has a probability associated with it and also each state has uh rules for which bases it can emit together with probabilities so a probability of "
    },
    {
        "start": 1098.32,
        "text": "emitting a versus immediate c etc and so what you do when you pass the sequence into an hmm the sequence is a series of a's and c's and g's and t's and as you as you move along that sequence you are moving along the sequence of states deciding which state to go to and deciding what base to admit at each point so this is that gives you a path through the hmm which is this red line and if we zoom out so this is the whole hmm that's representing it's representing pdj rearrangement so a path through this hmm is equivalent to say in this sequence it's a particular naive arrangement or a particular annotation this particular path it's chosen this v gene and it goes along the v gene and jumps out a little bit early so there's a deletion here it jumps into this non-templated insertion state and loops around a few times insert a few bases and then it jumps into this d gene uh and skips a little start which is also a deletion and goes on to do entirely analogous things over here so this just to say it again this this path represents a particular annotation "
    },
    {
        "start": 1159.76,
        "text": "or naive rearrangement and the html has a rule for just giving you the probability associated with that so if all you're interested in finding is the most likely annotation for a sequence you just choose the most likely path there's super well established algorithm dynamic programming algorithms there's a viterbi algorithm that defined you they can look through all the paths and find you the most likely one in a very efficient manner for clustering what we're more interested in is the total probability of the sequence given the model and this is just you sum the probabilities over all possible paths um and happily enough there's a nice dynamic programming algorithm called the forward algorithm that also calculates this very efficiently the last piece we use we need before we can before we can look at clonal relationships to this is to represent how we can have two or more sequences be clonally related in this hmm and the way we do that is just force the sequences to take exactly the same path through the hmm and we call this the multi-hmm because at each point "
    },
    {
        "start": 1220.08,
        "text": "the only difference is that then we're emitting two bases or three bases because we have two or three sequences there so to decide whether two sequences or two clusters are clonally related we just form this uh this ratio here where in the top is the total probability where we're forced all the sequences to take the same path through the h m and in the denominator is the total probability well the sum of the total probabilities will force them to take individual paths through it and use this for clustering uh we you do hierarchical conglomeration which is shown on the bottom here where we you first put together the two nearest sequences and the next two nearest and then the two nearest clusters and so forth and you keep going until this ratio for all for all pairs falls falls below one so when you want to go and measure how well this kind of clustering method is doing um typically you have to do most revalidation on simulation because on real data one problem is that you almost never "
    },
    {
        "start": 1281.6,
        "text": "know the right answer and the other problem is you just never have very much real data so to really have confidence in a method like this you need to test the method in all regions of parameter space um and the data really only exists at like a few isolated points so you need to you need to make simulation to really do a good job of spanning the whole space to do this so this is what we're doing here this is this is showing um clustering performance uh on a variety of simulation simulated repertoires where the variable we're scanning across here is kind of the clonality the repertoire so on the left side of the x-axis is a very diverse singleton repertoire on the right side is a much more highly clonal one that consists of just a few large families and on the y-axis is the f1 score which is what we're using to measure how good the clustering is and it's just a number between zero and one where one is perfect that encompasses both over clustering and under clustering so sensitivity and specificity in one number and one other thing before is that so that um on the left is showing repertoires that "
    },
    {
        "start": 1342.96,
        "text": "have typical mutation levels and on the right is ones that have much higher than normal mutation levels and that's important because kind of in any vcr inference problem the mutation level is just this knob where you turn it down to zero and the problem is totally trivial there's really nothing to do and you crank it up high enough and the problem is just impossible so this is kind of just two regimes where one is like typical slash easy and the other one is like much more challenging so the red line here is the hmm-based method uh partis that i've been talking about and in the easy um for the easier inference environment is bumped up near the top which is nice and then when you move to the more challenging environment uh as you expect it it moves downward and has worse performance but it's in a fairly well controlled way it's still it stays fairly horizontal there and the main method i want to compare it against is the light blue called the vjcdr3 which is the the kind of naive approach that's by far the most common practical papers still is light blue and as i was kind of alluding to before "
    },
    {
        "start": 1402.96,
        "text": "the problem with this is that it it dramatically over splits anything with significant shm and one way to see that is that for repertoires on the left side here uh where the true answer is just the singleton repertoire the light blue does really well because if you oversplit everything uh if the correct answer is is to split everything into single times then you'll do really well the other thing just to mention is that you can see what i was saying earlier i think that when you have higher mutation levels it does really particularly poorly so it gets close to down to the bottom here which is just random chance um because it is so gratuitously over splitting things so once you have clonal family groupings the other main step we need to get back to some representation of reality here is to do phylogenetics and the way i do this original one this is making it look like we're sampling all of these ancestors here and probably a more typical case is where we're mostly sampling leaf sequences and then we need to use the phylogenetic program to infer these sequences here that are grayed out "
    },
    {
        "start": 1464.08,
        "text": "and there's many many many different biogenetic programs out there the ones that are most relevant for this are ones that take into account the two main peculiarities of bcr development the first is that the mutation is context dependent which means that the mutation rate doesn't just depend on the base at a particular position it also depends on the bases on either sides of the two or four bases on either side and also you you usually want the program to also take into account the imputation process might be different in the v division to what it is in the j region and the the program that we really like to use for this is called linear ham this was written by one of our students a couple years ago named conrie it's a bayesian phylogenetic hidden markov model so this is it's using an hmm that's it's a totally separate hmm to what artist was using and it's as you can hopefully tell from this picture it's simultaneously inferring the the annotation here uh at the same time as doing trees and the fact that it's like fully probabilistic um and bayesian it means that you can sample from posterior what it's doing is "
    },
    {
        "start": 1524.96,
        "text": "sampling from posterior on trees and annotations and inferred ancestors and this is really useful for what i'm going to show you later so a lot of the data we've been running on and using in actual papers um with biology that has been made by julie overbough's group group that's also with the fred hutch here and all of all of this most of all this data has been from a particularly unique cohort um of hiv samples that julie was involved in sampling from uh from kenya 20 or 30 years ago and the reason these are um very interesting from the perspective of trying to develop an eventual hiv vaccine is probably because this is they're from before the widespread availability of antiretroviral therapy um so there's a mop the infections are just much more uncontrolled than they would be today and also there it's a very high risk cohort so there's a fair amount of super "
    },
    {
        "start": 1585.679,
        "text": "infection with different um different clades of hiv and both of those things um are associated with being more likely to develop really good broadly neutralizing antibodies that can neutralize a bunch of different clades of hiv so the commonality between all the papers i'll be showing here is that julie's group in this case laura and cassie they go and take the they they take some they unfreeze some samples and they go and use wetlab techniques um to pull out a bunch of really good antibodies by some criterion so here uh what they're doing is using micro neutralization this is just the x and y axes are two different replicas of this and the ones they're choosing to go test more thoroughly i think is all the dots in this plot but the ones that are highlighted in blue are the ones that turned out to be really good and then the next step uh is to unfreeze some more blood and they do deep sequencing on that and then they pass that to us and what we do is for each of these antibodies that turned out to be really good we pull out all the clonally "
    },
    {
        "start": 1646.48,
        "text": "related sequences from the deep sequencing data and then analyze those families so this is just a simple scatter plot showing all those families um in mean shm versus the size of them and there isn't a lot to take away from this but you can just in a simple plot see that uh the the families of interest which is where the good antibodies are do tend to be in larger families and have higher levels of mutation uh and just just quickly the reason this particular data uh was very interesting to us is because this this is from um a mother-daughter pair where vertical transmission occurred which means that so the mother infected the infant after birth and this is um besides being super tragic this is very interesting from the perspective of potentially developing an hiv vaccine because in most hiv transmission events the person who is getting it is totally naive to hiv they don't have any hiv antibodies but in vertical transmission between "
    },
    {
        "start": 1707.52,
        "text": "because just generally speaking mothers transmit antibodies like are share their antibodies with their infinite especially through breast milk that when there is a vertical transmission event um the person who is acquiring hiv already has a bunch of antibodies to it that are potentially very good which is much more similar to the case that you would have if you had been vaccinated for hiv and then were exposed to it so talking a little bit more in detail about this hypothetical route to an hiv vaccine the way this would work in theory is that you would find an antibody that you know is really good and broadly neutralizing so enab is the abbreviation for that um so you'd find a good antibody using wetland means and then you do some deep sequencing and reconstruct the clonal family and then use something like linear ham to to reconstruct this particular evolutionary pathway between these inferred ancestors back to the unmutated ancestor the naive sequence way up here "
    },
    {
        "start": 1768.159,
        "text": "and in the next step you would design some antigen that is designed to elicit the particular unmutated ancestor here and then you design a series of booster immunizations along here where each one is designed to um to nudge these ancestral antibodies sequences in exactly the proper direction to recapitulate this mutation opacity you end up eventually back at the mature bnab that you were aiming for and at least to me this this sounds very cumbersome and kind of unrealistic but that people can actually do this with mice um so it uh it is something that is actually being done at least in terms of a proof of concept level kind of thing so this is a paper where i'm showing a little bit more of the development going a little further with the development so so again uh here cassie laura have found a bunch of antibodies that they know are highly neutralizing i'm highlighting one here in blue called bf5 to 0.1 and then they do ng they do deep "
    },
    {
        "start": 1828.96,
        "text": "sequencing and we pull out the related sequences and make trees with them and then we run linear ham to give us a probabilistic view of what all of the inferred intermediate sequences and the the important mutations were that happen in that development path and that's summarized in this this this flowchart here where at the top we have the naive sequence at the bottom of the mature sequence and each oval here is an inferred ancestral sequence and the brightness of the red is how confident we are that it was really there and the darkness of the blue arrows is showing how confident we are that that was mutation that actually occurred and the next step in this process is to pass the sequences back to them so that they can of course very very labor intensively go synthesize them and test the neutralization of these antibodies and this kind of chart is really important to that because it gives you a lot of confidence which ones you really need to synthesize for instance this one is bright red so we're really sure we need to do that one but then when you get down to here you have these two alternative pathways and you need to test both of those to see which one is the correct one "
    },
    {
        "start": 1891.12,
        "text": "um the end result of that uh you can we summarize in this table here where on the left um you have the unmuted naive ancestor that it binds to hiv i think i'm remembering it correctly but it doesn't neutralize any uh and on the right you have the mature antibody originally pulled out that has very good neutralization breadth and about six percent nucleotide shm and then as we go along the chart to the right these are all the inferred the inferred ancestral sequences and the really exciting thing about this is you get to almost full neutralization breadth with only a third or a quarter of the total number of mutations here so this means that like the number of key that were really important to acquire that neutralization breadth is actually really small um and because of that uh and because uh you know if you have some friends that can do this magical structural stuff you can look at exactly what the mutations look like on the structures so here uh this is a cryo-em structure "
    },
    {
        "start": 1951.12,
        "text": "um it's showing the antibody binding to the relevant bit of hiv and then if we on the right if we zoom in to that to that contact surface what they're doing here is they can they can really just visualize where the mutations in red um so exactly these key mutations how they change the way that the antibodies binding to ons and be able to enable the broad neutralization so most of the time uh when we're talking about another bodies we're talking about them acting via neutralization and that a typical way would be that the ab and body directly binds to the pathogen and stops it from entering the cell an example of this being important is that you might have seen in the last few weeks the meta-analysis is coming out so we have a really nice variety of stars kobe 2 vaccines now and you might have seen some plots where like on one axis they have the induced neutralization titer on the other axis you have the vaccine efficacy and they could draw really nice i mean large confidence intervals but a really nice best fit line through there so in these cases um the neutralization is "
    },
    {
        "start": 2012.96,
        "text": "really all you need to predict how effective the vaccine is going to be but this isn't always the case um another way that antibodies can contribute to viral control is with something called adcc which stands for antibody dependent cellular cytotoxicity and this is illustrated here we have a cloud of antibodies binding to something they don't like here um and then some other effective cell affect yourself in the immune system is binding to kind of the back side of the antibodies um and then shooting laser beams of the cell and wising it to destroy it and uh a good example of a case where adcc was really important um is and i i think it was the the largest hiv vaccine trial date was the rv 144 trial and this was i mean the big picture result of this it was not an effective vaccine but it did achieve some level of efficacy at some level or protection and when they look at the correlates of what kind of could do the quotes of protection that could predict whether it was protective um adcc was a strong was a correlate of "
    },
    {
        "start": 2075.28,
        "text": "like like if you had a strong adcc response you were more likely to have protection um but the i think there was actually no correlation for neutralization so in this case at least adcc seemed to be a lot more important than neutralization so in order to relate adcc activity um to the inference that to the infrared ancestors i'm talking about i need to go into a little more detail about antibody structure so this is the bit that i was saying is this is the variable part of the antibody that determines the pathogen specificity and i mentioned the variable there's the cdr3 region here there's actually three cdrs here that determine the complementarity and the cdrs these are very variable with the blue stripe spikes here and they're interspersed with a few framework regions that are much less variable and in this image at the top right you can see why that is because the cdrs they're these loops are hanging off and directly contacting the antigen whereas the framework are much more buried in here in like the interior of the antibody "
    },
    {
        "start": 2139.68,
        "text": "so keeping that in mind in this paper this is the first part is just the same as i've been describing in the other two papers where in this case laura they go and find a bunch of really good antibodies in these samples in this case they're looking for antibodies that have a really good adcc activity not neutralization they do deep sequencing we pull out the families and then for the ancestors using linear ham doing this kind of thing then we pass the sequences back to them they synthesize them and test in this case the edcc activity not the neutralization to determine which of these mutations were important for allowing adcc activity um and we finally summarize what's going on or what we think is going on in this table here where these along the left side are all of the antibodies they're testing and then the columns it's looking at like um where are the mutations so in the left column is the mutations that we're allowing just binding to hiv and in the right column is the mutations that rely on really strong adcc activity and we're looking at where in the "
    },
    {
        "start": 2200.56,
        "text": "antibody those mutations occurred so if we just kind of like squint um i think it's fair to say that like the first column that's allowing bind just simple binding to hiv is mostly yellow so those are mostly the cdr which is the bit up here that's that's determining the specificity um so that that makes sense is binding directly to hiv on the other hand in the far right column um that that's the mutations that are giving you strong adcc activity these are more green and blue which involves a lot more mutations in the framework region which are more in the interior and that that that makes sense um at least hypothetically because the interior the antibody you could imagine be more involved with how it would bind to this effector cell on the backside of it okay so now um we're going to put our method development hats back on for a bit so antibodies have two parts here they have the heavy chain and the light chain and the locus codeine for the heavy "
    },
    {
        "start": 2261.68,
        "text": "chain here and the two different light chain locate the cap and the lambda they're on three completely different chromosomes um so that that means that basically all the deep sequencing data we've gotten um over the last 10 years or whatever uh we've had no idea which heavy chain pairs with which light chain and this is obviously a huge problem if you want to synthesize those antibodies because if you know if you have a heavy chain that you think is good and you want to go test it you have to just choose a bunch of light chains more or less at random and hope they work and you know most of them probably aren't even going to make a functional antibody let alone have the affinity that you're looking for another problem with not having this pairing information is that it makes for really inaccurate clustering the main reason for this is that the light chain um just has so little diversity compared to the heavy chain so in this picture at the bottom i'm showing that heavy chain here um on a average has about 20 bases that aren't v and aren't j so this is where most of the diversity is coming from whereas the light chain here on average has only one base so like lychee "
    },
    {
        "start": 2321.76,
        "text": "diversity is really mostly just a matter of choosing a vg and choosing a j gene and the effect of this is that on any reasonably sized repertoire you'll have lots and lots of cases where two different light change rearrangements result in naive sequences that are experimentally indistinguishable so you're just going to super over cluster the light chain in essentially all cases and we can see this happening on real data here's an example where i was showing the distribution of cluster sizes for the heavy chain in green and the light chain and red and blue and you know we know these are the same clusters so they should be basically the same really would be that the sum of the red and blue would be the same as the green but you know they should be basically on top of each other if the cluster is quite right but instead the light chain is trying to pretend to tell us that the light chain cluster is an order of magnitude bigger than the largest heavy chain clusters which is clearly just ridiculous luckily people have recently figured out how to do deep sequencing in a way that preserves the period information "
    },
    {
        "start": 2383.44,
        "text": "the most common way that people are doing that now i think is with the single cell droplet technologies from 10x genomics but in any case we worked out a way that we can use this parent information to basically correct the clusters of the heavy and light chains and a very cartoonified form here um the way this works is that you start out with the single chain heavy partition and the light partitions so these partitions have the same sequences here except they disagree for instance here on whether they disagree on whether c and a and b are clonally related and there's a fundamental asymmetry in this situation in that if the white partition says they're all quantity related that isn't really active information telling us that they're they are related it's just saying it doesn't have information that they're not on the other hand here when the split apart that means that the heavy partition is saying like c sequence is really different a and b so they really probably are so this means that like um when we want to correct like kind of joy together the information from these two what you want "
    },
    {
        "start": 2443.839,
        "text": "to do is basically apply the splits from one partition to refine the other partition and vice versa that the joint clusters you get out of this are going to be the splittiest ones so this the splittier ones from both cases and then to use this in practice there's some fairly complicated steps to incorporate these joint clusters back into the partition you're making that has a bunch of overlaps that i'm just going to completely gloss over here but the bottom line is that when we test this on simulation samples we can see that it is it's doing what we want it to which is basically correcting correcting the heavy chain partition with the light chain information and vice versa and i'm skipping over all of that uh simulation and just show you some results on real data where we don't know the correct answer but we can still see that it's that the the method is basically behaving how we expect it to so the first thing so this is heavy on the left and the two light chains on the right and the first thing is that the the single chain partitions in green um initially they start out in the light shade being super super "
    },
    {
        "start": 2505.119,
        "text": "over clustered so going out to the right but then as we apply the paired clustering method to get the red line it's much more that heavy chain and light chain predictions are much much more compatible with each other and the other thing to notice here so it makes a massive difference in the light chain but even on the heavy chain where you know on smaller clusters it's not making huge difference it does still make a really significant difference to the largest clusters here um and at least for the kind of biological applications that i'm showing the largest clusters are the ones we really care about so having something like this where a third of the sequences in your large cluster are spurious that that that means you've got a whole you've probably got a holding edge in the tree that isn't actually supposed to be there which means it's really going to screw up your ancestral inference um okay and the last methods thing i'll talk about here is um a way to choose antibodies that are likely to be high affinity based just on like the sequencing data so in all the hiv papers i was mentioning they take they use weblab techniques to "
    },
    {
        "start": 2567.359,
        "text": "extract pcr sequences or antibodies that they know are highly neutralizing or have a lot of adcc activity and there's a lot of different ways to do that you can use cell sorting neutralization or that kind of thing and all of those are really effective but they're also really expensive and really time consuming and it would be really great if there was a way that we could just use the sequencing data to predict which of those sequences in the deep sequencing data code for high affinity analysis because a lot of times we already have that data sitting around and we know some of those antibodies surely code for some of the sequences surely code for high infinity antibodies the basic the basic handle that we that we would want to use here for predicting which of these antibodies are high affinity is the evolutionary selection information so affinity maturation is this process where like it evolutionarily just like applies selection to increase the affinity of the antibodies and that should leave traces that we can extract from the inferred lineages in the final in the final product "
    },
    {
        "start": 2627.599,
        "text": "so in order to try and see if this kind of thing could actually work in practice well the first thing we put together which again i'm just kind of basically skip over here is put together this big simulation framework um that can simulate all the product the product the relevant processes inside the germinal center and like the selective effects of affinity maturation and then once we have that we can test a bunch of different metrics here that i won't go into in detail but there's a bunch of different metrics that we kind of test to see is well can this metric predict the affinity based on the evolutionary information in the tree one metric i will touch on quickly because it's i can draw a nice picture for it is called the local branching index and this uh it measures kind of the branchiness in the local area of the tree around the node or around the antibody you're talking about and the idea is that if you're a high efficiency b cell you're likely to have more offspring um and that means that the area of the tree that you're in will have a bunch of branching going on and this metric has been used a lot in "
    },
    {
        "start": 2690.16,
        "text": "the context of viral evolution to predict um to predict which flu strains are going to predominate in future years uh now also just if you see i've got a big list of variables here and i don't know which are good and they each have some independent information but it's not clear how much the natural thing to do is to throw them into some kind of machine learning approach and try and get that to work well so we suck uh a ton of time into into doing this um using a decision tree regression turned out to be an approach those seem to be working the best and this is just a chart that i won't mention that i won't like go through a detail but it has a bunch of the input variables um and at least to me the most uh interesting part of this whole exercise um was that it didn't work that well um we had no trouble trading the dtr that can do about as well as all the as each of the input variables but we completely failed to make one that did significantly better than any individual variable and we're pretty confident that this is just because of like the properties of how tree space works um so tree space like the space of all "
    },
    {
        "start": 2750.319,
        "text": "possible trees is like another one of these like spaces i like talking about that like um i can't really imagine using it but it's um it's very complicated and high dimensional um and the problem is that like you know you can trade a machine learning approach easily that so that if it knows exactly where it is in tree space it'll have extremely good um predictive ability it'll work really well but the thing is if i just hand you a single tree there's no way you can fur infer accurately enough the parameters at that tree to tell the dtr exactly where it is in tree space um so one example of that so one parameter that's really important to this is the selection strength and there's a lot of different ways to parameterize it but this is basically quantifying uh the relationship between if you have really high affinity how strongly is that linked to the number of children you're likely to have and this is really important for the performance of these metrics uh because there's there's some metrics that if the selection strength is high they work really well but if the select selection strength is zero if it's you're just a new mostly in "
    },
    {
        "start": 2811.68,
        "text": "neutral evolution that metric can be totally useless so in order for the dtr to function well it has to know it really has to know what selection strength is and there's lots of really fancy phylogenetic ways to calculate the selection strength like the phew h here is one of them um but none of them are anywhere near accurate enough to be able to calculate on a single tree where you are in the parameter space which means that the dgr has no idea where it is so anyway if you go through all of the simulation validation on this and kind of check what different metrics work happily enough it turns out that the almost basically the simplest method you could think of works better than all the others so this is the the consensus sequence so the for each family it turns out that the sequences that are closest to that family's consensus sequence are most likely the highest affinity and in retrospect this is not particularly surprising i think like the picture i have in my head is that the consensus sequence is each family's best guess for like the best ant body so far this is the "
    },
    {
        "start": 2872.559,
        "text": "best this is the best antibody we've found so far and then the family exists as this cloud around it um exploring the nearby antibody space trying to see if there's a better sequence nearby it and then another metric i'll mention just quickly because it's terrible is the number of mutations um so i mentioned this just because in a lot of practical papers it's really common to take a bunch of antibodies and when you want to choose some high affinity ones just choose the ones that have the largest number of mutations and on the face of it of course this makes a lot of sense because i just told you that affinity maturation is this mutation process that on average you know applies a bunch of mutations and to improve the affinity antibody the problem is that that statement is different to saying that within any group of antibodies the ones with the most mutations will have the highest affinity because the ones that have the most mutations those are by definition leaf sequences so by definition they have a bunch of novel mutations that have not yet been subject to significant "
    },
    {
        "start": 2933.04,
        "text": "selective pressure and most in any context most novel mutations are deleterious so if you want to find the highest affinity or the fittest high efficiency antibodies or the fittest bit of the tree it's true you don't want to be at the root of the tree but you also need to go back some ways from the leaves and you know not coincidentally that's basically what the consensus sequence is doing it's some perhaps overly heuristic non-optimal method of finding some middle ground some compromise between the root and the leaves so when we want to if we want to test this test the this antibody choosing methods on real data we run into the usual problems with validation on real data and that the sample sizes are really slow you don't have a lot of data sets any other sample sizes are slow for the same reason that we're really excited about having this method that is just really expensive and time-consuming to test affinity so there's a million uh public data sets out there that test affinity or neutralization on a handful antibodies "
    },
    {
        "start": 2993.52,
        "text": "these are just the two that i have in the paper um these are two well-known hiv vnab data sets one of these i think has like seven antibodies and the other one has like 15 or something like that and the way i'm quantifying performance in these plots is that we're kind of saying like if i choose the best like 15 of antibodies based on my metric on the x-axis here like what's the percentile in terms of affinities of those antibodies so like if we choose the best in terms of a metric are those also the best in terms of infinity which is what you actually want so the green lines here is showing that you know if we were perfect this is saying that if we choose the top one percent using a perfect method then we of course also get the top one percent in terms of infinity and the red dashed line here is random chance because you just get uh if you're choosing with blindfolded darts then you just get 50 percent all the time and just kind of looking at both of these these are very small sample sizes but you can see that like our blue line here is you know it's towards the it's doing better than random chance here and here it gets lucky and is actually "
    },
    {
        "start": 3054.559,
        "text": "just doing optimally and this is it's doing fine and as importantly this is entirely compatible with the expected performance that we see at simulation so this is a more recent thing i don't have any like final results to show you but we're doing so we've been applied the paired clip we've been applying the paired clustering algorithms and the antibody choosing things with some really exciting data from wesley google's lab um he's working on dengue also with the hutch so we don't have those yet but those will hopefully be kind of things that we're showing soon so in summary these b-cell receptor deep sequencing data sets especially with when combined with some wet lab techniques to like measure and find some good antibodies they can tell you a lot of really important things that are really a biological interest but you do have to be careful with certain steps in the inference process to really get actionable methods out of them uh so like first of course i'd like to thank eric he's been amazing mentor for the last few years and omri and i mentioned he developed "
    },
    {
        "start": 3115.119,
        "text": "he's one of our students and he wrote the linear package that has a really nice probabilistic beijing inference eli was a programmer in our group and you wrote this amazing visualization package called olmstead there's some links in the backup slides i'd be happy to talk about it or click through an example install it's this really amazing visualization thing that we use a lot when we're passing data back and forth with web lab groups because it lets you click around on a web server and zoom in and out on trees and label the trees by different things while you're looking at the wcr annotations and all the hiv data i was showing you is from julie's lab so working with data that cassie and laura and mackenzie have made the last few years really been a great pleasure um and as i was just mentioning the the paired clustering methods and the antibody choosing is something that we've been working on with leslie goo's lab also at the hutch so it's been really great working with data that lisa and jf made thank you "
    },
    {
        "start": 3181.44,
        "text": "stephen yep thank you very much it's a great presentation does anybody have any questions yeah hello duncan this is indica thank you so much duncan yeah thanks for listening yeah and and uh so duncan um during affinity maturation process is there like a common back moment regardless of the antigen certain things has to satisfy to become a plasma b cell uh is there a common what sorry like a common common i will use the word so i'm talking mainly in the context of transcription factors yeah so are there uh uh uh are there any common backbone regardless of the antigen that to satisfy to become a plasma bisa so is that the somatic hyper mutations are some sort of a refinement to yeah so um i'm not sure "
    },
    {
        "start": 3242.079,
        "text": "i let me know if i'm not answering the correct question but i think i think what i'm understanding is right past it um yeah so where's my framework picture there so like yeah when you're having mutations um in the b cell well yeah lots of naive rearrangements so lots of bdj rearrangements um yeah i think maybe like 80 of them result in just failures so like b cells that don't express like maybe they have a stop codon they're out of frame or just they don't fold correctly um and also when you have mutations you know if the mutations especially mutations their framework region those also result in like ineffective antibodies is that your question uh no i'm sorry uh no no yes but but i am thinking in the context of a naiub cell become a plasma b cell and during that process this is like a differentiation but no normal differentiation there is no somatic hypermutations right so in this b cell affinity maturation you need to change the sequence the small "
    },
    {
        "start": 3303.119,
        "text": "part yeah so but my question is the normal differentiation that certain things has to satisfy um is there common is there people know anything about that is there a common so cell has to divide right and and certain like a common backbone to affinity maturation um i i think the fact that i'm confused means that the answer the short answer is definitely that i don't know okay um the more speculative answer is that um i think we don't know a ton about exactly how infinity maturation in the gc and class switch recombination work um and going to like the g like people i mean there's a few things that people agree on between plasma cells and memory cells okay like plasma cells tend to be higher affinity memory cells tend to be more diverse but beyond that i think people are all just kind of like arguing about different stuff okay okay so okay duncan by the way i was a "
    },
    {
        "start": 3364.96,
        "text": "postdoc with mark gruden oh cool long time ago and type in with a question here uh really nice talk um uh i'm curious you mentioned you know sort of one of the things that pollutes you know your data or makes the the phylogeny reconstruction difficult is that you know like lots of these uh you know mutated sequences might be non-functional um i suppose some are obviously non-functional by virtue of you know having stop quote on you know nonsense mutation or something like that but i'm wondering if um you know deep mutational scans or tight seek or any of these kind of very high throughput experimental approaches which can measure affinity uh can those either kind of guide the "
    },
    {
        "start": 3427.839,
        "text": "inference process or do you see can you look at you know over the course of affinity maturation uh can you see like experimentally measured infinity going up kind of along the way at internal nodes yeah you definitely can uh i guess the first caveat is that i'm very fuzzy on the um the big picture of exactly what experimental methods would be good for previous things but i can't say that like yes that is the kind of thing people are working on the two projects that i think of immediately are you know gabriel victoria's group yeah yeah they do a lot of really amazing stuff from gcs so we're uh so will and eric especially have been working with them so that is that makes me think a lot of that a lot of their work in that those are like you know small families small sample size because of course they're having to do all the work by hand but that is where they're looking at each individual sequence and seeing how particular mutations change infinity um and the other project that that sounds like again i think i'm not 100 percent understand the question but probably we've been doing a lot of are "
    },
    {
        "start": 3488.72,
        "text": "you familiar with like fip seek and verascan yeah yeah yeah so we've been doing not me personally but other people in our lab with eric so um jared and kevin have been working a lot on fip seek data from stars especially stars kobe 2 with julie's lab with some people from julia's lab and i especially i think megan especially had the paper the paper that was actually out uh i could definitely point you to some papers on that where they are um that is much more high throughput um functional validation okay yeah great i mean um yeah no that's that's that's great i'll check some of those out for sure thanks again i have a uh a speculative question um in one of your slides you had talked about uh gently nudging um basically bcrs towards specific antibody targets yeah "
    },
    {
        "start": 3550.079,
        "text": "um in a series of steps and i'm wondering uh just generally if you believe it's necessary to gently nudge or if it's you might be able to push the vcrs towards that final state more efficiently or more aggressively what are your thoughts around i mean i am very far from an expert on this kind of sequential immunization scheme for hiv uh but i mean i was just googling trying to figure out which paper i was thinking of because i thought it was a recent paper that was like oh i thought i read this year that oh wow someone actually managed to do this in mice and then i started googling it yesterday and i realized that you know they did it like seven years ago for the first time so um but that said my understanding is that god i lost it um my understanding is that i think it's basically just a combinatorics thing um like you can't there's a lot of things in a person's um in your immune in like in in your genetics that would determine like what v genes you have and where the mutations "
    },
    {
        "start": 3610.88,
        "text": "are likely to be but to some extent is just kind of like you can't control it's just this random process that decides exactly where the mutations go and exactly what b genes you start with and if you have a process if getting from here to here requires 10 mutations like just combinatorically that's some number to the 10th power which is just a really really big number to have in your denominator whereas if you have breaking into a bunch of steps that's just a smaller number of the denominator so it makes it much more likely yeah totally fair thanks i actually had a quick question about um the antibody dependent cellular text cytotoxicity um and why um or the the data you were presenting that they were more likely to have mutations in the framework regions i was curious if those mutations in the framework regions could be used or are used for clustering clonal lineages and for like lineage reconstruction because i know in the past you you in previous slides you mentioned you "
    },
    {
        "start": 3671.68,
        "text": "know you're basically focusing on the cdr3 region um for that process and i was curious if the framework regions come into that at all um yeah so i mean in all the clustering and inference uh we use all the mutations and we don't um we don't even really distinguish between a cdr and framework in terms of there i mean all the models um have like perposition rates for the mutations so they know that like they would implicitly know that the cdr is more mutable than the framework region but there's actually a really good point that in terms of uh there probably is some signal there like semester a signal we could scrape out to like get some better performance by like taking into an account like this is the cdr and this is the framework so we know that fundamentally these mutations are not just i guess um not just that the mutations are less likely in the framework but that a mutation of the framework is probably functionally doing something different than one in the cdr okay thank you uh i also have a question uh about a hypermutation so are we assuming that the "
    },
    {
        "start": 3733.28,
        "text": "hypermutation always in the same rate uh in the whole process or that could be changed like yeah no it's variable rate it definitely changes a lot um and i guess i might be understanding but let me know if i'm not understanding right but like so one way in which so one picture of the way shm is happening is that so i said you're not ub cell you're exposed to your cognitive energy you go to a gc and then you go through a bunch of cycles between the light and dark zones so that's maybe like weeks or months your your mutation rate of your offspring is like you know maybe it's it's not constant but maybe it's kind of constant while you're in that gc reaction but then the problem is that like that gc is going to go away after some amount of time and then maybe you'll float around in the blood for a long time or your offspring wells of memory cell and then maybe you re-expose the same pathogen and you go to a gc again and start it all up again so this is potentially very stochastic process where your mutation rate is really high for a while and it goes down for a while "
    },
    {
        "start": 3794.319,
        "text": "and this is actually there's been some work on this um so in ebola you can have um latent infections where the mutation will totally stop for a long time and some trevor bedford's plots i've seen where they have like they have to take this into account we have really dramatically varying mutation rates so you get end up with seriously really long branches on the tree okay so uh so is it possible that in the like in the same in the same time constant and and when i like when the b cells divide they could like use different mutation rates right i mean yeah i guess it's kind of like the i guess it's i'm just a little uh just thinking on the fly like i'm used to thinking in terms of like like so a mutation happening on the micro level is this very stochastic process that happens individually like you know um an individual b cell is like "
    },
    {
        "start": 3855.839,
        "text": "it's in the dark it's in the light zone um and then it's given permission to go to the dark zone and it's kind of going back and forth there and one of those maybe mutation happens out of the generation and then so like thinking about like the parameter in a phylogenetic model where you have the mutation rate it's kind of averaging over a bunch of cycles and that well i guess the short answer is i haven't thought about that a lot like how to parameterize that but other people have certainly have i see i say but but in in your like uh hadamard model that you assume that the the rate actually is like 10 or 25 percent i guess oh no it's it's it's variable um but it is so in my hmm and the one and the html is geared towards clustering um it's even simpler for most purposes it's just assuming it's a star tree um just because that's what we have to do to make it fast enough to be able to run on the whole repertoire um and then we have to do more detailed phylogenetic things you're using something like linear ham that is much more accurate that you could never run on 100 000 sequences okay i see thanks "
    },
    {
        "start": 3939.359,
        "text": "okay if there are no more questions i think that might be the end of the seminar uh thank you again dr ralph for presenting today it was a great talk uh yeah thanks for having me that was great fun thanks "
    }
]