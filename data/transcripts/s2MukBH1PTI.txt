student now working in dr ramo's lab so today i'm going to talk about the algorithm our lab have been working on in the past year which is called this water yeah which is cause water it is a randomized approach used to solve a complex structural appearance so in this presentation i'm gonna talk about the following questions first we're gonna define the structure variance and complex structure variance and talk about why we're like spending all this time studying them and second we're gonna talk about the method we we developed like how does swelter like try to identify and also resolve sv and csb and then we're going to talk about the current result we have and what more we expect from the method uh first let's define structural variance like by definition structure variants are um the rearrangement of dna sequences that are usually larger than like 50 base if it's smaller we usually call them sniffer in those um so they're usually caused by a deletion duplication version insertion or translocation for each of them i have a like example in the next slide structural variance is sort of very important because the first of the they play roles in a lot of cancers as well as other uh very severe syndromes like this probably syndrome which there is a very long deletion on chromosome 15 which causes um something i don't know how to describe sorry so also like smaller like structural variants that happen on smaller region are also very important because they serve as a a way for a they play roles in evolution in population differentiation and also the first disease phenotypes so here is uh the five basic type of structural variance where the gray bar stands for example two while the black bars stands for sample one so compared to sample two for example here there is a deletion so there should be a dna material here on sample two but in example one this part got deleted or something like for like duplication there is one red bar here on example two well example one this part got duplicated and inserted somewhere else maybe close to the original copy but it may also be like very far in some other chromosomes maybe so like this is definition sort of relative like based on one sample compared to the other but like in reality we really do need some uniform uniform format of sample two which we use uh the human reference genome so when we define the structural variance we just we were actually we were talking about is the genome structure of the sample we have compared to the human reference hg19 to be specific um so as sv is important and interesting so there's a lot of method already there to study structure variance like like uh we got dali we got to make meerkat here which works pretty well and also we got brick dancers and we need her all this master has been published before they work pretty well i think in simple structural variants like one step deletion or one stabbing version since like this that in reality the situation is really not this simple for example here are two kids this is a the genome the genome structure from a germline genome in once in one person so we can say there is like a mosi eat structure between chromosome x and chromosome five so let's see there are none none 10 11 11 different regions from the two chromosomes that are like patched together and inserted here and also there's a very long material from chromosome x that got deleted from x and then insert it on chromosome five and here again like part of materials from five are in crimson x although this is more complex in second person where three different structs three different chromosomes are involved and it's too messy for me to describe okay so like this is for germline you know but one week it comes to the cancer genome and it's just more complex so here is a kodoma sample uh where we can see like five different chromosome chromosomes are involved here and the purple ones stand for the dna materials that are interchanged between the two chromosomes on both end of the purple line and also there's like yellow yellow line stands for the inversion uh the like the green ones then oh yeah there's also how to head and to tell various types of structural variants here and they are all messed up together so before we really talk about like how to solve this sort of situation let's first define that um so we didn't really that's sort of our next step to check for this but as far as we can see of course like the centromere and telomere regions are really messy so we we're not able to study for this but for the other regions i think they're handable for us so complex structural variants are defined as complex rearrangement on the chromosome that are resulting from either multi-step or overlapping events that involves alice's ray breakpoint so they are important as the plurals in a lot of tumors and congenital or developmental defects so uh i'm gonna give examples of how complex structural variants look like so in this case it just happened on one chromosome so there's two regions on this chromosome the purple one and also the right one so here what happened is like the purple one got duplicated and one of the duplication got inserted and the the right one just got deleted from the the part and all those others are like duplicated normally so this so this is like the the structure of the two homozygous chromosomes in one person where one chromosome look like this while the other look normal by normal we mean it's the same with reference channel here is a example of the overlapping uh complex structural variance where we have very different structural variants happened on both chromosomes that they're they have on the same region on a pair of homozygous chromosomes but the the arrangement there is different like for one chromosome there is a very long dilution well for the second there is a there is a smaller dilution with a smaller um inversion behind it so both of those are like real case in our study i'm gonna show like the the real kids later so for to solve this like situations like this this current approaches we have that have been published have their limitations because most of this method like when they start to solve this sort of problems they just they they already set some expected patterns of particular variance tabs and they're trying to search for this variance this very abnormal uh reads that could match their expectation but the problem here is so there's various different types of the variance we can have in the genome and it's really hard for one person to imagine all the possible situations right so that's why the current approaches have some problems solving the complex examples i've given before let's see this is a real case on chromosome 2 uh which is sort of popular in population uh this is how we we've been doing here um so this this is like what should have happened as we talked before this purple board got duplicated and inverted while this got deleted well for the current populations we have we have there is two studies that have detected there is an inversion on this on this region but they didn't detect there is like a duplication insertion somewhere else and none of these studies have detected there's another division very close to this part so here comes the voucher which is the method we've been working on so as the input data the router would take per and the next generation next generation sequencing data uh so with the initial band file we would first build the uh the node model based on the repair insurance the redapps and also the split read also we would consider the direction of a pair of rays which i'm going to explain more later so the output of the router which is i think is pretty cool is we will output the resolve the whole genome structure with so as the best structure we we output we define them as the one with minimized average remappings so here's a very brave introduction about next generation sequencing so when we do the experiment we share the whole genome into pieces that are within certain lengths and then we sequence the both end both end of the sequence the the dna pieces we have and here they could probably produce a fastq file for us and up with this file what we do is we map each pair to the reference genome so theoretically if there's like no structure variance on the sample we have they should map back very beautifully like this right like there is i'm going to talk about it here so for all the read their insert lens they should leaving some distribution as how the techniques set it we fit the distribution here as a mixed gaussian distribution and also for like each pair we would expect their direction to be forward and reversed because when we sequence uh when we do the sequencing we we just sequence the both end in the same direction right so there are three different situations like this they are all wrong structures and they are telling us there is probably some structural errors that happened where this reads mapped um also we would try to model the redax based on the negative binomial distribution because when we do the sequencing we we just randomly choose like a sequence from the sample to get sequence so theoretically they should follow like a poisson or an active binomial distribution but we found that this model could also be fit very well by normal distribution so like the euler's i mean someday when the method is publicly available the users could choose either negative binomial or normal distribution to fit their redapps model um so here it comes how this water really sell complex structural variants here is a very brave workflow of all the all the things and i'm going to talk about details later so what's rather do is we first define breakpoint where we think maybe like the two regions on both sides of breakpoint could be different from the reference genome and after these we define the blocks based on the breakpoint here we have four breakpoint here so we can define three different blocks three different potentially interesting blocks here in between and also we will add two anchor on both sides which is very important here and then we after we define the breakpoint we have this sort of structure we would randomly propose a uh structural variance a symbol structure like deletion or inversion here in this case we we propose a dilution so we would delete all possible blocks like here we delete a we did it then b and then c for this and from the new structure and for each structure we will map all the reads back to the new structure and we will score each structure like this also we will score the original structure so when we get the sort of score for each structure we will transfer them into a probability model and then we will choose a a mod a best structure from this four based on their score but so usually the higher a structure is scored the more possibility it is used for the next step but that's not necessarily all the cases so after this we will uh renew the structure to the picked one and then we start the next round here's a brave workflow just in word i think you know what's interesting right so um first let's talk how did we define the breakpoint so we defined the breakpoint based on the signals from average read pairs so let's first define the average breakpoints but apparent we mean first if if for a pair of read if they're in certain ones it's very abnormal if it is outside our exact distribution either too large on the right tail or too small on the left tail we would mark them as some potential sun and also the direction like in this case for this pair they are forward forward well for this part of their reverse rivers or we could also have like backward and reverse situation which is not good so we also mark these and also clip rates means for a for a read only part of the rate got mapped in this region while the other part got cleaved and it mapped somewhere else we don't know so we also take this as a very strong sign of break point after we define all these apparent reads like this see if there's like a cluster of aberrant read and they're clustered together means they come from the same red pair we could define a pair of breakpoints and the breakpoint has to be in front of the for the right most forward read if the all the reads in the cluster are heading forward and they have they also have to be on the very end of the left most to read in the if if a cluster is all consist of the rebirth am i confusing anyone also if we have a clip read or in that situation if we have several different collaborators that are all clipped in the same position we could have also defined a breakpoint layer after defining the breakpoint our next step is to cluster different breakpoints together because so in the other method it's not capable of running the whole genome like or we are not even capable of running one chromosome at one time because there could be like hundreds or even southern breakpoints on the chromosome and there are so many different possibilities that the the structure is rearranged so if we just run them in one algorithm it'll take forever to run so our task here is we would try to define the possible interesting regions or we can think it as a sv hotpot where we have a cluster of breakpoint defined together and we just focus focus on this local region and we solve this local region first before we integrate all the information from other local regions so here uh here is a case like how it looks on actually where you can set it with our whatever color like but here the colorful stuff stands for like clip rays well here the blue stand for the forward and the no this is green sorry grand stands for the forward read while the blue stands for the reverse read does anyone have problems with the igb okay cool oh also here it it is telling you the coverage so next step is to link different break points [Music] yes this is one of the case yes this is the chromosome 2 case which is sort of very popular in population and we can out there they just see maybe a lot of people here yes so this is a very interesting case we have four breakpoints linked together exactly in this format we're going to talk more about this case later um the next step we are going to link different breakpoints to form a cluster by linking we have a lot of different criteria like for this if two pairs are overlapping by overlapping women so for example these two breakpoints are defined by a pair a cluster of pairs so well these two are are defined like this well the region are overlapping so we would cluster them four together and we think there could be something happened with those four all involved and also like situation like this is also defined as overlapping or like if two break point defend are too close to each other maybe a smaller one than one columbus we will cluster this to end together with their linked breakpoint as well and there's another situation with which is more interesting complex we call for travelers we're let's see you have a you have a cluster of repairs and one of the read got mapped here well the other the mage of this wreath got mapped somewhere very far like maybe mapped in some other chromosomes or in mitochondria or very far end of the same chromosome in this situation we cluster this this fabric point together and we also would add we drag that breakpoint here and we sort solve this structure together does anyone have any questions so after we define all the potential uh interesting regions with all the breakpoints defund we're gonna be able to start a iteration step which we propose the rearrangement we we virtually rearrange them and map the reads back and we score and we start a new iteration i'm going to talk about this more here so in the very initial step we have a a lot of reads mapped back right among other three we have normal reads like the gray bars and also we have abnormal reads like this and the first what we do is with with a reference sequence and all the upper rates we scored the structure first and we defined the breakpoint as we described before and here we have the three different blocks together with the anchors and let's see we random propose our first rearrangement which is a deletion so we will delete delete block a first and then we map all the three back so this read could actually look like this for the reverse reverse blue pairs they didn't none parameters in them are changed well for this growing ones like their insert lens is much shorter because a lot because of the loss of a so if we delete b again this insert of the green pairs would be shorter they will be closer to each other but for the other pair there we got a problem here right so like they're they're made so this radar maps are perfectly fine that's good but for this they're mapped nowhere which indicating we're not doing we're not doing this right which tells us this is not a gift structure so we probably give a very bad score for this structure and also we could delete the c and in this case we got four different reads that got not mapped anywhere then we also get a very bad score after we got the score we will transfer them to a probability uh to a probability probability model which so we would first normalize the score into some certain range and then we will add the other four possibility to one and we will randomly choose a probability between one and two and whichever block defined by the scores our chosen point fall into we will take the structure as the right when we decide in this step and we will renew our structure like this so here we we just we choose the best score structure and we renew the structure we have and then we will start the next iteration then the next step again we first uh we will score the structure based on their current mapping situation of other reads and let's see we random proposed the second move called insertion so insertion is a little bit different from the deletion and inversion because let's see we could we could insert any possible blocks that are here and also we could insert some for travelers that are linked from our former staff if you remember right so maybe d is defined by um the breakpoint that are here while they're they're made a link somewhere else let's see here and next step will choose a certain block here to be inserted so here we can see let's see how c could be inserted if we insert c here on the first breakpoint we have other read will be remapped like this so here for this read it won't be changing also this so it looks nothing have changed expect for that the insert lens for the grain pairs are just longer but it'll produce definitely a better a bad score and also the situation here is so we can for this pair we can map them onto the c block here right but we can also map them to the block see here they're the same spot so in this situation our principle is we choose the the one that with the best stat probability so based on the insert lens model we built before uh this could be like maybe too long for us so it'll produce a very bad score for this structure but if we map the pair here like this the insert lines could make more sense for us and this could end in a much better struct score so we just kept this as a mapping situation we decided all this in our algorithm and also we could insert see here or here but they're all the same right like they would all end it up in the structure so we just do it once and we map all the three back again we have two situations to to map them and we will choose a better one and after we got the scores again we do the probability thing and we choose the best structure here i have to mention this in the part is we decided the best structure based on their score but it's not necessary that the best score structure is always going to be chosen because if we usually just choose the best fastest structure we will probably be trapped in the local minimum this way it gets us a very slight chance to like jump out of the local minimum and try to find the global minimum so then we renew the structure we add feedback here and we start a new run let's see we want to do an inversion here so for inversion we just could invert the first block the second block or the third block and we will check one by one how their reader map we can see here this is obviously a much better case because let's see the directions are all correct well for this the directions are wrong or wrong or wrong so maybe this structure shouldn't be scored very good but we have other parameters like maybe in the structure their coverage are becoming very bad or their maybe insurance become very bad here anyway this is the score we got and then we choose a score here based on the score here this is the situation where not the the best structure structure is not chosen well the maybe the words god chose here and then we we renew the structure and we remap all this and we start a new iteration again again so these are three uh basic structural variance situation so we have two more we have duplication and the translocation but we found that duplication translocation could be concluded by the three because think about it if it's a duplication we can replace it as an inversion right i'm not going to go back it's too long so if it's a duplication maybe if we want to duplicate b we can just insert to be back somewhere and this is like a duplication and also translocation we can delete first and then insert back somewhere else so i guess we all have the question is like how do we come from the mapping of all the three to the score of each structure here so our our score system actually are composed of four different facts factors the first is the distribution of insulins so we will calculate the posterior base probability of each pair based on this distribution for example let's compare between these two structure here we have in we have the insurance of this pair well here we have insurance two of this pair it turned out that insurance one located here is larger than like 600 base and the probability of of the observing such a insert lens is very low in this distribution or if we go to insurance 2 the probability is much better which costs a better insurance score for this structure compared to this one also like for the direction let's compare this to situation with c inverted or not inverted so in this situation we we have four apparent repairs that are forward forward and reverse reverse thus we have four pairs with our wrong direction so the penalty for direction here is four or the penalty if we invert c we will have a zero penalty for the direction so the in the decrease in the penalty score would also cause a much better score for the structure again we also consider the coverage and the number of repairs going through each breakpoint let's see here it seems that it got delayed because it gets a much lower coverage well c got duplicated because it gets a hair hair coverage okay here's the systematically format how we are scoring the structure we got insurance direction redux and the number of rate going through a breakpoint both the direction and the breakpoint these two are served as a actual loading an actual loading factor for the incidence and redapps so we we first calculate the posterior base probability of the insulins based on the mixed mixed gaussian distribution and we take the log of it and then for the directions penalty we will calculate the percentage of reads that got half abnormal directions and we time the log pl with one plus the percentage here and this firm our repair score and also here we calculate again the posterior base probability of the redups based on the negative binomial distribution or normal whatever you like and we take the log of the probability and we add this as as an actual loading on the score and we form redux scoring and then we will add them up with by giving them different weight and we from the final structure score which is what we show sorry it's too long back this is anyway this is just the final score we're gonna get for each structure so so far it's i i sort of have been done talking about the whole workflow we've been working on but we're sort of naive because we need to respect the fact that human got diploid genome so by only solving like one chromosome it's not enough we have to consider the pair of homozygous chromosomes together when we when we do the scoring because so for each pair each red pair we got they could be either mapped on the chromosome you get from your mummy or they could also be mapped to the chromosome you get from your daddy right so when we are manually mapping them back we have several principles we have to follow first if the two structures are the same of course we will try to distribute the repairs as evenly as we can because like in the real technique you can pick them from anyone you like uh but if there's the two the structure are different on the pair of homozygous chromosomes we are going to make some choices for example if we have a pair of reeds that are like this they are mapped not very good on this allele but if we map them to the other allele they get a better score so in this situation we will choose this pair to be mapped on this structure based on this precondition then we could do some modification to maybe take some read from here map back to this structure to make the coverage look better so in this way we are going to try to find the best score for each structure so here's how we how we actually do this so for a pair of homologous chromosomes we first propose a random a random move on one allele and we do the we we try the possibility then we choose the best best structure and then we would propose a second move on the other allele that has not been changed in the form of step and then we change the structure and we renew the structure on both chromosomes and we start the next rod so here's our funnel so usually the problem with the method we are now currently have is so after a lot of iterations our rhetoric just doesn't converge to the best score so currently what we do is we would run the first 100 thousand iterations and we will pick the structure with the best score so like in each iteration you pick a structure the structure may be not the structure may not be the one that are scored best among its own pool but when we take it out we will record a score and after a a hundred thousand iteration run we will pick the best cigar structure among this pool but there's a situation if there is one structure that are scored at the best for continuously for over 500 iterations we could consider it as falling in the global minimum and we just stop the iterations and output this structure as the final one like for example because our algorithm did not only aim to solve the complex structural variance we would also like to solve the very simple structures like a simple deletion inversion in this whole over in this whole like workflow for example we have just a block b that's interested in our local region and in our first step we just need to be heterozygous maybe and that arrives the red structure and that kept kept being the right structure for over 500 times so in this situation it is meaningless for us to run like a hundred thousand iterations we can just keep the result we have and output it as a funnel structure we want so here i'm going to show some primary results from swalter we currently have this is the real case in chromosome 2 which we showed before and this is a pedigree grandparents parents and kids so here in this sample they have a heterozygous structural variance on one chromosome while this in this sample they have a homozygous uh they have a homolyte this person have a homologous structural variance while their parents both have different sort of structural virus except for this one it has normal um both alleles and when we look at their kids this is the result we got so several kids are homozygous variants while some are heterozygous and there is no normal kids here and this the pedigree follows the mendelian principles even though we haven't tried to validate all the structures in the real lab um also we have we tried the bigger challenge we have here is the overlapping events we have on chromosome 3 uh so this is an overlap like with a long deletion with a smaller division inversion on the other chromosome and here's a result we have so for this sample the long deletion happened on both alleles well for example the the overlapping event happened together with this and in this family there all normal they're structural variants their structure are like what's in the reference journal and we can see in their kids there are several other kids that have one normal chromosome well for the other some get the bigger dilution or some got the more complex one and also this pedigree follows the mendelian principles which we think is red current and for this special case we also went back to look at the pack about data with longer read and that data also support our structure at least in the sample and then when do we selling it well that's all it and i would like to thank my boss and all the people in my lab um oh okay so for this case as a 10 000 duration usually takes like half a day to a day yes to solve one local local region but like if you have enough course you can run all these regions parallely and so i mean for the iteration step the whole chromosome are expected to get around maybe in one day if you have enough core you and also we're working on this to make it faster [Music] and the break point the searching staff it takes it's really fast the break point certain step takes like one or two hours so in and there is another step like when we we got all the reading we try to fit a new model based on the distribution of the insurance and redacts that step takes like half a day because it has to run the whole bam file to get all the parameters like like with your what would you really like use jtk for so in total this method currently take two or three days so for the no model settings step it is low because the band file is large the bamboo is like a hundred gigabyte and you have to go through uh each read inside and you have to see whether the insert lens is normal is within your expectation and you have to calculate whether the coverage is within your expectation right so it takes time to to go through the whole bam file and also like you have to know the higher coverage we have the better result we could probably get but the larger the band folder is going to be so the longer it takes for the iteration step so i think it takes long because in each step you know we have to remap all the reads back and it's not just the abnormal reads think about it if you change a structure there are some reads that are originally like normal mapped right they could be abnormal so if there is like in this case so if there is a read maybe mapped mapped between one one pair uh one read of the pair mapped here or the other mapped here well you insert a d here and the insert lens will get longer and it'll become abnormal and it will reduce the score right so you have to consider all the situations in one step it takes some time is just the anchors on the left and the the sea dragon is there they are probably inverse yes i know there is definitely some hint from the read we have and also there's a very strong signal from the coverage we don't yeah so that's a very good idea thank you but uh doing that you really cause more trouble than the increase of efficiency because like if you want to do a inversion here on c like also you have decided like you whether you invert it or you copy and invert it later and you have to decide like where you want to insert it so like by doing the randomized master you will reach the structure anyway yes yeah i can so um also we didn't take the hint from the coverage anyway for this randomized approach it is just totally randomized we don't take any assumptions or anything like that we didn't get the best structure yeah that's another problem we have so in most cases so we when we try to cluster these uh breakpoints together we we try to control the number of breakpoints clustered in one in one group so usually if the number of breakpoint is less than eight we can definitely find the best structure in a hundred hundred thousand iteration which tried that many times and we think that's good enough for us so the the key point here is just we we control the number of breakpoints in a cluster but if there's some situation like the clock the the region is really too messy you have like 10 or 20 different break points here we will maybe make set a larger number of iterations there to run so this would not necessarily be a great tool for mapping something like the product really angelman region which is incredibly complex even in the the reference sequence there's a lot of um nested and overlapping duplications and rearrangements within that within that region even even in the reference sequence um i do think it works well so like if you have a very messy region but not all i mean not all the blocks in the region are necessarily linked together so maybe there are some parts like some local part that all these regions are like there all this uh rearrangement that happened within this block then you can solve this block first and then expand it to a larger region like you can just solve this local region and fix it as the best structure you think is there and then you continue to add a different additional breakpoint and try to solve it in the larger scale seeing is map isn't correct however this is expandable to larger insert libraries such as mainpair jumping libraries there are 3kv which do allow for amount of coverage to resolve some potential okay uh yeah so um so far we've we've just tested the performance of our breakpoint color which is far back so of all these breakpoints we could define within our method we could detect like compared to the original of the already defined simple deletions we could find like around 70 or 80 of that breakpoint so within the file itself they have like 10 plus positive so in total we may be able to get like 80 or 90 accuracy in defining the breakpoint well for the later step like for the iteration step car so far we've tried this on the i shouldn't have done this we will try the iterations on the families on the pedigree sorry on this pedigree and we have three cases on chromosome two three and seven so this is the case in chromosome two we we haven't validated in the white lab but we scope we think like look at this structure it follows the mendelian principles mandelin rules and also we try to verify this with the packbile data we have and for this case we already look at the packed about data and the pack file data first support all the breakpoint we have and also support this structure so for the next step we will like send the samples to the lab and to validate all this other structures in this assembly is based off of an old standard set that was derived from many algorithms and validation sets so compared to other algorithms on that same set that's actually pretty good it's not like 80 percent of a single algorithm in this aggregation of multiple data sets um but this was like a highly validated 10 to 20 so yeah i i actually go back to check so we got the limited first like if the region's too messy we have too many signals here we're gonna be able to tell us for example if a breakpoint is very close to the centromere or close to the telomere or if there's like a 10m duplication like the satellite region so i mean the read are going to map back very messy so you have maybe a lot of the clip rates the collaborators are everywhere but if we define a breakpoint we we require maybe at least the five wreaths to be clipped at the same position which is impossible for this region so we just missed that this is one reason and the other is we set some arbitrary uh parameters in the algorithm we are looking for breakpoint for example if a breakpoint is defined by a cluster of aberrant reads we require the cluster to be no smaller than consisting of five reads and we actually did these like by setting different parameters like 5 or 10 or 15 or 20 and we try to fit the roc curve and we we just pick the best uh the the best parameter with like the best false positive and false negative but it didn't get it cover all the breakpoints we get also like by defining the breakpoint we didn't consider the coverage so if there is a very strict coverage change there it could also indicate some breakpoint there but because adding the coverage redux calculator is gonna take the two hour run to two days so we just get rid of that and we sacrifice a small a very small uh proportion of the breakpoint we can define so i write all this uh scripts myself i'm trying to maybe this summer anymore