[
    {
        "start": 16.01,
        "text": "through my earpiece again there's a sign-in sheet I will bring it back with me if anyone didn't sign in just over me or do it I'm so sorry so today's a huge fear shop Thank You Martha for the invitation and um as I meant getting older and older so I get more opportunities to speak outside of the boundary of our status and statistic community I really appreciate this opportunity to have this invitation and talking to people you know working quite expensive with the boss that the fishing about now really working on a methodology so I mean we have shared responsibilities and how welcome to analyze data and deal this complex data and design good studies and so on so "
    },
    {
        "start": 79.86,
        "text": "forth but I don't think I have many opportunities to really talk directly with the people in computer computational biology and and health informatics I think we should have more interactions because we share all of the common responsibilities and our interest so today I like to talk something simple I built that I only have 15 minutes so that mean I won't be mais mathematical formulas or arguments and just white 2% the framework or you know technology that has been derived on this framework talk about how we can have a different ways to overcome data sharing errors so this is research founded by those my are one and as nostalgic well so to begin my talk I like to really introduce my lab as we "
    },
    {
        "start": 145.39,
        "text": "sort of pursuing a statistical method and technologies that can help people to an advanced medical and public health research a lot of projects we work at my lab are strongly are very closely related by biomedical research projects um I work a lot with people in the nephrology and also environment Health Sciences and so we have several are one grant founded by an Asian ID became my DHS to pursue those methodology development and how we can do better for our data processing and modeling their analysis so if you're interested you can visit my lab webpage so this whole research interests are even sort of overcoming data sharing very beginning with the panel discussion October 26 "
    },
    {
        "start": 208.23,
        "text": "2016 there was a meeting workshop uh I shall precisely speaking kind of very small panel discussion organized by I think fourteen an IH Institute they really say the the importance of data sharing and this has been a thought one of the top priorities in IHS NIH and other federal funding agencies had invest so much money to general data and they found form in many different studies and a lot a lot of data have been collected in the research studies then they always won't think about how the we can use the data to complete the transition data into knowledge that was a very small panel discussion Erica and I were invited universe Michigan two participants panel "
    },
    {
        "start": 269.07,
        "text": "and that that I can't I knew little bit about their Sharon but no primary in this field and I learned our very strong sense and urgency at that panel discussion NIH and other funding agencies like Baja an answer want to push this very hard the boundary of data sharing so that they can speed up this research discovery and see our cost in general so so that after that panel discussion that I feel that as the tutition what I can do better to facilitate of the process of this data sharing strategy research and and other you know rapid a method that can really overcome this how there is so so that that's a beginning of my research interest in "
    },
    {
        "start": 329.85,
        "text": "this field happen so of course the the idea of data integration or data combine data from different studies is very simple people want to create bigger pay that is richer information so that they can do better research right so if you go to Google you know search the keyword data sharing you know they're you know this number it's just huge right so there are a lot of interest not only in the scientific communities but also e industry right so they want make good use of existing data um you know you made a ways that they can do that relevant database to address some of the commercial needs were scientific hypothesis and so on so there's a tremendous interest outside our not only university but also outside University campus to do data sharing so "
    },
    {
        "start": 393.46,
        "text": "in talking about the scientific community that we know data sharing is essential for better scientific research so essentially a lot of you know bioinformatics study where people want you know to do this sort of data equation for a very simple purpose to increase sample size because you know if you want to find really rent 100 subjects or two when 2,000 subject will not be sufficient you have to you know you know do some you know they're increasing convert to increase sample size so they have better power to detect some signals that otherwise cannot be detected by a single study right so this is a very simple way of the you know our idea of the motivation to increased emphasize and so another thing is that you want the data to be more "
    },
    {
        "start": 454.27,
        "text": "representative to the general population by combine a different data sources together for example I work on asthma study where one study focus on only kids from elementary school neither study focus on kids from middle school but if you want to have a more general conclusion of your research findings that you like to combine the data together to cover a broader range of the general population you know for some kind of you know intervention that you want deliver so so combined data has a scientific goal where you won't combine two that the cover broader range of population so that you conclusion could be more generalizable of course that you know you want to say research cost by avoiding collecting new similar data that being collected in other studies this is a very I want the primer inches "
    },
    {
        "start": 514.8,
        "text": "you know from an H and other from the agencies they say we already found this study collecting this omics data why we need found the same are static by the same I'm ecstatic and you know that's way too collected from others that'll be used by this study you know somebody is combating and so they asked the question why we need to you know are invested a lot of money to collect the same similar data okay I mean there's a general interest from funding agencies for the purpose of studying cost okay and also you know data collection is really a kind of consuming process you if you have this experience you know that you lose you know happy you know recruitment and even the IRB takes so much time that you start you know this you know there's recruitment and all you know data "
    },
    {
        "start": 575.86,
        "text": "pre-processing and then indecent data quality QC procedure it takes so much time to finally reach the analyzable dataset that you can start to find something meaningful so this is really common consuming process if you really storage something from scratch right so sometimes you really want to say hey can I use something that that's being cleaned and processed and so on so forth that I can save a lot of time so that I can speed up my research so there are a lot of you interested in others reverse I think you all have this kind of experience that how important that if you can you know have somebody who can help you to clean that they collect data and clean the area and so on the very role of the our interest in 10p community to have data sharing going up you know on our intent that the data sharing berries are so existed and I in "
    },
    {
        "start": 638.92,
        "text": "general are so on a few too many reasons I mean I work for a universal Michigan and Shanghai Shanghai Jiaotong University rather collided collaborative project where we're trying to terrible looking at how air pollution would affect the post our kidney transplantation graphics Ivalo so this is student project between universe Michigan and Shanghai Jiaotong University that University has seven affiliated hospitals so two years ago we went down there and talking to the pis who are involved being the Shanghai side of this project and it's very interesting story the this five out of the sin of hospitals have their pay that "
    },
    {
        "start": 699.4,
        "text": "managed to buy one data management and company so physically the hospital data are also physically in one complete data storage facility that they'd never share that data even though the data are physically stored in the same computer like this but this five hospital never share the data so we're saying hey we need the you know a patient data who received you know patients who receive transplant and so we can look at hello air pollution Association air pollution and post transplantation graphic survival and was a very interesting discussion about how welcome how we can come up with some strategy to share data one person from one Hospital asks who is going to be the first author of the publication that we are going to have what I never saw this question but where the ask this question who is going to be "
    },
    {
        "start": 760.9,
        "text": "the senior officer the first also have a paper that you're going to use our data to do the analysis I say I cannot promise you to be the first often talk about that but you know there are a lot of reasons beyond this data privacy and confidentiality a lot of time we're talking about the you know the patient's data privacy a conviviality but there are a lot of incentives people don't speak out the the increase is one various due to me reasons as lives and ownership of data is not clear and one time we'll discuss about this so I have no authority to share with you who has the authority nobody knows right so it's not very clean and clear defined so who can push the green pardon to to let the data be shared of this ass though and I already mentioned when you have "
    },
    {
        "start": 824.6,
        "text": "some of the findings who will be having this sort of credential those research without and any more are to cure reasons and something like me you'll notice this is not easy issue to overcome the current platforms of data sharing is really here that under centralized database when you think about this whole to the mandate data sharing a priority button like other funding agencies there always want to build up centralized database so ok you have data come has a day that you has a data just bring them together right don't beautiful eyes database that contains the identify the raw data from data this is actually the kind of thinking of data sharing and so you were to avoid the data privacy and "
    },
    {
        "start": 887.36,
        "text": "conveniently long time before people put release their data to the centralized database they just transformed data for example be one would be very are sort of common use the strategy is to lower down the resolution of data I have a continuous variable but I don't want share by continuous original measurement I just want discretize this you're still getting from the variable but you don't get ultimately measured variable you'll get a discretized version of that right so another thing is the to add a little bit noisy boys cheering a strategy where they say okay I don't want to you see the original variable per measurements I add a little noise on that so the variables amassed by some noise and so on so this is very active "
    },
    {
        "start": 947.629,
        "text": "research field in computer science something called a differential privacy but the router impulse the noise in the part in the proper way so that a particular type of status analysis like mirror version is not going to be affected as I can inject noise to the variables but if you receive the sort of noise data contaminate data you still get the same result as if you use the working there but this noise injection our strategy is analysis if you do a push model you have to do inject the data in such a special way so that you can keep the same set of findings as if you run the origin if I run register equation eyebrow relation you have to do something different so this is really analysis dependent noise injection this is not a universally so we thought the same sort "
    },
    {
        "start": 1009.79,
        "text": "of strategy you have to study all different strategies depends on what analysis you want to do so so that that's that's the that's the you know there are such strategies but all the strategies are viewed about this concept of centralized data sharing so for example I said this is very bad idea of more than this sort of resolution so I just be the simple simulation where I had an innovation model where symphony nailbrush model so x1 could be you are macmorrow could be issuing or methylation normal table whatever you have a confounder I say H so you have some health outcomes so you in irrigation so that you know this tube predictor so Cougars are correlated and then you're interested at the effect "
    },
    {
        "start": 1070.9,
        "text": "size where you make some analysis association between this biomarker and your health outcome so then you suppose you have sample size 200 and you repeat this analysis five sometimes so one one Alice's here is that you discretize this confounding factor because HH is somewhat like a sort of identifiable variable that's a something associated the privacy as a facility so you want to release the origin measurement of age of subject data sharing process that you want to discretize it suppose you will do this economization in protein two to two categories we carry weights motoric so here's the result if you don't do anything using the data is right so you get your right estimation and if you do "
    },
    {
        "start": 1130.9,
        "text": "the model 2 which is the case where you dichotomize you confirmed it into two categories you convert you continues XH variable into a binary a young oh okay so you get estimate that this if you do this three categories their categories so you can see that if you do this transformation of your variables to a lower resolution okay in the day our journey will be bringing a lot of strange things in the data analysis as a result you have over asked them the fat size Association okay this is a serious issue it's not just like you you you know you come you fulfill this application of data sharing in the same time you protect the data privacy but "
    },
    {
        "start": 1193.03,
        "text": "same time you create a misleading status in answer okay this is very serious okay so the massive rise don't actually channel data for the sake of the identification if you don't know possible consequences people do this all the time there's no understanding of the consequence of statistics is like things I cannot give you the organization I just discretize I still gave it right but you can see that for this is very simple nina reverse smaller you just don't get the right solution okay so that's very seriously issue that you have to think about so now here's come to our social solution however them to do this okay so I call this cavity interest a consists of simple are sort of elements first this is the distributed that is inference in a sense that this the test inference is "
    },
    {
        "start": 1254.74,
        "text": "conducted under a collaborative agreement of data analysis plant or common analysis protocol tell you you have 200 hospital or 20 hospitals or two hospitals you plan to share data to do certain data analysis let's see them together not talking about data sharing likes to work out a they analysis protocol what kind of analysis will be planted you okay so how we're going to prepare our data how we're going to choose our model for analysis and then how we're going to pretty result so you have all the elements in the protocol okay take that up and all agree with it okay so that's the first step you do after you do this protocol designed they you say okay I'm a statistician I'm not going to receive all the raw data from this when a hospital what I need is in me just summarize give me me "
    },
    {
        "start": 1319.99,
        "text": "all standard deviation or some information matrix I only carotid okay we are great with each other that weapons running near a bush model just give me the estimate and the L of your regression coefficient I'm sitting here in the universe in Michigan and I want to get you rather the raw data are still stored in your local this okay I only need two summers temples to me then I will ask for the colored the summer statistic I don't need to ask access to raw data at individual site okay that's easier I don't I don't need to do raw data and so this analysis is not based on centralized database I'm not going to approve all the you know local data set into a centralized the database they are still are just stored in the local places I only need you to "
    },
    {
        "start": 1382.15,
        "text": "send me summers according to analysis protocol and so they the now system will be operated separately and remotely in a distributed fashion so the resulting analysis I'm sitting here as I buy job here is you can mean summer statists I request I'm trying to find our optimal way to combine information so that these are analysis it's not going to have any loss of statistic our okay so you suppose you have the centralized data you do the analysis you have a power okay you can repeat that or something like you wanted Lucas that is the power there now I request summary statistic I will retained a sense to the power as the centralized analysis do deal see I "
    },
    {
        "start": 1445.87,
        "text": "don't want access to Raleigh but I can still do this analysis that has the same power as if you do this interest so that's something I want to advertise so it's something can you do this for this architecture so you firstly work on the protocol you have like in this case 8 different researches sighs were you know you have consorting of eight participating sighs each site holds their own data and operating data at locally so what around here is to request some summary statistics meaning as derivation is something like that there I can combine them to general result here and the result would have they same they do that this analysis will have the same status power as if you combine data together to run the centralized the bats thing your platform "
    },
    {
        "start": 1505.87,
        "text": "I want to do so but my first question here is why we need the common analysis critical because this is very very critical right I mean you know so so the first question here is why is a common analysis protocol needed okay here is a simulation using the second matter setup where now X 1/2 mile marker is noise based a now marker it has no it's not the important of know that you like to I mean of course you will do this on the biomarker analysis the olds wrong mean you're fishing and testily of the Association suppose X 1 is a non marker it's not associate Lisa T or why I still have the second sitting now I'm just saying that so if you decide like for the model one where you you say that I'm not going to include confirming factor in the analysis okay so X 2 is my "
    },
    {
        "start": 1567.97,
        "text": "convict manufactured so if I only do just marginal analysis X 1 without using confirming that's the key statistic distribution this is the distributive distribution we loosely tomorrow and this orange one is the one that you decide to draw a marginal okay so so you can't decide to run marginal without adding company but all places you have created okay that's what I'm saying now if you say our different places have their different way of doing this your ex - there your t-distribution is running around where's morally that you can't up split the graph is just the power together the T that's the statistic you have dichotomies ation you you have one place that actually use this compound you factor analysis I mean then this distribution is wandering all different "
    },
    {
        "start": 1628.42,
        "text": "places you don't really get the right the distribution that can really help you summarize information to take advantage of you know data sharing data aggregation your digital is everywhere right so then at the end you you're defining it would be problematic Thanks what I'm saying that you have to upgrade what advances you want to do in and advance so that this distribution can become narrow on our get more and more power because if you increase the sample size is different our treatment of X to you cannot really strength the precision of this distribution in more power because of the distribution like one way all different places right you have to become a motor model is usually combined so you don't really take an advantage of emphasize or increase okay so but you have to agree this so that's basically the problem about make analysis nowadays "
    },
    {
        "start": 1689.23,
        "text": "many people just take a turn her interest out of the published their paper not talking about the recursive publication just talk about just using the you know a popular result you don't know what x2 has been handled they have different structures handled if you only just pour the information from x1 to come back to me analysis maybe you enter this something like that right so in my personal view main analysis been largely abused in the medical application even more seriously abuse Tempe value okay so a lot of discussion about p-value nowadays you know we theme is this a community they wish you now use Kiba you should use p-value we should interpret few very in different ways from suppose the wrong and controversial arguments discussion about the meaning the people "
    },
    {
        "start": 1751.78,
        "text": "mail or boundaries that p-value should be used however I think main analysis is even more serious issue there about I just see the Google search there about over AB 100,000 publications using made on method in the medical journals but if you if your different studies using different way of doing this confounding factor you will energy something or even something finest you have we're not really are interpreting so you have to have a common program in place so if second question here is this more critical question from a statistical point of view what summary statistic we should pass around so I said that summary statistic but the question here is which summer statisti actually carried the critical information okay "
    },
    {
        "start": 1812.14,
        "text": "and passing um should be shared it's this sort of crap inference so I gave you a very simple example to illustrate this is the way that is doable so example the consider very simple we have two independent normal samples this a common parameter that's the mean population me okay average height or something a blood pressure you want estimate and some hydrogenous parents you have two populations your sample are n subjects from one study and you stand for in subjects from the sickness study like seeking hospital so then you believe that they have you are interested in the population average actually which is the name of that right so you want a Slurpee so what do you like to do here is of course you want to use this to dataset together to estimate the mean parameter P okay so first up "
    },
    {
        "start": 1875.35,
        "text": "then play here so now you do centralize the method so okay so hospital one a hospital agreed to share data they put their excel sheet together so that you can have two data combined to create a centralized database and the mean can be easily calculated right so we just add the first data together and measurements together and divide the total sample size very simple and this can be decomposed in such a way that you know py is a sample mean of the first data set and b2 is the same coming of second data set okay but anyway this this the first one is the way that you will calculate you know when you have the centralized database you just add them together divided by total sample size that's first before but what I'm saying here is that the sample mean on this statistic is distributable in the sense that I don't need to raw data if you are new to studying the population average "
    },
    {
        "start": 1937.17,
        "text": "mean estimate if you only pass me the meaning of the first dataset I don't need to draw the other one pass me the meaning of the second data because you calculated the mean exactly as if you have found this using the centralized data right the reason you can do this decomposition what this a distributed calculation is me is that New Year function of your data so if you have a linear operator of your data on your operational data you can always lead them into chunks or segments and processing Sicari in the dispute fashion right though so this is great it tells you that if you want to calculate sample me you don't need really theta from all raw data from the D you know don't call me temple me but the question here is is this um clarity works for logistic regression "
    },
    {
        "start": 1999.36,
        "text": "because in the logistic regression you have a nonlinear regression model regression coefficient is even have closed form expression how do we do this B composition if you run Cox regression your estimator has to be solved by newton raphson iterative algorithm you don't have close man expression of your solution or I can mean of course some expression you can compose it using this you know very elementary mathematics you know but if you have Cox regression you'll have an render text model you have the richest regression how do we do this your solution cannot be even write it out can only be solved numerically using iterative algorithm so one side that is it's great that you see that this beautiful on the other side that we would ask the question okay with this M "
    },
    {
        "start": 2062.55,
        "text": "for me enough for me is us to to solve this problem or is this the way that optimal because I said that I can properly inference will keep the same power as you do the centralized this is my centralized a solution which can be just beautiful if you do this way but my question here is that is this company this way of combination give you optimal solution so you can't get that you know the power so that's the question we need to solve right so so so what is the optimal way to combine this two pieces of sample mean so this back to the Fisher's usual paradigm 1935 Fisher published very interesting paper he called us a feed usual but this never be any course is seen in statistical I don't probably something some of you have had never heard of this okay so "
    },
    {
        "start": 2126.69,
        "text": "I think it's very good way of thinking of course Fisher is father of statistic he did a lot of thinking and very early time of statistic his thinking is not from mathematical technical way he's thinking was very philosophical and think how we are going to best use information how we are going to best extract information from data ok fiducial is one way of thinking okay he said that well we first need to generate knowledge of the new parameter from one data set because I have two data set coming from two hospitals he said okay how I'm going to first jet what what if I generate the knowledge of min parameter from one data set then on attend a update or posterior estimate be using the second data set tell he define the source equation of strategy I use first data set to generate knowledge and then I I it's not a general form first "
    },
    {
        "start": 2190.829,
        "text": "data into the second data set in optimal way so the question here is what kind of knowledge we should generally from the first data set so according to Brandi Ephraim he called this a Holy Grail I can you find out Holly grill okay from there you have so this solution was present many many years later by Efron in the 1993 component out in a study of he saw bootstrap so when you think about bootstrap which that essentially is a method of information aggregation you generally do many many kind of we have this regenerative model you stimulate from this regenerate model some kind of bootstrap samples then your question here is a how they're going to optimally combine the standard four samples "
    },
    {
        "start": 2253.14,
        "text": "together to get your your estimate and from has to face the question have data integration in the booster so he look at that a kind of issue and publish a paper in nineteen its PC hey this holy grill is something called a confidence distribution is a distribution estimators isn't that medical that is needed and used to construct confidence interval any confidence level so what's the knowledge about this parameter from my data if I'm able to construct confidence interval of the parameter of the interest at any confidence level well I should say that I have a full understanding about this parameter so my summary information is sufficient for me to make a first so he called conferencing the book and this is contained all knowledge about it inference so then what is this guy the the Holy Grail that in the meaner "
    },
    {
        "start": 2313.31,
        "text": "situation he Efrain said that is this one is the distribution of your printer this the same coming as the mean parameter and the bear sex okay so this is a distribution that people say oh this is prior but this is different prior right this is a distribution derived from your data you will start with nothing okay and you only observe this data set there you say I'm going to learn my parameter from the first dataset you know and what's the best knowledge of the maximal knowledge you can extract from the first data that concept is this one this distribution this is the distribution you used to construct confidence interval of the parameter name price based on birthday okay so if you technically you can treat "
    },
    {
        "start": 2375.44,
        "text": "this as your prior then you have your second deal I set it right oh you know I have my first date of that create a big prior a prior this holy grow then you can use Bayes formula to calculate a back is estimator okay then I just use the base for meter okay so given that prior food price that's the result okay we can see that fiducial estimator is also a minha combination of this two sample mean the b1 is the sample mean from first dataset a b2 is the sample mean from second dataset that they use different reading this reading is very familiar to you this is the may pass meter weighting the femurs we made our estimate the reading makes sense that way to make sense in the sense that the water is ms y -2 this is actually the "
    },
    {
        "start": 2437.9,
        "text": "variance of sample me okay I mean the receptacle of the sample mean so what what do you say if p1 has the day the first data has very low quality meaning has very large variance long noise you should value it this that sample me in the contribution of the combination very natural right so if I have lot of noise in this data set I shall now use this data set as much the way to control the contribution of that for data set is through the specific of the sample variance of that time for me okay very nice way but this is really a a disputable thing is you only need to use two sample mean to overcome this but the question here is is this optimal apparently that this one has different coefficients in a meaner combination well this combination is optimal because "
    },
    {
        "start": 2500.839,
        "text": "I said that I want keep this tends to get the power okay this comes to the work of Gauss our marker we call in statistical gauss-markov theory okay they still a guarantee that the way you should construct the combine estimate is optimal okay this is math but I like to drop over just really intuitive okay Hamas is supposed to be ready to to very very data-driven okay so I have to sample means I want combined and as a creator combined estimator so I have the run go fishing sequencing - defined by my centralized the method I just showed you and also I have services determined by my 5000 Fisher's reduce you argument don't remember many other coefficients I can use but the question here is which pair of C 1 C 2 that will give me smallest there's normally the highest position or is compiled as much this is "
    },
    {
        "start": 2562.74,
        "text": "a very bad thing I have two pieces information I want to combine this together so which way I'm going to combine so that I can have most precise so you want to find optimal C 1 C 2 so that the combined estimate has the smallest there's ice the precision you need to have this constraint sequence across one because you want the combined estimate to be unbiased you don't want to create advice after you do the combination so this constraint guarantee that the combined estimate is a advice you don't have pies right so that's what it what if you do the simple you know like arrange a sort of optimization the solutions test what is this this is call this is exactly same as you yes but what I'm saying here is that you have the three pieces of masterworks together "
    },
    {
        "start": 2626.03,
        "text": "Fisher's thinking of the usual sequentially how do you are accurate information sequentially there you have everyone's work on how you're going to generate knowledge and you have the gauss-markov to tell you this is optimal okay this must be a good combination with all the good minds in specific mathematical immunity I really enjoyed it as I said oh my god that's something we have to do some further generalization okay so that I want to sort of recap over the features reduce your estimate of population is disputable in Hadoop spark up watch our rock 10 platform basically you don't need access to raw data it's most precise one about all the unity combined estimator I guess approved and the centralized design for me is optimal only on two variants as parameters are same in other "
    },
    {
        "start": 2686.11,
        "text": "words this interest estimate is not optimal if it is obtained under the full access to all raw data so how do you believe that that thing that collected from two hospitals would have the same there's no hydrogen alt it's not possible right so so I think there are Christianity exists in actually data will always have the case that you have his genera these so that that the centralized method as we did that may not be the optimal okay so what I'm saying here is that this cracker influence is winning situation first it doesn't require to access the raw data we just store your data as you like I only need requests summer status you can't be that this is very reasonable request just give me me I don't make your there right they can i I'll do that this way I could give you MMO solution "
    },
    {
        "start": 2749.35,
        "text": "okay that I'm gonna do see anything okay don't think about I'm losing anything there's nothing okay so now that comes to this only one slice I'm talking about tools and technology a clean the thin of this thinner so after Einstein versus very basic thinking from those you know masters then we say how could you price this okay but that's what we did in my lab terrorize in the future paradigm to you post at his model in our model terrorizing your model including like well just a regression in every question and welcome in your negative binomial I know that a lot of people working on this in our bioinformatics there are how do you do qotsa regression how do you do longitudinal or generalized estimating equation and ran effect small or quantile regression structural equation model for mediation "
    },
    {
        "start": 2810.589,
        "text": "analysis so I just show you how this is possible feasible to do me parameter well my lab work out all the solutions for all this important models this is the tools and technology I'm talking about today I don't have time to go over all the details okay but you understand that what I try to achieve and what have you that you if you if you come to this data sharing sort of the issue and like to overcome the barriers you just tell I have a colleague at new M okay you know just require summer stitches remember our data okay just give me so much this is I came to produce the same way so even better power than what you know centralized method can do so this is take-home message to the collaborative inference has no losses that the power or even gains higher power with data "
    },
    {
        "start": 2871.16,
        "text": "huge emails in compares to centralized analysis this no need of sharing Robin okay so that's what we have been working on in my lab so I just want you a quick simulation longitudinal study okay it's very as just want to that we have this toolbox available for longitudinal study so we have a concerning of 200 hospitals each Hospital has 500 patients collected and you can have a centralized analysis basically combine the data from 200 Hospital together and you can run after you have centralized delivers you can always wrong the spend on our said like number makes the fact model generalized estimating equation using the our package sass part of you know mix right so here is the analysis is and then you have the collaborative inference I just talked it "
    },
    {
        "start": 2931.479,
        "text": "out we call this raw method because we're motivated from sale ross idea of how this can be down so we call raw estimator so here we do not require any raw data in the analysis we only request without point estimation and information majors like fisher information as standard output from yours you know our site our individual site analysis then you can get the result when you look at relative efficiency or disorder this this this is the one that you use of centralized and if you use our method that you have a small stand error as expected because our method is more efficient in which the lowers the smallest bears so we can prove that mathematically locally also shows in this very simple longitudinal study our "
    },
    {
        "start": 2991.63,
        "text": "besser can produce new prasad the estimator of this higher precision with not accessing rather okay so there's my concluding remark so elaborate inference has not need to build up centralized database so raw data can be stored in a disputed fashion in local study size in this way verbal will now be changed to low resolution your max of this artificial noise so you don't need to change your stuff you don't need to dichotomous you will age variable you don't need to add noise on some of the other just give me summary statistics right and this is very comfortable to out of the pis who will be probably agreeing merely to you know i'll share some of the analysis are very common by the protocol so as i said that we only "
    },
    {
        "start": 3053.4,
        "text": "require summary statistic and then those no losses that the power and sometimes can have even higher power and centralize the analysis and so this is a new shared platform associated technology um you know for many many imports that is models that we work in the past a few years thank you very much for patient questions comments it's great you know you know way but to know I mean I think it's sort of a huge "
    },
    {
        "start": 3116.279,
        "text": "day to step forward that you only need some reason right and so on and so forth so I guess you know from statistical standpoint how amenable do you think it is do you know what other types of analysis by the network analysis castorini and classification do you think there are a lot of things we can explore we haven't done anything about classification and clustering yet but if you do model-based clustering is basically the logistic regression some extent right but there's just a regression is fine but you may have mixture models like you have this much "
    },
    {
        "start": 3177.599,
        "text": "smaller I don't know like we haven't really look at it but in the field I will come to allow Association Alice's and costly inference that we lose a lot of conditional with wish model and something like that so so we folks um a lot of them the regression but machine learning is not a field I mean the computer scientists process this data differential privacy sort of things adding some more seeing today I don't like that because the noise is additional randomness in addition to sampling randomness so a lot of times people just take this injected noise or uncertainty as there's no uncertainty because if I I do I mean different people may have different ways to add more variable so so when people do this analysis they have to count this "
    },
    {
        "start": 3238.88,
        "text": "uncertain in norse injection or in jittery of strategy in addition to sampling error so they just come up with some of this sort of differential privacy idea and prove this concept in very simple setting but I you know for us we run a lot of more complex situations right so um oh I I really like something that would be analyzed this no noise injection so just keep the origin old measurements find different ways but yes we we should explore more other setting of X immersion very that's not be done yet so I'm from the the library I oversee our research in services first I want to say "
    },
    {
        "start": 3299.819,
        "text": "I really appreciate the thought and attention that you give in to to data sharing and how do you think it happened so I'll ask this question with really little knowledge of your field how much work does it take you to do to get to theta the summary statistics that you generated in addition to so what you would ordinarily do so the higher center your method correctly the different data sites would host the the datasets but then would you contact them they would have they would deliver though they didn't you need it would it keep them you know but they do Network ahead of time anticipating that that you would request or where they had to do that at the time of her class and therefore after figure out right well was a great question I think they we're talking about implementation though so that motivation can implement this right so you can set up a centralized sort of calculator or analyzer people can put their data there this is the password "
    },
    {
        "start": 3361.579,
        "text": "the roundest they steal the result is rather act so so i can spread the computer that's called configuring the system architecture nothing is that the lot of the places they have their own constitution because in the medical studies right you have consorting that consists of many Pacific side they have their own institutions and then we can all agree about you know some analysis they can run out as passed this is to me so there could be a automatic rail or some kind of human evaluate but all can sort of give you implemented and so that's great things that how this can be implemented in different way Romania "
    },
    {
        "start": 3422.67,
        "text": "certainly is very tiny bit more NH money to infrastructure you know people can access and rounders and and and that u s-- our are with them to come by thank you very so this way we can get the result right and fast "
    }
]