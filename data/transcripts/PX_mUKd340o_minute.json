[
    {
        "start": 0.799,
        "text": "um and thanks Marcy for that introduction thank you for inviting me here today and thank you all for coming to this tools and Tech seminar I've been excited for several months now to present a couple tools I've been working on um in Lydia folino's lab uh I spend most of my time really thinking about how we get the most out of our sequencing data and how to help biologists get the most out of their sequencing data and designing tools that will be really helpful and intuitive to biologists to use and uh to interpret the results that we get out of them and I think we're getting close with these two tools and rator and shap me um so what these tools really aim to help biologists do is to understand protein DNA interactions at a at a couple different levels but I want to point out how important protein DNA interactions are to our understanding of a wide wide array of biological processes and phenomena so here I've just got hey got a this toy example where we've got some "
    },
    {
        "start": 63.0,
        "text": "unnamed pink protein floating around in space and genomic DNA here and this protein has a specific binding site indicated in red um and whether this protein binds that site or not will have an impact on whatever biological process the protein's involved in so let's us assume it's involved in regulating gene expression it could promote gene expression by recruiting RNA polymerase it could repress gene expression by blocking P recruitment or blocking its progression along the DNA if it's involved in any number of DNA metabolic pathways such as DNA replication it could promote or inhibit DNA replication it could engage in DNA repair the point is all these processes you see here and many more that you could think up and imagine uh are underpinned by protein DNA interaction and these protein gen interactions will in large part Define the behavior of many of these biological processes so throughout biology protein gen reactions are really essential for "
    },
    {
        "start": 123.84,
        "text": "us to understand whatever process we're studying all right uh so understanding the interactions requires us knowing where they occur in genome space usually uh to really understand them holistically we need to know where they occur uh so here I've got another toy example where we've got kind of simplified perfect looking chipseek data if you're unfamiliar with chipseek the intuition you need to know about it is that it identifies for us where a protein of Interest binds DNA so for this protein of Interest it binds here and here all right so we know where this protein binds that's great now what if we want to know how the protein binds the DNA one method we can use and we often do use as biologists is to look for what we call motifs that are predictive or even perhaps Define that protein's interaction with the DNA and I've drawn that as just an orange box here representing some some sequence element in the DNA that encourages the protein to bind that site specifically "
    },
    {
        "start": 184.519,
        "text": "so knowing where the protein binds and how it binds are really important to knowing what this protein is doing and what biological process is affecting and how all right so it's not enough just to have chipseek data right that doesn't give you an inference on where the protein is binding we need to process the data and current tools for processing these types of data fall a little bit short in my mind really giving us the inferential value we seek from processed chipseek data and I'll go through some intuition with you as to how that happens and how this tool enricher Ator solves the problems we get with the current methods of analyzing chipseek data um so enricher Ator what it basically does is infers enrichment of the biological signal of Interest so here we're talking about protein DNA interaction relative to some baseline expectation and we get that Baseline expectation just by sequencing the entire genome right so that's kind of the Baseline we expect by random chance "
    },
    {
        "start": 245.079,
        "text": "we just get from The genome sequencing data and from chipseek data we can then use those two pieces of information to calculate the enrichment of the of the signal of interest uh gaps in current methods that the major gaps to me that exist are that very often with most tools that are available and most ways that we handle these data we handle the count data that sequencing data are as if they are continuous in the math we use to kind of infer enrichments of interest and you'll see in this talk how that can be extremely problematic uh and the second major limitation in current methods is the vast majority of them completely ignore the fact that sequencing data has a structure to it when we sequence DNA we're sequencing fragments that have a defined size and that structure can actually tell us a lot about the coverage we expect to see in our sequencing data and we can use that to our advantage when we fit models to the data to infer enrichments of Interest the second tool I'll tell you "
    },
    {
        "start": 305.24,
        "text": "about is called shap me that stands for shape-based Motif elicitation is designed to be a complement to current sequence-based Motif inference tools it's not designed to replace them but instead of looking only at sequence of DNA for motifs it looks at local shapes in the DNA uh for motifs and you'll see the GitHub repos for both of these are on this slide if you're interested um and you should be take a look at those repos all right so I want to give you the upshot right off the bat as to how enrich rator can be useful compared to the old way of handling these data so I'm abstracting a lot away from you but this is a real world example that actually inspired me i' I'd say forced me to write enricher raater in the first place where what we have let's just call it chipseek data it's not but that's the method I think most of us are familiar with so let's call this chipseek data on the Y AIS for both of these you have um basically inferred log fold enrichments "
    },
    {
        "start": 367.639,
        "text": "of this protein binding that we're interested in and it's the same genome position we're looking at in both of these plots so it's exactly the same data um that I processed and the top I used kind of the old standard way and the bottom I used enricher rator to process the data all right so right away up on the top I think what you'll appreciate is we see a bunch of these atically unrealistically negative discreet spikes in the inferred log scale enrichment um and we know these are unrealistic just based on thermodynamics basically right like there's not zero enrichment of the actual signal of Interest there's something there like if I took a coin and flipped it three times and got heads each time none of you in this room should say that coin will never flip a Tails right you know that's unrealistic and yet the math does not right the math that we're all using to analyze these data does not recognize that property of the count data that we "
    },
    {
        "start": 428.919,
        "text": "are using but enricher Ator does and what it basically just does is fits a generalized linear model to sequence and count data and it does this using uh an inference engine called variational inference you may have heard of this called variational Baye um it's a way to fit a model fairly quickly with basian techniques and you get samples from what called an approximate posterior distribution out of that and so a couple things that are nice about enricher Ator in addition to providing more realistic enrichment estimates than you see with the old way uh you get these Point estimates of your enrichment represented by the solid lines and you get these 90% credible intervals uh basically for free from the samples from the approximate posterior distribution and so we get really intuitive estimates of our confidence in our enrichments along with more accurate estimates of our enrichments right so that's kind of the intuition behind what enrich rator doeses on these data and by the way those data I was showing you they're not they were not bad quality data they're actually abnormally high "
    },
    {
        "start": 490.639,
        "text": "quality um and yet the old way causes these kind of issues with these dramatically negative unrealistic spikes in the in the inferred enrichment values all right so I've given you kind of the intuition for what enricher Ator can do with our data and I want to now build in some intuition for the old way actually gives us these unrealistic values that it does uh so here's a separate toy example here's a given genome the DNA is just this line this red protein has two specific binding sites in this genome so there are two locations where it is highly enriched so the actual biological enrichment of interest that we are trying to estimate with chipseek would be represented in this you know fake line that I just drew by hand where effectively there's zero in mment at the non-specific sites everywhere in the genome except these two specific sites where we have a high enrichment so that's the actual signal we are trying to estimate when we do chipseek all right what the actual data "
    },
    {
        "start": 552.68,
        "text": "may look like then is when we do our whole genome sequencing which we often call input DNA when we take our input DNA counts we' see this is our Baseline expectation for just sequencing coverage RIT large across the genome if there were no enrichment at all when we do our chip SE we do see the reads pile up at those specific binding site you know this is a probably the best case scenario where we have a decent antibody the experiment works well and then we see you know some other sequencing reads kind of work their way in they could come from a number of processes DNA might stick to the tubes that you're using in the lab you know that would be coming from some noise process they could actually come from True um non-specific binding of the protein of Interest right so we don't know exactly where these come from but they come all right and now we see with this and this would be what I would consider a fairly high quality chipy data set and what we see is there are regions of the genome where the coverage drops to zero right so "
    },
    {
        "start": 614.64,
        "text": "especially in these regions of the genome we get real problems when we start to assume that the sequencing data are continuous values because they are absolutely not they never are but they're definitely not at these low coverage regions of the genome where the counts are 012 and so you can see that in the math here that you know when we just calculate these enrichments in the old way we take some normalized measure of the chip seat coverage divided by some normalized measure of the Baseline expectation input coverage and because again we're pretending these are continuous values and we couldn't be pretending that we have a little workaround to avoid dividing by zero or having zero in the numerator where we just add a small pseudo count like well we know this is unrealistic math for the actual data generating process but we're going to just add the pseudo count to try to pretend that it's realistic we're going to make it look more realistic than it is um all right so we do that and what "
    },
    {
        "start": 678.2,
        "text": "results is that these regions where we get zero coverage in our chipseek data set you can see when we take the log scaled uh enrichment using continuous map the way we did we absolutely just get these strong negative spikes so again this comes from a couple of things where using math that's not really appropriate to the problem um and also we are not sharing information between local nearby positions of the genome so by show of hands I know several of you will raise your hands here but by show of hands who here has worked with like sequencing count data like chipseek before in your own research raise your hands high so I can pick on one of you you in the back if I told you that the sequencing coverage at position I is 100 what would you guess the sequencing coverage at position i+ one is it should be very similar it should be very similar there should be about 100 plus or minus a couple right we "
    },
    {
        "start": 739.24,
        "text": "automatically know this if you have any experience at all working with sequencing data like these you know that there is a high degree of information shared between nearby genome positions that if you know the coverage at one spot you automatically have a very good guess of what the coverage is at the nearby spot but the current math does not appreciate this it just can't it's not written into the assumptions of the model okay so enricher Ator actually does it solves both of these problems where it shares local information in a principled way based on the structure of sequencing data that we know exists it's in our mental model it should be in our mathematical model right and it also handles count data appropriately so gives us these more realistic estimates okay so let's get under the hood a little bit now that you have the intuition behind what enricher Ator does and what it's designed to do um I already mentioned it uses varial variational Bays to fit a model to the data and again this is just nice to handle these data in a basian way so that we can get samples from an "
    },
    {
        "start": 799.56,
        "text": "approximate posterior to give us kind of intuitive credible intervals on our Point estimates it's also nice that we can use basy and stats here because we can apply a shrinkage prior on our enrichment estimates if you're not familiar with what that means uh don't worry too much about it you've probably heard of correcting for multiple hypothesis testing though um what shrinkage priors do is basically that in a basian way right so that we don't have to then you know correct our enrichment estimates post Hawk the math can do it explicitly uh with a prior that basically assumes there's very little enrichment unless the data has a strong reason to pull our posterior away from that shrinkage prior all right it uses a negative binomial likelihood so that we properly handle our count data um and here's the math so basically at every genome position for every genotype we look at for a given sample type whether it's input DNA or chipseek DNA and for "
    },
    {
        "start": 861.36,
        "text": "each strand if you have strand specific counts some methods allow that for something like chipseek you usually don't so you know you would just have non stranded data and for each sample ID for every sequencing Library we we sequenced we say that the observed counts are drawn from a negative binomial distribution centered around the expected counts with some overd dispersion F and we fit a separate overd dispersion for each sample type here because we know a priori that chipseek data should be more over dispersed than input sequencing data we just know that to be true based on experience and every time I fit this model to any data that's actually borne out in the posteriors um so that's appropriate to have that separate over dispersion term globally for the different types of sequencing data we're working with all right so now our expectation here uh with its log link since we're working with count data U basically is expressed in this way where we have a certain "
    },
    {
        "start": 921.839,
        "text": "input DNA abundance that we fit so the model kind of learns the input DNA abundance and then based on whether we have chip seek data or not we either consider or not respectively beta here so we learn beta based on the actual chipseek enrichment over what we get in our input DNA and then here CJ is just an expression for it's an offset term uh to I account for Library size it does the library size normalization for us so if you sequence one library to two million reads and one to 1 million you expect the one in my left hand to give you half the counts on average at each genome position compared to the one in my right hand and this term accounts for that explicitly in the model here okay so you don't have to normalize your data basically before putting it into en rerat it's built into the likelihood okay and then again I'm going to reiterate and this is really I think the novel The truly novel thing about "
    },
    {
        "start": 982.72,
        "text": "enricher Ator is it uses sequencing fragment length information to locally pool information in our enrichment estimates estimates and this is how it does it so I already mentioned this Alpha term so this now is an alpha it's a vector Alpha it's a dense Vector for every genome position this Alpha has an estimate of the input DNA abundance then if we look on the right side we have Alpha Prime this Prime is not meant to indicate a derivative we're not taking a derivative of anything it's just meant to say this is a different Alpha than this Alpha but they're related so Alpha Prime is a sparse Vector so basically instead of looking at every genome position what we're actually going to do is fit a value for every maybe fifth more likely every 50th genome position depending on the actual fragment size in your sequencing libraries so if the fragment sizes are long we're going to infer Alpha primes wider apart than if our fragment sizes are "
    },
    {
        "start": 1042.839,
        "text": "short and then we build a weights Matrix to basically take the dot product of what I'll call a pooling Matrix and I'll explain with a cartoon in a slide here what this really looks like but we get this pooling Matrix so the dot product between that Matrix and the sparse Vector Alpha Prime gives us our dense Vector Alpha of actual genome scale resolution uh enrichment estimates or in this case input DNA abundance estimates um and then for depending on whether again we're using input um input values or hold genome sequencing counts for Alpha you know we get a week weekly informative prior on those values but if they're enrichment estimates betas we we have a shrinkage prior that we apply at this point before taking this dot product to get back to our genome resolution um enrichment estimates so in a picture here's kind of what this looks like where Again The Spar Spector Alpha Prime is up here and "
    },
    {
        "start": 1104.559,
        "text": "it's um represented by these circles so each value that we learn in sparse Vector Alpha Prime is represented by a color up here and then the pooling kernels in the weights Matrix um W Alpha are represented as the different color exponential kernels these double exponential kernels right and what I intend to convey here is that again we're looking at input DNA in this case but for the enrichments for the chipseek you could imagine you know we'll have different fragment sizes and so these distances will be different but for the input DNA the distance between each inferred value in sparse Vector Alpha Prime is the input fragment length over two just half the fragment length and then the the way I construct these pooling kernels is such that the mean width of them is equal to the average fragment size in the sequencing libraries that you're working with so it's a very principled way to use the fragment sizes in our "
    },
    {
        "start": 1165.64,
        "text": "sequencing libraries the structure of the data to then take and mix the actual fitted values in the sparse Vector back to our genome resolution and basically what would be a deconvolution of this sparse Vector looks like we have a question why kernel sharp because it's a cartoon that I drew okay so so are they actually like G kernels oh so no they are they are exponential so they are sharp okay and the reason I do that is because I think you know there's a certain probability with which you know if you have a certain fragment you don't know where the protein exactly was bound along that fragment but it was probably in the middle so you know we could split hairs over whether it should be gaussian um or haming or what what have you laian whatever but I've chosen exponential um I think it just makes sense to me to consider there be an exponential decay from the middle of some signal are there any other questions at this point "
    },
    {
        "start": 1225.84,
        "text": "okay all right so now I just want to point out one other nice thing about enricher rator using basian inference here um that makes it really useful I think to biologists who oftentimes are interested in how a signal changes in a condition dependent way so these data are from a collaboration that we have ongoing with ammaa and Chris from Michigan State University uh where we're looking at this molecule Chris Waters lab they're experts in cyclic DMP which is a signaling molecule uh that bacteria have it doesn't matter what it does for this example they have it and it does stuff all right so what we've done is we've done chipseek on RNA polymerase to look at where transcription is happening along the Genome of this bacterium so this is just one genome position that we're particularly interested in and you can see that these are the enricher results for this position of the genome either with cyclic DMP or without so I "
    },
    {
        "start": 1287.679,
        "text": "think we can all agree that with a high degree of confidence we can say that when cyclic DMP is high RNA polymerase is more abundant at this location that's probably the promoter of this Gene it's a bit more abundant in the middle of the gene than without but wouldn't it be nice if instead of just looking or forcing someone to look back and forth between these we could actually calculate that contrast that we're actually interested in what we're actually interested is the effect of cyclic digmp not just one condition and another right we're looked at looking at comparing them so with these basian um samples from a from a proximate posterior we can actually do that for every sample we can calculate the difference between these conditions or the contrast between these conditions what that then gives us is samples of the difference from which we can get our Point estimate of that effect of interest and our 90% credible interval of the effect of interest and again we're basically getting "
    },
    {
        "start": 1348.76,
        "text": "for free this intuitive look at the actual factor that the biologist wants to know about for free because we're using basian inference here so there's a lot I think that enrg generator has going for it of course I think that I wrote it but I also I think objectively um this tool has a lot going for it um one caveat though is again because we're using basian inference it is a little slower than just dividing a bunch of numbers by a bunch of other numbers right um so I've been ragging on that way of doing things but there's a reason we've all been doing it and it's not because we're stupid it's because we don't have time to wait for our computer to fit a model for a week right so on a bacterial siiz genome with several samples you know a reasonably sized data set en rerat takes several hours to fit which is actually not bad right but if you're talking about a human genome enricher raor is not going to be useful for you frankly you know we'd have to figure out some way to speed things up "
    },
    {
        "start": 1409.559,
        "text": "significantly for like an S VCA genome yeast genome I I think it could be useful for you um so keep that in mind as you look into enrich rator and decide whether to use it also as I work toward publishing this uh I'm working on including a noise term because right now all the data um affects you know the input abundance and the the enrichment estimates but we know that some reads come from these noise processes like sticking to the tube that we using and so the addition of a noise term should really help the predictive performance of this model um and in addition to that people are starting to use more and more often these spike in normalization methods rather than just normalizing to library size and we're working on incorporating that into the enricher RoR model as well are there any questions about enricher rator at this point before we go on to shap me go ahead yes so there's example you asked about earlier like where you have 100 counts at I and then you probably have close to I location right yeah with this "
    },
    {
        "start": 1471.399,
        "text": "algorithm are you worried about losing spikes that are biologically relevant yeah that's a good question and in case the owl didn't pick it up I'll repeat it basically the question was are we concerned about loss of resolution like genome space resolution with this kind of pooling local pooling of data um and you know I every time I fit this model you know I actually look at the posterior samples you know we get posterior predictive samples of the sequencing counts to see how well they match the actual data and they're just and richer is just doing a very good job and so I think yes I I think absolutely we are going to lose some intu not intuitive um informative spikes that actually mean something biologically but if those spikes are informative and are in your data and are well supported by your data you have short enough SE ing fragment sizes to actually your smoothing should "
    },
    {
        "start": 1532.279,
        "text": "just work out to capture it um and if those spikes are in your data and your actual resolution doesn't support their existence you don't want to infer that those spikes are biologically meaningful you want to smooth it out and enricher rator will do that so there you know as with anything there's a given take and benefits and drawbacks to to what we're doing here but I think overall you're you're going to get a better view of what your data actually support the existence of by pooling the information that enricher rator does any um NCH um so for your prior distribution for like the the alpha like vector is you said you had like a like a variance of four right there is that like hardcoded into like all the models or is that like supported by literature or is there any like type for that yeah yeah that's a good question so Mitch asked about um basically the choice of Prior on Alpha essentially DNA abundance "
    },
    {
        "start": 1594.159,
        "text": "relative to library size which is what that that prior really represents um so that choice it is hardcoded and honestly I didn't put that much thought into the width of that prior and I Pro I really should because for input DNA we really don't expect massive changes the type of wide changes that that prior explicitly denotes we're expecting so I probably should narrow that down to something that's better supported by experience with these types of data um so for like a bacterial genome we'd expect two to fourfold change across the genome if they're replicating their genome and this prior far exceeds that kind of difference so okay let's go on to shap me then I mentioned that it uses um shape DNA local DNA shapes to infirm motifs that are predictive of protein DNA inter action um so most of you will be familiar with this kind of view of a motif this is a sequence Motif they're "
    },
    {
        "start": 1656.6,
        "text": "defined by position specific weight matrices uh this is just an example of a motif I downloaded from a database called The Jasper database where for this transcription Factor af2 what the logo here means and we call these sequence logos or some people call them memes I think but sequence logos um what this means is is you know if you see a piece of DNA that has a or G here followed by c a CCC you really expect aft2 to bind that piece of DNA um and that's just what this means it's a very intuitive way to look at um how sequence is predictive of protein DNA interaction some issues with these are that they assume that each position of the DNA is independent from every other position of the DNA uh and we just know that's not true biophysically right if we change a base pair at one position you'll get local changes in the shape of the DNA up to two or so nucleotides away from that that single changed base pair so it's "
    },
    {
        "start": 1717.64,
        "text": "just not true that these positions are independent of each other um and of of course sequence motifs they only capture or sequence itself only captures a fraction of the actual information content in a piece of DNA so our hypothesis has been that local shapes of the D May outperform sequence motifs for certain proteins that's actually a pretty conservative hypothesis I think actually most people in this room will appreciate that it's probably a foregone conclusion that that is going to be born out in the data so I'll just give you the upshot right off the bat it is true it is born out in the data that shap me using shape motifs can perform better at predicting protein DNA interactions for many proteins than sequence motifs can so let me explain what you're seeing here the y axis here is just a given human transcription factor I went to the encode database and downloaded chipseek data for each of these factors and looked at okay where are the peaks in the human genome for each of these and "
    },
    {
        "start": 1778.44,
        "text": "let me fit shape me to the data to try to infer sequence motifs which are in the red or shape motifs which are in green or in the blue shap me actually allows you to run mixed models with both sequence and shape and then in the final step of shap me it'll kind of do lasso regression to do model selection and pick from the shape and sequence motifs that were identified which are the most informative right so it might report to you one sequence and one shape Motif or all shapes or all sequences when you run it in this blue both mode then the filed circles are each of the five folds for fivefold cross validated validation I did and the open circle represent just running uh the performance of a model trained and evaluated on the entire data set all right uh and then on the x- axis I've got basically each model's performance for each protein where performance is measured as the area under the proced recall curve so one would mean perfect performance and for "
    },
    {
        "start": 1839.6,
        "text": "these data sets the dash line indicates random performance or just garbage basically all right so what I really want to point out and again here's the upshot of what shap me can provide to biologists for this specific transcription Factor po 5f1 we see that shape performs really well you know with aprs of 75 which is great sequence motifs also do pretty well aprs are above 0.5 that's great when we run shap me in the both mode we get really good performance but it's just it's only as good as the shape performance was in the first place and if you actually dig down and look at the motifs shap me is returning to us the shape motifs so what this indicates is that for certain proteins like PO5 F1 sp4 yy1 um any information that sequence was able to give us was probably just shared information that sequence has with shapes and if we look more directly at the shapes we get a more inform informative view of protein DNA "
    },
    {
        "start": 1902.36,
        "text": "interaction all right so now you kind of see again the upshot of what shape me can do let's let's take a step back we'll talk about what these shapes are and how we're using them to infer motives all right so by local shapes here's basically what I mean uh DNA has a minor minor and a major Groove so we can look at the minor Groove width for B form DNA on average is about four a little over four angstroms propeller twist is when the pairs the bases and a base pair uh twist oppositely to each other along their base pairing axis roll is when they twist together the same rotational Direction along their base pairing axis and helical Twist of course is for every pair you move along the helical axis of the DNA how many degrees does the Helix rotate importantly these shape values at each position in the DNA can actually just be calculated basically deterministically from the sequence all right and if you're interested in how "
    },
    {
        "start": 1962.44,
        "text": "that works that's not what this talk is about I would point you to these citations okay so for now for the purpose of this TR talk trust me this works all right and wanted to provide another bit of intuition for why we think looking at the shapes will provide a dramatically different view of the DNA than just looking at sequences so here I took and I looked at every single fiver that exists for DNA and I calculated in the lower triangle the edit distance or the Hamming distance between each pair uh paired up F uh and these distances are just scaled so they kind of are are able to be projected on this color scale evenly between each other um but if every position of the DNA is different that's an edit distance of five if every position is the same that's an edit distance of zero whereas the upper triangle I've taken all these fers converted them to shapes and calculated the pairwise Manhattan distances between "
    },
    {
        "start": 2023.76,
        "text": "all of these where the Manhattan distance is just the absolute value of the difference between each shape value at each position all summed up to give us that one Manhattan distance so what I think you'll all appreciate is that the distance landscape for shapes just looks dramatically different than the distance landscape for sequences all right now for some proteins sequence is clearly what matters like if you ever use a restriction enzymes in your work you know that like that thing recognizes sequence right so this is the way that some proteins appear to be looking at the DNA so to speak if you'll forgive the anthropomorphism here of a protein and this is the way that some other proteins are looking at the DNA more more on the shape level right so again shape me is more of a complement to sequence-based Motif inference not a replacement for it um so here's the basics of the shap me workflow um I'm going to start by just showing you what the input sequences look like and the scores that you can "
    },
    {
        "start": 2083.839,
        "text": "give to the shapy algorithm and then we'll go through a few more of the details that I think are kind of important to understand uh so first of all we'll just look at the sequences and of course in real life you would want thousands of input sequences May hundreds you could probably if you have you know um a really good you know clean data set maybe hundreds could give you some good information for here we just have three for the sake of instruction so for each of these sequences that I would input to shape me in a fast a file I could have three scores or well one sorry for each sequence there's one score right so the scores could be binary um the most intuitive example here is like is this sequence from a peak or is it not right that's a binary input score that shap me could make use of the scores can be categorical maybe you have different types of Peaks um you know you can think of other examples the scores can be continuous but shap me will actually discretize these and bend them to make them categorical scores on "
    },
    {
        "start": 2143.96,
        "text": "the back end when it works with these data so it's pretty flexible is the point here the different types of scores that you can give it to associate the scores with the sequences all right so once shap me has your sequences it will convert each sequence to a shape and this is the way that I represent shapes these days um where for this seven M um each position has one of five shape parameters shown on this plot and you'll see on the y axis these have been zc scored already um and so like EP stands for electrostatic potential of the minor Groove I know it's technically not a shape but it's useful for this so um bear with that helical twist here minor Groove width propeller twist and roll and I know the resolution isn't great on this but we have the colors so you can kind of tease apart which icon belongs to which shape parameter so for each position we have each of those five shaped "
    },
    {
        "start": 2205.2,
        "text": "parameters and each one has a value on the Y AIS of course so that's how we convert these sequences to what I'm calling shape seeds of a specific width so once we have all those shape seeds we can take every seed in the input data and run it down and compare it to every sequence in your input data which is fairly computationally intensive process but it works works very well so for every sequence we can say okay does the Manhattan distance between the seed and the reference sequence fall below some threshold to call it a match so if we look over here on the right you'll see for you know this top sequence we're pretending frankly that this seed matched on just one of the two strands and so for that we give it a match score of 01 for the next sequence it matched on each strand so that gets a one one and for the next one it didn't match anywhere so that's z0 we can express "
    },
    {
        "start": 2265.76,
        "text": "that then as just a hits vector and now this becomes very useful and this is really the heart of shaping if you will where we can take this hits Vector of just categories and compare it to the scores Vector that you gave to shaping right that's your ground truth and we do this comparison using a metric called adjusted Mutual information where if knowing this hits Vector tells us everything gives us perfect predictive power for predicting these scores and the scores Vector the adjusted Mutual information score is one if there's no information shared between them the score is zero so I want to build out a little more intuition about what neutral information really is the classic example of neutral information is well today it's very sunny so you would all I think assume that it's not raining right so sunniness and not raining have a very high degree of mutual information just knowing that "
    },
    {
        "start": 2327.92,
        "text": "it's sunny on a given day lets you predict right away with high accuracy whether it's raining on that day so that's the classic intuitive example of what Mutual information does for us I also want to point out that you know you could say well why not use like a correlation coefficient or something right so mutual information doesn't require a linear relationship right and it can handle data much more flexibly it can handle categorical data right so it's just a very flexible way to basically assess the similarity between two vectors okay um so in the real world when you actually prepare your sequences to feed into shape me this is kind of what it might look like you know we don't actually know where the motifs are a priori right this would just be a black line of genome space we know we had chip SE uh Peaks along this DNA so we gather up the sequences under those Peaks represented here and we gather up a bunch of randomly chosen sequences that are outside of Peaks as our "
    },
    {
        "start": 2389.2,
        "text": "negative set so then we get our Peak Vector so these are the scores again that you input so you got the sequences here and the scores those are the inputs to shape me and then this final Vector is kind of that hits Vector between a seed or a motif and each sequence that we input so here you can see you know if the motif was actually under these Peaks 100% of the time and rarely was it outside of the Peaks there's a high degree of mutual information here and we say that's an informative Motif right and so here's the map for Mutual information if you really looked at it for a while you would notice that it doesn't just measure like the simil it between these vectors per se it's more like how often do they if they change do they change together right and that's really the the information that it's giving you all right so we put our data into shap me it gathered up all these seeds of SE of shapes it got these hits vectors it calculated the adjusted neutal information between every seed "
    },
    {
        "start": 2450.319,
        "text": "and our input data all right that's great now we have usually tens of thousands to hundreds of thousands of seeds that we need to filter through and decide which were actually informative and of those that are informative which are distinctly informative and which are redundant with seeds that we already identified all right and I'm not going to go through the details I'm just going to tell you we use something called conditional Mutual information for that which just addresses the question of you know whereas Mutual information says if I know one vector what do I automatically know about another conditional Mutual information says okay if I know one vector and another now what does this other one tell me about the first right it's conditioned on one um it just says you know does this does this next thing provide additional information to the model that the first did not and that's how we prune the Redundant seeds then we go through an optimization routine to tune "
    },
    {
        "start": 2512.48,
        "text": "every seeds shaped values and the weights that are applied to those values when we do a Manhattan distance calculation um so I'm just going to show kind of the intuitive representation here hopefully it's large enough to to see um where you know if we take this again just toy example seed you can see that after optimization some of these values have been nudged up or down but not only that you'll see that the opacity of these rectangles has changed and what I'm encoding there with the opacity is the weight associated with this value of this shape at this position in our Manhattan distance calculation so for example here propeller twist at position one has a weight of zero basically so it's transparent so the optimizer basically figured out that the value of propeller twist here just doesn't matter so we're going to drop it out of the distance calculation entirely not even consider it so now this is what I consider a proper Motif it's not a seed anymore "
    },
    {
        "start": 2573.0,
        "text": "it's it's grown up and now it's a motif um so after that final optimization of all the seeds what we often see is that some seeds will converge to actually be the same Motif we can further prune the Redundant one from that using another round of filtering using conditional information followed by lasso regression for our final model selection uh and then shapy gives a nice HTML report to the final user so they can kind of open that up in a web page um and the goal here is to get a web app running so that people can just submit their sequences online um and get the result and download their results so that the biologists can use this very easily all right so I just want to show you now that you kind of have seen the ins and outs so to speak of how shap me works I'm going to show you that it works uh first of all I generated a number of synthetic data sets where I just put known sequences at known places and shap "
    },
    {
        "start": 2634.72,
        "text": "me gets those back at find them the most interesting test data set to me that I created was where one where I took 50 of the most diverse sequences I could find that basically all had the same shape because that's really the Crux of shap me that's the thesis here right that we're identifying shape motifs no matter what sequence they came from so in this test data set when I run shape me in shape and sequence mode it finds one Motif and it's this shape motif right so it does exactly what we expect on that type of data set and just for reference when I run it in sequence only mode so in this case it's actually just a wrapper for stream by the way stream is a currently existing sequence Motif finding tool I'm not rewriting stream right I'm just using shap me as a wrapper for that um so when running sequence only mode it does actually find some fairly informative sequence motifs it's just kind of interesting that the optimization routine I used to identify "
    },
    {
        "start": 2694.88,
        "text": "these sequences I maximize sequence entropy and minimize shape distance it just happened to kind of converge on GC Rich DNA which just kind of interesting I maybe from a biophysics angle I don't know worth thinking about some um so here's actually the quantification of the performance of these various modes of running shap me on that test data set where again I'm showing you area under the Precision recall curve on the xais random performance is indicated by this line um and here the circle is the mean of the cross validated performance and the bar is uh plus minus the standard deviation of the fivefold Cross validated performance so we see shape only performs the best sequence still performs fine um but sequence and shape performs as well as shape does which again we expect that given that sequence and shape is actually just using shape information for this data set um I already showed you the kind of human transcription Factor um data when I kind of showed you know here's what "
    },
    {
        "start": 2755.079,
        "text": "this does we know that for some proteins shape really matters for some sequence really matters and for some there's a combination again this is really compliment to current techniques uh and then we have these three bacterial proteins that have been really hard to identify good sequence motifs for um and shape me does a okay job finding shape motifs for them which is kind of interesting especially our lab is really interested in this HMS protein which coats bacterial DNA and we're finding a a decent shape Motif for that in our chipseek data sets as well so overall I think shap me is going to be really useful for biologists um you should try out en rator and Sh if you have the appropriate data to use them with um here are their links uh go check them out we have a singularity container for shap me that hopefully will make it very easy to use with the instructions that I've supplied on GitHub but if you try these out and you find the instructions confusing or the container difficult to use please reach out out and let me know "
    },
    {
        "start": 2816.96,
        "text": "I'm very interested in hearing that feedback uh and thank you everyone for your attention thanks to uh everyone in the fredolin lab of course but especially Lydia Mike one of our former graduate students and Rebecca for all their work on the various aspects of what I presented today are there any questions at this time yeah any rockus Applause has a question or comment to relation which also be alternativ by joil yep yeah yeah absolutely absolutely so what chenin is pointing out is that "
    },
    {
        "start": 2879.92,
        "text": "basically calculating these shapes from the sequence is really just a convolution of the various kmer up to like four to five Ms um that you could just express it as sequence cers in your input data basically if I'm understanding your your point correctly so others have done this so people have and it won't surprise you to hear that people have Incorporated two three even former um basically um design matrices if you will into their sequence Motif inference tools and they perform pretty well the problem is that your parameter space explodes exponentially when you do that and so you're really getting your model becomes more and more sparse whereas by directly looking at the shapes we don't really explode the parameter space as as significantly okay great well thank you Mitch did you have "
    },
    {
        "start": 2940.52,
        "text": "a I kind of have an out there question also question like is there a way to sort of kind of do the reverse of what shap does in the of like say you have like a protein structure whether or not it can like recognize or will interact with a specific like shape of like certain DNA reverse I think the entropy space is just way too way too large there I I don't think so not currently but you're right Chang Shin would be the expert but based on the look on his face I'm gonna say no okay well thanks again everyone "
    }
]