let's get started so hello everyone my name is Xiao I'm a second-year students from Welsh lab thanks very much for showing up and I hope you are doing well at home today I will be talking about my recent work the topic is iterative refinement of cellular identity from a single set of data using online learning please feel free to ask questions the talk is outlined as follows there are four main sections background methods with outs and discussion so let me first introduce you some background about single Sarah Isaac technologies so this figure demonstrates the key advantages of single cell basic in comparison to the Oder about our logistic technologies the POC RNAs take data displays an average of each expression patterns across thousands to meetings themselves and it's my home security power to play evaluative differences between ourselves in contrast the single cell our Isaac enables gene expression to be studied at a single cell level resolution revealing the cellular heterogeneity next let's look at how researchers determine the cell identity so cells have long been cognitively characterized by combinational features such as mythology tell surface proteins and their broad function so give them multi modal single cell data available researchers are now able to quantify identify cell types and cell states using transcriptomic and epigenetic profiles here are three major challenges in integrating multi modal single cell data the first one is to only one modalities are available for each individual cell both similarities and differences among the SS of interest and a symbol size is large in the rapidly growing therefore the um there are some researchers in this area have put a lot of effort into addressing these issues and a number of tools have been developed so uh people have published this early this year listed top straight bar correction methods after benchmarking 14 of them they are like ER which is our published walk of our lab how many and destroyed two versions three although the ocean could perform at is there are limitations all those three methods are not efficient in scaling to massive datasets and not able to incorporate new data without recalibrating front squash while over how many is not designed for integrating multiple data modalities those things our work is based on like or let's talk a bit more about it the core of the library package is intuitive non-negative matrix factorization ion and having short the figure shows assuming we have and gene by cell input datasets X I a video same or different measurements of cells this shows M sub genes then I will go is to join to the decomposes matrices into shared managing factor SW there's a specific humidity versus VI and the cell factor loadings H I so the part of the left of laboratory function focuses on matrix reconstruction widely this part controls the sparsity there are two parameters we need to specify here in a lambda k is the number of dimensions of the latent space in other words how many meanings were looking for larger k leads to more authorized managing z-- k depends on the complexity of the biology underlying the data whereas two the lamb that determines how much we want to paralyze the ferocity that we use five by default to before time into a solution to IMF let's first look at the existing algorithms designed for regular non-negative matrix factorization problem so which is usually formulated as this so we want to find the non-negative integers W and H to properly construct a matrix a the problem service non-convex by the subproblem will we solve for one component while keeping the others fixed is convex lens down proposed multiplicative updating rules two decades ago the idea was pioneering but they do a personal guarantee the convergence you know long to our dresses researchers introduced two popular approaches in the broad coordinate descent framework the first one is or nating nonlinear squares in OS which updates the matrix blocks in coordinating fashion the second one is Hiroki coordinating least squares which updates vector Bronx there are faster algorithms with guaranteed convergence multi live updating rules in there as a house all batch algorithms as the xx of holding a set of the situation or training during like a package the patch I wisdom in OS was used for swapping INF so given and the sets English Delta M chings put in a set each today I in order to minimizes opposite function the matrix blocks if I double na VI updated using this formula is already a bit into a convergence to us this is really grueling we're in need of a scalable tool for integrating single cell data oh look it's inspired by the idea of online learning from my rock paper which is originally designed for regular and a sparse coding as mentioned earlier the iterative approach method access a whole dataset at a generator iteration while the proposed on I approach process one data point or mini batch at a time it provides first convergence it uses a fixed keep memory and it does not have to recover it from scratch in the thread for in the process of new leader so our goal here is to extend the online earning to the integrity of non-negative matrix for today shinai LMS we envision using online IMF to integrate single-cell dataset in three different scenarios the figure here illustrates scenario 1 assume that there is a large and a fully observed the algorithm access mini-batches from audio sets at a same time and I repeatedly updates the finishings sample to loadings each can be revisited throughout the multiple reports of training a key advantage of a center one as compared to partial algorithm is that only a single mini batch needs to be in memory at a time depending on the available computation or resources it is controllable by manually specifying the size of the mini batch um scenario two is demonstrated in the left hand side the input data cells are arrive sequentially and the own language amuses each cell exactly once update managing without repeating data already seen the cannibal nutrient rich without the focalization is efficiently you find the newly arrived without requiring a capo ehh time on the right hand side which shows narrowness 3 it allows us to protect a new data into the linen space most of it most perfectly sterile managing w already learned without fielding the new leader to obey the mayor Jing's in practice we first use on my IMF to learn the many things as in Scenario 1 and 2 then uses german urgings w to calculate the cell factor loadings for new data set the source scenario is highly efficient incorporating data allows users to query against a curated reference and provides increased robustness your data set differences in newly arriving data now let's jump into the methods in order to address of the penetration problem of IMF we carry out their steps iteratively at each iteration with simple mini batches of ourselves Veronica said Tom four separate loadings using the major things WI and vw8 a VI the na step they emptied managing factors using the latest a try so I was hardly used to combine previously discuss in areas and house methods so for each simple mini batch we update the cell factor loadings by matrices using alien LS so it is addressed by the block principle pivoting method developed by King and his colleagues the formula is showing here then we update the managing factors using house algorithm so in this problem walk there are 2 + + 1 times K vector blocks we have did each column VI and W during the displayed formula notably a smoked by the bread boxes the updates for WPI Japan of the previous acing data X I and their step aerobics H I a nave implementation would be required storing all the data as well as the sepulcher loadings in memory these are judging by cell and K by cell matrices when this though hence it is not efficient at all to do so with a large number of cells however we knew here certain house a place depend on the X I and H I only stood matrix products if I try a transpose and X I if I if I suppose in online IMF let's assume we have many patches from excited erosion T the size of the issue a batch API and I've learned cell factor loadings lowercase a chai tea do we say try I try suppose an excited if I trip of images is a and B the dimension of a and B are readily small they are clearly like a and the M by K elements with decorate allowing efficient storage and can be computed incrementally using is formula with incorporation peach new data point or maybe budget X I editorship duration T here is overview of the online IMF algorithm was proposed given the inputs with first specified K and lambda parameters and if initialize managing stability WN API the invoice information with sample minibuses from each data set we first obtain a sailfin loading suiting a nos the updating which which is a and B use them to learn the managing factor of stability I saw house and in the end we obtain the final I try using the latest matchings through analyze those things we expect the inputs to large to filling the memory by designer prism they have saved on disk being actually a five format mmm for example when the minibar size is five thousand and ten thousand billion data from disk showing blue as many more overheads as compared to other interactive on memory and color in color by yellow so before I jump into the results you see any questions about the background and methods let's move on so I'm going to pretend the results in poor parts so in part one we compare the performance of online MF and batch IMF then we benchmark IMF along with other popular algorithms in part two so in the third part with demonstrate on IMF on a large dataset last but by no means the least we show online mobility to interminably refining settlers so let's start so is part one is for this part we want to show our IMF converges faster and that's now news accuracy as compared to the batch IMF we use the nos in multiple odd multiplicative methods for comparison the data that we'll use ourselves for our Arab Mouse pronto and posterior cortex they're both single Sir Isaac datasets mmm the data collection is called table base created by Sanders and her colleagues and there are nine different regions in total sequence the former but now we're just doing positive part of the dataset for online and I've we tested different many by sizes ranging from one thousand to two hundred thousand we use K equals 40 and lambda equals 5 then we use eighty percent of the data for training and the rest of it for testing the panel a and the panel be show that the online algorithm marked in red converges much more rapidly to a similar or better object function value compared to the Bosch such methods you know less in Goering and a multiplicative in blue on both training and the testing stance in panel see the box plots compares of their function values are shifted by online and patch pack rhythms after a fixed amount of training time so we can see that the Alaia resemble is shows significantly lower object function value within around 500 seconds then we run a grism using different points of mini bite size from energy and the e we see that the convergence behavior of the on I IMF is nearly identical forming the by size from 1000 to 10,000 and the panel F shows that for fixed test at the wrong time needed to reach convergence remains relatively constant wants the film number of cells exceeds approximately 50,000 cells this behaviour likely occurs because for several population of fixed complexity for example a future of containing 10 cell types only some fixed number of observations are required to effectively learn the main urgings therefore the entire set to update the managing each iteration becomes increasingly inefficient as its size is larger than the minimum threshold needed to learn the Banerjee for the next experiment will use the same collects data plus human PBMCs and human pancreas data collections the parameters for implementing all I have shown here after we obtain the self of the loadings we visualize visualize them using you map in the first and second coordinates then we color the data points by data sets and by publish to cell type labels so online IMF and patch IMF NFS here for those very similar visualizations decision that two approaches give every similar data set alignment and the cluster preservation we will further confirm this all relative of the basis using qualitative metrics next parts so in the second part I will go here is to show online I have you know single cell data integration using lists and a memory but also producing comparable mm analysis results so the methods used for benchmark here - IMF arrest algorithm harmony and turrets versions to iterate is one as for the benchmarking metrics we use a time and a peak memory usage for evaluating computation or efficiency to qualify the data set alignment we used the K nearest neighbor flash effect test Quebec Quebec mythic and the alignment score the key beds uses a chi square that is statistic to test the null hypothesis that the nearest neighbor thesis in at alliant billion space do not differ according to bash thus a higher average p-value indicates banner decoration the alignment score Neelam's the mixing level in the local neighborhood of each cell for my badge correction produce a large alignment score near one wire if there's no correction at all then we'll have gone near zero we use identical keypads and alignments metric parameters to evaluate of approaches XX to access to assess the clustering performance we applied the new vein community detection to the alignment and spaces of type event by old methods we then compare the results in clusters with the published cluster assignments using class or purity and adjust a trend index they are I will utilize the same set of data for benchmarking mouse colleague 0 set the to human chemistry 10 SS + 8 human PP pancreas innocence and the settings for running on IMF industry analysis is shown here first for to benchmark time and a peak memory usage we generated 5 data sets of increasing Silas temperature from the mouse codex data to impanel a we can see it the wrong time required for a lie IMF rayline does not increase significantly as a size grows and the peak memory usage is constant although not displayed here we later found that a gap in run time between harmony green one and our proposed method will become larger at the data set size fully increases like of our meeting cells the pic memory usage of honor MF with a mini per size of 5,000 cells and a 48 factors is around 350 megabytes no matter how many cells in total processed but the panel B and C shows the results on the human PBMCs human pancreas respectively so as we can see here the online and back EMF algorithms aligned PBMCs and Pinker's stands as equally well and beating hominid and asteroids though online in Patrick of IRAs and produce comparable classic results the other approaches in terms of class of purity and a RI so overall the results of the lysis here in the case let the time and memory efficiency of online does not sacrifice the raise out quality in part 3 we're going to showcase the plication of online IMF on a even larger data set using the fixed memory and peak there are nine single CRN 16 assets tickle is still in the from 9 different mouse brain regions do something about 697 cells so we're an online algorithm for three epochs with minibar size to be 5000 the KLM double use here are 40 and 5 respectively it took about 54 minutes and Iran 250 megabytes of RAM on a laptop I use MacBook Pro with I seven processors so if we perform this analysis using our [Music] previously abduct abduct adopted - I am FM OS algorithm it would have taken around three hours and they're required over 20 gigabytes of RAM do we first realize in the embedded cells in panel a the cells are called by the published data type labels such as near wrong in the city or cells my hopefully our jalapeno sites of society in etc in the first and second you map Coordinates and really disparate display separately for all nine mouse brain regions such as corn eggs cerebrum hippocampus stripes Trey Liam so clearly the online IMF I'm going to pretend to the a structure as weak as here the cells within each class grouped together very well then we use a factorization to group the cells into 40 different clusters by assigning each cell to the factor on which it has a largest loading which we term the Max Factor total assignment so in panel B we fully exam differences in regional proportions of each cell cluster the neurons in original sites to the most regional variation in composition we notice that the two new propulsion apology no size varies by region but individuals of groups of a size and not region specific the income trust the individual sub groups of neurons are highly regional specific reflecting the divers with you know specializations in neuronal function in panel C we also investigated their biological properties of our cell factor loadings the class of diamonds larger planes the type withing the parolee subclasses as expected neuron shows most diversity with eight subclasses in contrast macrophage microglia a dog a Miro tells each correspond to only one thing go faster so one of the most appealing probabilities of our online or ISM is the ability to incorporate new data points as they arise this mobility is especially useful for large library of efforts to construct comprehensive sell the lands so in part 4 we demonstrate an IMF IMF by analyzing tell ourselves that is sequentially arrived the cells used here came from the mouse primary motor cortex they were generated by brain initiative self conscious sensors network Vict n so they are a data sets in toto um and they come from five four different modalities the single-cell are a sick single nucleus our Isaac single nucleus a toxic in a single nucleus dick Thalia about only talent sells in toto so after selling here shows a running online algorithm eye on it process 5,000 cells preservation only one epoch for each dataset and that we still use K goes 40 and wind x5 as shown in the figure following stereo to our approach successfully incorporated if new single cell of single nucleus are a sick dataset 16 total without representing previously process cells mm selves occurred by patella says and this is germinate chronometer awfully we try to incorporate the single cell nucleus a toxic data set which is a 7s datasets in a same way but it did but unfortunate it did not allow as well as our only PSS following 0-2 this may be because the toxic is a completely different modality in those neighborhoods who may not be the best by Jane for incorporating the toxic data because we also integrate the signal nucleus toxic data according to a scenario save by the parent by the managing project protection so we first performance online I live on the RNA seek data they used to managing W the project attack take data into assembly and space assault or insignia showing panel B mm and the data points from the taxi data occurring green and we can see here the new data points are well aligned to the previous data points Berlin great hmm then we included the constellations data in a same way shown in panel C and the Kilwinning very satisfied is through doubts so this relatively produce excellent equation results in penalty we were able to point it identify 17 different cell types from all a DSS delima buildings reported in other studies in summary but Allah IMF has three key advantages the first law it converges faster without loss in accuracy second it decompose the memory usage from the whole exercise there are 40 allows the researcher is to carry out the single cell analysis equation prints on up top and lastly it is able to availably refine the cell identity um as a single cell psst crucially arrived so there are still room for improvements those specifically we want to improve performance when querying at the similar type of data sets in scenario two um so that pretty much covers my talk today and I would like to say thanks to my mentor George and all the love members for their continued support throughout the project and I will thank you all very much for your interest in attention oh it's a q and A's yeah there was a question from Damien Furman and I was online I and implemented is it in our package or a Python um so currently the online algorithm is implemented in our package and the our and to improve the completion speed part of it is implemented in our CPP and our member Lulu is very working on making the our package into Python package so that great question the questions and if anybody wanted I'm sure you could take it offline with thank you all for tuning in thanks very much but you don't want