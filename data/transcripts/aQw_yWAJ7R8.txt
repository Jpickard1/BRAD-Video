I think we'll go ahead and get started just the introduction my last you can sort of get settled in welcome everyone tools the technology seminar series I think you've pretty much been here before there's a sign in sheet going around please sign it as a reminder it's just to help us with the pizza so that we can prove that we had people here and therefore continue to get pizza for future years or semesters I guess anyway he's to introduce speaker today Alex Clinton who's a graduate student and DC MMB and it's going to talk to us about a neural networks for biomechanical image analysis thanks Marcy thanks for having me and yeah so I'm first year our fourth-year PhD candidate in the department and I was already told that my team is matching my origin that everyone is big and red because I was born in USSR I guess that's a success already and what I'm going to talk about today I'm going to talk about I'm gonna give an introduction in the neural networks at artificial neural networks deep version of them and the kasam convolutional architectures and then briefly mention some of the exciting applications in biomedical image analysis and then in the end if I have time left I will also talk about some of my work on a sparse commercial neural networks for nuclear shape classification so before we start it in the traduction I would like to ask you guys how many of you do not know how artificial neural network works like show all right fair share I guess so yeah I actually did this is the biggest part the introduction so I actually try to be as clear as possible on these little steps introducing the architecture so I probably spend the most time most time here so you have this the intro so basically these these models then there are machine learning models there are family of biologically inspired algorithms invented around 50s later they have been outperformed by ACMs and random forests that became more popular more like to go tools for analyzing data for classification and regression but later into thousands alex debt and some other new networks architectures started that so-called deep learning Renaissance and now they are super popular and the working where you will and then usually there are a couple of reasons that people talk about when they say why they're working now versus 50s and you know previous century because we now we have lots and lots of labeled data that is available for you know for for people for researchers and we have much more computational power including some specialized higher hardware and this specifically I will talk a little bit later about the progress in in GPUs so and then when I mean that they're working now that means that many computer vision applications many machine learning applications these neural networks outperformed all these models including you know previous versions SVM's random forests and established new state-of-the-art for example this renaissance started it alex net when one of the biggest computer vision competitions on image recognition 2012 this application this application of this neural network dropped the previous result error by 10% and it was the biggest drop you know they were gradually reducing the error year after year but in 2012 using this net they were able to achieve 10% increase in accuracy so that why that's why this brought a lot of attention to these kind of models and they are a very coarse model of neurons in the brain so this is what was sort of developed in early 50s 60s based on the how people would try to imagine very simplistic realization of the human brain neuron and so this is still you know valid to the day so and then this is the central part I think of the any neural network functionality this is how it works and it's fairly simple actually because you know you have the axon coming in from some other neurons and it comes to to this neuron and then it brings some kind of a signal signal has a value then in a synapse there is another value called a synaptic strength it basically regulates how much of this signal comes into the neuron into the cell body and then this neuron has multiple you know multiple inputs from other neurons that are connected to and they're all burning the signal and then this synaptic thanks is just a multiplicative coefficient that gets multiplied by the input so you get some portion of the input coming in into the system body so this is I think fairly simple concept when you have when you have many inputs and this many inputs get multiplied by some coefficient those coefficients their property of the neuron and then in the model not in the human brain but in the model we consider them parameters so they are learnable we didn't know them when we start learning from the data and then so body also very simple things happen we take this coefficients multiplied by inputs we sum all of them across all the inputs that this neuron has and then we add bias and bias basically is just another value that is corresponding to each cell parting and that it means it's basically in a ability of each neuron to fire so if the bias is large then even given the small input that probably is this neuron is gonna probably fire these biases or even negative then it's probably not gonna fire even if it's received a strong signal from other from other neurons and in the end there is a activation function which basically means this function that the function determines if neuron is going to fire given the input and given the bias given this summation so basically output of the neuron is going to be this function applied to separation all the inputs multiplied by coefficients and the bias so basically if this sum is above certain threshold if you consider for example this output be in binary for simplicity just for a second let's assume this is binary then this function is basically a threshold that decides if this big enough to fire or not and if this sum is big enough than neuron fires if the sound was not beginning of the new another fire so this is all fairly simple okay so let's consider a binary neuron and then for historical reason was called perceptron in 60s so consider this perceptron consists of just one neuron it has two binary inputs so x1 x2 only can take zeros and ones the values can be 0 and 1 we know the weights let's assume that we already you know learn them or we know a priori these weights are both - to the biases 3 and it's a step activation function that very simple step activation function everything below zero ends up being zero everything above the zero ends up being one this is the function f that we're going to apply to the summation of this you know these inputs and the bias and so let's write different inputs for this neuron consider this binary input 0 + 0 X 1 and X 2 so we buy basic calculation you multiply 0 by -2 0 by -2 at 3 you get 3 and then 3 you apply F 2 to 3 that you ended up getting here you get 1 so when you have 0 + 0 this neuron outputs 1 so basically it fires if you take 0 & 1 0 by negative 2 is 0 1 by negative 2 is negative 2 plus by as 3 is 1 and F of 1 is 1 so it's 1 again basically neuron fires again the other way around is the same because the weights are symmetric both negative 2 so 1 0 is going to be the same outputs one neuron fires again if you consider 1 + 1 it's going to be negative 2 plus negative 2 and plus 3 so that's going to be negative 1 and then the our activation function of this negative 1 is going to be 0 so only when it's 1 1 this movie is not firing it always fires except when both inputs are 1 and that corresponds to response to simple negative end operator basically you only get zeros when there when there is only get once when there is all of them is zero but one both ones they're zero and this is the opposite of logic and logical operation such a negative end so basically this shows how using this very simple binary perceptron you can model logical gates and negative end or you know exclusive or any of these operations can be melted by just one neuron and when you start considering the many layers you can do what complex operations so basically what we said work is lifting we build these weights we have data that are inputs and we have the light correct labels or output of regression and we don't know this weight so we want to approximate this weight the problem is perceptron models that given the binary way binary inputs and binary outcome we don't know have much of a small change in the weight will lead to the small change in the input because input is always gonna stay either 0 of 1 and then even if you change your weights a little bit gradually by learning here if output is always gonna be it doesn't matter if you move from one or two or three it's always going to be just 1 so it doesn't provide you feedback how changing the weights reflected and they change of the outcome so this is necessary for learning to know how change in this changes this weight how does it change the output so we can have a feedback from the model so basically we cannot learn from data in perceptrons unless it's a completely binary data because output is binary so yeah I mentioned that and the solution would be approximated as binary step function with some continuous function to replace this function with something continues that gonna provide you not just binary response but would actually show you how much if you change your weights how much our output is changing and some common activation functions that are providing the alternative to step function are sigmoid or logistic logistic function basically squashes real number to the range between zero and one so basically you have from zero and one and this sort of like a smooth version of step function but the good thing about the sigmoid is that you get real value output is not just zero and one you get you know basically continued continuous values so if you change you change your weights a little bit and weights are not binary you can actually get mad binary response how much your output is changing in the sigmoid function couple of versions of similar functions are hyperbolic tanh squashes real numbers range between negative 1 and 1 actually zero sanphet which is good for optimization purposes and this thing is also very similar but it's actually scaled version of Sigma also very similar to step function also smooth outputs the real values and more recently introduced and more commonly used these days is rectified linear unit it's also sort of an approximation of the step function except it's it's the same zero below zero except it's the linear with the slope one when it's about zero and calculates like as maximum between 0 and X and what it gives its zeros everything that below 0 when the neuron does not fire but it still gives you the continuous response when you have the positive positive summation under the under the function so these functions are commonly used in practice people use really a lot but we're going to consider some examples using the sigmoid because this is classic function using logistic regression that many of you are hopefully familiar with so consider this single neurons still one neuron sigmoid binary classifiers so we're going to consider binary output but you're gonna model it is the real real values and actually actually this model is exactly modeling binary logistic regression it's exactly the same what your typical logistic regression classification does you have the inputs or your features it have weights or your parameters you want to weight each feature with the parameter you have total bias and then we have the same function but instead of step function now we have the sigmoid and the sigmoid function is going to be because how it's calculated 1 over 1 plus exponent to the negative x we're just gonna put our this part put it in the function and then it gets this so output of our model is going to be like that it's a ballistic interpretation is very similar to when you do a classification with logistic logistic model basically this gives you a probability of class being present or or or this particular label being of class 1 given your inputs and given your weights basically it's because logistic regression outputs the values between 0 & 1 it's a probability for class 1 and if you take Phi 1 minus that probability you get the probability for the second class so it's a simple binary estimation of probability for two classes you get one value and the second value is going to be one minus one and the Sigma it is very nice for the purpose because it's exactly between zero and one so when you get the value it's always you know normalized you don't have to scale it and usually as a as in logistic regression with this classification for this parameters W are estimated iteratively using some kind of minimization of cost function for example negative log likelihood which is maximum maximum likelihood estimation typical method for logistic regression with some gradient based methods and Newton methods or you know already in descent or something like that so using those math you can have the data you can have the labels and you can estimate the parameters in the same way as you would do it in a logistic regression or logistic classification and then when you want to move from binary logistic classification to multinomial or multi-class logistic classification you just basically have your parameters ever going to be a matrix and then your X is going to be the vector of inputs but it would be also a vector by the number of classes and it's all the same you just you just have multiple inputs instead of one for example instead of two classes you're going to estimate three classes so your model is going to output cable and vector of probabilities so probability for the first class probability for the second class probability for the third class and as in multinomial logistic classification of logistic regression your normal probabilities are going to be normalized to one so when you sum up all the three probabilities for all three classes they're going to add up to one and this is also a neural network consists on one neuron exactly corresponding to binary which is generalization of by a logistic qualification to multinomial so when you when you doing this again you take your you take your W switch our K by D multiplied by X which is d by 1 so when you multiply K by D by D by one you get K by one idea biases by this is K by one you get the cable and output put it in the function and this function is going to output cable and vector of probabilities so that's when you take those probabilities you do not know which is problem means with which probability each class is possible for the given input and for given parameters ok so I'm just gonna show you a little bit of the demo or one you know let's hope this works so this is Google tensorflow online demo or playground as they call it where you can build your own network online on their website and then you can run it on different input data so let's just create this simple one you're on your own network and there are two two inputs X 1 and X 2 and X 1 is basically the values on the x-axis this is the data so you have one group of points another group of points and they're both in you know class 1 and class 2 and X 1 is just x-axis so from minus 6 to 6 and X 2 is just Y axis also from negative 6 to 6 so this point for example is going to have X around 3 and Y around 3 and 1/2 so what we're gonna do is we gonna run it and see how it it's the data so fairly simple under 60 iterations already one neuron is able to separate two classes so you can see the X one is you know X 1/2 what is it negative 1 on on a left and a positive on the right and X 1 X 2 is negative at the bottom positive at the top and one neuron you can see the visualization of the these weights that are multiplied and added the bias this visualization of one neuron is exactly the linear classification so as you're on logistic regression this is exactly the same process and then when when you train it the as I said iterative training repeats multiple that multiple the maximum likelihood estimation so you can play with it and try it on a different data sets for example if you have these classes that are you know one class hardened by another class and try to fit the model here it does minimize the the loss function for some time because it tried to grab as many points of one class into into one section without other ones but the problem here it is not able to separate these classes linearly so that's the problem if you take the logistic regression if you take just simple inputs you know without without any dependencies between them it is not going to be to fit the model on this kind of data if you this supposed to be squares I think if you add the feature squared and try to fit this is able to fit it because you provide the information not only about the features themselves but you also provide the information about their square so it's able to build like the equation of the circle using those features and if you provide the x1 x2 to that feature multiplied features then let's also be able to build quite complex nonlinear signatures however however it's not going to be able to fit this data and the other way around it will able to fit this data like this but it won't be if you give just squared features you won't be able to fit this data because it only can do this circular shapes not possible to do this this distribution so basically what it says you have to you have to play a lot with the features and then as many of you know in a logistic regression that's what you do you choose choose what features to use and you have to choose the power of the feature you can't have to consider different interactions between features and manually filter from all this endless possibility of feature interactions or features what features to use for the specific data and then you know when you fit your model on data you actually don't know the underlying distribution so it's actually really hard to do and the solution for that could be if you just start using two layer networks so the trick here what it allows you to do it allows you to have multiple neurons in the layer organized in a layer such that each neuron gets all the inputs so every every neuron gets on the inputs but neurons don't talk to each other in the layer so everything every learn something like one neuron was learning in previous experiment each killer left sounding but then you can consider interaction between between these neurons in even higher level so we basically take what the first year and learn what the second neural learned and then you consider the combination of them in this neuron and this neuron looks at all of these learned neurons in a previous layer called hidden layer and then it can combine the decision boundaries that these neurons produced and produce some more complicated or complicated decision so basically in lady in any multi-layer neural network there is a full paralyzed connection of all units in the json class so input layer - hidden layer hidden layer - output layer and why it's important to to make it why in Whiting deep is that it's mathematically proven and there's a famous paper on the neural networks that no such neural network with one hidden layer one input layer and output layer can approximate any continuous function so as we considered before before you can approximate very easily with one neuron some logic gates like L or and negative end but actually this kind of architecture given the you have enough neurons in the hidden layer it can approximate any continuous function and it's actual rated to 30 ins Hilbert separates problem that was proved by our load russian mathematician in the end of last century so basically the width is important to increase the representative power to let the network be able to represent some complex continuous functions because well in the end what we trying to do we trying to estimate those parameters that will tell you what's the function of the data given the inputs and given the parameters at the same time just having width is not enough because it it's not guaranteed the training of the rhythm we'll be able to find that specific function even if you have endless hidden layer and you know unlimited number of neurons and hidden layer it's not guaranteed to converge to the correct function and then this layer when it's to because maybe unfeasibly large the computation become very expensive and then it might over feed the data that you have so it also very hard to properly generalize without overfeeding well this empiric ID lines there is no there is no different algorithm how to have to choose the architecture in terms of depth and it it's mostly empirical there are some theoretical insights that are started looking and how if you add one more layer but you use the bits that would help for specific process of function but there is no you know global theory behind that and it's mostly done empirically over time over the course of years the many interesting paper recently was a paper called learning to learn by gradient gradient descent by gradient descent so basically they took a neural network and it trained the neural network to estimate the sizes or architectures of other neural networks to make them the best for specific datasets so there are people are trying many different things how to have to figure out what to use yeah so basically this these are outputs for classes as it was for them for this guy yeah this case so this is output layers basically the probabilities per class for binary you need one right because you you get one value and it's going to be probability let's say 60% then the second class is going to be one minus that probability 40% so with those tangent functions or the others you don't need like two to the N output neurons like to calculate even we what do to them so if they're binary and you have like 16 classes you have to have to the form right out of all the combinations to get 16 yeah I mean you can have as many other boots as you need basically per class you can have thousand outputs neurons for it for each class and they're going to be because how you formulate the logistic regression they're going to be normalized so they're going to be from 0 to 100 single one of them every single one of them so that's very common when people have thousand classes of images something and they basically you do you classify your output the probability for particular image being of one thousand classes so you get a vector of thousand probabilities here and that's the highest probability is going to be the most probable class for that image yeah so here basically as Jeff said it's hard to choose the network and there are some caveats you know there if you increase using that increase the capacity of the network increase the representation of power so you can see here that for example tricky then you're on 6000 neurons you it fits better but at the same time you start over fitting the data so you lose your generalization and start pinning specific data set to avoid that you can use regularization like simple l1 l2 will do also there are specific things called dropout when you switch off some neurons from time to time for example this is l2 organization with different regulation terms you can see how smooth is a little bit the decision boundary depending on how much regularization you give so this is usually done by some kind of cross-validation on a training set you estimating these hyper parameters and trying to find the best combination for specific tasks for specific situation so and when you learn the weights in this model is very similar to as we discussed before in logistic logistic model logistic regression weights are randomly initialized there are small random numbers to avoid the situation when each neuron learns the same so they initialize a little bit different just sample from some uniform distribution and something called forward pass they performed and former pass forward pass is what we already did you take the inputs you pass the values to the hidden layer you multiply by you know the coefficients you add the bias computer a function here and then you output the probabilities after that gradient of the loss function that you have like a sigmoid logistic function loss function for example that is basically calculates how your outputs are close to the true values so basically estimate the error of your model compute you can compute the gradient of this loss function and then you can what they call to back propagate the gradient because you want to know the gradient of loss function given to respect to all the parameters but you not only have parameters here you also have the parameters here so you have to but propagate the gradient backwards to estimate all the gradients and after that you can use your favorite optimization techniques such as stochastic gradient descent to minimize the to maximize the likelihood and minimize the negative log likelihood and there's back propagation I'm not going to talk about it how it works how you estimate the gradient over the whole model but if you want you can google it separately it's a beautiful algorithm it's one of the best algorithms that I seen in my life it very simple works by the chain rule and it was not even proved to the current state until I think 80s or 90s only in Ages or 90s they actually came up is also a relatively fresh fresh thing but it makes it easier to train many many many error models by propagating the gradients and then let's [Music] anything needs to jump out of the local minimum yeah so that's a good question that's a moment which I'm not gonna go for because I'm out of time already but the short answer is that empirically it was showing that when you have multiple layers and many neurons in the layer the data manifold that you that you are trying to optimize on the manifold that you has the local minimus and Maximus it's relatively smooth so if you're using good optimization techniques such as as the SGD vis momentum or some of the modern ones like a de Delta at the atom there have they're able it's so smooth that are able to get even even get a local minimum they are able to get out and then it the only way these guys are converging because you're able to find a global minimum right so they're the surface of the manifold is so smooth that generally it works but again you have to guess your architecture right if your architecture is wrong if you have wrong number of layers run number of neurons might not converge yeah discrete demo so let's have now two neurons on a second layer and then I guess we're not gonna do the simple classification let's start with this one it's all well it can it affects it affects the order that you get information about about the manifold yeah yeah yeah that it might especially because if you use a stochastic gradient descent which takes leading patches it doesn't look older all the things at the same time then yeah then it's you know it's randomized so every time you restart even here if you every time you restart you don't converge to exactly the same solution unless it's a very simple problem so yeah basically having two neurons you see how they learn this boundary in this boundary so instead of boundary now we have two so it's actually able to pick up you know most of the points still having problems here in the middle with some blue ones and then still having some orange ones on the side so apparently the capacity of this New York is still not enough to learn this kind of data set so what I can start doing is it can start tweaking the tweak in the network and see how many neurons you actually need to learn enough there is to represent this situation well now if you have three you know they have this part of this or this so it has like one two and three decision boundaries so one two three still close but not enough but the simple yes would be if you have four of them across and then from bottom up that would be easier so yeah basically it converges relatively well trying to with the blue dots you can actually add the regularization here I'm going to do this right now but that would improve avoiding this you know this situation add more will fit even better so five neurons fits perfectly maybe even out overfitting instead of doing that you could try add one more layer and see what that does so adding another layer also helps because this neuron in the second layer is combining two of the two of the decision boundaries and this combination helps to segment out this part and this neuron combines the other way around so it combines two segments out this so very easy to see that more neurons you have especially in a second layer it's able to build very complex where very complex combinations of decision boundaries so again linear decision boundaries one two three and the second level is learning combination of Oh base two of these two and this learns combination of all three actually but is it as a chole so yeah also that's a pretty good job however the capacitive network is not good enough to learn this kind of data because especially because it's it makes a full circle and then has a little bit in the middle so just combining lis many linear boundaries is not able to figure out and actually come lab converging so that's one of the one of the situation when it cannot find proper representation cannot hit the local of the global minimum so what you can do we can either build a very wide network as we discussed before and then it will try to converge and I will try all the different things and you can add regulation to sort of smooth this situation when tries to come up you know some some solution for for this problem at the same time it sometimes it's easier to add layers if you reduce this but you second layer might be surely let's do a little function when you have a big of networks sigmoid and tan edge become not very stable so really is more popular in for practical purposes and for converges probe purposes and then you would have to wait a little longer but yeah so I think this is enough so you can see already you know it does fair in a good job there is a little bit of the still not fitting here or you know not true there is a this data in that area yeah so it's still still not zero but pretty close so that's a pretty decent job on the on the spiral data so that house here shows you have the capacity of network either one layer very big one layer or multiple layers kit like see if you have one layer with four neurons but we have two layers with two needle each do the same purpose no not exactly so sometimes because you have to have enough in a simple decision boundaries to start making the combinations if you make the first layer very small it's not going to be enough for a second layer to build these configurations so second layer is building like a combination of first layer decision linear decision boundaries so if you don't provide enough then second layer might not be able to build enough or if second layer is too small maybe those combinations are won't be enough so you need to like you need to build three combinations and you only have two neurons and a second layer so it won't be able to do that so that's why people started to do the deeper networks because as you go you're you hit the layer if you doing everything on one layer the computation grows much more expensive so it's very expensive to compute one big key to the layer versus a couple of couple of layers with this smaller number of neurons and then it was hard before the perfect mid propagation algorithm but it's much better now because of that efficient algorithm how to evaluate these deep networks so and then I'm going to talk a little a little bit about the convolutional neural network and then basically it's a fully connect so this fully connected multi-layer networks don't scale for images because images they're data that has two T's 2d structure so for example for one 256 256 by three colors RGB image one fully connected neuron that we discussed before he would connect to every single pixel in there and that would have almost 200,000 parameters so you take number of neurons and number of layers and the simple idea is a restricted connection between neurons so not not every neurons connected to other neuron or input such that if each hidden node is can unit only connected to small subset of the inputs also you want to use the the patterns in the image that there are structures within local region for example you want to look at just onto one pixel but rather other pixels around in this pixel and then images are stationary meaning that some features that we can find in one place in the image we also can find another part of the image like edges if you have an engine this pretty much you might have the same edge in other parts so the same feature you might reuse and have to do that basically these layers of the neurons they are reorganized in the 3d volume so instead of being just one array of you know linear layers you put them in the box that has the width the height and the depth and how do you do that and why you do that there are two or three two reasons for that or two tricks the first is you connect each neuron to only a local region of the input volume so this neuron that is you know one of the depth depth blocks in the in the neuron block each neuron is only looking to the corresponding part of the image it doesn't look to the whole image so it doesn't have to have the parameters for pieces here pixels they're only for this little part in this can be four by four by three so that you'd only have time sixteen times three parameters instead of having 32 by 32 by three parameters and basically that was the area that he sees it's called receptive field or filter size because it basically how many parameters you have for neuron and that depth the depth of this receptive field is always equal to the depths of your image so for example if you have RGB you have three colors you might have medical image that is volume that has bigger depth so it's always going to see the full depth but limited X&Y dimension and depth of the output volume so the size of this block is actually a hyper parameter corresponds to how many filters how many this neurons that would look at the same area in the image you want to have so this is like five features of five filter that would all of them will look only in this area but they will learn different things from that area so this might learn an edge this might learn some circle structure this might learn some noise or something like that so all of them are looking only in one area it's a step it's a next so originally with CNN's you're assumed it consumes all the depth the whole hold that they will take for my four by 50 slices that you have in the image definitely you can you can go fancy and start making the propagation and dep you can do four by four by four and then snap only slide because this guy is going to be sliding across the beam across the image but you can also start sliding in three dimensions so then that's like a next development and the second trick is parameter sharing and this one is maybe a little tricky to understand but this is basically saying since images are stationary so the same features that is useful in one location can be useful another location let's limit for each of this filter or for each this depth plains or depth slices let's link the parameters for all neurons in one slice so basically all neurons in one slice are going to be have the same set of parameters so humans are looking here mutants are looking hit in different like one urine is looking at this for another one is looking at this for that look here and here all of them are going to have one set of parameters so because they're looking at the at the areas of the same side of the same size all of them are going to have for two by two by three number of parameters and the set of parameters is going to be shared between all the locations basically what it means each each filter is learning something corresponding to different locations in the image so it can learn an edge but it would try to learn edge here edge here edge here edge here and it will learn it would work like an edge detector so that corresponds actually when you do this it can be computed as a convolution when you take this weights or you take the neuron and then you measure you convolve it with the area here then you look here look at the area here then here and then here slide that neuron across the image so that corresponds to convolution that what people in signal processing and image now this will do before what it does is fairly simple operation and it's not going to work unfortunately what it does when you look at the one area you take so big numbers are image values and the small numbers are your weight values or your fuel filter Mouse what you do you multiply element-wise your values of the image by your corresponding weight weight values and then you make a summation and you output then what you do your move your filter to the next to the next 3x3 part of the image and you repeat the same with the same values of the filters so for example this filter actually learns a cross pattern because it has 0 here 0 here here and here and once on the agonist so it learns the and then it learns the cross pattern and this value somehow represents the how much of the similarity this part of the image represents to a to this cross pattern if there is a cross pattern it's going to get bigger number if there are if the part of the image has exactly zeros on the diagonals and once otherwise that would actually have 0 so no cross pattern so that's and then you keep sliding this across the image and then you fill up the control feature no it's it's hyper parameter they're usually fairly small from 2 by 2 I think the smallest and then you can go the people go to like 7 by 7 if you have a large image because you want to slide quickly and then pick up the big patterns if you have a big image and you have to buy to filter it will take forever to compute all the convolutions there is a nice explanation of the convolution so here yes not fitting everything so basically that's a dynamic dynamic demo so you have input volume and then this volume is just basically the depths of the volume these three colors that cut them different different pieces so you can just look at one color for example you have the one filter corresponding to a set of neurons in first depth slice when you when you have the volume of the filters this is the first depth slice and this filter these filters are for all neurons in there what you do you take the filter and then you slide it across the image and then you compute this convolution that we just discussed by element-wise multiplication and you put the output to the corresponding output volume and again because you have filtered filter number one and filter number two in your output volume they're going to correspond to depth slices so it's going to be you know first slice of that depth output and the second slice of the depth output so when you perform this convolution when you finish this convolution yeah you can you can do know you can do this in real life you can go by one step you can jump one you can jump to for example if you have a very large image again but your features are relatively sparse you might not go everywhere every single step but you actually wanted to jump and find something or if if that make sense yeah so what they actually learn if you take some of this so so that green input that was you know showing it's actually slices here so one filter here another filter here when we learn those values and we want to look what they actually learning in a multi-layer network you can look at the output of the you know a few first layer filters and these are on the RGB natural images so like photographs so they learn some structures and patterns and some speckles and stuff stuff like that you know doesn't make any sense but you can see how there are some different colors in different patterns so it does learn different things so it converges to learning different patterns if you look at the second layer already start showing some of the more complex structures some of this I don't know what's this some circles of some particles like that some other more fine green textures and if you look highl a network for example fourth or fifth convolutional layer you can see that's much much or complex patterns are learned in in the higher levels so what it does because it's again using the combinations of the previous layer output to come to combine them and build more complex representations it that combines these guys to build these guys and it uses different combinations of these with different coefficients to build different combinations of this and etc etc so deeper your network more complicated structures you can learn and then if you take a imagine if you take a simple you know not very deep network is like what 12 layers or something like that you visualize that filters but activations the outputs of the layers you can see how in a very in the first layer using the random set of parameters it does get sort of detect like a silhouette of the car and then when you apply the rail in a nonlinear function to it you get output some kind of features and you continue doing that and you continue combine the outputs or previous layers to to the next ones in the end you get some kind of output of very specific features that correspond to these images and then the specific features go on the output layer that produces probabilities so what sort of produces the original image to the most informative image the features that it was able to pick up from here so it's not using the full image in the end it uses only what was filter down so it sort of doesn't like automatic feature selection not only feature extraction but also feature selection based on the training and then as a regard as before so that puts the bunch of probabilities and then for five class classification would be something like that that outputs probability for a car probability for track so none of them are zero actually but if you look at the maximum probability it would correspond to one of only one class so that's why it's actually a regression if you want to do regression it is a regression but in probabilities but if you want the classification just pick one that has the highest permeability and how deep is the deep it is smaller so these are the it is smaller so every next book of activations these guys that you get they're getting smaller and smaller and smaller because you come off and when you come off you get the output that is equal to your original image times your step with which you convolve and then negative one so it gets smaller and smaller every time yeah you can look it up there is a it's it because because of the convolution operation it gets smaller and smaller every time you can do padding and then I'm gonna finish so how deep is the deep for example 2014 was very deep commercial neural network who's nineteen layers and and these days last year for example that was a paper that was state-of-the-art 2016 a 1200 layers and this is important for very specific Asian problems is very large datasets and then as revolution of depth was negatively correlated is the error on one of the famous datasets the big challenge and the computer vision recognition it has I think few million images and a thousand classes so the error was 2010 was 28 dropped significantly and now it's below 4% and 4% is like a huge human error on that data set so now this lepers perform better than people on this specific data set so I think I'm gonna skip structure the summary there are some challenges associated is using these models because they require lots of data and then you can address this a little bit with some specificity specific techniques but still you have to have a decent amount of data unsupervised learning doesn't work so if you have very little number of layers that might create a lot of problems for you it's not a silver bullet doesn't solve any problem and then a lot of people tend to use it as the overkill so they could solve something using similar algorithms like linear regression or and the first or something so you know it not necessarily to get all that complicated you should try the simple things first at the same time it's not Nautilus works in other words to try and you know you need GPUs to train them and then there was a large body of work in theory visualization interpretation on how to interpret the outputs of these models there is a lot of lot of different frameworks you can use and then I'm gonna skip most of the applications because the amount of time just want to mention 2016 so we can do you can do the histology images you can do CT scans this is a 3d CT scan when they slide the filters of three dimensions instead of two dimensions and then you know have a very good organ organ segmentation quality so again 3d so yeah for example this 2016 twenty seven twenty seven layer CNN trained on the budget extracted for whole slide images of these cells I think it was breast cancer yeah and then a you see of neural network was or ninety two and a half and then board of pathologist also did evaluate them and then average pathologist was ninety six and six and then when the combined your network and pathologist because they were not very well correlated they actually were able to push the accuracy up because so pathologist and neural network perform better than just pathologist there aren't 85 percent reduction in this reality of humor rate and if this is interesting work also from last year they combined this detection of of something oh I think it was lung in lung there they were detected lesions or I guess that here I don't remember so but the fact they were training together with the vocabulary that doctor is used to describe images so when it was detecting these things they would find the lesions inside the neural network is outputting the response did the probability but also the description so this is machine generated text that would describe what the does machine see in human language so when it comes to interpretation this is very exciting work that you know doctor doctor not only sees where it found it in the probability but it also describes in the human language some features of that couple of Google papers from last year detecting diabetic retinopathy in eyes with 99% accuracy with 48 layer CNN and this this year paper in nature dermatologist level classification of skin cancer so they took two different types of skin cancer compare it to 25:21 board-certified dermatologist and then CNN was performing on par with 21 dermatologist so we can take 21 people you can take 48 layers of CN and it get pretty much the same result so that provides a lot of excitement in the industry there are a lot of stars that startups they're doing this kind of stuff and I think I'm gonna skip my work I'm gonna go straight to questions it's okay what work is unpublished so too early anyway sorry I got a little overtime that means age at the at the edge of your speaker is more difficult to learn second when you do convolutions you are drinking yeah so does that means stuff at the edge of your input layer is in the middle right right so wait where is the convolution yeah so when you when you have this edges of the image as it was correct mentioned before you can path with zeros here so when you start the first convolution it would it would Center right on right on this pixel and as you move you would Center on the each pixel neighboring to it or even if you don't pad these values are still contributing to this so there is a contribution from edge values here so when you start the next layer and you start involving with this this already have like a representation of the edge information from earlier stages so it sort of does like a summarization of these patterns into the next edge edge parts but yeah in in the demo that I was showing this actually does the actually does the padding you can see this image is actually has zeros on the on the outside so that's that's the padding because they actually image here but if you want there is to have more information about edges you can do this kind of trick necessarily I'm actually a couple of models that I was using there in different frameworks but you know there are so many of them so as and then gets one of the researchers tweeted this this month he said but like we saw 2012 Catholics of 2013 theano is 14 torches 15 and tensorflow is already sub 2016 because there is no library just came out last month my torch and that combines Python and torch which is was Facebook deep learning library and that was very fast very easy to use so tensorflow is going to be a big player in the game and then yesterday actually Google released version 1.0 so it's like an official release now so it will be definitely still big player in the game but you know the landscape in changing and people are using many different libraries the Charis library was developed separately from tons of flow but yesterday they announced they're going to merge these two ones so it's going to be chaos inside of tons and tons of flow and I used I used CDN NIU storage I used I didn't use standard form for practical purposes carries for crack caris I used a little bit yeah I would say tons of flow is definitely if you want to like start from your you get familiar with now you know network standard flow is definitely a safe bet is going to be around for a long time and it's already like a stable version stable API so it's a good place to start good place to try using it for for your problem also my torch worth looking at I think all right