[
    {
        "start": 165.569,
        "text": "that you're welcome everybody my name is Tom Schmidt I'm a faculty member here at Michigan in the department's of internal medicine and ecology and evolutionary biology and study the gut microbiome it's my pleasure to introduce today's speaker dr. Patrick chain Patrick's scientist at Los Alamos National Labs he leads a group there that studies metod genomes and metagenomes from microorganisms and has been hanging around an hour some recently because his wife is a resident in pathology and so once I heard of that I thought well Greg this is a great opportunity to hear from Patrick Patrick I both worked on microbes and microbial communities in a number of environments and the classic approach in microbiology the way we blown that I think we still know the most about microbes by isolating them and growing them in the laboratory that's how we study microbes the problem "
    },
    {
        "start": 229.26,
        "text": "is is that lots of microbes that are in the environment in the human gut or elsewhere we haven't yet cultivated it's my personal bias that it's because these microbes have formed networks with other microorganisms and when we pull them out of that network it's difficult to provide everything that they had been getting from other microbes in their environment so one of the approaches that has been developed to circumvent that requirement for cultivation is just a reaching of the environment grab a sample extract all of the nucleic acids there and sequence them so great don't have to culture the organisms to learn something about them but the challenge is now what do you do how do you interpret those terabytes of nucleic acid sequences that you get from the environment so Patrick is one of the people who is really pushing the "
    },
    {
        "start": 289.32,
        "text": "boundaries on how to do that how do we make sense of these shotgun metagenomes and transcriptomes the human got in from up from other environments so glad you're spending some time in Ann Arbor and thanks for joining us today thanks Tom is this on you can hear me yes good so thanks for the invitation it's a pleasure to be here though I'm in Ann Arbor quite frequently these days but still a pleasure to be on campus and talking a little bit of science with people that I can see not be a video screen which is where I conduct most of my work these days I'd asked Tom what I should present or what flavor I should present how much background and he suggested I should just go with my gut so I so you can blame him for anything that that you don't like about this yes I I know I "
    },
    {
        "start": 352.58,
        "text": "know yeah yes that's what I was inferring so for those of you unfamiliar with Los Alamos I thought I'd give you a brief two second tour since I was told by some individuals they weren't entirely sure of the environment but it's located in this lovely place which was from what I understand the only secret city in the US and it was a just a postal code in Santa Fe for quite some time before it became an actual city of Los Alamos I mean it's in northern New Mexico we we're at 7,000 feet elevation we get nice snow Wow sometimes we get nice snow in the winter ski hills about ten minute drive from my office elevations about 10,000 feet at the top but you know that's just the environment this is the the city the county itself is only about 18,000 plus people the the lab itself employs almost 12,000 so you can imagine "
    },
    {
        "start": 412.58,
        "text": "the types of discussions that go on in the food aisle at the grocery store however despite this large number only about 200 people in bioscience division which is where I'm located it and most of the people Atlanta also asked me why why are you here and what what do you do because it's a surprise to them that biology atlas Alma's so this is the type of slide that I opened up in in any talk that I give at los alamos is to try and poke the physicist and and the the modelers in terms of why genomics might be a type of science that that the national lab should really invest in and embrace with open arms so it's it's a big data science with a proposition to change the term astronomical to genomic oh I'm a proponent of and you know the microbiome is one the cover whereas supernova machine brain and fusion energy are just relegated to the top so I present this normally as my first "
    },
    {
        "start": 474.21,
        "text": "slide at Los Alamos so this large data capacity or this big data for genomics is driven primarily by these new technologies we were not stuck but we were limited to one sequencing technology for about 20 years and then all of a sudden all these newer technologies came out with a far higher throughput at a reduced cost and this opened up a number of different avenues it opened up avenues for different types of research we're not just doing single isolate genome sequencing anymore but we're doing RNA sequencing there's chip seek there's amplifon sequencing there's a whole slew of different types of goals or applications that we're applying this these technologies towards and now there are centers around the world that are doing exploratory research using genomics and pushing the boundaries of whatever science they're conducting in biology so with the advent of so many "
    },
    {
        "start": 535.53,
        "text": "new technologies especially with big data comes an influx of algorithms to try and deal with this new type of data and so this is a nice list compiled by noon open CEQA saying or EBI listing the growth of number of just alignment tools these are tools that grab the individual sequence reads coming off your machine and throwing them against a reference or a reference database and seeing what sticks and where and so you can see that there's a challenge in terms of you picking what tool you might want to use given your particular goal of interest and so I've been answering this type of question for years whenever I'm being approached by colleagues or friends you know what what should I use and I'm glad to have seen that this bound from Attucks company in Germany came up with the same answer that there is no best reel/line read a line or it really just depends on your goals the type of input sequence the type of "
    },
    {
        "start": 597.28,
        "text": "chemistry and everything else that comes along with with science you you have to weigh what tool you're going to use depending on the question you want to answer and so I decided to spend most of my time not making better slides but researching Google with images and I thought I'd create this analogy for you so if you have a single tool there's a number of things that could go right there's also a few things that go can go wrong but you need a reference database for your tool even if you only have one tool there's a possibility you're not using it correctly or at least not as the user the designer of that algorithm or that tool intended perhaps you're using different parameter settings and that's why you might your results may vary of course there are different samples that may result and you know something that's similar but slightly different there's sometimes they're quite obviously different and then occasionally depending on how you look at the data it might look the same as what you've seen previously but really "
    },
    {
        "start": 658.9,
        "text": "if you take a different angle or slice of that data you might see it differently and then there's different tools and they've all the different tools have been primarily designed by grad students to fulfill their thesis project which is a specific goal with a specific data set with a specific sample type or series of samples so there's some cautionary use so with all of this complexity of these various technologies new chemistry's that are being thrown at us all these new applications that we want to use in various questions even if you're looking at the same sample this poses really a pretty grand challenge for most volatile medicines and most biologists you know what should I use and how and so I tasked this grand challenge to two of my very talented developers to try and solve and we created this what amounts to a suite of tools or a platform that we call edge "
    },
    {
        "start": 719.92,
        "text": "bound rheumatics which I'm biased I will try and sell to you but it's open source you can look at it yourselves and see whether or not it accommodates what you want to do given your data um so you might also be wondering why would someone at Los Alamos be playing in genomics and I posed this question to Brian and Tom earlier today but they said of course you know Los Alamos has been in genomics for a while many of you may not know this I did not know it before joining the National ID system GenBank was initiated at Los Alamos and handed over to NIH about six years later the human genome project really started at the National Labs and by the National Lab systems ostensibly to look at radiation effects on the human genome but they quickly shifted Tunes once NIH through much heavier weight and dollars into the project they shifted more towards microbial genomics metagenomics "
    },
    {
        "start": 780.069,
        "text": "fungal genomics and plant genomics to address other key Department of Energy missions but we've had a slew of relatively large highlights in the genomics arena at Los Alamos and our latest attempt is trying to push the boundaries of metagenomic data analysis using a variety of bound from Attucks tools which I'll tell you about soon so this is a cartoon depicting some of the workflows integrated within our edge system it was recently published last year it's a completely open source and we were tasked with this challenge to make all these labs that are acquiring sequencers become proficient in genomics or proficient in data analytics as you might know if you've ever used the sequencer it's pretty easy nowadays to generate a lot of data it's normally much tougher to analyze it so that was our task this is open source we've made "
    },
    {
        "start": 841.42,
        "text": "a cloud compatible you have the VMware image it's in a docker container you have plenty of documentation and we have two web servers the Los Alamos National Lab site due to security reasons is now prohibits you from updating or inputting any information or analyzing anything new but this other site is completely open and also harbors additional tools that weren't in the original publication in essence the workflow can be relegated to this all you really need is a fast queue file or a series of past few files which is the data that either comes straight off your machine or it's considered the raw data from any sequencing machine these days we have a number of different processes some of these require an additional input for example if you're choosing to compare your genome to an existing genome then you obviously need a reference fast a or a GenBank file or a GFF file and then we create a number of outputs I think the "
    },
    {
        "start": 901.72,
        "text": "trick here is that it's all encompassed within a web-based framework and you have page projects that display all of your data in the same way that you submit it now run you through some example really quick we essentially used or recycle the number of pipelines that we had either used for data analytics or that we created ourselves to address certain challenges using new next-gen sequencing technologies so this is one of the home pages that you see the only thing required is a project name and for you to point to your data you can point to an SRA file if you if it's open if it's open to the public and it's in the SRA repository or in a repository or you can just point to a mountain drive with all of your data and you can just select all the data from there and then you simply need to toggle on or off a variety of different processes there's documentation behind everything that's being done and all the defaults and you "
    },
    {
        "start": 962.32,
        "text": "can modify some of the default settings by clicking on any tab so this was an example that I ran for live or what conference I didn't think I'd be able to do a live demo here since I wanted to cover other topics but you're welcome to go to this site and test it out or these sites so if you just plug and play this it'll also point you to the full-fledged site once you set up a run and you've selected what modules you wanted to run it does real-time tracking of the data processing and they'll tell you what step you're at it'll also tell you their usage of whatever server that you've opted to choose you can modify a number of these settings as well and you can look at a live raw log of all the commands that are being issued it's also stored in a log file you can delete projects share projects publish them to the open etc as you go through things will keep progressing until it's all done and then you have these tabs that "
    },
    {
        "start": 1023.209,
        "text": "you can click on you'll get a summary of how much time anything took and whether or not it completed successfully you get readouts of whatever step that you used we provide summaries that we find useful but if you find something else that's useful it's actually easy enough to modify this output file so that you can view it regularly but here we have provided a pre and post analysis view so this is just a QC module where we're trimming and removing poor quality data and then you can just click through some of the some of the graphs that you might be used to or might not be but all the explanation is online on how to read some of these graphs to assess the quality of your data and how much data is retained this is a taxonomy classification module I've I'll go over some other examples from other tools but here we're running a number of different tools we have more integrated into edge if you want to use them but we have limited to some of the ones that are "
    },
    {
        "start": 1083.99,
        "text": "being commonly used out there as well as to our favorites we're biased of course because we developed some of the algorithms ourselves but I'll show you the use cases of some of these but we provide a variety of plots we're working on making the heat map interactive we have the radar plot we provide Crona pots and dendogram that you might be used to everything is toggleable also if you click on the directory you get access to all the raw data outputs as well as the converted PNG files or we also have access to genome browser straight from the website and you can see either the annotation of your context or the reference genome and it's annotation and any pileups that you've run using your read mapping or contig mapping to the reference and you can look at either variance or polymorphisms within the community that you're sequencing or within the isolate we've also integrated into this full-fledged phylogenetic "
    },
    {
        "start": 1144.86,
        "text": "workflow one thing I didn't mention is every single module will accommodate either context or weeds and so all the analytics are done two ways to hopefully converge on a single answer or at least highlight where you might have some issues in analyzing the data or some complexity in your data so these are just examples out puts from from the site and hopefully both your read and your contact analysis come out at the same location for your followed by logic I thought I'd expand upon this because we're mostly because I'm trying to pressure my postdoc into writing a paper which were almost done and so he's provided me these slides last night which I've yet to completely have gone through but I'm assuming he did a good job here we allow any type of input so reads contigs or reference genomes you need at least one context set or complete genome set in order to do everything here so in general we just find a core genome alignment we generate "
    },
    {
        "start": 1207.169,
        "text": "this sniff matrix I mean for a phylogeny using either and maximum likelihood or maximum likelihood like method and then we also tacked on a molecular evolution package to be able to look at positive and negative selection in one or more genes in your genomes of interest or lineages of interest and so I thought I'd run you through some of this quickly we have five major steps like I said you need one type of reference it doesn't need to be a full reference it can be context we run nuclear which is a relatively rapid complete nucleotide aligner you can also run bwa mmm and then you can just do find all the sections of these genomes that align to themselves we have a pre filtering step to remove all duplicated regions within a genome that might confuse evolute evolutionary inference and then we map reads onto the reference we actually have a mash algorithm to try and choose "
    },
    {
        "start": 1268.85,
        "text": "this is the best reference to map to first but then we map that and then you calculate the core from the core genome we extract all the columns that contain snips and from the snip matrix mean for a tree and then from this tree and the alignment we it also needs an annotation file so we decided to test the general methodology see if it works on math there are a growing number of approaches that try to look at more of a complete picture looking at whole genome snips but there are they're few and far between very few of them are I think one of them might have a GUI most of them only work on reads one of them might may work on context as well or they only work on context or reads this one has a lot more full-fledged set of features just to allow a bit of flexibility in your analytics so this is a tree of e.coli just recapitulating all the known "
    },
    {
        "start": 1329.78,
        "text": "clades in the right order the core genome size and other statistics are all available to grab from this this is a almost invisible tree that also includes a number of genomes from Salmonella Shigella McCoy the goal of Fame because it's a nucleotide based end of the alignment based tree inference is really meant for fairly closely related genomes so we wouldn't normally go outside of genus boundary or below 85% identity itself but we recapitulate all the proper groups we also discovered a few issues with nomenclature of us curricula that fall outside of Salmonella even one of them was renamed just a month ago so you feel that this is on the right track I also did this we also did a tree for a very phylogenetically confused genus of bur "
    },
    {
        "start": 1389.87,
        "text": "cold area there's a number of different species that within Berkeley area that are very tough to resolve using a number of different methods and so we want to dig into this I don't expect any of you to read any of the stray names but I want to use this one in particular as an example to show that we so we grabbed all completed genomes that were published and in the database we also grabbed all the initial contact assemblies that we had done it with in Lionel's since we generated some of these genomes we also grabbed all the read data sets that we deposited in disarray and we threw them all into the same phylogeny just to be able to show you that in green or all the reads and that you can see that there pair matched with either their contact set or with with the completed genome so we feel that you can do your analytics straight off of the reads there's no need for assembly ahead of time because we place it in the same location as we placed some of the other ones we've hope this "
    },
    {
        "start": 1450.29,
        "text": "is just more Burkle dariya we zoomed into the brook of there is a patient complex was the years of interest at least to one of the medical doctors at University of Michigan john liPuma has done a lot of work in in this particular group and we have identified a few miss named organisms that fall outside of the entire clade have seen as a patient now I'll really only bring this up to highlight the fact that genbank and refseq as good as they are they are littered with a variety of different issues one of them is miss naming I found bacterial genomes that are in the viral genome database simply because the strain name matched a bacteriophage name that was oddly named after that strain also I think c.difficile was mentioned during one of my discussions today and c-diff was changed to PD pepto Clostridium difficile and of course because the medical community wouldn't be able to "
    },
    {
        "start": 1510.59,
        "text": "accept not saying seed if they reach Ange that - Clostridium difficile so so that's an accurate taxonomy label now but the point is that things can change and so so many things are very difficult in analysis for example if you're using one tool your favorite tool but the database was created a day before CDF changed to PDF or maybe the day after and now it's changed back and you use two different tools and you're trying to look for confirmation obviously they're not going to confirm the same name because the same name does not exist so these are some of the issues that we're trying to tackle as well but I will go into that a little bit later we also applied this to sacrum ices cerevisiae and neighbors or the entire genus of sacrum Isis as well as to the recent Ebola crisis genomes that came out so up to 1,500 genomes and then all the datasets that are available Saccharomyces so we feel like we expand "
    },
    {
        "start": 1570.68,
        "text": "the Tree of Life in terms of having a tool that works on any type of data set and any type of input because this is an alignment based method you also tested its use in metagenomic samples so we had a couple of samples that we acquired from CDC or that we sequence with CDC from the 2014 German I should say German the e.coli outbreak that happened in Europe and the two Americans came back with symptoms of this 104 isolate and so we sequence these one of the samples was chock-full of e-coli the other one was moderately had a moderate amount of of data over 300 million leads were generated each so that was a full lane of Illumina at the time we at the time we assembled the data we fished out the coli context and then we created a phylogeny out of them and that was published the next year but here we just "
    },
    {
        "start": 1631.49,
        "text": "threw all the data as a raw fast queue file into this tree so we're searching for e.coli we know that we want a reference of e.coli so it shouldn't be that difficult it's the same way of fishing out contents we're just doing it with reeds instead and as you can see here we clearly get a location of one patient right near this eco is 88 the other one fits right into the outbreak so this one was positive for that particular strain this one is not and on this right side I just show you a raw read mapping of the data so if you just took leaves and you knock them against all genomes and you simply ask where does this read fit best not to all possible locations with its best this is the distribution of reads that you'd find for the two samples patient one which is right here and you see all the data mapping to organisms in this clade the patient two actually had another organism perhaps a commensal the data set but because this organism is so much "
    },
    {
        "start": 1692.959,
        "text": "more abundant the parameters used for doing snip calling or variant calling get this e.coli in our pipeline placed this sample right here even though it's a metagenomic data set so as long as you have a clonal I slit that's a dominant in your community we feel we can process even raw metagenomic data using this tool so in summary for the phylogeny part you know we've this is designed for closely related genomes it's based on whole genome core alignment and whole genome snips it accommodates hundreds to thousands of datasets relatively easily on a traditional server can process virtually any type of input it works across the Tree of Life or at least small eukaryotic genomes we have solutions for larger ones as well we track coding non-coding as long as an input file is given for the reference or for one of the references so that we can identify synonymous and nonsynonymous snips we "
    },
    {
        "start": 1753.289,
        "text": "can identify pretty easily nomenclature issues or issues and some of the deposited isolates and we're using it now to try and identify incorrectly named organisms throughout NCBI taxonomy we can process like I said metagenomic data and we output essentially all the output that you might want to use we do not provide the best tree as possible visually but we're still looking at an array of different visualization methods for phylogeny and even for geographic mapping which there's very few software items out there okay so this is if you know what you're looking for and you know what you want to do so you have a target your favorite organism you're looking to see where it's in the tree or you're looking at a diversity study you have a number of isolates or maybe you're looking at epidemiology and you want to track the evolution of an output or something you can use a phylogenetic approach for that but for other cases you may want to "
    },
    {
        "start": 1815.77,
        "text": "simply ask what is present in my sample for many environmental studies this is already done but they use as a proxy 16s or ribosomal RNA typing but people are turning to shotgun metagenomic methods to try also to recapitulate what might be in a sample another reason to use meta genomics is maybe not an environmental sample of the more clinical use trying to identify whether or not a pathogen is present at any burden throughout any stage of the lifecycle and the infection and tissue type correct so 16s would never be able to discriminate there are many pathogens with very close near neighbors that where you wouldn't be able to discriminate with a zonal army so we've included a number of tools like I showed you earlier we generate a heat map we also do a form of contact based assignment and you simply categorize them by organism at any level "
    },
    {
        "start": 1877.69,
        "text": "of taxonomy and give you a plot where the contexts that are labeled as a particular taxon get clustered together based on GC and fold coverage which is what you'd expect for going assemble so we've revamped the tool at least twice almost in its entirety because we had limited scope at first and scope in scope crawl or creep and that expanded so things are much more secure we've added a number of new tools we have antimicrobial resistance in the virulence database attached to it now that are standardized and we've had tools to screen both contexts and leads against these two databases we're creating a full-fledged pipeline for pathogen detection and characterization based on this and the other tools that are already in there and we're looking at amino acid based classifiers as well to try and capture what some people call virus hunting which is to try and discover new viruses that are the cause of some outbreaks we vet it we have an "
    },
    {
        "start": 1938.7,
        "text": "interactive pathway viewer that accommodates a number of different type of omics data we have a post assembly binning emphasis to try and in all the context of interest into species level bins which is itself a challenge we have an RNA seek differential gene expression pipeline which accommodates metagenomic the genomic data as long as you have a reference a set of references we've incorporated we've essentially transformed all the tools that I just mentioned into being able to take long read erroneous data so we also allow the inclusion of just recently of nanopore generated rocks or an input data into it most of these tools and we're beta testing some of the other tools that are in there including the phylogeny program and we're creating a framework and an API for third party integration of workflows or tools into this system as well so we've integrated our this major effort with two other efforts that we "
    },
    {
        "start": 2001.159,
        "text": "have one is international engagement we have the project were engaging a number of different labs across the world to try and get them up to speed on genomics and on the analytics the bioinformatics so we've already distributed this package to a number of groups and from our website we know it's already been used by about a thousand different labs so you feel like we have number of groups that we could might be able to rely on to provide data for biosurveillance so we have a completely different effort on both surveillance where we would like to get knowledge as to what background levels of potential pathogens might be in any area now ideally this would be environmental as well and we will eventually get there but there are many farms already that screen animals and USDA just routine screening of different funds and then in the clinical setting one might expect or it's been proposed "
    },
    {
        "start": 2061.19,
        "text": "numerous times that sequencing is going to hit the clinic anytime now we're not too sure where where or when that will hit but at some point there will be sequencing being done relatively routinely for some samples and so we think this is a great source of information to try and gather also there's tons of data being deposited into the SRA so we've got about 10 terabytes of data or Tara bases of data being deposited every month this is just an pecan and shotgun at a genomic data in blue and so our goal is to try and funnel data directly not even we don't even need to wait for sra but we've created a submission tool now as part of Edgware you simply click a button and all the metadata as well as the information of what's identified in your sample can be pushed to a particular server and then that'll be displayed on that on a geographic map with the timeline so um in order to do this you "
    },
    {
        "start": 2126.62,
        "text": "actually need pretty good and reliable taxonomy tools so you don't want to say that there's Ebola everywhere in the world that might cause some panic but you would like to detect it if it's present so I thought I'd spend the rest of the talk really talking about metagenomic taxonomy classification which is kind of a very high active area of research and with a number of groups being used with by a number of very proficient groups using a number of different and examining different types of samples so like I had said with the haircutting example you get what you play with and so your mileage may vary depending on your goals and the type of tool that you're using so I thought I'd show you a nice investigation by the Mason lab looking at metagenomic classifiers actually have a lot of critique for all the types of tests that were done because many more can be done but at some point you have to call it they "
    },
    {
        "start": 2187.66,
        "text": "investigated maybe 20 or so tools but as you may be aware there's tools that are published almost every day and so there's this is a year old slide omics tools doesn't even have this layout anymore and so there are hundreds of different tools and just to do this one task and so these are the ones that were tested I'll tell you the Gotcha algorithm is ours and so we are a bit biased but and I'll let you know a little bit more about how we fare here this is in terms of greater accuracy they're looking particularly this F score and there's precision we call when here as well in the area under the curve there's a different metric or an F score metric or both filtered and unfiltered data as well in terms of accuracy is up or to the left we've selected tools so all the tools indicated in red are natively encompassed within our edged bomb from attic software and that's because they've run rather quickly so we "
    },
    {
        "start": 2249.04,
        "text": "did this both for speed as well as accuracy and then here we actually chose a slew of different tools in part because some of them are better for some types of goals or some types of samples than others and I can get into the details later as well this is one last chart looking at relative abundance quantification there is no standard for relative abundance of quantification nor is there a good method to test this yet at least not holistically so whatever they tested this is the the order that things came in I could come up with a series of other data sets where the order would almost be reversed so why is specificity key so high accuracy I thought I'd give this as an example this is sequencing a bacillus anthracis strain so we are labid sequence a lot of these over the past so this was one strain that was pulled with a number of other strains in a single weighing of alumina and then sequenced "
    },
    {
        "start": 2309.25,
        "text": "and you know did the columns are different tools and you can see that some tools will identify a large number of organisms even if the tail-end is very low or where our biosphere members let's say and so the top hits are pretty consistent across the board we have a couple of viral taxonomy profilers if you're wondering why those don't line up but there are a few top ones so I'll zoom in here so bacillus anthracis is indeed the top one at least in terms of average for all the tools there are a number of vassilis pages which also makes sense given the input and this Staphylococcus phage is also known to be in this source and then you'll see bacillus thuringiensis bacillus cereus as well as a few other organisms so you know we were curious why we you would see a Yersinia pestis that makes no sense same with Francis Elif Phillimore aza or cross tritium and it's only seen while it is seen with a a number of the tools "
    },
    {
        "start": 2371.46,
        "text": "our tool which I'll explain to you further how it works but essentially if we have a complete genome database which I realize we don't the algorithm will not give you a ball deposit so also if there's no errors and weeds which you know doesn't happen but at least with those two caveats we know our algorithm is extremely specific but we also identified we didn't we did not identify this bacillus thuringiensis which is one of the nearest neighbors to n places which we were happy about but we did find serious and all these others which were confusing until we look at the other samples that were run in the same Lane and if you've heard of barcode cross talk you will know that the barcodes that you use to label your samples are not pure barcodes they're mostly pure there's only point percent contamination frequently but the tools will pick up those cross talks of barcodes other barcodes that are used for other samples that are identified in your sample now this does not matter if "
    },
    {
        "start": 2433.27,
        "text": "you only care about what's among your sample what's truly relevant in your sample perhaps but if you're being sequestered away into one tent if you have Ebola well not this matters a great deal and so you'd like to be very accurate in your determination from clinical service so I thought I'd run through you this is a couple it's in the manufacturers there are we're aware of one company it's spin-off company from our software and our wet lab work that has asked for completely pure barcodes they also do dual indexing which helps reduce a great deal and then they have a couple of additional tricks to reduce this to virtually zero so there's a number of issues with the barcodes but you know if you don't care about that point a one percent or you're sequencing a nicel it you know that contamination will never show up in an assembly and won't really matter in terms of anything you really do yeah so a number of things "
    },
    {
        "start": 2496.84,
        "text": "that have been carry over but once Illumina implemented that washes between between their runs the only additional lab carryover would be lab contamination and which still could be but with some pretty good robust techniques you can eliminate that pretty well yeah so you can you can bypass this we don't use the Illumina coding software so you try and bypass that but you can be very very specific in terms of how you bin your weeds in two different barcodes you can do the assignment later oh yeah right yeah I'm I can't remember any "
    },
    {
        "start": 2558.829,
        "text": "workarounds to this but yes that's a pervasive issue that illumine is very well aware of that but um you know there's only so much they can do to cater to the 1% the 0.01% of of their industry of their market in any case so um I'll just run through you are in terms of a visual of how we created our database the only thing really special about our methodology is the database that we create and for this if you imagine for every single organism or every single genome in the database we identify at the very root of the tree all the external branches we identify any overlap between all of these genomes and your genome of interest that single genome of interest and so we identify these locations we remove them from the database or actually we simply notate that coordinates and put them aside we "
    },
    {
        "start": 2619.7,
        "text": "do this for every level of taxonomy up to the strain level and what remains is the unique fraction that's completely unique to that strain given everything else in the database so this reduces the complexity of your database by a large amount and so now we can use essentially any tool to map these reads back to these leftover fragments to see where they might belong into which genome they might belong at the strain level we can do this at every level of taxonomy actually so that you can still look at the core that's conserved within all of ecoli for example so so this is the the set up there's a number of reference unique segments that remain at any level of taxonomy that you care about and you can count the number of reads that map on to these data most of the tools out there will simply give you read count as abundance if you're comparing a virus with a bacterium or with a eukaryote maybe read count not be the best metric for relative abundance in your sample um "
    },
    {
        "start": 2680.029,
        "text": "so we've come up with a couple of additional metrics which is the percent of the linear coverage of your unique segments that you're covering as well as the fold coverage within that covered segment and it turns out that these two metrics can really aid in your relative abundance calculation this is presence/absence only however we do not imply or we do not put any cut-offs on any of the tools is that all defaults outside of outside of bwa and blasts where we used at end of 5 or 10 cut off but here's the results so you can see that some of the tools have an extreme false positive rate this is a artificial community that was actually sequence so it's a physical strains that will mix together sequencing on Illumina so we know the composition and these are the results anything negative was actually just black missing from the databases so they're actually very good at finding whatever is present in their databases but some of them are less good at "
    },
    {
        "start": 2741.079,
        "text": "telling you only what's present in your sample so we use this and this was a couple of years back though and we've been working on it since but there's a limitation to the method that we've just created which is despite going all the way up to Kingdom level we have all of these databases that we need to probe to do this and actually calculating this database is somewhat difficult although we're really trying to work around it so we thought of a normal regular approach but based on lead mapping so that you did not need to you could create databases a little bit easier on the fly instead of trying to pre-compute an index or a database and then have to map against it but from what I understand there's maybe a method developed here at the University of Michigan which I'm anxious to try formatting as well in any case so we've learned a lot from from our efforts we have collated a pathogen database oddly "
    },
    {
        "start": 2802.16,
        "text": "enough no one has a complete pathogen list not even CDC or WHL so we've collated our own and we were trying to automate that process so we don't have to do it again next year we have our Janee genome uniqueness database which comes from the previous tool I just went over you can enter in any host you know we may be concerned about ourselves but some people are concerned about their cattle or other organisms we have a particular control file we can add a custom string taxonomy since we realize there's a very big difference between our idea of taxonomy and actual phylogenetic evolution and metadata for all the pathogens that we've described in our package and database we still look at all the levels of taxonomy similar to a last common ancestor low of common ancestor algorithm we also do a rank specific maket mapping based on what rank they are most similar to which "
    },
    {
        "start": 2865.46,
        "text": "I'll get to in a minute and we also tried to put in a scoring method that was less or not reliant on abundance so if you look at any type of tool and you know what makes you confident that something's present in your sample it's natural to think that the more data you have the more confident and that's great unless you're looking at low abundant pathogens in a sample you would like to be very confident that a pathogens in your sample and despite the abundance or if you expect that something should not be very prevalent but you should expect it present you'd like to have it there should check that time okay so I'll run through this relatively quickly I apologize so this is like an LCA method we map data to all of these genomes and I'm just giving you a road example made-up example of Reed Reed that may map to all three of these strains or read that map only to this E "
    },
    {
        "start": 2926.3,
        "text": "coli strain and I read the maps to this e coli strain in this bacillus in same strain this is the matrix that we work off of and then we normalize the data by the number of references that every Reed hits we normalize by identity so most of the tools don't accommodate percent identity of their actual map they just call a presence of absence and we can normalize by both how much depth is enough we have because we have the uniques information we have maybe some idea of priors as to what will be mapping to signature regions and so we have some expectation as to the distribution of the data along the genome and so we estimate the probability of this genome given the distribution of the reads at every level of taxonomy and we know that every genome has different fractions of it that are unique at a different level of taxonomy and so we essentially take the inverse of this and calculate "
    },
    {
        "start": 2988.57,
        "text": "probability distribution from this and we can come up with a score on what's the expectation of finding you know a distribution that we see throughout any given genome in our database and we've tested this with over 700 contrive samples that were spiked with four different types of pathogens at different levels we've also tested it on hundreds of different samples that are synthetically generated it appears to have a fairly decent balance between sensitivity and specificity I didn't mention the other scoring function which is to give it a negative control so if you have a healthy sample from day one and you're sick day two you can compare to yourself in any case I thought I'd just run you through our idea of challenges you know in terms of taxonomy classification once you have the tool you think it might be easy it's really not and that's mostly because we neglect this entire side of the conversation the "
    },
    {
        "start": 3050.8,
        "text": "database side and the databases are growing that's why this prediction was out there that genomics will surpass all other big data analytics combined even NCBI has stopped the use of being able to explore all of the WGS using blast because it's simply too large and too resource intensive there are a couple of publications on problems with too much data and I just thought I'd run you through the different databases so when I say NCBI or refseq and you say NCBI rxe if we may be talking slightly different languages this is the actual distribution of data in the blast and T database this is refseq this is GenBank in blue this is the WGS WGS does not overlap at all with GenBank just in case you were wondering refseq compare has many genomes in in the GenBank in Blast NT but also outside in the WGS in the whole genome shotgun archive bacteria here outnumber and I'll paste even the blast "
    },
    {
        "start": 3111.37,
        "text": "and T database and oddly well interestingly bacteria archaea and viruses are very small fraction of this because there are a lot of eukaryotic particularly human reference genomes in here but most tools really only look at a subset of this information they do not look at all of this so we've revamped our Gacha to algorithm we had to stop using a variety of different languages and turns straight to C++ and a number of computational science tricks to be able to do this and we now have a database for all of refseq including the eukaryotes of course that was last year and so this year things have changed some of the databases has essentially doubled in size and so this is a constant growing challenge our current estimate for database construction will be about 500,000 CPU hours and we need at least you know 50 terabytes of space "
    },
    {
        "start": 3172.48,
        "text": "just to be able to construct the database so this is a grand challenge I only talked about bioinformatics we have a number of individual funded projects with other companies or spin-off companies we look at other technologies we have natural product research goals interactions or soil metagenomics biosphere or plant metagenomics human metagenomics and a number of other things and obviously this is not me and my group alone my group it's fantastic a number of great postdocs and politicians we work very closely hand-in-hand with our wet lab genomics counterparts and our microbiology group and with that I'll take any questions this is just a video of live me live doing my demo at a different time thanks "
    },
    {
        "start": 3232.99,
        "text": "I just recorded so we'll try to capture your questions with a microphone on that side in this side while you're thinking of the question say Patrick well the views that you gave in the metagenomes was with the pathway viewer what what meta meta about like potential was there of course I mean lots of us are not are measuring the metabolites - are you giving thought about how you're gonna handle getting them at a blight data eager great in with that yeah yeah that's a great question we've thought about it we haven't come to a conclusion so adding trash transcriptomic data is easy proteomic data pretty easy to that omics viewer map adding the metabolite data I think metabolomics is you know that cutting edge field that's very tricky particularly for metagenomes particularly for non typical metagenomes "
    },
    {
        "start": 3294.88,
        "text": "let's consider human microbiome as a normal environment that metabolomics is being applied to so we know a lot more of the metabolites there we can identify a lot more of the peaks in terms of actual metabolites and so we can place them on a map but they're in other environments most of the peaks left uncharacterized you have no clue what what what metabolites they are and how do you still track those and yet and try to display them they can't be displayed on the map I don't I'm not sure I'm I'd welcome any ideas we are not you know what our focus has been genomics primarily as you've seen we're starting to play with other types of datasets outside of sequence drive data but I'd be welcome to collaborating with anyone who has great ideas on you know how to visualize how to transform the data and really push the boundaries on what's "
    },
    {
        "start": 3354.88,
        "text": "possible given today's work and we have a couple of interactions with PNNL that have a very large mass spec facility and they're experts both in metabolomic s-- as well as proteomics and we're trying to work with them to try and to try and explore what's possible it's a really cool set of work so um when you were showing all the plots like this right there sorry I just went down um plots where you're comparing performance for several different tools or um the identification of species they're present how much can you gain if you just take another prediction or a consensus across all these different tools that exists yeah so that's almost what the heat map does is it kind of rank order is based on the average for all within tool normalized relative abundance criteria you know our personal "
    },
    {
        "start": 3419.17,
        "text": "preference I mentioned my bias considering we developed our tool is to turn to our gotcha analytical tool or what's really present you know virtually guaranteed to be there if not the nearest neighbor to that thing and nothing else closer to it is available in the database so that's our first go-to if we're more in an exploratory mode for an organism that we think should be there but we don't find it in ER a list then we go down to a different tool that might be a bit more lenient in being able to identify organisms we turn to our pen geo tool now we've turned to Kraken to BW a different methods might be appropriate but can you take like a formal consensus and look at how it performs on those metrics you were looking at what do you mean well so just let all the methods vote for example about whether a given thing is present "
    },
    {
        "start": 3480.07,
        "text": "right and then how does that perform on your standard statistics compared with any of ya it should be better I'm sort of wondering how much better and especially can you push the boundaries on you the sensitivity of this conversation with other groups that are I think some other groups are trying to to do that we have not looked into it in part I mentioned this taxonomy issue that happens to be quite frequent not because of changes in taxonomy but more because of when the databases were constructed some of the tools that are prevalently used in some facets of meta-genome research or microbiome research like meta phlegm you cannot create a new database you just have to wait for one therefore unless you know what's in the database it's very difficult to ascertain why you may or may not find or may not find particular organism in there so we have not really considered I think that would be appropriate "
    },
    {
        "start": 3540.26,
        "text": "especially if you normalized all the databases but we've turned to trying to optimize one or two tools to be able to catch for the capture the diversity but still be very specific still be pretty accurate in your cause to try and limit the number but looking at consensus yeah we may be I can't ask someone to that as I do nothing myself so I feel like in general especially when we're trying to validate these tools that are supposed to go down at the screen level frequently people are using artificial communities that never have strain level conflict and you mentioned that that actually might be a problem also in this case for your tool can you talk a little bit about if you were able to validate in cases where you have close relatives in the same sample and then also what you expect to happen using this "
    },
    {
        "start": 3601.28,
        "text": "algorithm when that occurs yeah sure so first the algorithm question with our gotcha algorithm we look for perfect cameras to remove sections that are identical but when we do the search we accommodate and tolerate a few additional mutations to accommodate possible evolution or diversity of strains we actually use a larger camera than was used to remove similar cameras so we use I think like a 24 or 26 murder removal you use a 30 motor for actual mapping but we accommodate to variabilities that's default settings you can augment that we have tested how that works in real situations to identify new strains we feel that that does an okay job of identifying we haven't tested what the limits are you know if you only have ten reads in a sample that map you may get "
    },
    {
        "start": 3663.03,
        "text": "rid of a few additional reads and so what your what is your lead out of that there is a big effort on strain challenge that's being run right now by I believe it's maybe FDA it's hosted by DNAnexus I believe and so that challenge is supposed to get to oh no it's a Johnson & Johnson or Johnson Johnson derivative that's looking into this and the goal there is to look for what kind of tools and utilities are really good at strangest drain discrimination I don't know if the taxonomy classifier should be truly used down to the strain level for strain level it's far better to have actual alignments with sequences and maybe use a phylogenetic approach instead to really place or characterize some of these sequences but but there is "
    },
    {
        "start": 3723.3,
        "text": "an effort and some of them use ideas like phylogenetic approach to try and deconvolute strains that said we still have used our gotcha tool and as long as it is one of the strains that's in our database you can actually place it I've given enough data Patrick thanks a lot glad to put you face to face with some people and hope you see around campus more yeah join me join me in thanking Patrick again [Applause] so since we stopped recording I can also say synthetic data "
    }
]