[
    {
        "start": 33.76,
        "text": "[Music] [Music] "
    },
    {
        "start": 127.569,
        "text": "okay I get started I just want to remind everyone that this is the last seminar of the semester we will start up again the fall and of course I'm looking for speaker so if anyone is interested and willing to talk please let me know and I think yeah I don't think I need a microphone even though that was on is it recording so is it on it is okay yeah so it's a it's a pleasure for me to introduce Ricardo Lunas he is a a third year PhD student in the bond phonetic program here at the University of Michigan started off doing a rotation at the end of his first year and has been working in the lab ever since and has made some nice progress on the project is gonna tell you about today why the idea is trying to identify in high resolution how transcription factors interact with DNA specifically using an assay called a taxi the the projects "
    },
    {
        "start": 191.09,
        "text": "he'll talk about are are unpublished so far although he is working on writing it up now and I think he's even going to share you that github URLs or this code is open and available for anyone to download and use and not advocate for people to give it a try out especially hopefully you're interested after you see some of the performance metrics that Ricardo will show relative to other competing software programs finally at the end he'll introduce kind of a more provocative idea of measuring transcription factor residence time on DNA using accessibility data only so I'd encourage you guys to speak up and ask questions especially at this stage of the research right before we're going to submit for publication we would appreciate any kind of feedback you have so thanks for coming in alright thanks Steve yeah all right can everyone see the screen from here good so as Steve said when I talked to "
    },
    {
        "start": 254.069,
        "text": "there a little bit about chromatin accessibility and how we can use this kind of data to predict genome-wide transition factor binding and in the end residency time so motivate this a little bit we know that a majority of diseases and traits associated genome-wide Association studies the genetic variants are located in non-coding regions so that means that rather than breaking altering protein protein structure they are affecting some regulatory measure in the in the cell I don't think the majority of I think yes for your loss is common disease right so it's kind of Geno's is the methyl I mean it's implicit in this state but she was yeah "
    },
    {
        "start": 318.6,
        "text": "you can't see you're very selfish okay and for Mendelian normal you have some sort of transcription factor being directly broken and that's why the effects are so high for a single variant so we can see that those this associate variants there in reached in regulatory elements such as open chromatin region or known transcription factor binding sites so this shows the regulatory role and also suggests that they may disrupt the this regulatory architecture by disrupting transcription factor binding sites so I'm showing here here's the stretch of chromosome 8 where we have at the star here this RS 5 oh wait for a 19 snip which is linked to ink 1 this is a gene expression and to type 2 diabetes and you see that this region closely resembles the steer for binding motif so therefore is the transcriptional "
    },
    {
        "start": 379.77,
        "text": "repressor this snip falls right in this position a where you have sorry position 8 where tier 4 is expecting to find MA in the in the binding site and you can break the so looking at what the non risk and the risk alleles are you can see that Aidan on risk you leave the motif intact but for G the risk allele G is not expected in this tier 4 motif so therefore I won't be able to recognize and then you would disrupt the binding site here and we have they're showing that this this is exactly the case so like one expression is higher in the risk allele so the transitional repressor is not binding and therefore is not repressing the gene and this is probably the mechanism by how this snape is linked to type 2 diabetes so being able down to predict where those transcription factors bind in a given condition can be a powerful tool for uncovering the "
    },
    {
        "start": 440.799,
        "text": "effects of these non-coding variants so essentially we're trying to understand the underlying regulatory grammar after cell regulatory architecture and the goal to is safe for this would be chip seek so you can say a transcription factor that is a factor binding advanced using this this technique but it has some limitations and the main one is that you can only say one transcription factor at a time so if you're thinking of all the possible 1,300 in fact non-transition factoring in the cell you're going to do 1,300 sequencing essays which is not practical so an alternative approach is seen for a transition factor binding using chromatin accessibility data so my chromatin accessibility data and we'll be talking mostly about attack seek so this is the say for transpose a is assessable chromatin which in for accessible regions using transpositions so it's very similar to DNA is one when "
    },
    {
        "start": 502.509,
        "text": "you start off at the na 6 sorry when you start off using the DNA is one using a transpose a is that's already loaded with the Illumina diapers so you can use less input material so instead of millions of cells that what you need for DNA sig you can do it with thousands of cells and the readout is essentially the same okay so okay alright so the question we're trying to ask to Lancer here is can we accurately predict bound more different occurrences using this chromatin accessibility data so I'm showing here this IRF eight locus where we have different different tracks here the track number one in orange is the gem to eight seventy eight ataxic data so the chromatin accessibility you can see that it varies across the focus now into here we have the ataxic Peaks for for the P calls for this region and see then those "
    },
    {
        "start": 563.899,
        "text": "regions with high chromatin accessibility got called as Peaks and then we have this sp1 motif so these are regions where the sequence match the recognition site for sp1 and we also have for this vision the chip seek data for for sp1 so we know where sp1 is binding and you can see that there are four four or five motifs here that are pretty that are bound by sp1 so the question is is there any feature in this orange track that we can use to predict which of all these sp1 motifs are going to be bound and not only for sp1 but for all the other transcription factors there are couple methods to do that the most widely used one is there are the footprinting base methods so they are searching for regions of relative protection on open chromatin so what does that means in turn away here I'm showing this empty p n locus and I have "
    },
    {
        "start": 626.94,
        "text": "the four I have the chip seek data for Ni f1 listen the transcription start and white below I have the DNA is data so the chromatin accessibility for the same region and you can see that there is an overlap between those two peaks so this is the small signal so we actually zoom in in the to cut the cleavage pattern on this on this region so that's a white box here you're going to see that we mostly have a high density of cut but right at this locus here we have you see this depletion so this is what a footprint in matters are looking so that the assumption is that ni f1 is sitting at this region and is protecting the DNA from cleavage right so between matters are looking for open chromatin a dip in the signal and an open chromatin again then there's also this voice signal approach which is the one we're going to discuss more in depth so it has been known that if you just sort your motifs "
    },
    {
        "start": 688.59,
        "text": "by the by the density of cleavage or integration events in the in the vicinity of motif that's actually a good predictor of whether that motif is bound in the chip seek data so I'm showing here this ctcf motif on the x-axis you have the relative position for the motif in plus minus 50 base pairs and the colours indicate low to high DNA's cleavage right and increase sort by the number of cards in this on the y-axis you can see that there is a good correspondence of the most occupied motifs by chip seek will also be the ones with highest cleavage okay so they already a couple of methods that do that I'm showing here data from this who's more wear out paper is this method called hint this is one of the most recent matters that we have it uses this "
    },
    {
        "start": 749.28,
        "text": "hidden Markov model for loops for looking for those dips in the signal and this is a figure four in the paper where they compared their method to a bunch of other available methods and all the x-axis here we have different performance metrics and the ranks for those methods and their method they found was the best performing method for compared to all the others a couple of caveats if you thinking of this footprinting apology so first of all this footbed base matters they only look at regions within chromatin accessibility Peaks right so is this an important limitation do all factor the bound events are in open chromatin second more important twelve factors we food bins are going to see that depletion in signal for every single factor and then if you ask for both of those types no furball does is "
    },
    {
        "start": 811.339,
        "text": "there a better more general approach this is not the first question do all factors bind in open chromatin this is going to depend on the factor so I'm showing here this bar plot on the x axis I have different factors and on the y axis I have the fraction of bound motif so chip seek bound motives relative to ataxic Peaks so this is looking at the Brandywell sodium twelve eight seventy eight there and I'm showing blue the fraction of bound motifs that are within ataxic Peaks and in R and the fraction that's outside attacks it Peaks and you can see that for some factors like brca1 100 percent of the bound events are within open chromatin regions but other factors such as if you have one srf almost half of them are outside so if you're only looking within Peaks you're essentially missing out all those red parts of the graph and to my knowledge "
    },
    {
        "start": 871.37,
        "text": "there's I should do this experience for every single factor there's no way to predict whether your factor is going to be binding outside Peaks or within right so this is this could potentially be a limitation yes so yeah but this is a very bad proxy to something else yes yeah this is a limitation but I think it's for every single one of them MLS yeah then you would do a chip seek for those guys but footprinting you won't be able to do unless you know what is the motif of the cofactor in question but yeah this is question 2 this one you show the dependency on the factor what about tissues sorry pounds on tissues I don't think it's the same for "
    },
    {
        "start": 933.47,
        "text": "every tissue for everything yes you're gonna have different factors active at different tissues so that's a tissue specific profile yes so we have data for another jumper with 78 ataxia sample and we pretty much see this pattern but I didn't run that to another another tissue again we have to have to look at that that's a good point so max dwarf actors the footprint and Panther this is not the case or at least it changes between factors so this is from our garden Haggar paper from garden Hagar's group where they are showing what is the amount of protection for different factors so they so far this three factor ctcf ap1 and glucocorticoid receptor they are showing on this "
    },
    {
        "start": 995.089,
        "text": "there's essentially the blocks here are showing the motif at the center right at 30 base pair region on each side of the motif and then they have the cut counts so how much cleavage events do you have for those motifs right and they're showing what's the difference so the first - Horizonte - line here is the average background on that motif and what is the protection at the motif right and so the further away those lines are farm each other that means the higher get the protection yes by that factor so more of a signal depletion it's causing and they found this it seems that's correlated with residency time so factors that bind longer to the DNA they leave deeper fur they protect the DNA better then factor so glucocorticoid receptor is known to bind for a second seconds sometimes maybe less than a second and then you can you cannot even see a footprint very well where ctcf is known to bind for "
    },
    {
        "start": 1056.95,
        "text": "minutes at the time and you see this really nice protection and so this is a limitation not all factors leave footprints so I'm going to talk about that but you can only do that to experimental assays so you can only you can do that to single molecule tracking so you design a gfp-tagged version of your transition factor and you look at the microscope and see it how long does it bind the DNA and there is this photo bleaching assay which I'm going to describe later around that gives this information but still it's experimentally complex because you have to design a gfp-tagged version of yours of your facts or express that into a cell line so it's not you cannot do that for every factor oh yeah here so that's "
    },
    {
        "start": 1118.36,
        "text": "a good question but it depends on where those factors I are binding so a p1 for ESS is binding at promoters so we would expect to see a higher baseline just because promoters are more open chromatin then forces to pooh-pooh corticoid receptor those are known to be pioneer factors so they might you should be binding at regions that were previously closed so the baseline will be lower and ctcf it's its own environment that's also very open chromatin so just after that you're absolutely right but also in this plot they're looking pretty close proximity to the motif right plus all is tens of faces if you were to zoom out the hundreds or KB Savoy then they would all come down that means it's because like one bad thing you might have some ambiguity big baseline of this "
    },
    {
        "start": 1179.76,
        "text": "so mapping buyers yeah yeah so I don't understand the question so I repeat Steve's explanation is because we're just looking at very small we joined mm-hmm so when you see a lot of fluctuation in a small region instead because you have some fires in your methane yes so they account that's actually one of the things that those methods try to account so if you look except so this is an aggregate pot plot so you're looking at here forty thousand or thieves twelve thousand motive six thousand motives right so with all of them this region at zero is essentially the same right because it's the same motive but the region around it's going to be slightly different and you see so you see these like Peaks here for glucocorticoid receptor this is the integration bias after the of the "
    },
    {
        "start": 1240.64,
        "text": "technique so sorry the cut bias for DNA's right so they these models actually try to try to see what are the preference for DNA is binding so they do like DNA is on a no and make itself so so without a deeper denies so and try to model what is the what are the preference and take that into account for the model but see you cannot open too much in the region that you're looking on either side because you can also have other motifs close so if you look at promoter there are several motifs co-occurring right so if you start opening too much then you're ready looking at the motif next door so it has to be close by yeah but those matters they take they try to take into account there's DNA is bias the Magnum is not making sorry cleavage bias all right so we had some issues with the way they compare there in that hint paper and we decided to look at look at ourselves so we compared "
    },
    {
        "start": 1301.03,
        "text": "five different methods on ataxic there and that's an important point that method was compared in DNA's data we're looking at attacked which hasn't been done before so JM private probably 78 and we're also including human pancreatic islets so looking at clinical samples so we are now I think that paper so the 2016 one that I mentioned early DNA's 2tf that's the paper from the left on the previous plot centipede from where peaky Reggie who's currently at Wayne State next door here so centipedes is not a foot printing method it's actually unsupervised clustering method so it looks at the pattern the cleavage pattern around each motif and try to separate between bound and unbound motifs using this multinomial model so it's not dependent on looking at within pigs is a motif based approach centipede partition which is a different implementation that we use for the nature communication papers paper sorry there takes designs taking the the "
    },
    {
        "start": 1363.76,
        "text": "cleavage pattern around the motifs is also taking the fragment sizes so if you have parent ataxic data you cannot include the fragment size into this and finally this new signal based approach that I'm going to explain so how's it feel sexually different that these methods that you waste in your method from other methods that produce a transcription factor binding you see or thank you that you know define all of those things that recently came out and with their genders visiting speaker all the masses that they use you know DNA sequence another may be information made information to predict if you defines are not only use gypsy foundation so have put printing and then what you do is there as I like a conceptual difference in the task no it's not the same because and this is my notes family "
    },
    {
        "start": 1426.19,
        "text": "those methods are supervised so you need to train them right and we're not sure exactly so if you like if you train on ctcf data that you have plenty of ctcf datasets around maybe will be very good at predicting city CF but this is not generalized you don't know how that to work say for instance you're going to use the cities you have more than one a fox a - all right so so all these methods they do need any training so you can just take your sample and stir it up give to them you don't need additional there so if it's a cell type that you don't have other chip seek there anything you can straight up do those techniques and we're gonna see how well they perform that answers yeah so interesting part and the interest part is for example be buying they they show that the method can generalize between different transcription factors in between different cell lines so you can "
    },
    {
        "start": 1487.21,
        "text": "do like a transfer learning kind of part so if you train on one roll on a number of transitions I can remember so wise use the same model to improve your predictions in another transcription factor in another that's interesting you know sort of there is something common between those different fractions difference alliance there is some kind of information can be reused again but of course if you can achieve that with my guess is that both those methods are learning the raw signal and that's what we kind of see we centipede it wasn't actually learning a lot of better and it seemed that is just learning the amount of signal on those motifs so so that's that's definitely something that we can test in the future but right now I'm just going to sequence so you don't know the transcription factor is present in a "
    },
    {
        "start": 1547.779,
        "text": "particular cell type necessarily this is independent of that saying things were bombed because they're interfering with the integration of the transposon so you actually have data that's looking at the state in whatever cell type you're looking at it's not for specific transcription factors it says this is a protein yeah and I guess I would add to that this transfer learning I think will be will work recently in a few instances but I don't think it's going to be brought yes for example Terry Fury's group just published a paper in NER about their method called DEFCON or something will they transfer learning of gf and one subtype and they perform very well in the same key up across the other cell type right so you could imagine TM generally generally right not but all these 17 RTS in the human genome some "
    },
    {
        "start": 1607.98,
        "text": "have very similar protein to me though they're not very different so I don't think TF is the number all right okay so let's see those are fancy methods can you come up with something simpler that's like you don't need PhD in machine learning to run and like even if you say that they are user found it they're not right so that's easy anything that they say it's easy to use - my experience is never easy okay so how is this grassing approach that we're talking essentially we just count the number of fragments that are occurring in the vicinity of the motif "
    },
    {
        "start": 1669.38,
        "text": "with the fancy part of the model is that we just discard the fragments that integrates directly on top of the motif so because there are sequence bias here that's the only part that's common between all the motifs we are discarding those so I'm showing here - ctcf motifs right though and then we have this extended region plus minus 100 base pairs so the raw signal score for this guy here would be 16 so there are 16 tags occurring the vicinity or this kind in these twos and then for this other the raw signal score would be 6 and then we just prank them by that and see how they perform all right so how do we evaluate those predictions for all those methods we have so this is jump over it's empty there's this richness of encode there we have chip "
    },
    {
        "start": 1729.51,
        "text": "seek data for several factors so we intersect the motifs with the chip seek mix and we classify those motifs as true positives or false path right so they're bound or unbound and then we look at the mattered prediction so saying is this motif round yes no no yes and we label them through positive to negative false negatives and false positives so essentially the data that we have for each method we have this table where we have the motive prediction score rank so the p-value or whatever score the method outputs and whether that motive is a true positive or not so a good method would have a lot of true positives early in the list and a not-so-good method wouldn't wouldn't be able to discriminate between the two boys so they will be like evenly distributed across this list and then you can evaluate them this list using either ROC curves or precision recall curves so our curves are plotting the true positive rate versus the false positive rate after they are set and then I'm showing "
    },
    {
        "start": 1791.789,
        "text": "here for each of those math so this is elf one on when we also jumped over it 78 on our region so not only within Peaks I'm doing genome-wide and you can see that we can take the area under the curve of this ROC curve and this is going to be this is going to be a metric of which math is better than the others then we have ranked here a signal 0.97 Centipede not sedition point 96 and then last hintondean a suit EF could sees those are unbalanced data set which means you have a lot more requires much more two negatives than true positives we're going to focus instead on this precision recall curves so they're giving at any particular week also if we look once we get 24 set of all the possible through politics in the data set what is the precision that we're calling them so let's say for DNA CTF when we find twenty percent so "
    },
    {
        "start": 1854.429,
        "text": "as we go down that list and we got 20 percent of the two politics what is the precision so that's around 0.7 so that means that 7 7 out of 10 of everything we're calling is true sort of but for a raw signal let's say at 20 percent that precision is 90 person ID right so 9 out of 10 are real and then you can imagine a perfect method would do 100 percent of the recall with 100% precision so be shifted to the upper right here and again we can take the area under the curve and directly compare those methods so I have here based on the opportunity or you see what Sigma the first followed by centipede hint centipede partition and DNA's 2tf calculations or from precision and everything else is it true that you use the chip seek data as the gold standard yes only yes which means you optimize to the systematic errors of chip seek not to the truth yes so then "
    },
    {
        "start": 1917.85,
        "text": "ships you has a huge error wait that's wrong wouldn't it be better to have something more reliable than ship seek as a gold standard and do the same thing I'm open to suggestions on that so what could be an alternative well a combination of chips some of the other methods that you showed and if you look at that would find a few more true positives that are missed in chips simply because of the cutoff mm-hmm you would get rid of some false positives our pile ups of for whatever reason and well you've done you've done this basically but it's really your optimizing something to the error rate of gypsy here yes and that worries me a little bit because that's such an elegant approach it shouldn't be based on a faulty method and gypsy is base faulty method we just don't have much "
    },
    {
        "start": 1980.01,
        "text": "better the point this I think this is a great point and something that I discuss extensively with Roger wicker AG and other people and this is the problem of the field as a whole I I don't think I haven't seen any discussion on what could be I see a lot of people complaining exactly about not replace one faulty method by another mm-hmm because then just optimizing to a different error I would try to compensate the error it's very simple thing if you have an error rate of 10% and you use one method you have an error rate of 10 percent to which you're optimizing if you have 10 methods with an error rate of 10 percent in the end you have 1 percent error that's really providing that's the idea behind it so a combinatorial approach for the gold standard and it might be worthwhile investing a little bit into getting a better gold standard here because everything that follows yes it just might have set my was sort of along the "
    },
    {
        "start": 2044.96,
        "text": "same lines at least so the transcription factors are all right because what what I'm worried about in if you go back one slide for this false positive or negative it is conceivable that there is a binding site for a transcription factor and it's not a false negative prediction most positive prediction but that the transcription factor in certain circumstances you know there's a repressor that's preventing it for binding or that they have to somehow interact there's no configuration of protein interactions with each other and we know that there are many transcription factor binding sites that are like 2 or 3 and active operatively or repressor in inhibits and all of that is not in there it's just assuming so my guess is sort of modifying what you're doing is for certain transcription "
    },
    {
        "start": 2106.22,
        "text": "factors like ctcf maybe that are very common and that bind just by themselves this will be good but for any complex factors you may get too many folks yeah so these are all valid concerns and I think of this method not as giving your definite the answer this is you get your sample and you just want to survey what's the binding landscape there you don't if you want if you really want the tails on a particular factor then you're gonna go into chipsy acquire create the problem if you do your method and you say this is my projection in space and chipsy fine but if you start comparing for other methods that might not make the same mistakes and then they are less good because they don't make the same mistakes that's where the issue starts and what Mark said is just part of that and there's much more to that which is subsidizing the trip seek our rate mm-hmm and that's all fine for a single "
    },
    {
        "start": 2168.17,
        "text": "method but comparing methods on a standard that is off the truth is unfair because if another method is closer to the truth in comparison it would fall fail so it would be better in reality and that's when whenever you do a comparison you have to be extremely careful about your standard mm-hmm well I think all this is great feedback but can you forward to the next slide okay I you know the current state of the field has chip seek is about as good as you're gonna do it let you go to chippings I work something a little bit more high resolution right and I agree chip seek is not a great essay but it's kind of the workhorse that we have today and all of these methods that he's comparing against are using the same input data it's all motif scans and all chromatin accessibility it's just different ways of analyzing motifs ability so given the imperfections of chip seek and that all these methods are using the same input data if those "
    },
    {
        "start": 2229.34,
        "text": "curves are shifted in one way or another I would think that that is meaningful in terms of having better performance so some of these things we can actually go back into the lab and validate and we've dug deeper into the chip seek yeah this is an example the chip seek damages looking really really crappy right or this is a false positive unnecessarily because we actually see signal on the there just didn't meet their threshold for Cox right so there's all these caveats that we've seen deserve but I want back in the truck so I think that it's great but I based it on the same refers back to where I was saying about the transportation fight by pushing these curves up and then you know matching your whatever GTD is you still picking up something that during generalizes well that means you know you catch an "
    },
    {
        "start": 2291.05,
        "text": "important concept that's you know be below this you know this biological processes which is which is a good thing to do so not kind of the gold standard is one thing but then you still want to be better and another thing some papers also do like this is directors baseline some papers also do comparison methods recall at the peaks FDR so you fix your FDR at some you know particular level depending on 10 percent because I think that that magic is important for what you want to do after you do your essay so what we want to do is with it for downstream analysis you're recording except you are so if you are experienced do you think that the area under procedure Goldberg is a good starting point but later gonna discuss the f1 squad and I think that's the magic you should be using because then you're taking your cutoff into account and your compare dealer actually what is called "
    },
    {
        "start": 2352.97,
        "text": "the Stroop as to positive by one method versus what's called is two parts by the other even though then you might do our line here for us you know and maybe here for hand so that's what you want to compare because those metals they have defaults for where do you draw the line so that should be the watcher compared and I'm going to show this day later but first so I get the precision recall you see not of one but for all the factors gonna go kinda quickly here so first of all let's look at when roster data within Peaks which is where all those methods are operating so for centipede and I'm sure each point here so on the x axis I have the precision require you see for those different methods centipede centipede partition DNA stay a firm hint along the y-axis we have the precision recall for for raw signal and points that are above the line means that the precision recall is higher for our method and you can see that pretty much across the board all those points are above the line with the exception of those green square which "
    },
    {
        "start": 2415.25,
        "text": "correspond to ctcf data sets so ctcf is the only method that's predicted better by this other part the other the only factor that's predicted better by these factors this mattered sorry then if you look at all regions basically see the same region the distance is higher for hint now because it's missing all those binding events that are occurring outside Peaks then we did the same thing the same analysis using our own jam for 878 there so comparing the same well I'd sing a cell line but a different biological sample and then we see pretty much the same result ctcf is the only one that's below the line right if we have time you're going to discuss they have to run to this then we also did that on human tissue samples which is where we actually want to do and then the diff the distance is actually greater than what you saw so we have two different islets cadaveric donor samples "
    },
    {
        "start": 2476.72,
        "text": "here on the on the world and the different methods on the on the y axis this is chip seek data from this nature paper of quality paper that came out a couple years ago and unfortunately stock matched our sample but we please see all those points above the line right so answering alex question how do you establish the threshold for footprint we see that just by taking the score factor it works but how do we draw the line here so for each motive on this measure and this raw signal approach we get all the raw signal scores then we randomly sample motifs outside Peaks so those are most likely to be unbound so this is our know set and on there's no set we fit the negative binomial distribution on them and then with this negative binomial distribution we can calculate the p values for all the observed values in the data and the bound motifs will be the ones that adjusted p value by "
    },
    {
        "start": 2537.7,
        "text": "bonferroni correction is less than point zero five all right that's why I like using my own computer okay anyway it's just going to show the sample data in the negative binomial and you see that they work and this is annoying it should be there but anyway so we can so the negative binomial is composed of two parameters the mean and dispersion right and so this is the mean and dispersion so I'm doing a scatter plot of the distribution that we need for it for ctcf and I want to look at how is this distribution similar to other factors right so "
    },
    {
        "start": 2598.9,
        "text": "there's this whiskers here saying we did this random sample we fitted this distribution 150 times so we have an idea of how those values behave so let's see how it works for the other factors and you can see that there is not much overlap between the negative binomial distribution so this suggests we should be using a per motif negative binomial instead of a general negative binomial and now we went compare that both around you can talk to the other methods so I'm using this f1 score which is a function of the precision and we call at that specific cutoff comparing to the cutoff suggest so for Centipede denis.df hint and also this within peak so just saying if a motive is within a pea is bound how does that perform and you can see that the per motif negative binomial f1 score is higher across the mark for all of them and we can show the proportions here so the percentage of time that the raw signal f1 is higher "
    },
    {
        "start": 2659.989,
        "text": "than the other methods close to 90% Allah every method so this is all for this cut off is better than the others so next subsampling comparison so all the data centers that we're looking at sequence very deeply so let's look at real data were not sequence e to 300 million weeds how much sequencing is that Society for accurate transcription factor prediction show you data here for elf one on the grimoire so data this is just one of the sample side of the pool data and different sequencing gaps here and you can see that around 30 million 20 million weeds it's already close to them so I'm showing the precision recall you see on the y axis and then as we increase the sequencing the F we start making better predictions right so what 30 million seems to be already closing to the max then we can look not only about signal but on the other methods centipedes got a similar but it's "
    },
    {
        "start": 2720.259,
        "text": "stabilized early on and hintondean a city after very bad at low coverage so that's a you cannot call peaks with low coverage and they cannot look at the day right because they're blind here and then what we saw 30 million years is a good number for us for them is almost 60 50 million this is just far off one here and I'm going to show now for all the others so we can break those methods at different tabs and I'm going to show those rank plots so at 60 million weeds which is close to the maximum of this data set we I'm showing the precision recall you see rank so one is the best fourth is the worst for all the different factors the signal mean bad centipede in blue and you can see that they are basically bossy you know is dominating the first place here followed by centipede and then hintondean ace therefore are not a basic computer for the third and fourth-place if we look at 30 "
    },
    {
        "start": 2781.31,
        "text": "million reads which is more like an actual sample that you would the competition is just centipede embassy right just on the Precision's ago and hintondean a stay after not they don't even try to get the second place so we should be using this approaches and then you if you look at the f1 scores unfortunately I didn't have time to run this deal for today so it should be probably what we saw with the full data set with raw signal dominating the first place okay finally very quickly so quality an architecture information content so can we predict residence time from a toxic data so garden Hagar already gave some hints that those factors with high residence time leave this deep for it means but how do we get this data from the first time so answering your question the most common method to get this is this fluorescence recovery after photobleaching so the idea so you generate this data set this "
    },
    {
        "start": 2843.97,
        "text": "transcription factor with a GFP tag and Jeff B if you shine a laser on it it's going to burn out all right and so basically what you can do so you can picture your nucleus at the steady state level and you burn out half of the nucleus and then you count the time that it takes for the autumn of other molecules to go to the other side of the nucleus and this is directly a phone is this is going to be directly a function of how long the factor interacts with DNA so here I'm showing this time force from this parallel paper where they have this each MGM one wild-type protein that binds to DNA and you can see after bleach takes more than 20 seconds for you to go to the other side of the nucleus but if you oblate the DNA binding domain of this protein it takes less than 5 seconds is just a diffusion speed of this protein right so this showing that it's very dependent on the DNA interaction and there's data for a "
    },
    {
        "start": 2906.44,
        "text": "handful of factors for this assay imagining a factor that binds for DNA for a long time what would expect to see in the in terms of chromatin architecture so a vector would sit at the DNA it's all wrapped by nucleosomes so once you see it in there it's gonna start shuffling nucleosomes around because sitting there for a long time is gonna hit give time for it accomodate so if you think of the accessibility landscape at this at this particular locus the regions protected by the by the proteins won't be accessible to transposition all right so let's put our y-axis here that's fragment size and let's how we would imagine seeing the fragments around in this locus so we would see small fragments small fragment on both sides of the transcription factor before the adjacent no-go zones then slightly large effect fragments on overlapping the transcription factor item color coding blue is to the left of "
    },
    {
        "start": 2968.999,
        "text": "the transition factor black to the right and rather than ones that are overlapping and then as you go higher in size we're gonna see this mono Lucas own fragments and even larger Dinoco some fragments and and so it goes when we can represent this data just showing the fragment midpoints and stabbed off the other fragments and we will see this sort of V pattern right so this is a high presidency time factor that rearranges chromatin architecture do we see that for every factor and the answer is dope but in a comparative varies between transition factor so this is the same idea of the plot I showed before it's centered at zero at the motif Center on the y-axis is the fragment size and each dot here although you can see it's a fragment midpoint all right and you can see for p53 there's no discernable pattern here and we can actually measure the information content of this V Plus and maybe there's a little bit of enrichment right at the middle here but if you look at ctcf there's a very striking pattern here so "
    },
    {
        "start": 3031.099,
        "text": "the chromatin is rearranged around ctcf and these measures this matches that idea those experiments that show that ctcf bites for a very long time and because if it doesn't so can we do that for our factors and this is where I'm showing so we got away here is just a recap of that that schematic and I'm showing here not only p53 ctcf but Crabtree which is an intermediary factor and if we just sum up the information count at around 100 base pairs and do that for the fact that we have there is actually a very good correlation with the frappe residency times so factor ctcf is the highest fret we're covering time is also the highest chromatin architecture information content and I've gotta be you 0 1 2 P 53 is a low one and you can see this so now we have this meta symmetric that's a foxy of the residency time and then we can start looking at other factors which "
    },
    {
        "start": 3092.28,
        "text": "you don't have this experiments so this is a good question because what is influenced by the concentration so there's a lot of moths mathematical modeling going on those frappe assays and the data is that the model we have it this is actual residency time so there's single molecule tracking experiments showing that the factors with high fret residence time are also the ones that take longer like in a single molecule tracking you see them sitting for a longer time so it's not a matter not an a function of concentration that the actual interaction time with the factor with the DNA oh all right so we have this index the chromatin architectural residency time score chart score is the "
    },
    {
        "start": 3154.32,
        "text": "the some of this information content around the motif you if you plot the predicted residence time versus the precision recall we see we see that there's a positive correlation for all the methods so this is saying all those matters they are sensitive to factors with high residency time alright so this would make sense for the food printing because the footprint they live deeper food so they are easier to predict and you ask why ctcf was different ctcf ism is the one with the highest information button that we have here so it is bound for longer than that for therefore the other methods can predict them so we should just put all those lines together you can see that the correlation is positive for everyone but the intercept of the design is actually higher for our signal so even for a factor with low residence time we do a better job at predicting them than the other methods and answering the question this is DCF the rule or the exception in "
    },
    {
        "start": 3214.5,
        "text": "the transcription factor world yes the exception this is their highest information content so all the other factors are have very lower predicted residence time so we shouldn't be using those footprinting base methods for them something else oh and certainly even within peaks rustic north performer does as well as the other matters the only exception is ctcf because of the hybrid si time this is significantly simpler to implement compared to all the tested methods we are not going to go into details but I mentioned already that they are not straight for it's just not a package that you download input your data and it just works out of the box so the way we're we're deploying this this raw signal approach is just a command line that takes a vampire or big file motive file and gives the input the number of reads and the p value of the favela and you can just trade use that no no "
    },
    {
        "start": 3275.94,
        "text": "problem easier to run and troubleshoot we have another formal benchmarks but it's also faster and parallelizable compared to the other methods relatively robust samples with flow sequencing the F which is a problem for those footprinting based masses because you need to call peaks on the sample still sensitive to residence time although not as much as the other methods and then future the reactions we are still working on understanding which regions of the V part so that blue red black plots are predator von on there the Rhine chromatin architecture unfortunately out of time already but that square doesn't generalize they're well across samples so we'll make it more stable so that we get the same number across different different brackets different biological samples also some matrix some footprint validation quality controls using a little Hakeem balance at high information content positions also taking account integration events at the "
    },
    {
        "start": 3338.47,
        "text": "motifs right now we're just discarding them which can actually use this metric as a quality control and also using non Express transitional factors with no motif as negative controls and then finally correlate to start correlating structural domains with residency time so now take that histogram and see what what makes a factor have higher residency time than predicted residence time than the others and then we can for help to syntax without tools so won't have time to go that but this is our github so Parker lab get to peek at the well there's the attack decay which is a toolkit for working with ataxic there so right now there are signal methods is unpublished but we have some tools that are rarely available so the trim adapter is used for removing the Illumina adapters from the from the ataxic deal if you're doing that make up matrix is making the input data for centipede which is not straightforward to do so we have a tool that does that for you and "
    },
    {
        "start": 3399.43,
        "text": "then some quality control plot that you can analyze your data and also the attack you see which is our quality control and visualization tool which I hope it's going to work yes all right so with this you can look at your actual libraries where it's alright and you can see the fragment distribution of your samples everything is interactive so you can like put your your mouse over and see which sample we're talking about what's the fragment length the fraction of all the reads can see some nice metrics of distance from "
    },
    {
        "start": 3462.7,
        "text": "from a reference distribution so if your is your sample over transpose or under transposed now everything has its own help here you can see the fraction of fragments at transcription start site so this is a good metric of whether your samples high-quality or not can also get other fact reads in Peaks big territory mapping all the distribution all the stuff nice tables with the poor the percentage of high quality autosomal alignments everything that you might want to look when you whenever you get the data set for the first time so everything is run locally you don't have to send the data to our server so we don't have to worry about us stealing your data or wherever anyway so you can also sent a collaborator so they can directly download the metrics or the experiments on pieces this is a companion to any attack seek there with that everyone you "
    },
    {
        "start": 3535.66,
        "text": "guys for being here and like tolerance to the apocalypse gives great an answer nice to work with the labs composed of John has their staff scientists would make sure everything works like clockwork Yoshi chewin on the postdoc together with Jacob kids and that generated some of the ataxic libraries Irish in theatres our PhD students yeah Nandhini many manikin our lab tech and also Laura Scott and Jacob Keaton and for for experimental an analysis support open to questions five minutes take a little comment on the last point your controls work normally transcription factors and their binding "
    },
    {
        "start": 3595.66,
        "text": "sites mm-hmm a great idea if you take into account that transcription factors are a little bit too misc is many transcription factors that can bind to the binding sites of other transcription factors so you have to make sure that none that is capable of binding is expressed not only one other than that great idea yes yeah so that's we can derive some of this information from the position weight matrix from the from the chip seek the error so we have it started looking into that but definitely you see some factors that the PWM is knocking so it's basically buying binding anywhere but others are very very so this is definitely we have to know this [Music] "
    },
    {
        "start": 3670.25,
        "text": "[Music] "
    }
]