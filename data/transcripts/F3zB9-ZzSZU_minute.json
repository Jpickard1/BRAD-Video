[
    {
        "start": 0.0,
        "text": "Kahless look this the bad taste a spinach looks good cook anything any  that heat a bit of olive oil and then sprinkle a little bit of salt a little bit all other times when it's cooked through you really can't go getting getting the tools the technology seminar I think most of you guys have been here before but in case we have anyone who has this this is just a venue to discuss tools technologies methodologies currently in development newly developed or just abused to researchers may be used in a novel way there's a sign-in sheet it's going around please sign it if you haven't already it's just to help us with the pizza so we can justify continuing to have the pizza um so I'm pleased to present today's speaker Ian class he's a graduate student and electrical engineering computer science in general we love and we're pleased to have thank you "
    },
    {
        "start": 63.359,
        "text": "introduction so I just when preparing this presentation I wasn't entirely sure the sort of proud that I was going to be presenting to it's nice to see a fairly full rooms in particular I'm going to be talking about methods involving deep learning and just to give her a sense how many people heard of deep learning before quite popular how many people would use these learning or have a good sense of how it works ok so it looks like I pitched this just about right I was concerned I get all the experts in deep learning and this would sort of be well but it might be might be good so my research I had a third year PhD student working with jennyline's is primarily on applications of representation learning to sequential data in particular sequential data derived from healthcare applications although I also do some work in sports analytics this turns out "
    },
    {
        "start": 124.32,
        "text": "that that's not a good way to access some special data let's start off this presentation just by giving an inch action you know unless the basic structure this is going to be I'm going to talk about sort of what neural nets are and why everybody likes them these days I'm going to talk about how we actually learn with neural nets and then I'm going to briefly talk about some of my research in forecasting physiological signals that's first off we need to think about the sort of fundamental question in machine learning and that's how do we represent our data this is something that people and machine learning have known is the sort of fundamental challenge I think AI more generally basically for as long as the field has existed it's pretty clear that the difficulty of a problem depends a lot on the representation of the data from what you derive that problem and in particular if you have a good rich set "
    },
    {
        "start": 186.239,
        "text": "of features that are very discriminative derived from your data then it typically is quite easy to learn how to predict various outcomes with that data as a little example of this now maybe we're interested in classifying pictures as either motorcycles or non motorcycles now there are a bunch of different ways that given an image of a motorcycle we could choose to represent it right here we're going to focus on just some two-dimensional representations for the sort of ease of visual presentation but the most natural form of representation for an image or perhaps the least sophisticated form of representation would be looking at the pixel values so here we're looking at two pixels and again one right there one right there and we can imagine that these axes represent you know sort of the intensity of the pixel this is a grayscale image or you have some sort of color combination of those pixels we have a bunch of examples of motorbikes and non motorbikes and we're interested in learning a "
    },
    {
        "start": 248.01,
        "text": "classifier that will distinguish between the two of them now if we look at this example right here it's pretty clear that we're going to need a sophisticated classifier in general we think about the capacity of a classifier as being a measure of how sophisticated its decision boundary has to be where I draw some sort of I need to be straight or curved or whatever everything on one side of line gets classified as X everything on the other side of line gets classified as y as we can see in this example it's going to be very non-trivial to find a mapping that will sort of adequately distinguish motorcycles or motorcycles even more concerning Li just especially given some royal amount of data that we're showing here it's not at all obvious that a classifier we learn on this training data will generalize to new pictures of motorcycles because you know we could I could draw a whole bunch of little circles around each of these positive dots and that will work perfectly well for my training data but I don't see from here that there's there's any guarantee that future points "
    },
    {
        "start": 308.4,
        "text": "that I sample in this space or it all fall you know within those little circles if they happen to be motorcycles this is a pretty difficult problem to long land now if we look at a more sophisticated feature representation we see that this landscape changes pretty drastically in particular here we have two sorts of handcrafted features let's just imagine for a second that if I give you a picture of a motorcycle you can instantly and automatically tell if there is a handle in that motorcycle or if there's a handle in that in that picture maybe in a motorcycle and may not be of a motorcycle in a wheel in that picture and if I have this sort of more discriminative feature spacing it may be this is a probabilistic classification so this feature demonstrates our probability that a wheel exists and this demonstrates the probability that a handle exists in that image once we cache things into this space suddenly the problem becomes a lot simpler right we saw all of our negative examples are sort of clustered together in one "
    },
    {
        "start": 368.85,
        "text": "well-defined subset of the space and all of our positive examples are in another set the space that sort of makes us know we would expect that motorcycles have handles and wheels and we would expect that things do not have handles and wheels are not motorcycles so this is you know first off easier to learn with a simple classifier we see that a single straight line perfectly divides our training in our test set and we can also sort of intuitively think that because there's such a simple pattern here in this limited subset of the data we're more likely to get better realization two new images of motorcycles or not of course in machine learning we're not always interested in classifying motorcycles or not but the general structure of our provinces were given hopefully large amounts of complicated data we don't really want to work with manually and we want to try pino learn mappings from that data to useful things I want I just described to "
    },
    {
        "start": 430.409,
        "text": "you I said okay so we have we have some sort of handle and a wheel and that sort of intuition there it's all well and good I can for this problems motorcycles it's not really what we're interested in we're interested in general techniques for complicated data and when I was sort of describing there this this process of handcrafting features for your task it can be very effective and it can be very important it's been like sort of the cornerstone of most machine learning research prior to maybe the 90s or so but it requires expert knowledge so I need to know what a motor cycle is and isn't first off beyond just to sort of the mappings that are present in the data I need to have detailed understanding of the anatomy of motorcycles it can be time-consuming to serve hand-tuned these expert features member I just said ok so you have some automated way to tell if there's a handle in the image right but I mean that could be just as complicated or the task is determining whether or not the thing is a motorcycle or not that was we "
    },
    {
        "start": 490.889,
        "text": "guys with further down and the granularity of features it becomes easier and easier to think of a real like automated rule that can distinguish between those cases but suddenly we need to worry about building handle classifier and the Pandavas fire maybe we need to get like that I don't know cylinder class fire and the cylinder classifier energy edge classifiers and we know how to do X classifiers from they are made you can build off your way but it's gonna take a pretty long time we don't want to do it so this process of having to handcraft your features it's been argued us as one of the major limiting factors aids are preventing the widespread adoption of machine learning in that it's just you know these systems are very complicated to implement they often require a tremendous amount of involvement have a deeper amount of understanding in their field and it's just in general you know it can work for some problems but it's not really a general purpose solution people are more interested in general purpose solutions "
    },
    {
        "start": 551.64,
        "text": "and I will bring us to know that's the first I want to show off a fun little thing here and this is this is just like an app or it's a website you can go it's called what dog neck and this is you know probably the greatest triumph of machine learning that would say it is a website which given a picture will tell you what breed of dog that's interesting right I mean it's it's it's not all obvious that you know dogs they certainly for a human it is sometimes easy to tell hog out breeds apart some of them look pretty similar right but in general it's pretty easy to say like you know okay this is a beagle and we can recognize that it's a vehicle and a variety of different poses and lighting conditions and different features you know the beagle might be fact people might be small the beagle might be old regardless of those different traits we seem to be able to tell pretty quickly that things are or are not beagles right and now there's no that's our thing our brain is a very complicated thing and there's all sorts "
    },
    {
        "start": 613.0,
        "text": "of complicated priors built into the machinery there but it's it's not obvious that like you know with snake identification or anything there's any pressing concern for our brains to be able to identify beagles right I mean beagles didn't exist before people right so there's no real reason why we would have an incentive to be able to recognize Eagles so the fact that we have such an easy time distinguishing dog breeds suggests that in all sorts complicated stuff that's going on in our visual cortex we're automatically deriving interesting and useful feature representations for different images and that's all for together but different dog breeds can look very different and what this is an example of is actually the performance of this system a system online it is a deep learning system so all of this is basically trained as by I have a bunch of pictures of things that are beagles a bunch of pictures of things that are Miniature Schnauzers I trained it and it can do the sort of multi-class "
    },
    {
        "start": 674.86,
        "text": "classification interestingly it also has a class of you know no dog because I could presumably give this website any picture and it you know if I give it a picture of a table I don't care what dog breed it says it is they're all going to be equally wrong right now the the dog breed classification problem is non-trivial that's actually a sort of where you look at these pictures I know if you if you sort of don't pay attention to it pretty difficult to figure them out now when you look a little more closely you can usually tell the difference between Chihuahuas in muffins or sheep dogs and moths but it's just an indication that in order to build systems that really operate well in the visual realm you need not only to have like sort of good intuitive these representations but you also need to have pretty sophisticated representations or to distinguish between some of these things like it's insufficient here to say okay I'm gonna "
    },
    {
        "start": 734.95,
        "text": "find three large black dots and if I do that then I found myself a chihuahua now I do want to point out that you know we can we can make all these grand claims we want about deep learning and I'm sure if you've read any amount of tech journalism you've heard it all already but we're still not exactly at human level intelligence yet for most problems so you know given this image or the thinks that it's a favorite I know if we can see where there's there some reasoning behind that right like the hair here is in fact pretty similar to that fur that you would see on a poodle and for some other examples it's maybe a little more puzzling there's a big thing right now where with all of these representations you can see very good performance and you can always see almost perfect training performance and you can see good performance in a lot of related data so like you know this this network will probably work very well on pictures of dogs but they can be oddly susceptible to pictures that operate an entirely different regimes of our sort "
    },
    {
        "start": 795.85,
        "text": "of feature space when we have pictures of politicians instead of dogs it can it's not all clear sort of where it's deriving these judgments from but slowing a fun aside we should actually get to what a neural network is now no network it sounds very biologically inspired and I think it was indeed sort of going back to the original formulation it has it had some night of inspiration from biology but I think that's often a little overplayed these aren't neurons they don't behave like neurons do really what a neural net is or how people might call them multi-layer perceptrons or people may call them various other things but what these what these are essentially are compositions of linear classifiers with nonlinearities applied between the non linear classifiers so here we have a one layer neural net and it's composed of three different parts the input layer where we're actually "
    },
    {
        "start": 856.93,
        "text": "sort of sticking in our picture of a dog this hidden layer where we derive a representation from that and then the output layer where we use that representation to make a classification decision how does this work it's pretty simple actually at the input layer you'll notice that I've added in a little bias term to that input layer here this is just a sort of ease the ease the presentation here you might also think of like yeah there's no actual +1 node here but each of these weight connections carries with it a separate bias term and so the every one of these nodes like this abused e 1 Z 2 Z 3 is deriving a linear function and right now I'm not talking about trainers I'm just talking about sort of running through the balloon say a forward pass of the network is just using our train network in order to make a classification so each of these intermediate nodes computes a linear function a linear combination of the inputs right so it's got some weight times x1 plus some weight times x2 plus "
    },
    {
        "start": 919.35,
        "text": "some bias term and then on top of that it computes some sort of non-linearity so here you know I've derived some sort of value from this linear combination and I'll do something like apply a sigmoid to that value or any form of nonlinear squashing function I don't always need to be squashing functions in fact the most popular thing used right now is something called a rectified linear unit which is a very fancy way of saying you take the max of 0 and the value so you just if your value is negative then you just wipe it out but the idea is that we want these nonlinearities to be differentiable or sub differentiable so we want to be able to define a derivative once we've got that we have from each of these nonlinearities and they sort of act what we might call a some stages or activations the hidden unit activations and these get passed through to our output and our output layer is actually just another linear "
    },
    {
        "start": 982.09,
        "text": "combination you take a linear combination of these different activations bias we might apply some sort of non-linearity on top of that and that's the output of our neural network that's all that a neural network is it's very simple the idea is just we had this composition a bunch of linear functions with nonlinearities between them now what I'm showing something calling you a single layer fully connected neural maps we can add in additional layers just by adding in more sort of layers of these hidden units and they will operate just like this layer right here except they will take as input the activations from the previous layer of hidden units and this idea of composing all of these different layers together provide some of the justification for our use of this non-linearity here if we didn't add in that non-linearity we just be basically composing compositions of linear functions which is of course just another linear function so that wouldn't be very interesting there's a whole bunch of interesting results on just "
    },
    {
        "start": 1042.6,
        "text": "these sorts of shallow and simple fully connected networks maybe most notably a single layer hidden neural net with an arbitrarily large hidden layer so I was a whole bunch of hidden units is a universal function approximator so I can train it to arbitrarily closely approximate any function that you might wish and these fully connected neural mesh you can have sort of fair - your versions like convolutional neural nets for current neural nets but those are all sort of just dressing on it and they're all derived identically to this very simple case just from simple combinations of simple linear functions yeah a whole bunch of those not sort of where the magic happens one way to think about what's going on here as I said I've sort of motivated this all by a representation learning perspective is that what we're doing is learning a representation with this hidden layer which is then used by our final output layer there's been a lot of work you know so a lot of tangential work and different branches of machine learning about what's the best way to learn with data the approach taken by "
    },
    {
        "start": 1105.12,
        "text": "the deep learning community is that you shouldn't focus at all on how complicated your decision boundary is when you're actually classifying your data all of your work should be in learning a representation for your data because really what we're doing here is we're saying okay we are going to use the simplest possible classification scheme word one did it just define a linear function and use that linear function to classify our data and all of our sophistication goes into how we're going to represent our data and the real genius of deep learning is trying to automatically learn discriminative feature representations just from data so we we remove that handcrafting experts step yeah so the we can think about this you know look this fee here is something that you might see if you're if you're used to working with kernels and things like that it's just a way of saying we have a raw input representation X this fee gives us a representation of that X which we're going to learn with and here that representation is the intermediate "
    },
    {
        "start": 1166.44,
        "text": "hidden units there now we've got ourselves a little bit of an example so I've been I've gone sort of making all these hand wavy claims about learning representations and things like that this sometimes useful to ground yourself in reality here we've got ourselves a very simple binary classification but I'm not too simple right I mean we see here we've got all of our negative cases and all of our positive cases and there's no one line there's no linear classifier which is going to do perfectly in separating our training data right oh yeah we want this to be linearly separable cuz that survived the whole idea we want to get representation where it is easiest to discriminate and we can do that using one on that so here we've got two neurons this is Z 1 and Z 2 and our nominee areas that rectified linear unit that I was talking about earlier where we just take the maximum zero and our activation the question is how can we use this representation in order to sort of cast this or how can we use this "
    },
    {
        "start": 1227.669,
        "text": "setting to cast this into a representation where everything is linearly separable if you just look at it it sort of becomes clear what the what the answer is because we have these two lines that have already drawn in defined by Z 1 and Z 2 each of these are just the zero line of our linear Ness of a hyperplane I've defined these neurons and we see the positive direction here in the positive direction there and then once we once we sort of look at this and we think about how this will look in this plot well he comes clear that it's going to be linearly separable I just want to take some some time now just think about you know what do you think this is going to look like cast into this this new representation if you all have hypothesis here we go so what we can see is that because we have our two dimensions defined by this activation on "
    },
    {
        "start": 1288.27,
        "text": "Z 1 and Z 2 and the positive direction of this hyperplane is in that direction the positive direction this hyperplane is that direction I means everything in this space is going to be negative in those dimensions and also you know every point over here is going to be negative with respect to this dungeon every point over here is gonna be negative with respect to that dimension but our nominee here he takes care of that and just as we're just W zero so basically what we're doing is just casting all of this data producing a projection onto this basis here and we can see of course all of our negative data is gonna be precisely at 0 0 it's pretty simple to draw a line that will perfectly separate that data so here's just a very small concrete example of how we can learn better representations of data that's all good right so I've sort of hopefully motivated that no elements are interesting we're thinking about there's a big problem right like I sort of started all of that by saying okay we've got this train knowing that we've got this neural net which defines these two "
    },
    {
        "start": 1349.65,
        "text": "boundaries I mean how is that any different from handcrafted feature designs the answer is that the difference is we don't need to assume that we have that were given that neural net we can learn that neural net automatically from the data there's a bunch of different ways that you can do this there's a whole bunch of interest and I think maybe biologically plausible mechanisms of training these things or like re circulatory methods of training these things but it turns out that none of those really work as well as a very simple technique developed and like the 1970s I think called back propagation back propagation it's probably what most GPUs that aren't working on mining cryptocurrency are doing most of the time always when they're doing machining they might also be you know playing video games or something but it's a very very very widely used technique and it's also quite simple to understand there's nothing sophisticated going on here that's maybe a theme running throughout "
    },
    {
        "start": 1410.07,
        "text": "deep learning is very little of deep learning is sophisticated from a human engineering perspective and that's that's hopefully the elegance of it right that we don't need to be geniuses to understand what's going on here we just let the data sort of speak for itself and that will hopefully result in something good he owners have a little optimization because the idea is we're going to start off with some sort of predefined architecture for our neural net and then we're going to optimize the weights of that neural not in order on some sort of training data in order to achieve national performance now it's the simplest mechanism of optimizing - the simplest be like zeroth order authorizing just picking random points seeing how well you're doing and then reporting the best one but that's not super interesting because we have a system ember that we took care to make sure that all of our nonlinear all of our nonlinear functions are sort of activation functions there or differential our sub differentiable and all the linear combinations was also "
    },
    {
        "start": 1470.52,
        "text": "differentiable we will have a differentiable manifold or there's a sub differentiable manifold on which we can work and once we have that you know that basically means that we can think of things sort of in this this hill analogy right things are more or less smooth and so we can just use you know like some form of gradient descent in order to optimize maybe of gradient descent being you know if I'm standing in the dark on a hill I can see maybe a little bit out in front of me any direction if I want to get down to the bottom of the hill as fast as possible I just follow the direction of steepest descent right that's all we're gonna do here so we just know well find the gradient with respect to the weights of our neural net on our output and then we update our weights taking some small little step I know that sort of step size can be important because if I have in the dark on some sort of hill you know maybe maybe a very long legs or something I could take tremendously long steps but since I only have a sort of limited view of my world I don't really "
    },
    {
        "start": 1532.32,
        "text": "want to go super parks it could be the case that the lowest point is actually just a few feet in front of me if I take a very long step I could be suddenly you know even higher than I was to start off with as ISOs we'll be doing some form of gradient descent optimization and the idea of gradient says we have some sort of function with respect to inputs we take the derivative that function with respect to the inputs that's just define the derivative and then we're going to move in a direction such that we are minimizing right because the gray will give us the slope there are a couple of different ways that you can take gradients right and you can do a numerical approximation just sort of what we're getting at here or you can take analytic gradients you typically want to take analytic gradients because numerical gradients are very slow you're gonna have to do a lot of work already to optimize neural nets by back propagation so you don't want to really do the numerical approximation now before we go much further I just want to talk a little bit about computational graphs the computational graph is how we organize "
    },
    {
        "start": 1593.46,
        "text": "sort of all of our thinking about neural nets the different components that it's about right a computational graph is just a manner in which we can sort of organize a series of operations that were performing on some form of data so here we got a computational graph which is defining bias this wasn't really defined much at all but I've got one up here yeah here we've got a computational graph defining this function here these slides are just to sort of make a point that what I'm about to go through and it's these computational graphs any level user divided any form of computation to a bunch of simple sequential steps it can be used to represent a lot of very complicated things and before I was talking about single a layer hidden that's that's very simple you've got this thing called Alex net this was maybe what kicked off the current boom and deep learning where this architecture which sort of ironically in every image of it is cut off part of it I think it's something you noticed it's like every copy of this "
    },
    {
        "start": 1654.88,
        "text": "is its car I don't know why yeah it's all copy I think in the original paper they had to cut it down again to fit in the page limit so I don't know many silly but this convolutional neural network structure was really revolutionary and that it demonstrated a tremendous improvement in performance on a very important computer vision problem called image net if you're interested in that sort of stuff we actually have one of the professor's here in each department yeah Charlie he was the creator of image - okay yeah so pretty exciting not only in that but we can get computational graphs for these fiendishly complicated things that haven't really been demonstrated to work super well yet called neural Turing machines and the idea here is you want to create a super general neural net that essentially operates as a computer it's you know sort of Turing complete now there are other like you know there are simpler architectures which are technically turn complete or can "
    },
    {
        "start": 1716.02,
        "text": "simulate turn complete operations but no Ithorian machine knows was more direct form of that and there's the stateful things so you run them thousands and thousands of times on different problems but their whole operation can be encoded as a computational path a neural Turing machine just as easy reacts it's a neural networks to derive from a neural net and why are they called neural nets because yam people add sort of more grandiose biological patient's back when these things were getting cough I here are going to talk about performing back propagation on a somewhat simpler computational graph and this cuz the only out such time here we've got a conversational graph which is defining the operation z times X plus y right pretty simple operation we have three inputs XY and z we take x and y we add them together so minus two and five here's our particular example we have three and multiply them together with this Z which is minus four we get minus well that equals I right so if we want "
    },
    {
        "start": 1776.65,
        "text": "to take the derivative of f with respect to x y&z how can we do that first off we might observe that you know it could be useful to define this intermediate node here this addition as its own sort of unique property or this its own sort of named entity q q is equal to X plus y all right so if we want to think about the derivative of Q with respect to X so it's gonna be one root of Q with respect to Y it's just a wine rack I'm not blowing anyone's mind here and F is Q times Z the derivative of F with respect to Q is Z the derivative of F with respect to Z is Q that's good if we derived a whole bunch of different derivatives here we have a whole bunch of different values yeah it's all and it's all it's all so simple this is just the chain rule but let me show you and that's the revolutionary back propagation algorithm so if we just want to sort of keep a store of the errors here in this keeping a store is going to be important I'll explain why in a bit we then go in with a fill-in so we have "
    },
    {
        "start": 1838.99,
        "text": "this one here is our sort of gradient value we fill in that derivative of F with F of Z that we calculated earlier Q is three remember so that's three now you want to update the the gradient for Q well it's going to be 1 times the derivative with respect to Q which is CNN convert Z is minus 4 and hopefully you're seeing that all of it I'm doing here is just sort of shuffling numbers out nothing fancy going on after the respect to why I was the derivative of F with respect to Y we can use the chain rule to find that right and that's just you know this is this is back from calc one days right the derivative of a function with respect to some sort of second level input is just the intermediate derivative times that local derivative easy as 1-2-3 right and we have the derivative of F with respect to Q we have the derivative of Q with respect to Y so that means that we had the derivative of F with respect to Y and we just plug it in minus four right similarly for Z minus four and there we "
    },
    {
        "start": 1899.429,
        "text": "have it we now have the derivatives of the input with respect to the output and if we want to minimize this maybe it doesn't make so much sense to talk about minimizing this function because pretty clearly we can minimize it indefinitely right just take Z to be some arbitrarily large negative number and take the X and y to be arbitrarily large positive numbers something like that well I think we're operating with some sort of constrained field and our various ways in which it's much more sensible but this is just how we can impact propagation on our computational path and the real key with back propagation you see the idea that we can use our input seems to take the chain rule to eventually derive the outputs that's completely obvious not something that sort people have known since people knew about the chain rule right and what back propagation is saying is just okay well we want to it's it's essentially dynamic programming you want to sort of fill in from the back first so that we don't have to recompute say the derivative of Q or the derivative that we respect to Q multiple times before actually if you were doing this sort of in the naive forward manner and in "
    },
    {
        "start": 1961.83,
        "text": "general if we've got some sort of differentiable function here and we want to back propagate gradient through that all we do is we first figure out what the local gradient is the derivative I'm you know ax with respect to our function here or that other function with respect to action the derivative of the function with respect to Y we look at the incoming gradient so the derivative here that L is representative our our loss which is sort of the final output of our network we take the derivative of our loss with respect to this input which is very easy to do sort of going back to this example here if we've already pre computed everything in the general right so if you've already freaking this thing is pretty simple and then just use the chain rule again sort of pop it out the back so we will compose the local gradient with the sort of global gradient and there we go we have the gradient flowing backwards from the snap and importantly you know we can see this doesn't need to be the sort of input layer here this can be any node anywhere in your neural net and I don't care if this is you know this "
    },
    {
        "start": 2022.07,
        "text": "could be these sort of linear layers that I was talking about earlier and a fully connected net this could be some sort of convolutional filter base could be some sort of current thing so long as I can you know so long as these elements are well-defined doesn't matter what this function is right I can pass the gradient off through that's why this is such a general thing so once we do that I mean I hope I've convinced you now that we can find the gradient of weights with respect to some sort of output loss and once we have that gradient again yo then it's just it's just gradient to simply have that gradient we take a small step in the direction of that gradient and then we go back and do everything all over again right now there are a whole bunch of different way of people people talk about different sophisticated optimization schemes or things like them adaptive learning rates or adaptive step sizes there's a whole lot of pretty sophisticated stuff but interestingly recently there's been a good deal of evidence that nothing much really beat simple gradient descent or "
    },
    {
        "start": 2082.58,
        "text": "stochastic gradient descent I will say also people rarely do what's known as full gradient or batch gradient descent where you find the derivative of weights with respect to the full input because typically hopefully the data that you're learning with is massive and so you can't fit all of your data into memory let alone function gradients on it so you'll take sort of the derivative with respect to subsets or mini-batch gradient stochastic gradient descent so now I want to talk a little bit about my particular interest in deep lines of deep learning very useful very general very exciting I think and I'm using it for physiological timeseriesforecasting in particular I'm looking at forecasting blood-glucose data so this is collected from people with type 1 diabetes type 1 diabetes is basically just one year it when your pancreas is not producing insulin it's fairly common and diabetes on the whole which you know "
    },
    {
        "start": 2143.05,
        "text": "also comprises type 2 diabetes and like station 'old IEDs and things like that diabetes is the seventh leading cause of death there are more than a hundred million Americans who are diabetic or pre-diabetic I think a pretty when I first saw it I thought there's some type oh my god I can't believe I'm 1 in 3 yeah so it's it's pretty remarkable now luckily diabetes is is not the death sentence that it once was and pitar type 1 diabetes in that with the application of insulin usually control your blood glucose there's a lot of very strong evidence that tight control of blood glucose values to between I think around 140 and 70 milligrams per deciliter of blood it goes if you can keep your blood glucose in that range most of the time then you are much less likely to have a lot of the various negative outcomes that are associated with diabetes unfortunately it can be kind of difficult to keep your blood glucose "
    },
    {
        "start": 2203.95,
        "text": "under control all the time a big thing that people are interested in then is a development of some sort of artificial external control loop that will sort of manage your blood glucose values for you an artificial pancreas is what it's called and a component of the artificial pancreas of narrowing down further and further into my research component of the artificial pancreas would be a good model for predicting the action of blood glucose so if you know what the future of the blood glucose signal is going to be then you can take better actions to try to prevent that future if it's going to be something bad like a spike in one dose so the basic tasks that I'm having here is a time series forecasting task we have some sort of signal that is the input to our model the output of our model is our predicted guess for what future values in that signal might be and the error or the loss that we're going to be training to and trying to minimize is the difference between our prediction and the actual outcome "
    },
    {
        "start": 2265.499,
        "text": "here's a somewhat standard forecasting framework it's something that I'm sort of building off of here where the idea is that you know we've sort of inherited this model as there's a whole bunch of other stuff which is looking at previous times in our signal down to whatever time we're interested in but when it comes time to actually making a prediction will use our input we'll put that through some sort of model here I've been using a s variant of a recurrent neural map which is just a neural net that has some sort of notion of state which persists between predictions useful for sequence level tasks that neural net learned some sort of representation that goes to our little output laying out some sort of simple linear classification scheme and the output of that is hopefully the next value in our sequence now if you want to know here we might not be interested in forecasting only one step in the future we might we might want to forecast multiple steps in the future so to do that we'll take our prediction of sort of the next step in the sequence and "
    },
    {
        "start": 2326.019,
        "text": "we'll feed that back into our model right so we have our prediction feed that in we get the prediction of the next time step feed that in get a prediction of the next time step oh they're right so we've got this this will hear this I'm not really showing it up this is a deep neural Max predicting blood glucose valleys it's gonna do fantastic right uh-huh maybe not so much so I'm comparing it against another a perfectly reasonable machine learning technique called random forests and using a random forest or grass versus this sort of recursive formulation of blood glucose forecasting I'm looking at a large data set of real blood glucose data collected for individuals with type one diabetes and what we see is that on all the data we actually perform a little bit worse I put this in here this is mean squared error of our sort of forecast versus the actual the actual value and this is sort of summed over six different steps were predicting within this 30-minute time horizon and we see that our recursive "
    },
    {
        "start": 2387.7,
        "text": "neural net actually isn't doing as well as a simple random forest although encouragingly on this sort of subset of the data the event subset of the data which represents periods where you have larger variations in blood glucose levels and that just repeat the subset of the data we may be most care about we see a pretty big improvement in performance so that's maybe encouraging right but still not the great now that's so great so since I just spent such a long time she's waxing poetic about the powers of deep learning it's sort of disconcerting that we're not seeing anything improvement I know maybe that's a common thing in machine learning there's a lot of things which are philosophically appealing but don't always get BIGBANG's over this job you know there's there are ways that we can crack this a little bit when it's you here with this sort of recursive forecasting framework is the idea of error accumulation and something that's well known in sequence modeling tasks where if I am using a sequence to you know if I'm using a prediction of a sequence as input for that sequence "
    },
    {
        "start": 2449.72,
        "text": "there can be a big issue where when this model was trained it was used to all these guys being actual observations there it was used to these things being correct and so it doesn't really know how to recover from errors that it makes because you know if it makes an error here that error is automatically corrected at training when we feed in the real next value of the sequence whereas here it makes some sort of error so you can imagine you get very good performance forecasting so one step at a time right if you're always given the real next value even if you are consistently slightly biased above the real value but when you do this recursive formulation predicting multiple steps in the future those small biases add up more and more until eventually you wind up with pretty poor performance so I was looking at a multi output forecasting framework for deep neural nets and as a bunch of different ways that you can do that so one of the ways I've sort of removed all of these these states you can imagine they're still there actually I'm just focusing on one at a time instead of having a single output at each node particularly the next time step we can have multiple "
    },
    {
        "start": 2511.52,
        "text": "outputs predicting multiple steps in the future simultaneously so now our training we're forcing our model to learn a more consistent estimate of the dynamics of the signal because if it's consistently biased and slightly upwards sort of a th time step then we sort of see here when we get to that final one that oh man we've gone way out we need to collect that or as you don't see with the recursive forecasting framework patients on if we have this where we are sort of independently decoding our hidden state into a bunch of future predictions it could well be the case that we'll see a large amount of variation on these outputs and for example if I'm predicting a spike in maybe this little segment of the output there's no way that the network will know inherently so different because if these especially these decoders are trained separately that spike of the coding so that know to sort of predict the continuation of that spike at future points in the prediction and we can ease that restriction just by allowing the nodes to communicate with each other in some ways like here we make a prediction "
    },
    {
        "start": 2572.03,
        "text": "at time plus one we use that prediction plus the primitives plus information here to predict time plus two and then so on we've also we're developing a technique which i think is pretty interesting where we're taking a slightly different approach instead of operating in some sort of time prediction scheme where inherently you know it's harder to predict the future as it gets further and further ahead that's pretty much universally true right cuz entropy increases and all of that the error of time only goes one way so what if instead of trying to centrally predict the future further and further where were sort of guaranteed to get accumulating errors in one form or another we tried to predict a function which generates the future now if we assume that this function sort of locally approximates the signal well then it could be the case that by predicting sequentially instead of observations parameters for this function which is generating the sequence we could get around which could at least disentangle the issues of the future being more difficult to predict "
    },
    {
        "start": 2633.38,
        "text": "and sort of sequences being more difficult to predict as you predict out a sort of error accumulation that I was talking about before we could trying to disentangle that somewhat now of course you know there's there's no free lunch anywhere this only works if we assume that whatever function class were estimating parameters for accurately models the future and it will only ever accurately model the future to some time horizon right because if the future becomes harder and harder to predict more chaotic what we saw were these different techniques finally as I should name them this I called a baseline multi-output this is an auto regressive multi output and here we're assuming this function is some sort of polynomial it is a polynomial multi-output method we found that these approaches tended to do fairly well we see in particular the baseline multi-output gets a big improvement in predicting these sort of normal cases while maintaining good performance in the event cases and we sauce with further improvements with the autoregressive thing although it didn't do terribly well in the event similarly "
    },
    {
        "start": 2693.92,
        "text": "with the polynomial prediction Oh interestingly so although these two methods neither of them really crush the baseline multi-output method on either of our data sets first I left off a few other some data sets because things get a little complicated which they might have done better on not to say it's kind of small but interestingly when we were looking at mostly people don't use individual models for prediction they need ensembles of models we're looking at ensemble turns out to be very important to include these different things so they seem to be capturing some important elements of the dynamics of the sequence and then that's fashion say these are so if we if we remove these guys train a couple of different baseline multi-output things so that they're different we ensemble all those together we get worse performance than we do in the ensemble all these different sort of multi output methods together and together they're on ensemble does in fact beat the random forests pretty convincingly in both tasks so that's nice right we've demonstrated that deep learning does in fact have some amount of promise and also this is justice or "
    },
    {
        "start": 2755.99,
        "text": "the final result that I had I was talking about error accumulation right here but I never really backed that up with data and here's just seeing the root of the sort of median squared error of our prediction by the place in the prediction whenever I'm saying a particular tournament in the future at five-minute airfoil so this is predicting five minutes in the future ten minutes fifteen so on and we see the error by all of our different methods obvious before all other methods actually error does increase over time again saying if there's really no no way to avoid that but interestingly all of our multi output methods they tend to have a decreasing rate of error emulation relative cursive giacomino I showed here that we're getting much better but really if you look at I mean the Pritikin fathers in the future or actually doing worse than with the recursive forecasts the recursive forecasts are saluting a better five minute forecast it's just at all these guys are learning much better say 30 minute forecasts I'm sort of interesting and that's all the slides that I have "
    },
    {
        "start": 2818.569,
        "text": "thank you for humoring me throughout this this diet file about deep learning [Music] yeah so what does that combine my love so let's see I think yeah typically go back see how it does so in yeah we're sort of preparing a paper for this work in that you know there are cases I will say you know there are cases it's not it's not the event that where our combined model is a much better forecaster for all time series for all of these for all these blitz glucose as it could very well be the case I think this forecast was generated by the "
    },
    {
        "start": 2878.569,
        "text": "random forest which tends to have pretty low rate of response and like it's change which is why you see it it has pretty bad performance in these sort of faster changing paces but maybe what guess reasonable enough performance was the recursive is more willing to predict drastic changes based on which predict that the blood glucose does not change simiiariy estimates so we looked at that and we did find that the random forest beat it out by a little bit but not by a tremendous amount I mean I think that the the means where they're just predicting the sort of past is the future values it was something like 95 or maybe a hundred on the full dataset there are cases where the random forests will be able to sort of interpolate out linear transferring of actively so if you see the good blood glucose to start inching up and the random force will continue that prediction it's not always going to be a completely flat line yeah so if you use "
    },
    {
        "start": 2942.8,
        "text": "a strap duration method you still give special inant do you guess similar or worse so we tried we also tried an extrapolation method and we found is the extrapolation as it before much worse than just predicting the past values because blood-glucose tends to have these sorts of sharp peaks and rises and falls if you just do an extrapolation approach over over a window which is small enough that you tend to see sort of meaningful slopes and not just always predicting the same value anyway more or less what you'll see is that it'll get tricked occasionally by these sorts of peaks and it'll go off and predict these outrageous valleys and so incur massive error prediction based on one individual well so these predictions these long he's got we've got some input data right we've got some readings you trying to predict the future uh-huh so we graph this is that like from one person versus date up um so "
    },
    {
        "start": 3004.119,
        "text": "this is this is I think yeah this is data from one of the people or this is this is one person two different people have different things going on how they are responding over time yeah so that's I had a versus X but does that confuse things as to making prediction so there is certainly I guess what we would say interfacial variability or interpersonal 'ti in the gluco regulatory system although it tends to be less drastic than just variation that you might observe within someone's glucose signal throughout the day anyway because sort of no matter who you are the these sort of glycemic Peaks and trials that you might fall to might be a little different the rates what you guys and fall might be different but the primary driver is going to be sort of external phenomena so like if I have a hundred grams of carbohydrates and I'm a type 1 diabetic it doesn't matter who I am you're still "
    },
    {
        "start": 3064.81,
        "text": "gonna see a sharp value unless to take a right amount of insulin to prevent that okay instance pumps with some sort of modern similar models and prediction so now that's it's somewhat proprietary there are some insulin pump manufacturers have like open source models typically things are done more with a control perspective what you know what's the difference between machine learning and control theory what department you're in basically right those of you yeah typically typically insulin pumps will operate based off of mechanisms like mister proportional proportional integrative derivative controllers or those sorts of things a little bit outside of my area of expertise we did compare our method against I so I will "
    },
    {
        "start": 3125.92,
        "text": "say that the the random force we're not to sort of randomly choosing a model to compare against for choosing a random force because previous research has demonstrated that that's the most effective sort of standard machine learning model in predicting for glucose values and sauce problem yeah most insulin pumps to my knowledge do not use highly sophisticated models for predicting future blood glucose values now that doesn't mean if they don't work well again as every sort of see here you can get this is maybe not unreasonable performance you get pretty reasonable performance in predicting these sorts of things just with very simple models some of those extrapolation it's also an interesting thing right we're here I'm presenting mean squared error that's not actually really the error metric we care about right I don't if I'm if I'm a patient with diabetes I don't care if you're saying my blood glucose is gonna be ten points higher than it actually is if I'm well within a normal blood glucose level anyway what I care about is do you issue predictive alerts "
    },
    {
        "start": 3186.43,
        "text": "Wow or does the insulin pumps or operate appropriately in administering insulin during cons when I actually need Izzie how can it recognize I'm heading towards a clash of blood glucose values and alert me sort of preemptively is to have some carbohydrates or can it tell that I'm well on my way to a glycemic spike and administer insulin appropriately now for insulin pump that's actually somewhat of an easier problem if you're sort of looking at hypo versus hyperglycemic or low and high blood glucose prediction tends to be much easier to predict periods of hyperglycemia because they tend to be much more stable why become hypoglycemic it's very rare that I'll be hyper glycine in some short period of time I'll sort of creep up into hypoglycemia and then stay there for a while then maybe come back down especially if some and so that's the main test which the insulin pump is interested in because it wants to say well when you're at that pretty high level let's start bringing you back down with insulin and so that's somewhat easier with hypoglycemia it's a bit "
    },
    {
        "start": 3246.52,
        "text": "trickier because the damage done by hypoglycemia is much more rapid and that you know if you're if you're really hypoglycemia if you start feeling dizzy and you can follow unconscious you can go into a coma or die it's particularly severe pretty quickly and so with with hyperglycemia if I'm just monitoring your current blood glucose if I even don't really predict the future at all you can still get a benefit just from you know being able to detect hyperglycemia and right away serve administering insulin as opposed to waiting for the person to do a manual blood measurement with hypoglycemia there's I think maybe a bit more interest in sophisticated predictive models you know to be fair it's best to not going to hypoglycemia at all so if we could use a sophisticated predictive model and prevent that all together hey that's Dylan yeah so recent results and and what seems like similar to what you're doing and sequencing coding kind of worn out that the most effective to the day architectures are these combinations of you know the local "
    },
    {
        "start": 3309.79,
        "text": "encoder and some couple of convolutional layers so with DM and some attention on the top of the whole things like wave net or target Rondo's architectures so does your data allow or like you do trying in these approaches the yeah so it's a wavenet is an interesting architecture that's something that I'm working on with some other related stuff with glucose and in general representation learning but what if it's also I mean so it's it's a it's a very cool result it's also a a somewhat simple result right and that all that wave that's doing is is looking at sort of a dilated at the causal convolutional neural network basically convolutional no that where you you spaced out your convolutional filters I might not be anything that people don't know what a convolutional layer is but it's it's nothing super fancy what's what's interesting about it is just that I think one of the most interesting things about it is that it's it's really and this is sort of maybe a bit of a tangent but I'm very interested in forecasting not just for sort of clinical "
    },
    {
        "start": 3370.67,
        "text": "applications but also is a representation learning thing because if I'm trying to learn a representation of a sequence I think that forecasting is a mechanism of self supervision to learn a representation of that sequence which is going to be particularly effective as opposed to sort of standard methods of just like glossary reconstructing a sequence like an auto encoder or something like that and I think that a lot of the game that you're seeing in wave now is because it is inherently using the forecasting as a representation learning tool in that a lot of what they're doing with wavenet is like conditional D coming so they're not interested like in sort of raw forecasting but in using a sophisticated representation sort of speech signals in order to construct mappings between actual sort of characters and those yeah so we we tried working with a common paper on this data we were never able to get it to generate particularly reasonable results and I don't think "
    },
    {
        "start": 3433.04,
        "text": "we've ever we ever found several pages of tinkering that we could get results that sort of came close to the random forced upon us and other people have have done maybe more thorough amount of comparisons of common filters to end for us and found that random forests tend to win out yes sir I mean that's good issue creates a space this is thin cat it's like that I just filled up maybe they're coming out even models that don't have particularly good performance in and of themselves can be very useful in an ensemble so long as the ways in which they make mistakes and with me "
    },
    {
        "start": 3517.58,
        "text": "here I hope that you learned something out deep learning I hope that now you can go out and sort of impress everybody with your knowledge so yeah I know I know you have a large amount of data here you try to eat Bologna it's fun it's not always easy it can be easy uh-huh if you're working with natural images it can be quite easy and it's it's what's generated the most promising results recently in machine learning across a range of different tasks so thank you "
    }
]