[
    {
        "start": 0.08,
        "text": "thank you everyone for attending on today's tools and tech seminar uh my name is ji tao today i'm going to give a talk on building a software pipeline for developing and evaluating time series machine learning models using electron electron health record ehr data uh so a little bit about myself so i uh i got my bachelor of science in biotechnology in 2013 and then i came to university of michigan school school of public health obtained my master of public health in epidemiology then i had three years of work experience and joined the pips dc phd program in 2018 so i'm in my third year of my phd life currently i um my dissertation thesis is mentored by two professors dr karen gibson who leads the machine learning for learning health science lab and dr kevan nigerian who leads the biomedical and clinical informatics lab "
    },
    {
        "start": 62.719,
        "text": "and this is dr singh and this is dr nigerian and my research interest and focus lies in clinical predictive modeling using ehr data and we are particularly interested in bringing models to bedside so the packages i'm going to talk about in today's presentation is developed in machine learning for learning health science lab the major contribution is from dr ken deep singh okay so let's first talk about the ehr data in 2009 president obama signed the health information technology for economic and clinical health act into law to promote the translation to ehr and health information exchange starting in 2011 an incentive program was developed and led by cms to promote meaningful use of ehr nationwide this figure shows here here shows that "
    },
    {
        "start": 122.799,
        "text": "as of 2016 over 98 percent of u.s hospitals have participated in this system and here at michigan we are 100 percent and when we use ehr data to develop clinical models there are typically two types of predictions we can make one is a one-time prediction this is very basic in a typical for example we can predict a patient's seven-day mortality as their at their time of the mission or we can predict their 30-day readmission at the time of uh their they are discharged they are discharged from the hospital and this uh this kind of prediction task is very similar to what we learned in the practice in many statistical learning machine learning courses because it only did one it only does one time prediction and it's very simple but in clinical practice what clinicians "
    },
    {
        "start": 185.68,
        "text": "really care about is to do some sequence prediction so uh so that we can monitor patients health status continuously during their hospitalization so in the figure here the top panel shows the patient creating measurements during uh the patient's hospitalization creatinine is used in natural nephrology as a biomarker and important measurement to diagnose patients kidney disease for example the acute kidney injury i marked here and ideally clinicians want such a an algorithm that can give a continuous prediction of patient to risk and in an ideal case the algorithm can predict a patient's risk of getting the disease um in a little time "
    },
    {
        "start": 247.36,
        "text": "uh prior to the onset of the disease like the bottom bed panel here shows the model predictions uh given by an algorithm and it captures uh patients elevated the risk of having aki earlier than the real aki where aki really occurs so this kind of sequence prediction is what i'm going to focus on in today's talk and then let's look at what kind of challenges exist in ehr data analysis and the raw ehr data has very special characteristics first it is a mixture of fixed data in the temporal data here this graph shows some example tables that are components of ehr database for the fixed data that means uh the variables the features that does not change as time goes by for "
    },
    {
        "start": 307.759,
        "text": "example the patient to demographics uh like the sex race ethnicity but for certain variables like the age although it can change as time goes by but usually in prediction tasks in a rather short period we can also consider it to be fixed data and the temporal data are time series data they are something like lab results medicine they are stamped by a time by a time point during patients hospitalization and their values are continually continuously checked and changed during patients hospitalization and for temporal data it's a little bit complicated in ehr data it's usually stored in a long format like the example table here when i say a long table it means one patient can have multiple rows and each row is stamped by a time time point and only "
    },
    {
        "start": 368.24,
        "text": "displays one event and a value associated to it so you can imagine how long this table can be for all the patients in our system in our ehr system and unlike uh this temporal data is unlike uh data we collected from a designer study in a designed research study we can design to collect a certain measurements in regular intervals but in ehr data because this is real-world data uh usually it's uh the consecutive data points have in irregular intervals like the figure shown here and to make the table more uh intuitive to our human beings usually we want the detail to be reorganized to this way each row represents one patient's status and it can has fixed data and the temporal data and for temporal data there's a critical "
    },
    {
        "start": 430.88,
        "text": "need to transform them into a more clean the way so that it can be used as inputs for machine learning models for example we talked about these data points are collected irregularly and a common practice is that we prepare these longitudinal patient data aggregate them using some summary statistics so that they can have a fixed feature fix the length of feature vector so no matter how many data points in this wing observation window differs from the next observation window we always have a fixed length of feature vector so that's we end up with a tabular data so this is the first challenge you will find in our ehr data analysis so there's a need for data transformation tools so the next challenge is that ehr data typically contains "
    },
    {
        "start": 492.479,
        "text": "uh both the structured data and unstructured data the structure most of us are more familiar with structured data like this tabular data demographics vital science even the temporal data medication lab results they are structured but many of the medical information are contained in unstructured data like the clinical notes in the images in the videos they are also unstructured data so we next identify that that there's a need for clinical text processing tools in clinical natural language processing uh specific tools are required to complete the following typical clinical notes on lp tasks like named entity recognition medical concept mapping negation detection sectionizer so the idea is that we want to convert this kind of unstructured data uh into "
    },
    {
        "start": 553.6,
        "text": "structure the data format so that they can be used for machine learning algorithms and currently there are many existing clinical nlp tools in python and we see that there's a need for similar clinical nlp tools in r and lastly we also find that there is a need for clinical predictive model evaluation tools so when look when evaluating a clinical predictive model there are different aspects you need to take into account the first one is discrimination so discrimination means how well the model can discriminate a data with outcome from with outcome and most often you will see auc as a single metric used to compare models to show the model performance but we also see there's some pitfalls about auc so first there's no threshold "
    },
    {
        "start": 615.12,
        "text": "information if you only look at auc and although you can plot the roc curve to show the sensitivity specificity how they change as uh at a different threshold we still think there's a limited ability to compare models conditional on threshold using a roc curve only and because it only shows sensitivity and specific specificity it can potentially provide you misleading information the second aspect people will look at a model performance is called calibration calibration shows you the agreement between predicted probabilities and observe the probabilities i'll talk about it in details in a later slide when i talk about our package about eval model evaluation and lastly once you know your model is has good discrimination good calibration there's some decisions "
    },
    {
        "start": 675.6,
        "text": "you need to make to bring it to to backside for example a given a model there are many thresholds you can choose and for your practice what is the optimal threshold that you want to choose and as this chosen threshold what is the net benefit you can get to using this model so based on my review of the background information about ehr and the gaps we identified so in our lab we have developed three tools one is to transform ehr data for machine learning training purpose it's called a wizard next is to help uh researchers handle clinical text processing uh using r the third is to um the iop tool is called clean spacey and the third one is used to evaluate and compare prediction models we call it runway so i have the links here so you are free "
    },
    {
        "start": 738.88,
        "text": "just feel free to search it on github and we have tutorials for each of the package and due to the time limits um and there's more background information you will need to understand nlp tasks today i'm going to just a focus on the data transformation tool wizard and evaluation to a runway yeah but just feel free to contact me for the nlp package i'd be happy to answer questions offline so let me first talk about the wizard package wizard is abbreviated for windowed summarization for autoregressive data so in this package we provide the following functionalities um we our package can help you transform your ehr data in the following ways first it can help you segment "
    },
    {
        "start": 801.32,
        "text": "hospitalizations into time steps uh here i use hospitalizations because this is the typical time range we will use in clinical predictive modeling but you are free uh you are welcome to define your own start and end time in our package next uh after the hospitalization is segmented into steps the package can help you create rolling prediction and outcome intervals of varying lengths and calculates the associated rolling statistics so in our package we provide you sample data set for you to play our play with and you can also provide your own data the only requirement is that you provide the one virtual fixed data in one version of temporal data like i mentioned before in the background the slides and these two these two screenshots are the sample data we provided we provide in "
    },
    {
        "start": 864.48,
        "text": "with our package and they are made up so there's no sensitivity information so just be assured um okay so the first thing our package can do is to help you cut the temporal data into steps i use a graph to illustrate the process say this is when the patient is admitted to the hospital and like i mentioned before we want the algorithm to predict a patient's risk of having the disease every couple hours say like here we want to ask the algorithm to predict the uh risk every six hours there you define your step to be six hours this is the first step and here uh you just uh design your wizard frame provide the your fixed data temporal data start and time the id and the step "
    },
    {
        "start": 926.399,
        "text": "yeah and then you can also ask the function to write the output to your specified directory and next one is a little bit complicated so for each step we would like to generate predictors and outcomes and there are several concepts associated with them their window size look back and look ahead period and you also need to define summary statistics so i also use the graph to illustrate say we want to make a decision make a prediction at this time point like 12 hours after the patient is admitted to the hospital and we want to predict a patient's risk of having the disease one two three four 24 hours later so this is where we are this is what we are going to predict so "
    },
    {
        "start": 987.839,
        "text": "this period we call it look ahead period and the information you want to utilize uh we call it to look back period say if you want to pass the 48 hours uh data then you just say your look back period is 48 hours um because in some prediction tasks you may use a longer time of look back here a longer time of look back period and you might want to break down your data into smaller granular everything so we provide another option for you to define your window size say if you want to go back 48 hours but you want to extract a six hour window of data separately so that you can put more weights on the more recent six-hour data so that's that's also possible using our package and for each window you can supply the summary statistics "
    },
    {
        "start": 1048.799,
        "text": "you want to use for this window and uh as time goes by let's move to next time step and uh again we want to predict 24 hours later whether the patient has risk and the same look has looked back in window size and next step so just imagine how many times you are gonna do this kind of process again and again and given that ehr data has very large size um this uh data preparation um can be really tedious but with our package you just call the wiz at predictors and the with ad outcomes tell them the variables look back look at the period and the summary statistics um for predictors there can be different ways to calculate predictors yes so we offer "
    },
    {
        "start": 1109.679,
        "text": "some more uh specific uh type of predictors that you can utilize um yeah so just uh by calling these functions you will get this list of files in your specified directory and with these data you can supply them into model but there's one last step you need to do because they are in separate files and our package can also help you to combine them combine them to form a single data set for model input just call it with combine and you'll have this uh final data sets ready for model development so in this final model data set you have fixed data you also have a temporal data "
    },
    {
        "start": 1170.64,
        "text": "and this is the step we defined before meaning every six hours we are going to make a prediction and for every step this is one row this row is the data you are going to use this is the outcome variable these are all the predictors you can use yeah so this just may makes us uh makes uh ehr data preparation easier in our lab and another thing i want to mention is because uh ehr data is large and usually enabling parallel processing can speed it up so it is also possible with our package you only you just need to call plan multi-session from the future package at the beginning of the your script and it can process the data preparation in parallel and we provide the benchmark "
    },
    {
        "start": 1231.44,
        "text": "results here these two results are using multi-session and this one is just sequentially preparing the data and you can see that the model uh speed is faster with the multi-session so this wraps up my tutorial about our wizard package the next package i'm going to introduce is called runway this package is designed for evaluation and a comparison of prediction models so why is it called wrong way um i think you may have this experience that sometimes your model just look quite different it's very easy to tell which one is the best to use and you're like oh ho i i'm done i know what i'm gonna do next but other times you may see your models look the same especially when you are "
    },
    {
        "start": 1292.0,
        "text": "using very simple or single metrics to compare different models so we hope that with our package you will be able to like work working in the wrong or working on the wrong way and say oh i'm excited i know the model i know which model is great um for this package the imports data we require is very simple if you are just looking at a single model you just supply two columns one column is the true labels of your data and the other column is the predictions given by your algorithm and you can also compare different models the only thing you need to add is another column indicating which model it belongs to so these two are also screenshots of our sample data provided in our package "
    },
    {
        "start": 1352.24,
        "text": "so uh the first thing i'm going to talk about is discrimination like i mentioned before people usually use auc to represent the discrimination of a model but here we propose to use this threshold performance plot it is a form panel plot with a histogram and showing the threshold from zero to one and what's good about this kind of plot is that it carries all information of an roc curve because it provides the sensitivity and specificity and also uh in clinical modeling uh we often uh plot a precision recall curve together with roc curve uh precision recall curve is just a positive predictive value versus sensitivity curve because clinicians want to use this kind of curve to have a sense of force alert "
    },
    {
        "start": 1415.76,
        "text": "the ppv typically tells you what's the proportion of true positives out of the predictive positives if that proportion is low that means the clinicians need to evaluate a lot more patients to get a true positive so this is also very helpful in clinical predictive modeling and also in our plots we provide the distribution of the predictions to give you a sense like how it is distributed and we also in these two shaded area shows at a given threshold what is the proportion of positives and the negatives given by the algorithm so we think this um uh this kind of threshold performance plot gives you um a lot of information in one plot and you can pick uh whichever you are more interested in you uh "
    },
    {
        "start": 1478.72,
        "text": "you want to focus more on just from this uh plot and we also enabled our package to compare performance between different models using this function and here is an example and the next thing we will look at for a clinical predictive model is the calibration the idea of calibration is to tell you uh the consistency between the predictive probability versus the observed probability um when our model is calibrated you can we are confident to say that the probability value given by the the algorithm um can be appropriately interpreted as a probability because it "
    },
    {
        "start": 1540.96,
        "text": "is consistent with the observed probability um let me use an example say for a group of 100 patients the out the average predicted probability uh given by an algorithm is 0.4 let's say it's 0.4 for these 100 patients and if this value can be interpreted appropriately as a a probability that means we would expect to see 40 out of 100 patients actually have the disease so what we want to do is that we pull the true labels and account how many out of these 100 patients actually have the disease and the plot them on the y-axis and compared to the to the exit the x-axis predicted a probability "
    },
    {
        "start": 1602.0,
        "text": "and ideally a well-calibrated model will align with this diagonal line very well um but if i thought if a data point is below the diagonal it means that your model is over predicting the risk probability and in contrast if the data point is above the diagonal line that means the probability given by your model is under predicting the risk uh calibration plot is important it's an important metric to look at because non-linear machine learning algorithms often predict uncalibrated class probability so you should be cautious when calling them class probabilities or interpreting them and similar to the discrimination plot you can also compare the calibration "
    },
    {
        "start": 1662.799,
        "text": "plot plot for multiple models like the one shown here [Music] next i'm going to talk about the application of our runway package in a study we did last year for covet 19 patients so this is a study we did for adult patients admitted with proven 19 to non-icu level of care at michigan medicine the data is from march 9th to may 20th last year and we want to predict our composite adverse outcome including icu level care mechanical ventilation or death and because that's in the very early phase of the pandemic there's no existing uh algorithm designed for cov19 patients only so we were "
    },
    {
        "start": 1722.96,
        "text": "thinking that can we use some existing models to do such predictions and maybe repurposing it or at least the model can provide us some information so we found the epic deterioration index model it is a model developed by epic and also embedded in the epic system which is the ehr system we are using at michigan medicine it runs at the back end and every 15 minutes it gives you a deterioration index ranges from zero to 100 for a inpatient for an inpatient patient we were hoping that we could use such model to identify patients with high risk of experiencing this at adword composite adverse outcome or low risk of adverse outcome this is critical because in early uh pending pandemic the resources "
    },
    {
        "start": 1785.279,
        "text": "are very limited so we hope this kind of algorithm can help us prioritize resources and also this paper can serve as an example how we do model repurposing a model evaluation on a different cohort so we do uh like i mentioned in the previous slide we did two kind of analysis one is high risk analysis we want to see whether this index can identify patients with high risk of experiencing adverse outcome or versus non-high risk analysis and another analysis is to predict a low risk versus non-law risk and for the high risk analysis we exam examine the threshold from zero to one hundred hundred and compare the uh the "
    },
    {
        "start": 1847.2,
        "text": "performance of four different metrics at each threshold and the clinicians uh picked this threshold it's 68.8 meaning if uh in the system this model gives an indexed value of 16.68.8 or greater for a patient then we say this patient it has high risk of experience adverse outcome and at this this threshold we see that the positive predictive value is 74 and this means that for clinicians um they need to evaluate 1.4 patients to find the one true deteriorating patient "
    },
    {
        "start": 1910.96,
        "text": "and at this threshold the sensitivity is 39 percent not high but better than not having a model and for the load risk analysis the cohort is a little bit different so we only include patients who have not been discharged or experienced the primary outcome at 48 hours so the cohort is a little bit smaller than this one that's why you'll see the plot is a little bit a little bit different from this one and for low risk we examine the thresholds and uh use the the threshold of 37.9 um and if the patient index is less than 37.9 we think this patient has low risk of experiencing "
    },
    {
        "start": 1973.6,
        "text": "adverse outcome for the remainder of the hospitalization and you'll see that the negative positive value the negative predictive med value is 90 90 percent yeah so that means um the 90 chance of uh this patient is uh too low risk among all the or we predicted the negative patients and up here we captured the sensitivity of 91 meaning we captured the 91 of true high-risk patients so we use these kind of plots to give audience a sense of uh the overall performance of the of the model i think for clinicians "
    },
    {
        "start": 2035.2,
        "text": "this product provides almost everything they want and the need to look at a model's discrimination especially this distribution of predictions and like i mentioned before a calibration plot is also critical uh for this analysis this is the calibration plot we got so you'll see that all the data points are below this diagonal line meaning that um this in this in detail rating index is over predicting the risk of having the disease so when you say um oh here we converted range uh the the index it ranges from 0 to 100 we convert it to 0 to 1. so when you have 0.3 here we convert it back to index of 30. "
    },
    {
        "start": 2095.599,
        "text": "so when you have a patient of index equals to 30 that's it's not safe to interpret that the patient's risk is 0.23 because this index is over predicting the risk so this is something you need to be cautious when interpreting it and we would say this model is not very well calibrated for this population here is another set of figures we made to show how early this kind of alert can capture patients high risk so every dot here shows uh the alert uh the alerts um for high risk patients given by the algorithm and the red dots uh represents the first alert that was generated "
    },
    {
        "start": 2157.28,
        "text": "um so we see that the median lead time from when the threshold was first exceeded to when the outcome occurred was 24 hours and we have a zoom in for these patients and showed where their first alert was generated so the conclusion we have for this paper is that um the epic deterioration index can be repurposed to evaluate the risk for copying 19 patients to some extent but it is better than nothing it is better than having no model but the results should be interpreted carefully and the more efforts need to uh more efforts is needed to calibrate this "
    },
    {
        "start": 2217.52,
        "text": "model for the development of this model or design a new model for copy 19 patients and we are still developing uh our runway package and there are some more things we are considering to add to this patch package the first thing is that we want to make interactive version of the plots so that when you hover over your mouse onto the plot on the plots you can see the exact value of threshold and exactly that exact value of different metrics alert timeline is what i showed in the previous slide uh the red dot represents the first alert and the rest of the learns are in black dots so we got very good feedback from clinicians that this can be helpful to give your sense how the model would look work in the real life so "
    },
    {
        "start": 2279.68,
        "text": "we are considering adding it to the package and another thing i mentioned in the background the slides but has not been implemented in the our model is the decision curve so the idea is this is the idea of decision curve is shown in the figure here so basically you want to look at the net benefit um comparing different models and different interventions um say this is a the dashed line we call it a test you can think of as a standard clinical practice uh it's a like a diagnostic test and this is a net benefit given by the statistical model we developed and we need to compare them with two reference one reference is called an intervention for noun meaning uh "
    },
    {
        "start": 2340.8,
        "text": "we just don't intervene any patient another one is called intervention for or meaning we just assume everyone has a problem in interviewing everyone so we know they are not better strategy but they can use as baseline for comparison and what net benefit plot the decision curve gives you is that you can compare the net benefit of model compared to these two baseline and you can also compare to a standard test or other statistical models to see at your given threshold whether your model exceeds all other practices or other models in the if if that's the case it's a it's an evidence that your model is likely to benefit in clinical practice and you can use this as an evidence to tell people yes we should implement this model "
    },
    {
        "start": 2405.44,
        "text": "so as a summary um in our lab we carefully examined the challenges um for the ehr data analysis and from the data preparation of clinical notes uh processing and the model evaluation in a comparison we we have developed three packages uh first the wizard to handle the data transformation for the unstructured data the clean spacey um you can use it to complete a certain clinical nlp tasks and lastly a runway can help you eva do a comprehensive evaluation and the comparison of the prediction models you have so together they uh we build a data pipeline for uh time seriously "
    },
    {
        "start": 2468.56,
        "text": "electronic health data analysis in machine learning tasks so we hope uh this talk can help everyone else interested in ehr data a little bit and if you are interested in talking more details about the package and need some help uh just feel free to contact me offline or just throw a question here at the seminar yeah that's the end of my talk and my reference so if you have any questions feel free to ask it's really great job uh gee we really enjoyed that "
    },
    {
        "start": 2528.56,
        "text": "thank you brian it was great there's so much i mean how do you get at that epic deterioration data do you need special permission to get at that data i mean how does that work oh the epic deterioration index model so epic embedded in their system so that's something we that's already accessible for michigan medicine it's just running on the back end not showing on the screen not showing on the screen yeah it's accessible yeah but what's mysterious is that uh epic does not provide details about how they develop this model right that's why we call it yeah that's why we call it a proprietary deterioration index so the only thing we can do here is to validate it on the cohort we are interested and see if this model can be used for other "
    },
    {
        "start": 2589.68,
        "text": "purposes and how appropriate it is yeah yeah thanks and it's so does is that they run that every 15 minutes against every patient in the system yes yes no that's very they have different models as far as i know there's another model called sepsis model it's also run every 15 minutes on the back end so if physicians they want this kind of information they can use this this model risk model to do some research yeah with the sepsis you know you know that's related to the covet too so if these guys get into a you know ventilator or something like that get bacterial pneumonia as a secondary infection i mean they often get sepsis and this is what really could kill them you know with the lung involvement so you know it's just like i was just "
    },
    {
        "start": 2650.64,
        "text": "wondering about the sampling rate i mean you know so i guess what that does is that every patient in the system is looked at in some kind of aggregate form every 15 minutes and then tries to make adjustments as new data is entered by physicians or medical devices that you know tries to make an assessment on the status of the patient with some things like they move to an icu or something like that things go kind of go bad yeah it would be interesting to learn about uh god so did they publish i mean there's nothing inside you don't know how it works yeah so when writing this paper i i was trying to like find some details about this model and there's just the news articles about how great this model is but this is not we really want for a research paper we want to know the model details so yeah we find it interesting just to "
    },
    {
        "start": 2710.88,
        "text": "repurpose this kind of proprietary model very interesting experience yeah thanks so much this is it is very interesting i really appreciate it very exciting work thank you yeah are there any other questions so gee there's a question that just came in uh to the chat box i can read that uh to you it is yeah sure challenges in identifying actionable versus non-actionable predictors um so if i'm understanding it correctly i think the question is asking whether there's something we can tell uh a in a predictor is important or not is is that something you mean i think for the future "
    },
    {
        "start": 2771.68,
        "text": "importance the predictor importance and that depends on the algorithm you are going to use so here we only talk about uh the data preparation process um depending on your uh machine learning algorithms some algorithms provide you a interpretation of feature importance and even the algorithm does not provide you uh the future importance that i know there are publications of some model agnostic interpretation methods that can help you understand how important and feature a feature is in a machine learning algorithm yeah i hope this answers your question uh sorry it was my question that gets somewhat to it so you're saying that this model doesn't necessarily tell you which features are the most important as far as ranking but more so "
    },
    {
        "start": 2834.079,
        "text": "the challenge of um what sort of things are actionable as far as in the clinic versus non-actionable things that might be contributing but a a clinician can't might not have tools to to kind of change the patient care uh so maybe a better way to frame that yeah a better way to frame that might be to um what were the kind of actionable um what were the kind of downstream um changes in care that are supported by this model especially in contrast to kind of the holistic um of it's taking in a lot of different types of data but not all of those things are necessarily actionable in the clinic "
    },
    {
        "start": 2896.0,
        "text": "oh so you're saying like although we use different kinds of predictors uh supply them as predictors in the model but some some of these predictors are not something we can change when we exactly provide information okay okay i see i think the talk i'm giving today it mainly focus on like technical parts but when talking about actionable uh the concept of action uh action uh whether something is actionable from my talk i think one thing is that when you design the model you should carefully design the your look back here look ahead window size for example we know if the data is closer to the outcome the prediction is going to make it it's going to be perfect but just think about how far apart your prediction versus outcome "
    },
    {
        "start": 2957.04,
        "text": "should be to provide enough window for clinicians to provide some interventions i think that's one thing you can think about from the modeling standpoint and another thing i think uh is about our runway uh threshold performance plot is that um especially for the ppv i think this is um um many clinicians i've interacted with they care a lot about ppv in a clinical predictive modeling because they want to know like what their burden will be for example if you have to if the ppv is just like twenty percent you have to evaluate the five patients who got the positive alerts and you only identified one uh two positive patients and that can be a lot of burden to physicians clinicians and that "
    },
    {
        "start": 3018.96,
        "text": "may be something not actionable for them and we usually call it a alert fatigue in clinical predictive modeling i think these are two things we can consider for whether the model is actionable from our modeling standpoint but for the whether the predictors are actionable i don't think in my talk we specifically address this because i think that needs some uh input from experts like the clinicians they know better which variables are actionable yeah great thank you for that answers oh yeah thank you exactly can you ask a question about the net benefit graph yeah sure so i'm very interested in that graph it seems like the graph can tell us some information about "
    },
    {
        "start": 3080.16,
        "text": "whether the model should be implemented or not um can you can you please give me an example so for example uh what kind of like what kind of graph when you see you would say okay this model is okay to implement and and what kind of model that you use you think it's not very if it is not efficient to implement uh great question so the next benefit this decision curve is a little bit hard to understand that's why we are still developing it and clarifying uh getting clarification from the original author to understand it completely so this example graph i am giving here shows a good model because let's say for the most threshold we can choose here it is its net benefit is over intervention for all over uh greater than intervention for now "
    },
    {
        "start": 3141.28,
        "text": "and even for a standard clinical test so yeah just comparing their net benefit value but the authors specifically mentioned that so this graph is not for you to pick the threshold you should pick your threshold first say i want to use a threshold of 10 and then compare all these lines here and see if the model if if your uh developed model is the best has the best performance it has the best net benefits then that's how proof your model is super superior to all other methods so how does the net benefit calculated for the model so if you're interested you can search andrew vickers decision curve paper and it has yeah it has strict mathematical formulation basically it is a balance of "
    },
    {
        "start": 3201.76,
        "text": "cost and benefits you would evaluate in the clinical practice cool thank you so much can you please put that maybe in the chat uh yes at the end of talk i can put it in the chat sure thank you thank you sure thank you okay does anyone else have any questions i'm not seeing anything else um in the chat box and i'm not hearing anybody else unmute themselves so i'd like to thank g for her excellent presentation today and uh next week we'll be hearing from stephen linsley from indica roger poxie's lab so hopefully everyone can join us and thank you all for coming today "
    }
]