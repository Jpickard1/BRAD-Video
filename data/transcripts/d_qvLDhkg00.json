[
    {
        "text": "The basic function underlying a normal distribution, ",
        "start": 0.0,
        "duration": 3.243
    },
    {
        "text": "aka a Gaussian, is e to the negative x squared.",
        "start": 3.243,
        "duration": 2.877
    },
    {
        "text": "But you might wonder, why this function?",
        "start": 6.64,
        "duration": 1.7
    },
    {
        "text": "Of all the expressions we could dream up that give you some symmetric smooth ",
        "start": 8.72,
        "duration": 3.744
    },
    {
        "text": "graph with mass concentrated towards the middle, ",
        "start": 12.464,
        "duration": 2.383
    },
    {
        "text": "why is it that the theory of probability seems to have a special place in its ",
        "start": 14.847,
        "duration": 3.793
    },
    {
        "text": "heart for this particular expression?",
        "start": 18.64,
        "duration": 1.8
    },
    {
        "text": "For the last many videos I've been hinting at an answer to this question, ",
        "start": 21.38,
        "duration": 3.283
    },
    {
        "text": "and here we'll finally arrive at something like a satisfying answer.",
        "start": 24.663,
        "duration": 3.017
    },
    {
        "text": "As a quick refresher on where we are, a couple videos ago we talked about ",
        "start": 27.68,
        "duration": 3.965
    },
    {
        "text": "the central limit theorem, which describes how as you add multiple copies ",
        "start": 31.645,
        "duration": 3.965
    },
    {
        "text": "of a random variable, for example rolling a weighted die many different times, ",
        "start": 35.61,
        "duration": 4.233
    },
    {
        "text": "or letting a ball bounce off of a peg repeatedly, ",
        "start": 39.843,
        "duration": 2.679
    },
    {
        "text": "then the distribution describing that sum tends to look approximately like ",
        "start": 42.522,
        "duration": 4.019
    },
    {
        "text": "a normal distribution.",
        "start": 46.541,
        "duration": 1.179
    },
    {
        "text": "What the central limit theorem says is as you make that sum bigger and bigger, ",
        "start": 48.44,
        "duration": 3.702
    },
    {
        "text": "under appropriate conditions, that approximation to a normal becomes better and better.",
        "start": 52.142,
        "duration": 4.078
    },
    {
        "text": "But I never explained why this theorem is actually true.",
        "start": 56.94,
        "duration": 3.24
    },
    {
        "text": "We only talked about what it's claiming.",
        "start": 60.22,
        "duration": 1.76
    },
    {
        "text": "In the last video we started talking about the ",
        "start": 63.08,
        "duration": 2.452
    },
    {
        "text": "math involved in adding two random variables.",
        "start": 65.532,
        "duration": 2.348
    },
    {
        "text": "If you have two random variables, each following some distribution, ",
        "start": 68.26,
        "duration": 3.584
    },
    {
        "text": "then to find the distribution describing the sum of those variables, ",
        "start": 71.844,
        "duration": 3.638
    },
    {
        "text": "you compute something known as a convolution between the two original functions.",
        "start": 75.482,
        "duration": 4.218
    },
    {
        "text": "And we spent a lot of time building up two distinct ways ",
        "start": 79.88,
        "duration": 3.084
    },
    {
        "text": "to visualize what this convolution operation really is.",
        "start": 82.964,
        "duration": 2.976
    },
    {
        "text": "Today our basic job is to work through a particular example, ",
        "start": 85.94,
        "duration": 3.565
    },
    {
        "text": "which is to ask what happens when you add two normally distributed random variables, ",
        "start": 89.505,
        "duration": 4.968
    },
    {
        "text": "which, as you know by now, is the same as asking what do you get if ",
        "start": 94.473,
        "duration": 3.975
    },
    {
        "text": "you compute a convolution between two Gaussian functions.",
        "start": 98.448,
        "duration": 3.332
    },
    {
        "text": "I'd like to share an especially pleasing visual way that you can think ",
        "start": 102.52,
        "duration": 3.311
    },
    {
        "text": "about this calculation, which hopefully offers some sense of what makes ",
        "start": 105.831,
        "duration": 3.357
    },
    {
        "text": "the e to the negative x squared function special in the first place.",
        "start": 109.188,
        "duration": 3.172
    },
    {
        "text": "After we walk through it, we'll talk about how this calculation ",
        "start": 112.36,
        "duration": 2.894
    },
    {
        "text": "is one of the steps involved in proving the central limit theorem.",
        "start": 115.254,
        "duration": 2.986
    },
    {
        "text": "It's the step that answers the question of why a ",
        "start": 118.32,
        "duration": 2.517
    },
    {
        "text": "Gaussian and not something else is the central limit.",
        "start": 120.837,
        "duration": 2.723
    },
    {
        "text": "But first, let's dive in.",
        "start": 124.2,
        "duration": 1.64
    },
    {
        "text": "The full formula for a Gaussian is more complicated than just e to the negative x squared.",
        "start": 129.78,
        "duration": 4.66
    },
    {
        "text": "The exponent is typically written as negative one half times x divided by sigma squared, ",
        "start": 134.82,
        "duration": 4.663
    },
    {
        "text": "where sigma describes the spread of the distribution, specifically the standard deviation.",
        "start": 139.483,
        "duration": 4.717
    },
    {
        "text": "All of this needs to be multiplied by a fraction on the front, ",
        "start": 144.68,
        "duration": 3.201
    },
    {
        "text": "which is there to make sure that the area under the curve is one, ",
        "start": 147.881,
        "duration": 3.354
    },
    {
        "text": "making it a valid probability distribution.",
        "start": 151.235,
        "duration": 2.185
    },
    {
        "text": "And if you want to consider distributions that aren't necessarily centered at zero, ",
        "start": 154.02,
        "duration": 3.855
    },
    {
        "text": "you would also throw another parameter, mu, into the exponent like this.",
        "start": 157.875,
        "duration": 3.305
    },
    {
        "text": "Although for everything we'll be doing here, we just consider centered distributions.",
        "start": 161.54,
        "duration": 3.58
    },
    {
        "text": "Now if you look at our central goal for today, ",
        "start": 165.8,
        "duration": 2.605
    },
    {
        "text": "which is to compute a convolution between two Gaussian functions, ",
        "start": 168.405,
        "duration": 3.658
    },
    {
        "text": "the direct way to do this would be to take the definition of a convolution, ",
        "start": 172.063,
        "duration": 4.213
    },
    {
        "text": "this integral expression we built up last video, ",
        "start": 176.276,
        "duration": 2.716
    },
    {
        "text": "and then to plug in for each one of the functions involved the formula for a Gaussian.",
        "start": 178.992,
        "duration": 4.768
    },
    {
        "text": "It's kind of a lot of symbols when you throw it all together, ",
        "start": 184.22,
        "duration": 2.54
    },
    {
        "text": "but more than anything, working this out is an exercise in completing the square.",
        "start": 186.76,
        "duration": 3.32
    },
    {
        "text": "And there's nothing wrong with that.",
        "start": 190.56,
        "duration": 1.02
    },
    {
        "text": "That will get you the answer that you want.",
        "start": 191.72,
        "duration": 1.5
    },
    {
        "text": "But of course, you know me, I'm a sucker for visual intuition, and in this case, ",
        "start": 193.76,
        "duration": 3.744
    },
    {
        "text": "there's another way to think about it that I haven't seen written about before ",
        "start": 197.504,
        "duration": 3.652
    },
    {
        "text": "that offers a very nice connection to other aspects of this distribution, ",
        "start": 201.156,
        "duration": 3.421
    },
    {
        "text": "like the presence of pi and certain ways to derive where it comes from.",
        "start": 204.577,
        "duration": 3.283
    },
    {
        "text": "And the way I'd like to do this is by first peeling away all of the ",
        "start": 208.2,
        "duration": 3.237
    },
    {
        "text": "constants associated with the actual distribution, ",
        "start": 211.437,
        "duration": 2.428
    },
    {
        "text": "and just showing the computation for the simplified form, e to the negative x squared.",
        "start": 213.865,
        "duration": 4.095
    },
    {
        "text": "The essence of what we want to compute is what the ",
        "start": 217.96,
        "duration": 2.837
    },
    {
        "text": "convolution between two copies of this function looks like.",
        "start": 220.797,
        "duration": 3.283
    },
    {
        "text": "If you'll remember, in the last video we had two different ways to visualize ",
        "start": 224.46,
        "duration": 3.9
    },
    {
        "text": "convolutions, and the one we'll be using here is the second one involving diagonal slices.",
        "start": 228.36,
        "duration": 4.56
    },
    {
        "text": "And as a quick reminder of the way that worked, ",
        "start": 233.28,
        "duration": 2.622
    },
    {
        "text": "if you have two different distributions that are described by two different functions, ",
        "start": 235.902,
        "duration": 4.753
    },
    {
        "text": "f and g, then every possible pair of values that you might get when you ",
        "start": 240.655,
        "duration": 3.933
    },
    {
        "text": "sample from these two distributions can be thought of as individual points ",
        "start": 244.588,
        "duration": 4.097
    },
    {
        "text": "on the xy-plane.",
        "start": 248.685,
        "duration": 0.875
    },
    {
        "text": "And the probability density of landing on one such point, ",
        "start": 250.36,
        "duration": 3.707
    },
    {
        "text": "assuming independence, looks like f of x times g of y.",
        "start": 254.067,
        "duration": 3.452
    },
    {
        "text": "So what we do is we look at a graph of that expression as a two-variable ",
        "start": 258.0,
        "duration": 4.039
    },
    {
        "text": "function of x and y, which is a way of showing the distribution of all ",
        "start": 262.039,
        "duration": 3.929
    },
    {
        "text": "possible outcomes when we sample from the two different variables.",
        "start": 265.968,
        "duration": 3.652
    },
    {
        "text": "To interpret the convolution of f and g evaluated on some input s, ",
        "start": 270.56,
        "duration": 4.344
    },
    {
        "text": "which is a way of saying how likely are you to get a pair of samples that ",
        "start": 274.904,
        "duration": 4.799
    },
    {
        "text": "adds up to this sum s, what you do is you look at a slice of this graph ",
        "start": 279.703,
        "duration": 4.668
    },
    {
        "text": "over the line x plus y equals s, and you consider the area under that slice.",
        "start": 284.371,
        "duration": 4.929
    },
    {
        "text": "This area is almost, but not quite, the value of the convolution at s.",
        "start": 291.1,
        "duration": 5.22
    },
    {
        "text": "For a mildly technical reason, you need to divide by the square root of 2.",
        "start": 296.8,
        "duration": 3.36
    },
    {
        "text": "Still, this area is the key feature to focus on.",
        "start": 300.84,
        "duration": 2.6
    },
    {
        "text": "You can think of it as a way to combine together all the probability ",
        "start": 303.44,
        "duration": 3.972
    },
    {
        "text": "densities for all of the outcomes corresponding to a given sum.",
        "start": 307.412,
        "duration": 3.628
    },
    {
        "text": "In the specific case where these two functions look like e to ",
        "start": 313.3,
        "duration": 3.346
    },
    {
        "text": "the negative x squared and e to the negative y squared, ",
        "start": 316.646,
        "duration": 3.022
    },
    {
        "text": "the resulting 3D graph has a really nice property that you can exploit.",
        "start": 319.668,
        "duration": 3.832
    },
    {
        "text": "It's rotationally symmetric.",
        "start": 323.72,
        "duration": 1.96
    },
    {
        "text": "You can see this by combining the terms and noticing that it's entirely ",
        "start": 326.88,
        "duration": 3.932
    },
    {
        "text": "a function of x squared plus y squared, and this term describes the ",
        "start": 330.812,
        "duration": 3.715
    },
    {
        "text": "square of the distance between any point on the xy plane and the origin.",
        "start": 334.527,
        "duration": 3.933
    },
    {
        "text": "So in other words, the expression is purely a function of the distance from the origin.",
        "start": 339.2,
        "duration": 3.96
    },
    {
        "text": "And by the way, this would not be true for any other distribution.",
        "start": 344.56,
        "duration": 3.36
    },
    {
        "text": "It's a property that uniquely characterizes bell curves.",
        "start": 348.1,
        "duration": 3.18
    },
    {
        "text": "So for most other pairs of functions, these diagonal slices will be some complicated ",
        "start": 353.16,
        "duration": 4.159
    },
    {
        "text": "shape that's hard to think about, and honestly, ",
        "start": 357.319,
        "duration": 2.349
    },
    {
        "text": "calculating the area would just amount to computing the original integral that defines ",
        "start": 359.668,
        "duration": 4.257
    },
    {
        "text": "a convolution in the first place.",
        "start": 363.925,
        "duration": 1.615
    },
    {
        "text": "So in most cases, the visual intuition doesn't really buy you anything.",
        "start": 365.94,
        "duration": 3.42
    },
    {
        "text": "But in the case of bell curves, you can leverage that rotational symmetry.",
        "start": 370.36,
        "duration": 3.56
    },
    {
        "text": "Here, focus on one of these slices over the line x plus y equals s for some value of s.",
        "start": 374.8,
        "duration": 5.68
    },
    {
        "text": "And remember, the convolution that we're trying to compute is a function of s.",
        "start": 381.3,
        "duration": 4.54
    },
    {
        "text": "The thing that you want is an expression of s that tells you the area under this slice.",
        "start": 385.84,
        "duration": 5.26
    },
    {
        "text": "Well, if you look at that line, it intersects the x-axis at s zero and the y-axis ",
        "start": 391.7,
        "duration": 4.692
    },
    {
        "text": "at zero s, and a little bit of Pythagoras will show you that the straight line ",
        "start": 396.392,
        "duration": 4.521
    },
    {
        "text": "distance from the origin to this line is s divided by the square root of two.",
        "start": 400.913,
        "duration": 4.407
    },
    {
        "text": "Now, because of the symmetry, this slice is identical to one ",
        "start": 405.86,
        "duration": 3.5
    },
    {
        "text": "that you get rotating 45 degrees where you'd find something ",
        "start": 409.36,
        "duration": 3.442
    },
    {
        "text": "parallel to the y-axis the same distance away from the origin.",
        "start": 412.802,
        "duration": 3.558
    },
    {
        "text": "The key is that computing this other area of a slice parallel to the y-axis is much, ",
        "start": 417.64,
        "duration": 4.726
    },
    {
        "text": "much easier than slices in other directions because it only ",
        "start": 422.366,
        "duration": 3.336
    },
    {
        "text": "involves taking an integral with respect to y.",
        "start": 425.702,
        "duration": 2.558
    },
    {
        "text": "The value of x on this slice is a constant.",
        "start": 428.74,
        "duration": 2.7
    },
    {
        "text": "Specifically, it would be the constant s divided by the square root of two.",
        "start": 431.62,
        "duration": 3.14
    },
    {
        "text": "So when you're computing the integral, finding this area, ",
        "start": 434.76,
        "duration": 3.471
    },
    {
        "text": "all of this term here behaves like it was just some number, and you can factor it out.",
        "start": 438.231,
        "duration": 5.149
    },
    {
        "text": "This is the important point.",
        "start": 443.88,
        "duration": 1.06
    },
    {
        "text": "All of the stuff that's involving s is now entirely separate from the integrated variable.",
        "start": 445.28,
        "duration": 4.92
    },
    {
        "text": "This remaining integral is a little bit tricky.",
        "start": 450.82,
        "duration": 2.18
    },
    {
        "text": "I did a whole video on it, it's actually quite famous.",
        "start": 453.08,
        "duration": 2.12
    },
    {
        "text": "But you almost don't really care.",
        "start": 455.5,
        "duration": 1.4
    },
    {
        "text": "The point is that it's just some number.",
        "start": 457.24,
        "duration": 1.76
    },
    {
        "text": "That number happens to be the square root of pi, ",
        "start": 459.0,
        "duration": 2.645
    },
    {
        "text": "but what really matters is that it's something with no dependence on s.",
        "start": 461.645,
        "duration": 3.835
    },
    {
        "text": "And essentially this is our answer.",
        "start": 466.88,
        "duration": 1.6
    },
    {
        "text": "We were looking for an expression for the area of these slices as a function of s, ",
        "start": 468.78,
        "duration": 4.475
    },
    {
        "text": "and now we have it.",
        "start": 473.255,
        "duration": 1.025
    },
    {
        "text": "It looks like e to the negative s squared divided by two, scaled by some constant.",
        "start": 474.38,
        "duration": 4.46
    },
    {
        "text": "In other words, it's also a bell curve, another Gaussian, ",
        "start": 479.3,
        "duration": 2.909
    },
    {
        "text": "just stretched out a little bit because of this two in the exponent.",
        "start": 482.209,
        "duration": 3.411
    },
    {
        "text": "As I said earlier, the convolution evaluated at s is not quite this area.",
        "start": 485.62,
        "duration": 5.24
    },
    {
        "text": "Technically it's this area divided by the square root of two.",
        "start": 491.34,
        "duration": 2.82
    },
    {
        "text": "We talked about it in the last video, but it doesn't ",
        "start": 494.8,
        "duration": 2.101
    },
    {
        "text": "really matter because it just gets baked into the constant.",
        "start": 496.901,
        "duration": 2.339
    },
    {
        "text": "What really matters is the conclusion that a convolution ",
        "start": 499.68,
        "duration": 3.226
    },
    {
        "text": "between two Gaussians is itself another Gaussian.",
        "start": 502.906,
        "duration": 2.774
    },
    {
        "text": "If you were to go back and reintroduce all of the constants for a normal distribution ",
        "start": 507.56,
        "duration": 4.37
    },
    {
        "text": "with a mean zero and an arbitrary standard deviation sigma, ",
        "start": 511.93,
        "duration": 3.05
    },
    {
        "text": "essentially identical reasoning will lead to the same square root of two factor that ",
        "start": 514.98,
        "duration": 4.32
    },
    {
        "text": "shows up in the exponent and out front, and it leads to the conclusion that the ",
        "start": 519.3,
        "duration": 4.066
    },
    {
        "text": "convolution between two such normal distributions is another normal distribution with a ",
        "start": 523.366,
        "duration": 4.472
    },
    {
        "text": "standard deviation square root of two times sigma.",
        "start": 527.838,
        "duration": 2.542
    },
    {
        "text": "If you haven't computed a lot of convolutions before, ",
        "start": 530.98,
        "duration": 2.563
    },
    {
        "text": "it's worth emphasizing this is a very special result.",
        "start": 533.543,
        "duration": 2.517
    },
    {
        "text": "Almost always you end up with a completely different kind of function, ",
        "start": 536.38,
        "duration": 3.532
    },
    {
        "text": "but here there's a sort of stability to the process.",
        "start": 539.912,
        "duration": 2.588
    },
    {
        "text": "Also, for those of you who enjoy exercises, I'll leave one up on the screen ",
        "start": 543.26,
        "duration": 3.195
    },
    {
        "text": "for how you would handle the case of two different standard deviations.",
        "start": 546.455,
        "duration": 2.985
    },
    {
        "text": "Still, some of you might be raising your hands and saying, what's the big deal?",
        "start": 550.42,
        "duration": 3.52
    },
    {
        "text": "I mean, when you first heard the question, what do you get when you ",
        "start": 554.48,
        "duration": 3.264
    },
    {
        "text": "add two normally distributed random variables, ",
        "start": 557.744,
        "duration": 2.256
    },
    {
        "text": "you probably even guessed that the answer should be another normally distributed variable.",
        "start": 560.0,
        "duration": 4.32
    },
    {
        "text": "After all, what else is it going to be?",
        "start": 564.76,
        "duration": 1.6
    },
    {
        "text": "Normal distributions are supposedly quite common, so why not?",
        "start": 566.86,
        "duration": 3.38
    },
    {
        "text": "You could even say that this should follow from the central limit theorem.",
        "start": 570.24,
        "duration": 3.1
    },
    {
        "text": "But that would have it all backwards.",
        "start": 573.86,
        "duration": 1.62
    },
    {
        "text": "First of all, the supposed ubiquity of normal distributions is often a little ",
        "start": 576.18,
        "duration": 3.823
    },
    {
        "text": "exaggerated, but to the extent that they do come up, ",
        "start": 580.003,
        "duration": 2.597
    },
    {
        "text": "it is because of the central limit theorem, but it would be cheating to say the central ",
        "start": 582.6,
        "duration": 4.314
    },
    {
        "text": "limit theorem implies this result because this computation we just did is the reason ",
        "start": 586.914,
        "duration": 4.166
    },
    {
        "text": "that the function at the heart of the central limit theorem is a Gaussian in the first ",
        "start": 591.08,
        "duration": 4.264
    },
    {
        "text": "place, and not some other function.",
        "start": 595.344,
        "duration": 1.716
    },
    {
        "text": "We've talked all about the central limit theorem before, ",
        "start": 597.06,
        "duration": 3.24
    },
    {
        "text": "but essentially it says if you repeatedly add copies of a random variable to itself, ",
        "start": 600.3,
        "duration": 4.831
    },
    {
        "text": "which mathematically looks like repeatedly computing convolutions against a given ",
        "start": 605.131,
        "duration": 4.661
    },
    {
        "text": "distribution, then after appropriate shifting and rescaling, ",
        "start": 609.792,
        "duration": 3.468
    },
    {
        "text": "the tendency is always to approach a normal distribution.",
        "start": 613.26,
        "duration": 3.24
    },
    {
        "text": "Technically, there's a small assumption the distribution you start with ",
        "start": 616.98,
        "duration": 3.209
    },
    {
        "text": "can't have infinite variance, but it's a relatively soft assumption.",
        "start": 620.189,
        "duration": 3.031
    },
    {
        "text": "The magic is that for a huge category of initial distributions, ",
        "start": 623.22,
        "duration": 3.655
    },
    {
        "text": "this process of adding a whole bunch of random variables drawn from ",
        "start": 626.875,
        "duration": 3.884
    },
    {
        "text": "that distribution always tends towards this one universal shape, a Gaussian.",
        "start": 630.759,
        "duration": 4.341
    },
    {
        "text": "One common approach to proving this theorem involves two separate steps.",
        "start": 635.82,
        "duration": 3.48
    },
    {
        "text": "The first step is to show that for all the different finite variance ",
        "start": 639.6,
        "duration": 3.588
    },
    {
        "text": "distributions you might start with, there exists a single universal ",
        "start": 643.188,
        "duration": 3.536
    },
    {
        "text": "shape that this process of repeated convolutions tends towards.",
        "start": 646.724,
        "duration": 3.276
    },
    {
        "text": "This step is actually pretty technical, it goes ",
        "start": 650.0,
        "duration": 2.142
    },
    {
        "text": "a little beyond what I want to talk about here.",
        "start": 652.142,
        "duration": 2.098
    },
    {
        "text": "You often use these objects called moment generating functions that gives you ",
        "start": 654.52,
        "duration": 3.828
    },
    {
        "text": "a very abstract argument that there must be some universal shape, ",
        "start": 658.348,
        "duration": 3.239
    },
    {
        "text": "but it doesn't make any claim about what that particular shape is, ",
        "start": 661.587,
        "duration": 3.288
    },
    {
        "text": "just that everything in this big family is tending towards a single point in ",
        "start": 664.875,
        "duration": 3.779
    },
    {
        "text": "the space of distributions.",
        "start": 668.654,
        "duration": 1.326
    },
    {
        "text": "So then step number two is what we just showed in this video, ",
        "start": 670.62,
        "duration": 3.258
    },
    {
        "text": "prove that the convolution of two Gaussians gives another Gaussian.",
        "start": 673.878,
        "duration": 3.522
    },
    {
        "text": "What that means is that as you apply this process of repeated convolutions, ",
        "start": 677.4,
        "duration": 4.117
    },
    {
        "text": "a Gaussian doesn't change, it's a fixed point, ",
        "start": 681.517,
        "duration": 2.546
    },
    {
        "text": "so the only thing it can approach is itself, and since it's one member in this ",
        "start": 684.063,
        "duration": 4.279
    },
    {
        "text": "big family of distributions, all of which must be tending towards a single universal ",
        "start": 688.342,
        "duration": 4.605
    },
    {
        "text": "shape, it must be that universal shape.",
        "start": 692.947,
        "duration": 2.113
    },
    {
        "text": "I mentioned at the start how this calculation, step two, ",
        "start": 695.58,
        "duration": 2.982
    },
    {
        "text": "is something that you can do directly, just symbolically with the definitions, ",
        "start": 698.562,
        "duration": 4.134
    },
    {
        "text": "but one of the reasons I'm so charmed by a geometric argument that leverages the ",
        "start": 702.696,
        "duration": 4.239
    },
    {
        "text": "rotational symmetry of this graph is that it directly connects to a few things that ",
        "start": 706.935,
        "duration": 4.395
    },
    {
        "text": "we've talked about on this channel before, for example, ",
        "start": 711.33,
        "duration": 2.93
    },
    {
        "text": "the Herschel-Maxwell derivation of a Gaussian, ",
        "start": 714.26,
        "duration": 2.46
    },
    {
        "text": "which essentially says that you can view this rotational symmetry as the defining ",
        "start": 716.72,
        "duration": 4.291
    },
    {
        "text": "feature of the distribution, that it locks you into this e to the negative x squared ",
        "start": 721.011,
        "duration": 4.447
    },
    {
        "text": "form, and also as an added bonus, it connects to the classic proof for why pi shows up ",
        "start": 725.458,
        "duration": 4.553
    },
    {
        "text": "in the formula, meaning we now have a direct line between the presence and mystery of ",
        "start": 730.011,
        "duration": 4.5
    },
    {
        "text": "that pi and the central limit theorem.",
        "start": 734.511,
        "duration": 1.989
    },
    {
        "text": "Also, on a recent Patreon post, the channel supporter Daksha Vaid-Quinter ",
        "start": 737.06,
        "duration": 3.297
    },
    {
        "text": "brought my attention to a completely different approach I hadn't seen before, ",
        "start": 740.357,
        "duration": 3.475
    },
    {
        "text": "which leverages the use of entropy, and again, for the theoretically curious among you, ",
        "start": 743.832,
        "duration": 3.921
    },
    {
        "text": "I'll leave some links in the description.",
        "start": 747.753,
        "duration": 1.827
    },
    {
        "text": "By the way, if you want to stay up to date with new videos, ",
        "start": 750.96,
        "duration": 2.625
    },
    {
        "text": "and also any other projects that I put out there, like the Summer of Math Exposition, ",
        "start": 753.585,
        "duration": 3.764
    },
    {
        "text": "there is a mailing list.",
        "start": 757.349,
        "duration": 1.051
    },
    {
        "text": "It's relatively new, and I'm pretty sparing about ",
        "start": 758.72,
        "duration": 2.159
    },
    {
        "text": "only posting what I think people will enjoy.",
        "start": 760.879,
        "duration": 1.901
    },
    {
        "text": "Usually I try not to be too promotional at the end of videos these days, ",
        "start": 763.22,
        "duration": 12.441
    },
    {
        "text": "but if you are interested in following the work that I do, ",
        "start": 775.661,
        "duration": 10.055
    },
    {
        "text": "this is probably one of the most enduring ways to do so.",
        "start": 785.716,
        "duration": 9.544
    }
]