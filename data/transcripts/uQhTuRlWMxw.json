[
    {
        "text": "As you can probably tell by now, the bulk of this series is on understanding matrix ",
        "start": 10.24,
        "duration": 4.718
    },
    {
        "text": "and vector operations through that more visual lens of linear transformations.",
        "start": 14.958,
        "duration": 4.382
    },
    {
        "text": "This video is no exception, describing the concepts of inverse matrices, ",
        "start": 19.98,
        "duration": 4.368
    },
    {
        "text": "column space, rank, and null space through that lens.",
        "start": 24.348,
        "duration": 3.172
    },
    {
        "text": "A forewarning though, I'm not going to talk about the methods for actually ",
        "start": 27.52,
        "duration": 3.382
    },
    {
        "text": "computing these things, and some would argue that that's pretty important.",
        "start": 30.902,
        "duration": 3.338
    },
    {
        "text": "There are a lot of very good resources for learning those methods outside this series, ",
        "start": 34.84,
        "duration": 4.513
    },
    {
        "text": "keywords Gaussian elimination and row echelon form.",
        "start": 39.353,
        "duration": 2.647
    },
    {
        "text": "I think most of the value that I actually have to add here is on the intuition half.",
        "start": 42.54,
        "duration": 3.8
    },
    {
        "text": "Plus, in practice, we usually get software to compute this stuff for us anyway.",
        "start": 46.9,
        "duration": 3.58
    },
    {
        "text": "First, a few words on the usefulness of linear algebra.",
        "start": 51.5,
        "duration": 2.42
    },
    {
        "text": "By now, you already have a hint for how it's used in describing the manipulation ",
        "start": 54.3,
        "duration": 3.545
    },
    {
        "text": "of space, which is useful for things like computer graphics and robotics.",
        "start": 57.845,
        "duration": 3.195
    },
    {
        "text": "But one of the main reasons that linear algebra is more ",
        "start": 61.5,
        "duration": 2.756
    },
    {
        "text": "broadly applicable and required for just about any technical ",
        "start": 64.256,
        "duration": 3.003
    },
    {
        "text": "discipline is that it lets us solve certain systems of equations.",
        "start": 67.259,
        "duration": 3.201
    },
    {
        "text": "When I say system of equations, I mean you have a list of variables, ",
        "start": 71.38,
        "duration": 3.386
    },
    {
        "text": "things you don't know, and a list of equations relating them.",
        "start": 74.766,
        "duration": 2.994
    },
    {
        "text": "In a lot of situations, those equations can get very complicated.",
        "start": 78.34,
        "duration": 3.26
    },
    {
        "text": "But, if you're lucky, they might take on a certain special form.",
        "start": 82.12,
        "duration": 3.18
    },
    {
        "text": "Within each equation, the only thing happening to each variable is ",
        "start": 86.44,
        "duration": 3.48
    },
    {
        "text": "that it's scaled by some constant, and the only thing happening to ",
        "start": 89.92,
        "duration": 3.479
    },
    {
        "text": "each of those scaled variables is that they're added to each other.",
        "start": 93.399,
        "duration": 3.481
    },
    {
        "text": "So no exponents or fancy functions or multiplying two variables together, ",
        "start": 97.54,
        "duration": 3.854
    },
    {
        "text": "things like that.",
        "start": 101.394,
        "duration": 0.886
    },
    {
        "text": "The typical way to organize this sort of special system of equations is to ",
        "start": 103.42,
        "duration": 4.192
    },
    {
        "text": "throw all the variables on the left and put any lingering constants on the right.",
        "start": 107.612,
        "duration": 4.528
    },
    {
        "text": "It's also nice to vertically line up the common variables, ",
        "start": 113.6,
        "duration": 2.631
    },
    {
        "text": "and to do that, you might need to throw in some zero coefficients ",
        "start": 116.231,
        "duration": 2.943
    },
    {
        "text": "whenever the variable doesn't show up in one of the equations.",
        "start": 119.174,
        "duration": 2.766
    },
    {
        "text": "This is called a linear system of equations.",
        "start": 124.54,
        "duration": 2.7
    },
    {
        "text": "You might notice that this looks a lot like matrix-vector multiplication.",
        "start": 128.1,
        "duration": 3.08
    },
    {
        "text": "In fact, you can package all of the equations together into a single ",
        "start": 131.82,
        "duration": 3.809
    },
    {
        "text": "vector equation where you have the matrix containing all of the ",
        "start": 135.629,
        "duration": 3.532
    },
    {
        "text": "constant coefficients and a vector containing all of the variables, ",
        "start": 139.161,
        "duration": 3.754
    },
    {
        "text": "and their matrix-vector product equals some different constant vector.",
        "start": 142.915,
        "duration": 3.865
    },
    {
        "text": "Let's name that constant matrix A, denote the vector holding the variables ",
        "start": 148.64,
        "duration": 4.42
    },
    {
        "text": "with a bold-faced X, and call the constant vector on the right-hand side V.",
        "start": 153.06,
        "duration": 4.42
    },
    {
        "text": "This is more than just a notational trick to get ",
        "start": 158.86,
        "duration": 2.17
    },
    {
        "text": "our system of equations written on one line.",
        "start": 161.03,
        "duration": 1.95
    },
    {
        "text": "It sheds light on a pretty cool geometric interpretation for the problem.",
        "start": 163.34,
        "duration": 3.44
    },
    {
        "text": "The matrix A corresponds with some linear transformation, ",
        "start": 167.62,
        "duration": 3.514
    },
    {
        "text": "so solving Ax equals V means we're looking for a vector X, which, ",
        "start": 171.134,
        "duration": 3.998
    },
    {
        "text": "after applying the transformation, lands on V.",
        "start": 175.132,
        "duration": 2.788
    },
    {
        "text": "Think about what's happening here for a moment.",
        "start": 179.94,
        "duration": 1.84
    },
    {
        "text": "You can hold in your head this really complicated idea of multiple ",
        "start": 182.06,
        "duration": 3.239
    },
    {
        "text": "variables all intermingling with each other just by thinking about ",
        "start": 185.299,
        "duration": 3.239
    },
    {
        "text": "squishing and morphing space and trying to figure out which vector lands on another.",
        "start": 188.538,
        "duration": 4.062
    },
    {
        "text": "Cool, right?",
        "start": 193.16,
        "duration": 0.6
    },
    {
        "text": "To start simple, let's say you have a system with two equations and two unknowns.",
        "start": 194.6,
        "duration": 4.08
    },
    {
        "text": "This means the matrix A is a 2x2 matrix, and V and X are each two-dimensional vectors.",
        "start": 199.0,
        "duration": 4.96
    },
    {
        "text": "Now, how we think about the solutions to this equation depends on whether the ",
        "start": 205.6,
        "duration": 4.02
    },
    {
        "text": "transformation associated with A squishes all of space into a lower dimension, ",
        "start": 209.62,
        "duration": 4.073
    },
    {
        "text": "like a line or a point, or if it leaves everything spanning the full two dimensions ",
        "start": 213.693,
        "duration": 4.33
    },
    {
        "text": "where it started.",
        "start": 218.023,
        "duration": 0.877
    },
    {
        "text": "In the language of the last video, we subdivide into the cases where ",
        "start": 220.32,
        "duration": 3.86
    },
    {
        "text": "A has zero determinant and the case where A has non-zero determinant.",
        "start": 224.18,
        "duration": 3.86
    },
    {
        "text": "Let's start with the most likely case, where the determinant is non-zero, ",
        "start": 231.3,
        "duration": 3.545
    },
    {
        "text": "meaning space does not get squished into a zero-area region.",
        "start": 234.845,
        "duration": 2.875
    },
    {
        "text": "In this case, there will always be one and only one vector that lands on V, ",
        "start": 238.6,
        "duration": 4.193
    },
    {
        "text": "and you can find it by playing the transformation in reverse.",
        "start": 242.793,
        "duration": 3.367
    },
    {
        "text": "Following where V goes as we rewind the tape like this, ",
        "start": 246.7,
        "duration": 3.441
    },
    {
        "text": "you'll find the vector x such that A times x equals V.",
        "start": 250.141,
        "duration": 3.319
    },
    {
        "text": "When you play the transformation in reverse, it actually corresponds to a separate ",
        "start": 255.4,
        "duration": 4.53
    },
    {
        "text": "linear transformation, commonly called the inverse of A, denoted A to the negative one.",
        "start": 259.93,
        "duration": 4.75
    },
    {
        "text": "For example, if A was a counterclockwise rotation by 90 degrees, ",
        "start": 265.36,
        "duration": 3.671
    },
    {
        "text": "then the inverse of A would be a clockwise rotation by 90 degrees.",
        "start": 269.031,
        "duration": 3.729
    },
    {
        "text": "If A was a rightward shear that pushes j-hat one unit to the right, ",
        "start": 274.32,
        "duration": 3.699
    },
    {
        "text": "the inverse of A would be a leftward shear that pushes j-hat one unit to the left.",
        "start": 278.019,
        "duration": 4.461
    },
    {
        "text": "In general, A inverse is the unique transformation with the property that if you first ",
        "start": 284.1,
        "duration": 4.533
    },
    {
        "text": "apply A, then follow it with the transformation A inverse, ",
        "start": 288.633,
        "duration": 3.075
    },
    {
        "text": "you end up back where you started.",
        "start": 291.708,
        "duration": 1.772
    },
    {
        "text": "Applying one transformation after another is captured algebraically with ",
        "start": 294.54,
        "duration": 4.098
    },
    {
        "text": "matrix multiplication, so the core property of this transformation A inverse ",
        "start": 298.638,
        "duration": 4.323
    },
    {
        "text": "is that A inverse times A equals the matrix that corresponds to doing nothing.",
        "start": 302.961,
        "duration": 4.379
    },
    {
        "text": "The transformation that does nothing is called the identity transformation.",
        "start": 308.2,
        "duration": 3.12
    },
    {
        "text": "It leaves i-hat and j-hat each where they are, unmoved, so its columns are 1,0 and 0,1.",
        "start": 311.78,
        "duration": 6.3
    },
    {
        "text": "Once you find this inverse, which in practice you'd do with a computer, ",
        "start": 319.98,
        "duration": 3.98
    },
    {
        "text": "you can solve your equation by multiplying this inverse matrix by v.",
        "start": 323.96,
        "duration": 3.76
    },
    {
        "text": "And again, what this means geometrically is that you're ",
        "start": 329.96,
        "duration": 3.298
    },
    {
        "text": "playing the transformation in reverse and following v.",
        "start": 333.258,
        "duration": 3.182
    },
    {
        "text": "This non-zero determinant case, which for a random choice of matrix is by far the ",
        "start": 340.2,
        "duration": 4.116
    },
    {
        "text": "most likely one, corresponds with the idea that if you have two unknowns and two ",
        "start": 344.316,
        "duration": 4.067
    },
    {
        "text": "equations, it's almost certainly the case that there's a single unique solution.",
        "start": 348.383,
        "duration": 4.017
    },
    {
        "text": "This idea also makes sense in higher dimensions, ",
        "start": 353.68,
        "duration": 2.504
    },
    {
        "text": "when the number of equations equals the number of unknowns.",
        "start": 356.184,
        "duration": 3.016
    },
    {
        "text": "Again, the system of equations can be translated to the geometric ",
        "start": 359.38,
        "duration": 4.665
    },
    {
        "text": "interpretation where you have some transformation A and some vector v, ",
        "start": 364.045,
        "duration": 5.019
    },
    {
        "text": "and you're looking for the vector x that lands on v.",
        "start": 369.064,
        "duration": 3.676
    },
    {
        "text": "As long as the transformation A doesn't squish all of space into a lower dimension, ",
        "start": 375.74,
        "duration": 4.795
    },
    {
        "text": "meaning its determinant is non-zero, there will be an inverse transformation A inverse, ",
        "start": 380.535,
        "duration": 5.024
    },
    {
        "text": "with the property that if you first do A, then you do A inverse, ",
        "start": 385.559,
        "duration": 3.711
    },
    {
        "text": "it's the same as doing nothing.",
        "start": 389.27,
        "duration": 1.77
    },
    {
        "text": "And to solve your equation, you just have to multiply ",
        "start": 393.54,
        "duration": 3.034
    },
    {
        "text": "that reverse transformation matrix by the vector v.",
        "start": 396.574,
        "duration": 2.866
    },
    {
        "text": "But when the determinant is zero, and the transformation associated with the ",
        "start": 403.5,
        "duration": 4.171
    },
    {
        "text": "system of equations squishes space into a smaller dimension, there is no inverse.",
        "start": 407.671,
        "duration": 4.389
    },
    {
        "text": "You cannot unsquish a line to turn it into a plane.",
        "start": 412.48,
        "duration": 2.98
    },
    {
        "text": "At least that's not something that a function can do.",
        "start": 415.98,
        "duration": 2.08
    },
    {
        "text": "That would require transforming each individual vector into a whole line full of vectors.",
        "start": 418.36,
        "duration": 4.62
    },
    {
        "text": "But functions can only take a single input to a single output.",
        "start": 423.74,
        "duration": 3.0
    },
    {
        "text": "Similarly, for three equations and three unknowns, ",
        "start": 428.4,
        "duration": 2.808
    },
    {
        "text": "there will be no inverse if the corresponding transformation ",
        "start": 431.208,
        "duration": 3.36
    },
    {
        "text": "squishes 3D space onto the plane, or even if it squishes it onto a line or a point.",
        "start": 434.568,
        "duration": 4.572
    },
    {
        "text": "Those all correspond to a determinant of zero, ",
        "start": 439.92,
        "duration": 2.28
    },
    {
        "text": "since any region is squished into something with zero volume.",
        "start": 442.2,
        "duration": 2.96
    },
    {
        "text": "It's still possible that a solution exists even when there is no inverse.",
        "start": 446.7,
        "duration": 3.94
    },
    {
        "text": "It's just that when your transformation squishes space onto, say, a line, ",
        "start": 450.72,
        "duration": 4.3
    },
    {
        "text": "you have to be lucky enough that the vector v lives somewhere on that line.",
        "start": 455.02,
        "duration": 4.36
    },
    {
        "text": "You might notice that some of these zero determinant ",
        "start": 463.3,
        "duration": 2.676
    },
    {
        "text": "cases feel a lot more restrictive than others.",
        "start": 465.976,
        "duration": 2.324
    },
    {
        "text": "Given a 3x3 matrix, for example, it seems a lot harder for a solution ",
        "start": 468.84,
        "duration": 3.781
    },
    {
        "text": "to exist when it squishes space onto a line compared to when it squishes ",
        "start": 472.621,
        "duration": 3.945
    },
    {
        "text": "things onto a plane, even though both of those are zero determinant.",
        "start": 476.566,
        "duration": 3.674
    },
    {
        "text": "We have some language that's a bit more specific than just saying zero determinant.",
        "start": 482.6,
        "duration": 3.5
    },
    {
        "text": "When the output of a transformation is a line, ",
        "start": 486.52,
        "duration": 2.711
    },
    {
        "text": "meaning it's one-dimensional, we say the transformation has a rank of one.",
        "start": 489.231,
        "duration": 4.269
    },
    {
        "text": "If all the vectors land on some two-dimensional plane, ",
        "start": 495.14,
        "duration": 2.933
    },
    {
        "text": "we say the transformation has a rank of two.",
        "start": 498.073,
        "duration": 2.347
    },
    {
        "text": "So the word rank means the number of dimensions in the output of a transformation.",
        "start": 502.92,
        "duration": 4.56
    },
    {
        "text": "For instance, in the case of 2x2 matrices, rank two is the best that it can be.",
        "start": 508.4,
        "duration": 4.32
    },
    {
        "text": "It means the basis vectors continue to span the full two dimensions of space, ",
        "start": 513.08,
        "duration": 4.212
    },
    {
        "text": "and the determinant is not zero.",
        "start": 517.292,
        "duration": 1.728
    },
    {
        "text": "But for 3x3 matrices, rank two means that we've collapsed, ",
        "start": 519.419,
        "duration": 3.22
    },
    {
        "text": "but not as much as they would have collapsed for a rank one situation.",
        "start": 522.639,
        "duration": 3.821
    },
    {
        "text": "If a 3D transformation has a non-zero determinant and its output fills all of 3D space, ",
        "start": 527.24,
        "duration": 4.836
    },
    {
        "text": "it has a rank of three.",
        "start": 532.076,
        "duration": 1.264
    },
    {
        "text": "This set of all possible outputs for your matrix, whether it's a line, ",
        "start": 534.52,
        "duration": 4.1
    },
    {
        "text": "a plane, 3D space, whatever, is called the column space of your matrix.",
        "start": 538.62,
        "duration": 4.1
    },
    {
        "text": "You can probably guess where that name comes from.",
        "start": 544.14,
        "duration": 2.14
    },
    {
        "text": "The columns of your matrix tell you where the basis vectors land, ",
        "start": 546.56,
        "duration": 4.224
    },
    {
        "text": "and the span of those transformed basis vectors gives you all possible outputs.",
        "start": 550.784,
        "duration": 5.056
    },
    {
        "text": "In other words, the column space is the span of the columns of your matrix.",
        "start": 556.36,
        "duration": 4.78
    },
    {
        "text": "So a more precise definition of rank would be that ",
        "start": 563.3,
        "duration": 2.847
    },
    {
        "text": "it's the number of dimensions in the column space.",
        "start": 566.147,
        "duration": 2.793
    },
    {
        "text": "When this rank is as high as it can be, meaning it equals the number of columns, ",
        "start": 569.94,
        "duration": 4.55
    },
    {
        "text": "we call the matrix full rank.",
        "start": 574.49,
        "duration": 1.63
    },
    {
        "text": "Notice the zero vector will always be included in the column space, ",
        "start": 578.54,
        "duration": 3.732
    },
    {
        "text": "since linear transformations must keep the origin fixed in place.",
        "start": 582.272,
        "duration": 3.568
    },
    {
        "text": "For a full rank transformation, the only vector ",
        "start": 586.9,
        "duration": 2.453
    },
    {
        "text": "that lands at the origin is the zero vector itself.",
        "start": 589.353,
        "duration": 2.607
    },
    {
        "text": "But for matrices that aren't full rank, which squish to a smaller dimension, ",
        "start": 592.46,
        "duration": 3.647
    },
    {
        "text": "you can have a whole bunch of vectors that land on zero.",
        "start": 596.107,
        "duration": 2.653
    },
    {
        "text": "If a 2D transformation squishes space onto a line, for example, ",
        "start": 601.64,
        "duration": 3.488
    },
    {
        "text": "there is a separate line in a different direction full of vectors that get squished ",
        "start": 605.128,
        "duration": 4.579
    },
    {
        "text": "onto the origin.",
        "start": 609.707,
        "duration": 0.873
    },
    {
        "text": "If a 3D transformation squishes space onto a plane, ",
        "start": 611.78,
        "duration": 2.711
    },
    {
        "text": "there's also a full line of vectors that land on the origin.",
        "start": 614.491,
        "duration": 3.129
    },
    {
        "text": "If a 3D transformation squishes all of space onto a line, ",
        "start": 620.52,
        "duration": 3.22
    },
    {
        "text": "then there's a whole plane full of vectors that land on the origin.",
        "start": 623.74,
        "duration": 3.72
    },
    {
        "text": "This set of vectors that lands on the origin is called the null space, ",
        "start": 632.8,
        "duration": 4.245
    },
    {
        "text": "or the kernel of your matrix.",
        "start": 637.045,
        "duration": 1.735
    },
    {
        "text": "It's the space of all vectors that become null, ",
        "start": 639.36,
        "duration": 2.435
    },
    {
        "text": "in the sense that they land on the zero vector.",
        "start": 641.795,
        "duration": 2.385
    },
    {
        "text": "In terms of the linear system of equations, when v happens to be the zero vector, ",
        "start": 645.68,
        "duration": 4.266
    },
    {
        "text": "the null space gives you all of the possible solutions to the equation.",
        "start": 649.946,
        "duration": 3.694
    },
    {
        "text": "So that's a very high level overview of how to ",
        "start": 656.42,
        "duration": 2.466
    },
    {
        "text": "think about linear systems of equations geometrically.",
        "start": 658.886,
        "duration": 2.834
    },
    {
        "text": "Each system has some kind of linear transformation associated with it, ",
        "start": 662.3,
        "duration": 3.699
    },
    {
        "text": "and when that transformation has an inverse, you can use that inverse to solve ",
        "start": 665.999,
        "duration": 4.115
    },
    {
        "text": "your system.",
        "start": 670.114,
        "duration": 0.626
    },
    {
        "text": "Otherwise, the idea of column space lets us understand when a solution even exists, ",
        "start": 672.28,
        "duration": 4.96
    },
    {
        "text": "and the idea of a null space helps us to understand what the ",
        "start": 677.24,
        "duration": 3.601
    },
    {
        "text": "set of all possible solutions can look like.",
        "start": 680.841,
        "duration": 2.599
    },
    {
        "text": "Again, there's a lot that I haven't covered here, ",
        "start": 684.98,
        "duration": 2.417
    },
    {
        "text": "most notably how to compute these things.",
        "start": 687.397,
        "duration": 1.983
    },
    {
        "text": "I also had to limit my scope to examples where the ",
        "start": 689.8,
        "duration": 2.504
    },
    {
        "text": "number of equations equals the number of unknowns.",
        "start": 692.304,
        "duration": 2.456
    },
    {
        "text": "But the goal here is not to try to teach everything, ",
        "start": 694.88,
        "duration": 2.677
    },
    {
        "text": "it's that you come away with a strong intuition for inverse matrices, ",
        "start": 697.557,
        "duration": 3.537
    },
    {
        "text": "column space, and null space, and that those intuitions make any future ",
        "start": 701.094,
        "duration": 3.637
    },
    {
        "text": "learning that you do more fruitful.",
        "start": 704.731,
        "duration": 1.769
    },
    {
        "text": "Next video, by popular request, will be a brief footnote about non-square matrices.",
        "start": 707.66,
        "duration": 4.22
    },
    {
        "text": "Then after that, I'm going to give you my take on dot products, ",
        "start": 711.88,
        "duration": 2.797
    },
    {
        "text": "and something pretty cool that happens when you view them under the light of linear ",
        "start": 714.677,
        "duration": 3.671
    },
    {
        "text": "transformations. See you then!",
        "start": 718.348,
        "duration": 1.312
    }
]