full for this semester but I'm already starting to look for people for the fall so if anyone's interested please let me know and with that introduce our speaker so today we have a slanty das from biostatistics and he's going to talk to us about next-generation imputation methods alright so today I'm mainly going to talk about some of the efforts that me and some of our colleagues in that department have made like in addressing some of the problems in next-generation imputation methods they particularly like scaling to really big data issues arising from different kinds of data restrictions for some of the topics I usually end up making too many slides so if I find that I'm running out of time I might skip a few slides so I'm going to give an introduction a brief review about imputation then we are going to talk about mini Mac 3 which is one of the commonly used software's for imputation one of our imputation servers then I'll talk about a few utilities like we have like in order to let you know aid in the imputation process then some ideas about mini Mac 4 and conclusion I will try to skip I try to skip the details in most of the methods and just try to give a brief overview of the results and you know like how we are trying to address all of the problems so just to kind of make a nice segue to the topic so we all know about genome-wide Association studies in human genetic studies the main purpose is to find regions on the human genome that might be associated with some kind of binary disease or some kind of quantitative trait and the main method it is quite simple we get a big sample of cases and controls we find out the genotype of these samples all along the genome and then we do some kind of statistical methods to find out p-values so the Pope the second bullet point is that what we are going to concert it on today like how do we find out the genotype of a person so if you know like in general there are two ways of finding so one either we can do whole genome sequencing just sequencing or commercial genotyping and I'm sure most of you know what's the basic difference but just to give an idea making sequencing we usually get the whole stretch of the DNA whereas in genotyping the balance only had some common variants and also which are predator mind but of course genotyping is really cheap whereas sequencing is expensive which is why most really big studies they end up using chips instead of trying to go to a sequencing so this brings us to imputation because since in chips 99% not I mean different person based on the density of the chip but most a big percentage of the variants are missing on chips therefore we need to impute them back or estimate them back and that is what we call genotype imputation which means missing estimating those missing genotypes for samples which were typed on chips and trying to do that as accurately as we can and this fits us because it in helps us increasing power of the gos we have a much bigger set of variance to analyze it helps in meta-analysis mainly we are combining you know information across different studies for fine mapping again gives us a bigger resolution and this weather can also be used to estimate inder's or each other lilz things like that so this brings us to giving a brief review about how imputation works so once again coming back to the original problem so we have a sample that was typed on an array and most of it is missing and we need to include those missing values and the intuition behind imputation is that we know that if two samples they are related they end up sharing long stretches of haplotypes from the common ancestor however even if individuals are unrelated we drop from the same ancestry there's they they're still expected to share short stretches of chromosome between themselves and that is the main idea that we use we bring a huge being a big reference panel that has densely sequence individuals and we try to match this sample to the samples in the reference panel using only the type sites and thereby try to impute the sites in between so for example just to give a brief I like intuition what we do like say for example this is a sample that's typed on an array and here we have a big reference panel of densely sequence individuals so the idea is to like find segments of shared stretches of haplotypes between the sample and the G Spaniel for example if you observe here the first three L is TG NT they match with the second haplotype in the panel and but the fourth one C the C doesn't match with the G over here so we then try to match it next and we see that the C C and G kind of mat matches with the CC over here so we assume that you know it matches with that stretch over there and we keep going on like that so the idea is that this whole big sample our diva sample it shares this part from the second haplotype this part from the fifth haplotype and so on and this will help us to impute the sites in between where we can kind of copy the alleles from the sites in between now of course the things aren't as simple as that because we don't always find exact matches even if we do we might find multiple matches and which is why you know they think sine always discreet like this but which is why we have to bring in probability or probabilistic models and this is I won't go into the details of what the model does but just to give an idea like you so Markov model is something that Musa first know it's kind of its kind of a statistical process that moves we have H 1 H 2 H M each of the H is depend only on the last one then the idea of a Markov model is that if you condition on H 1 to H I minus 1 the distribution of H I only depends on the last position H I minus 1 it doesn't depend on anything else what we use for imputation is called the hidden Markov model which is similar to a Markov model but the underlying markov model that was done process that was happening that gets hidden we don't observe those variables instead each of these variables H 1 H 2 H M they emit some other variable si based on a distribution if I give an H I and we observe these variables instead and the idea is that we try to make inferences about the underlying H is based on what we observe so this is called the usually remark of a hidden Markov model has two main parameters it has the transition function that comes from usual Markov which describes how age variables depend on the one before it and we have a emission probability well zero to what I in general I mean yeah I should introduce the phenotype over here just in general their emission probabilities where which describes how the variables in size are emitted from the H is and the reason each ms are used for imputation is because this already good existing literature on how to like do parameter estimation using n algorithms or Markov chain processes and they all we focus mean to make the efficient in terms of complexity and it fits very well in our imputation setup so I can see we can understand that the a size are going to be our observed genotypes which we get from the array where as HS are going to be the underlying haplotypes which we are going to estimate and that is going to give us the imputed results so that leads us to minimize 3 which is one of the commonly used tools that people use nowadays for imputation just reduce how many make three came into being it is actually the fourth version in a series of imputation tools that people from our department has been working on in 2010 newly developed Mac which used to do facing as well as imputation then mini Mac was first developed and it came in it came in on 2012 Patricia on the second function of mini Mac let me listen 2014 and mini make three copies last year some checking the middle the they're all of the same kind indices they all work on the same idea of feeder mark of models but slight modifications have been made over time just to make them more and more efficient just to give a brief summary like you know what has been done in general so the final aim is to find out for each marker and specifically for each untyped marker to find out the posterior probability of each of the states the H is and this or if we have you know the complexity of this is going to be number of states times the number of markers so this is the difference between Mac and mini Mac comes up so people when met what's that we the idea was that it would take in unphased genotypes and so do phasing as well as imputation at the same time as their hidden state space for Mac was every pair of haplotype because we're trying to impute butta products at the same time and therefore for each for handling states are every possible pair of haplotype when it's in mini Mac the idea was that it was assumed that we can just face samples before instead of computing them and then use of spheres haplotypes through the imputation process that way we are imputing into half the types and not genotypes in that case the hidden state space becomes every reference haplotype and not every pair of haplotype so facing means so we do you have when you have genotypes of samples one after the other so the T it actually exists is to haplotypes right so if I say that you know for one has a beginning as small a and the second one has a big B and a small B it could be either that the capital the big letters are together and the smaller ones are together or it could be the opposite way so the phasing determines what you know what alignment they are like it essentially means like you get the haplotypes and you you just don't it's not that you just know that genotype 0 1 or B get smaller like that but you know which half the time they're lying does that make sense so the mini Mac introduced this idea of prefacing that when you have genotype if de you know just don't try to face and impute them at the same time instead face them first and then impute them and that way mini map was developed and since the state space reduces it it was you know now instead of doing aim times H square so one important point that the H over here denotes the number of half-lives in the difference manner and that is usually around thousands so a square is a big difference so H square is really big compared to H and which is why when minimax was tablet was much faster than Mac and with many Mac - it wasn't any kind of new genetic algorithm but Christian mainly worked on software engineering techniques like using a lot of visualizations are transposing the reference panel what are you threading automating multi-threading and stuff like that and based on that minimize to was found to be 18 times faster than many Mac Mini 3 so when effect 3 in spite of its PI tough you know around 2012 me miss to being quite fast so using some of the imputation methods using the current reference panel was we still take four years to impute just one sample so HRC is a reference panel just like thousand genomes or hapmap it's just is a very modern reference planner it has 30,000 samples whereas the thousand genomes only had like 2,000 samples so and in the previous slide since you saw that the complexities depends on the number of panels so the more reference - you have more the size of the reference panel the more time it will take of course increases accuracy of the imputation so this kind of gives us motivation to find out another faster method to do imputation but instead of bringing it down the accuracy so I'm just going to give a brief idea about state-space reduction which is the idea of reducing the state space of the hidden Markov model the ideas that are in small genomic regions the number of unique haplotypes is much less than the total number of haplotypes and it suffices to just analyze those unique haplotypes instead of all the different subtypes I can take like give an example if you have is a reference panel with nine habits and 37 markers and we found out that in the previous version in the previous of meaning like the latest complexity was H times M so that would be due to 53 constant operations when we set up what we observe is that we first become the whole panel the whole reference the some into for example in this case we break it up into three segments in the first segment we notice that all the we have nine haplotypes there are only three of them are unique for example the first four are exactly same the second two segments are again same and sorted so this is one but anyways similarly if we move on to next two segments we find that in segments the number of haplotypes are much less and this happens because of linkage between variants that are very close to each other so the idea is that instead of the nine haplotypes that 27 markers we can compress or reduce the state space so that we kind of end up with only three haplotypes preferring the first block we have seven markers and three haplotypes so that gives us 21 operations in the first segment in the second one we have 12 times 4 and we have 8 times 3 over here so initially when we had to do 243 operations now we have to do only 93 operations which is like a quarter of that and of course this you know 50 times faster isn't that big a deal but these ratios are much higher in real life examples but one important thing I miss that hidden Markov model usually works as a recursion so whenever we make these breaks over here it kind of breaks affection because for example this haplotype did not have any existence over here which is what you have to do some calculations at these junctions and yeah so this calculation that is going to add a few more calculations over there so this is just a schematic diagram of what we do in mini Mac 3 I won't go into the details of the formulas we take a segment we find the unique haplotype in that segment for example over here you know we end up with only 3 main box and we sorry we do for the probabilities we use the term fold because the state space is kind of being reduced initially with 9 now we have 3 model as a recursion only across these three haplotypes the three unique haplotypes and once we reach the end position you can unfold them back we call them again uncertainty because now we are coming back from the smaller space to the bigger space and again we move on to the next vlog so that way it kind of keeps working as a recursion yes that's a good question so the boundary is comes as we optimize a cost function so that problem the point is that if the box up top a the number of unique haplotypes is almost same to the number of total number of heretics so we are not gaining much right same thing with the box are too small the unique half is really less but you would have too many blocks and if this step the unfolding probabilities you know the steps we have to do at the junctions that's a very expensive step because we are doing that at all the original haplotypes at the order of age so if you have too many blocks we are having too many calculations of the junctions which are very expensive so we don't want too many blocks either at the same time we don't want too few blocks so we have a cost function we can device which is just you know the complexity of this process and we optimize that cost function and based on that we find the allocation we know no because like different panels you know based on different methods some are for genome sequencing some are more eggs on sequencing so it's better to not have anything that's biology specific but rather that's very more dynamic to the data reference panel that is very you know that doesn't have many rare variants then it will be it won't be a very effective measure in that case but main point is that we have we can do things faster but at the same accuracy we don't because it's still the same equivalent model that we're doing in the original space so these are some results of how this method brings up you know a speed-up so we use different reference panel sizes of thousand five thousand on all the way up to fifty thousand and we are comparing the time required and the physical memory used and between minimus three and mini meant to sew mini Mac two was based on a simple hidden Markov model and this is based on the state's mess reduction so we see that you know a bigger panel size the more efficient this method is going to be because in thousand samples you know the number of unique haplotypes might be 500 but in this case the ratio would be more so a thousand samples is just 11 times faster whereas to be bigger samples is almost 45 times faster of similar kind of train we see in the physical memory so these are some results were comparing minimize 3 to other companies contemporary methods like bigger 4.1 mini Mac 2 and impute - so the I have used different reference panels like thousand genomes aim deep Sardinia and these are precises so in most cases viewing all the cases we find that mini Mac 3 still the fastest among all the country many methods available being able is closest and the show gets as the panel size increases for example it's twice fast use panel sizes like per thousand but for the HRC panel it is a huge panel 330 thousand samples tribute one sample he still takes 8 days with mini Mac 3 whereas to take 2 months with the next fastest method and mini Mac - an impute to would always be infeasible to run with them so just to summarize or what we did in mini like 3 on increasing the panel about 50 times we find that you know it's the time becomes only 15 times whereas the memories 13 times for a really large panel like HRC its mini Mac - was 45 slower than our method impute we 60 times slower and bigger there was a time slower it also gives the highest imputation accuracy among the above methods I've skipped those results because because the mini Mac - was already known to be most accurate among them and we haven't changed any accuracy so so this emitted by that correlation so usually you do experiments when you have the sequence data you mask you know you mimic as if they were typed on AG was sorry like you mask some of the variance then you pass it through the algorithm and do a genotype correlation to do an accuracy so the imputation server is another feature that we wanted I wanted to talk about because that that just makes the whole process of imputation much easier the motivation when we needed a server like we realize analysis for still quite challenging especially for someone was doing it for the first time first you have to read online tutorials of the software you have to the tower and install the software you have to download reference panels which is the because there are lots of modern reference panels which are not public data so you can't always download them so you can you know but at the same time it's something good if you can if you should just can't use or you know have no benefit out of them but you know after downloading everything we didn't students often realize that you know we can't really run this on a laptop because it takes too much compute power then about custom management and finally they are able to run imputation issues because since 2010 included you know you have to prefix like I said like whenever you get genotype data from the sample you have to first determine what happened I swear and then do the imputation if France panel there are this quality control issues and lots of other data processing stuff which is why we thought imputation server would be the researchers will just upload their data this will be the server and finally they would again be able to download their results and yeah I am but then making something in your reference panel is a good model yes yes yes that does happen I feel like you know because even when you do g1 studies leaking often not all of them are not the same ancestor although they might report it to be and if in that case the reference panel that you are using would not give you much good accuracy yeah it might happen that you know if it doesn't that as European samples and I mean putting into an african-american population then we won't get good accuracy it does give it does report something and a variant level like that is my universe attraction of samples so each variant will have an imputation accuracy reported that can be used so they for example variants which are so rare of varieties the more difficult it is to impute for example in our sample if the variant is a single term it's obvious that we can never impute that from a reference panel because no one in the panel even has that variant so the pair of the variant is the more difficult it is to impute I have a few graphs in later on that shows some imputation accuracy maybe that will give a better idea so this is these are the main summary what the solver does so now from the side of the user we do serve in whatever format they have their data they just need to convert it to a VCF format and you know zip it and techniques index it there are tools on the server through which they can you know pipe it through and just check if the finest you know if it's running of course otherwise you know it might get in the queue and after two days they will know that oh there was some kind of quality control issues so this is the CH and there are lots of ways to share the data SFTP and even global sharing when you have lots of data you know and then seek you and the SFTP doesn't work on the software we have automatic quality control check we do prefacing in case the samples are not phased we do the computation and we encrypt the data and then finally notified the users by email and since you know most of the time the data is secure we provide a one-time password the data is also deleted after seven days and quality control reports available for download so the imputation server has been for over more than a year till now we have impeded over three million genomes we have over 900 users we have produce over 660 terabytes of data users paid almost all across the world and yet using for example just in the last four months we have imputed 1 million genomes out of the total 3 million genomes in a one and a half years difference - for imputation be a thousand genomes phase 1 and phase 3 we have CA APA which is a reference panel for african-american populations we have the hapmap we have the HRC so for difference panels in general are also publicly available so if you just wanted they could download the reference panels they could download mini make free and do the imputation themselves but HR says in the public panel so you know if someone wants to use the HRC panel they must go through the imputation server so researchers can exact what the imputation server and they have tweeted that's never like us correctly since there is since we have HIC almost more than 50% of the studies are being impeded like as they chassis but it is encrypted it has at least once at the pastry in new ways and even if they are not like you can always mail us and we will try to work around that and finally in the we see that you know this kind of services help us researchers in the long run has also been used to make other servers for analyzing mtDNA for tracing genetic ancestries both from our department and other universities that means the time so that the thing that brings us to some of the utilities that we have to kind of aid in the imputation process so like I have been talking about for a while I'll be referring to HRC I'll just show you what you can talk about what the HRC is it is an effects panel of 30,000 samples and that will be the next two topics which is if you have large samples we need methods that scale well even for simple stuff like saving the data doing simple data query or simple summary statistics like L in frequency alas table introduced a novel idea called meta imputation that also is kind of important given that panels like EHRs are not available for download so it's just you know it's a simple reference panel just to give an idea of what reference panels are so in 2005 we started off with half map which was one of the very first reference panels that were used for imputation then we had happen to have made three genomes came in 2010 it had two phases phase 1 and phase 3 there was an interim period which never got released for some reasons the number of haplotypes is increased from 420 to 5000 phase 3 the number of variants has also increased considerably and this brings us to the next reference panel that we are correct we have developed now so the HIV was instead of having since doing not scale whole genome sequencing is really expensive which is what kind of couldn't get much larger but different studies have you know like different universities or institutes have done studies all across the world so one good way would be to like you know combine them so that was the main idea HRC we had around 20 different studies some of the main studies are from cause disease in k2 MK Sardinia MD diabetes phase 3 thousand genomes and we took this data across 20 different studies we combined them together finally ending up with 32,000 samples and this big difference panic would mainly be used as a next-generation resource for imputation and facing so the next slide just kind of gives you a result so this is the first verse that I kind of happy shows you how imputation accuracy works from the y axis on the x axis I have the reference allele frequency and the imputation r-square imputation r-square means just a genotype R square so it's like so you can see that for common variants like the would be one person the accuracy is quite high around like eighty percent and as curious as we it gets read at the QSC Falls so the legend what I'm doing over here I'm company two panels HRC versus thousand genomes Phase three and I'm also covering two methods so minimag 3 is the method that we talked about in the last line and PBW is a competing method from Oxford I don't know if you know it's missing that works on burrows-wheeler transformation that's also an imputation tool so the red line kind of shows the result so in general we see that mini make 3 has more accuracy than PB WT the red line usually always above the blue lines and the solid lines show that copy to the dotted line show that the HRC HRC shows a much improvement in accuracy from thousand genomes for example using mini Mac 3 from it jumps from 65 percent to 80 percent and the top is even more for error variance so this already said this one this hoc data is not public which is why users won't be able to download it and you can always in the take advantage of it by imputing through the servers subset of it would be available because some studies they don't have it a restriction issues like for example so for those cases some a subset of that would be available to DB gap Nega so this brings us to the next motivation that we have been we end up with really large data sets like HRC simple things become really difficult just to load the HS is you know genome on the physical memory using a commonly used tools like VCF tools it takes 10 hours and the frequency calculation over the whole genome take 90 hours zipped VCF file is 80 gigs and unzipped it's 2 terabytes so which is why we developed a new tool called m3 VC of truth which is we're just going to do all these stuff much faster and also just because HSE cannot be shared and you just can't use them so whenever people if you want if someone wants to March the reference panel with their common private reference plan and then do an imputation they won't be able to do that so that's why we are introducing an idea called meta imputation which is like meta analysis for Association tests when you combine imputed data from anyways so then 3 BC of tools is just a fast tool for compression and analysis big genetic data just to kind of introduce like standardized file formats have been for ages the pedigree format that was one of the most you know original formats that link introduced the binary pedigree format in 2007 which was just making a binary file to save space the variant form and we see a first introduced in thousand thousand genomes the pinnacle process was you know so that you can also have biological annotations along with the variants but it also moved it to marker cross sample format rather than sample cross marker because that's looking much better compression when you run it through zip the Oxford universities have their own format half legend and sample format recently the gqt genotype query tool was also developed for the same purpose really fast query on genotype data and Haley who has been working on samples she also developed a tool called bgt but the second purpose to kind of work on some of the issues the you know already existing tools have like for example pedigree binary files in spite of being binary there are still four times larger than zip the VCF files which is why they're not they say which is worse if VCF files are way more using binary prevails VCF talks on the other hand is very very slow third group this is not a very genetic format and it's mainly used for their own tools it's not so much used worldwide especially not in the US and these two methods although they kind of work around some of these issues but they still don't work with linkage and stuff like that so which is why we developed in three VCF tools the idea is same as what we said in mini Mac three we take the whole genome we break it up into small segments and in each segment we find only the unique haplotypes and we saved them for example on the left is how we have a normal VCF format we save all markers and all samples but our main three VCF format we only saved the unique haplotypes and so these are some results or features it's also the CBC a format is tatics indexable for fast accessing it has a command line parameters as VCF tools so users who already have been using VCF tools for most of the stuff they don't need to change their pipelines it's faster so the advantages of our VCF tools it's 60 times faster for an in frequency calculation - 50 times faster for linkage even the files are much smaller for clean files it can store phase and multi and excites because plink still doesn't store have an app store phase information of my tenant excites and instability particularly being the fork reading like they do any frequency faster but the you see of tools is still faster and they don't do any kind of help or die based or square estimation so I'll just skip this slide for now and just come to the last thing so the meta mini Mac the last thing that I wanted to talk about is from a time pew Titian so that is that now you don't have to imputed data from two different reference panels the view that we can integrate them and in a way that does not interfere with the algorithm and it would also want to require access to the reference panel and the advantages of that would be that it would be the best solution to marching reference panels which is a big problem in general because when you have to reference panels the problems that happen when you try to merge them is how do you determine the final set of variants if its intersection the variants decrease the number of variants if its Union he will end up with missing variants you end up here I might have to do joint calling which is very lot of work and it requires access to the panel and of course marching to panels would give you a bigger panel you know that's an increased burden so that's why we have the idea are called meta computation which is just that is simple in the sense we just do a weighted combination of the individual imputed values but it's the way we determine the weights they are yeah they are based on some factors like and how the imputation works so skipping into the details of what those weights determine so for example just to give a result of how the metal imputation can help us give more accuracy so we included in 268 stubble samples is there B samples or african-american samples from the United States so these these Co textures are all from the thousand genomes data and we imputed and using two panels one of 500 samples European and 600 African samples because uhm the african-american samples might have shared ancestry with both the Europeans and Africans so and this is the claim the imputation accuracy graph so we see that in general the African reference panel is much more helpful for giving a bit see but it's not that the Europeans are totally still does help us here so when we do a meta imputation we find that if we kind of combine the joint imputed values of from these two panels without actually having access to the panel we can bring up a slight improvement in the accuracy and this is still a work in progress which is a very minor stage and we're just going to just working in simpler models and we hope that if you tweak around the modulus we might get more accuracy and if we compare that to the joint imputation so the joint is like when I actually can combine them together and impute so this is as good as it can get so I hope is that like using our methods of meta imputation we can make the redline come as close as possible to the joint results so yeah I'll skip the mini make formal easy this is the work in progress so I'll just skip to the last slide yeah so just to summarize like you know to summarize the minimax there are like tools in the series of imputation tools and they're mainly made for genotype imputation they've been bringing increasing speed and memory efficiency a negligible almost no loss in accuracy from Mac to mini Mac the main idea was you pre-phase instead of imputing into genotypes that we can impute into haplotypes for minimax to minimize - it was mainly software engineering techniques from two to three we introduced the idea of state space reduction which is just using local redundancy is in the haplotypes and in the minimax foresight which I skipped is a an IV con aggressive safe space reduction that is going to further bring down the computational cost and we might also exploit some other ideas like doing interpolation which might again bring up a further boost in speed but even like that the flights right fall in accuracy and to summarize like you know how these minimax of develop like what might have taken 3,000 years to do back in 2010 we can do that in 12 minutes now add almost the same accuracy so the biggest jumps way from Mac to mini Mac and then you know the fold increase has of course decreased but yeah and thanks to all my collaborators goes always my advisor Christian helped me a lot with all the imputation tools as well as he's the main person who works on the imputation server and maintain said the difference construction it was a collaboration between Michigan Oxford and Sanger so yeah that's pretty much it doing pre phasing and then haplotypic separately like it seems like when you're facing you're trying to discover some haplotype information so I'm confused why so good point so the technical that kind of depends on like how you do phasing so currently if you have so even when you use 52 phasing people often use different spanners they you should do that you know but now when you have really large samples like if you have say 5000 samples in your chivas data we no longer use reference panels anymore we can just face themselves like liquid in themselves and that is really fast because you jeewa samples have only one percent of the variance you know it's just like say 20 million variants you know or maybe five so that's very fast to face so the facing gets really fast and you don't need to like yeah so the whole idea of haplotype being only happens within the sample so yes yes just cause like I said I always end up making way more slides that I can talk about i skipped on that but i did yeah so far they were talking about an aggressive stage stress reduction so in essence are how we just compressed half in small regions and so in implications one important point we found that what we found but after the information the genotype sample of the Geo sample comes only to the typed sites the untyped sighting on the g1 sample doesn't provide any information right so our next idea for the aggressive state space reduction which I was saying is that I mean now we will collapse haplotype just based on the type side so I can just you know so for example like here it surely we had three haplotypes so in the first block you know because these three by now unique they are no longer them so for example this and the G don't match here so I can't merge them further but if the blue lines these are the just the genotype sites you know if I do you know the movements and the genotype sites the space reduction the idea is that we will merge them only based on these blue lines and as a result they still disagree in between for example here these two most half the types I don't agree which is why we have to go back and modify our calculations a bit but general look this kind of shows you this was the original space where we have 9 plus 27 243 marker you know constant operations this was me Mac 3 where we have three and three haplotypes in blocks and multiplying that linearly that gives us 83 which is like a quarter of 243 and this was this is supposed to be mini math for where these three has reduced to two you know even these two and these two will combine further the top group will combine here so the number of haplotypes has reduced even further in mini-map for making it 54 like these folds would be much more they would make much bigger numbers in real-life data but next idea mini Mac for but yeah it's just going to make the comp capsules more complicated because we have this new thing called fuzzy areas which are going to be served to half the banks which are now collapse like I said they might not agree at intervening size at the intermediate positions so those positions where they don't agree we have to keep track of those we are calling them fuzzy alleles and we have to properly speed up the speed up the probabilities when we are doing the final imputation so yeah that is supposed to be mini-map for question about so when you initially within your city inferring a haplotype so have you compared it with six a maximum this is a different booth altogether because we don't infer haplotypes any movie edge [Music] and we are just computing the sites in between so phasing is kind of like a problem the issue has been addressed already yeah in the the one that you have in the server so you have a couple of reference not I'm not in the fields I'm not sure but these might be the gold standard or ones used for the reference but it's a way to upload your own panel anyone can make their own hands but the point is we don't have a lot of difference panels around because they're expensive hapmap ten thousand genomes and we have UK team game but in everything is private so that's the reason people so that makes it yeah becomes difficult so whenever we have a big study each study will have almost all the time they will have their one reference panel along with a public reference panel just because whenever you do a do a disease specific study or something that's population specific for example the Han study which is based on population in Norway so we have found that minimal we will get into populations in Norway which is actually a kind of a secluded population although they are independent they are found that they get way more accuracy from a hunter if a standard or a sorry from a norm nor winning reference method then from something that's more common thing to claim that thousand genomes so yeah like of course this witches can always make the reference time and I use this it's someone minor point is are they gonna me make three as a tool but on the server I think chromosome makes the chance to deliver the other thousand genomes but the last panel agency I think you are still working on chromosome eight you explain a little bit about our even though you just created the home from going to chunks there is no loss of accuracy so those are like kind of detailed calculations which I've escaped general I there is no intuitive way to explain that because the like if I come back to this slide which I mainly talk about much was the schematic diagram of imminently so in this like these formulas they we devised these formulas in a waste so as to be get the same answers you know so we have a space and we had calculations that we did on this huge space now we are reducing something to a smaller space and we are doing our calculations on them right and so I try to say is they the way we do the calculation is on the smaller space or we have modified our formulas in a way so that to make it equally the reason behind that is just like either as some algebra are showing that oh yeah what we did in the reduced space is same as in the original space this just Angela this not much intention bond that it is I don't get how do I break them into chunks yes right that Junction like that's what so but initially we were supposed to move across this child you know normally like go from this side to this one and then this one by recursion right now we are not doing this on the original space they're doing it on a smaller space like this so once we come to the end so for example we have some probabilities for these three haplotypes we unfold them back this unfolding state gives us the probabilities for the original line haplotypes and it would give us the same numbers if we did the calculations on the original space so once we come to this end and then we get ready for the next clock so mean once again because the reason we have to move from block to block is different blocks we have different configurations right for example in the first block saying these four were much together in the second block you know we stored a merge together so I cannot keep moving from blocks to block without doing a jump in between so this jump in between is called the unfolding step maybe a foot back the probabilities we come back to the original space and once we are back to the original space it's a recursion again for the next block we use the same if it again you take the block you fold then move across the block get to the bar and unfold them and I didn't get ready for the next block yeah so the initial recursion was martyred by marker and now it's like you could talk look at it like a necessary question like the first recursion is block by block and within block there is a marker by mark records any other questions [Applause]