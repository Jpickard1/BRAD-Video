welcome everyone thanks for joining us today for the tools and Technology seminar series uh for those of you who are online if you have questions you can put them into the chat box and I'll be monitoring that and can let our speaker know if you'd rather ask a question verbally you could use the zoom reaction to the bottom right and raise your hand and then we can bring that up to the speaker too so they know there's a question being asked um so with that I'm actually going to turn this over to Mike santorpo who's going to introduce our speaker today okay thank you hi everybody out there on Zoom so my name is Maxine Bracco a faculty in the department of biological chemistry and the Life Science Institute and so we're you know not in the sort of typical tools and development lab I would say but we're allowed to use this cry OEM to determine protein structures but the reason that we're here today is that I wanted us to share Eli Lee's work so Eli UE is a post-opway lab who does computational methods development and we have this really cool Story related to cryo and data collection that I want to share with the department and the community here in general to get feedback we want to advertise for potential grad students or post-docs or collaborators to think about how we can keep bringing all these very nice algorithms into cry OEM in lots of different areas and so with that I'll turn it over to Eli foreign so I'm going to share a story about um accelerating structural biology with basically a machine learning approaches so all in a single particles you have the opposition we combine um so the framework is a combination of uh CNN and our reinforcement learning to help the Microsoft to steer to the good area to automate the process and also to achieve higher efficiency so it's an overview of program data section program is a technique that has been growing very fast in the recent years uh and uh it's been invaded in the space of traditional uh and focused uh if you know anything about structural biology it's just to solve the atomic report structures for large biomolecules and the idea is for again uh at a high level is very simple you just purify your sample and put it on the spirit freeze on this grid and you'll look at it from microscope but when you zoom in so this grid is about one millimeter in diameter uh it's very small but compared to single particle proteins or samples it's very it's very huge so when you're doing there are a lot a lot of squares because on top of the grid and then inside each square there's a lot of poles inside each Square and we collect micrographs what we call microgram which is the images we use to analyze later inside each of the holes and from there you can see single particles of proteins or samples that are interested in in it and then you analyze these microgravity data structure so uh so you start from a grid your sample is on the grid okay you have a lattice of squares and then you have letters of code inside which this is where endocratic one or more micrograph inside control and then uh this is a simplified Pipeline and then you pick some particles from your micrograph and you have a few thousand microwave or data sets usually you end up with a number of particles you'll turn your particles through the studio classification when you get to the basically get to the conservatives everything has higher higher resolution information in it and then use these particles from the two to the averages to do the 3D reconstruction so this is how you get a 3D Volume or a 3D structure from the data on the grid so a number of things could go around uh on the data collection side and for the downstream processor to be easier you want your option to be as clean as possible so showing here is the good example of um for the kind of the ideal case you'll have a monolayer of your particles and your eyes thickness it's good enough so you can see your particles and there's no aggregation not to dance but in many cases that's either an eyes thickness it's too eyes it's too thin so your sample get denatured or it's too thick you and the particle is too concentrated you'll see all these aggregations or not with high resolution information or to dilute basically you don't see any signals from your micrograph and uh pressing uh challenging data sets requires clean micrograms from the Upstream so example is this gpcr data set uh when we just do an initial name even analysis pick the particles from all the micrographs and because the particle pitching process is not 100 accurate you will end up with a lot of false positives so uh after you do 2D classification the 2D class average is I feel very noisy but when after we manually curate and this part 40 of the micrograph with visual inspection you can end up with a much cleaner to the customer disc so um the better uh better quality in the Upstream the easier for example processor and uh how is our data function now so from all the micrographs we collected from January 2019 to May last year in our lab about 50 does not contain any high resolution information at all so the red so even though we try to crack as much as good data as possible as possible there's still a lot of data that's basically just uh doesn't contain a high resolution uh and the efficiency of data connection greatly depends on expertise uh if you're a new user you'll end up with making a lot of mistakes during the data process during the data connection process and it waste a lot of time and the question is how do we crack as many as good micro graphics and limited time frame and uh preferably without an experts during the microscope so NY can just start the data collection and there will be an algorithm but as many as good micrographs uh and uh I didn't mention here but some of you might know that probably is a very expensive technique so the scope itself caused a few million dollars and installing for another few minutes and then uh to keep it running uh I used to combine all the costs uh the cost per day is around a few thousand per day so efficiency and automation is needed in this in this field but um so proud young grids are Complicated by the Landscapes as you can see from this whole ad from this Atlas image the Spurs and holes has a lot of verbiology so on this athletes you use some some regard thicker eyes and some are thinner eyes or maybe just empty or maybe the aspires are just ruptured and uh it really depends on the user to decide how to navigate through the grid so find it that way through and also keep in mind that the grid is very big compared to the um to a Toyota sample so for a typical data set with a few thousand micrographs usually you'll only end up image only uh one or less than one percent of the total grid and you need to start from this Atlas and uh with other medium or lower magnified images trying to collect a few thousand micrographs covered only one percent or less of the total grid and you want those microbes to be as high quality as possible so uh one so we're thinking to break this into two parts the first part is to get a landscape of the data quality uh across the across the address or at least across a part of the access so here uh we're showing different layers of um uh what's uh the different magnifications or process Atlas so this is on the right this is the Atlas image and when you zoom in you have like a multiple squares to zoom in once were you can see all the holes in it and it will take hash level images and this patch has some rough third holes so we don't think um this is please hold our good ones and we think probably these holes are a good one so we predict we sort of predict the quality of the data on the on a whole level images where we have more confidence and we take all this information from all these different patches and overlay you basically plotted them on the on the Square level image and probably that and probably that so ideally what we want is to end up with uh an atlas with um the for the data Quality Landscape according on top of it so um so the goal here is to cut as many as good holes in a limited amount of time and we want to the ideal solution will be resurvey and assess a list of thousands of targets so maybe a list of thousand Target is not enough to cover the whole whole address but since we're only connecting uh low and medium magnified images uh we save a lot of time by just assessing the quality of the whole level averages and we also want to balance the trade-offs in the time for switching to different catches first and the bridges of the assets with data quality so on one hand we want to optimize optimize the data party or press as many as doables as possible on the other hand uh switching the so basically moving the microscope to one place where another place that are far away across the bridge takes a lot of time so it also needs to balance the type of moving the Microsoft in a scenario and also we want to this framework to adapt to different samples and different query types all right so our proposes uh reinforcement learning guided part of the end data section uh the first step will be with do a survey across the grid and we assess the whole qualities and uh plug them back to the atlas image and then we start to predict projectory Academy based on reinforcement learning and from the trajectory with exposures and once you get that exposure you'll know the ground through of your data quality then you you if you are doing something right then you'll get a reward and then uh you learn your policy of trajectory planning um so this is a this is the training process where um you learned the data question uh the optimal data connection approach on this grid uh which is also called the learn data connection policy so there are several key questions first is how do we Define good or bad exposures and we have a uh answer to that and how do we assess code quality and uh we also uh we have a chief progressor which is a CNN based model to uh predict the quality of a whole um from the whole level images and three is how do we plan a path across the bridge and we use D2 Network which is very interesting one of the reinforcement learning methods that we use here so first question um uh how do we Define good or bad exposures we use a CTF max resolution and uh for those of you who are not program experts this is basically qualifying the um the signal at the forward space that's which so uh you're gonna have high resolution and then you'll see more signals in a forward space in the high frequency areas if you're if your microgram has lower don't contain any it doesn't contain any like a high resolution information in the forward space you don't see uh as many of the tongue rings uh in the higher frequency areas so the good thing about this this isn't about unbiased approach and generally it's partially to the equality and the benefits of capitalist once you have the micro wrap the bad thing is that it's unrelated to a particle quality and there's also ice thickness dependent so if you have say a particle with aggregation in many cases you would get very good cpf resolution but not a usable micrograph but we just go with this uh this CTF resolution because we think this is a this is basically the best we can do to uh Define or uh evaluate the book or a good microgram next question is how do we assess poor quality um and the answer is we use a CNN based resonant based on the progressor to do that so we take all the whole images combined with the ground through CTF resolution and friend of the progressor uh based on that so in the in the following experiments we use two different regressors first one is a grid specific regressor which is Trend um over 13 000 posts and um and basically it's trend on trend on the steps on a subset of the data set and test it on a on the on another subset of the data set uh the other is a general regressor so we also want to test the generality of this framework so we train the general deep progressor that's a trend on uh over a hundred thousands of holes and CTF max resolution pairs uh and this is the so showing here two different plots are the results for the regressor so as you can see ideally you should um all the points should be scattered along this diagonal line but um we don't so because we are trying to predict very high resolution information from um the pixel size with over 100 apps from over 100 and stuff so uh it's anticipated that we don't get a very perfect result from R2 progressor but I'll show later that you don't need to have a very good prediction from a different from this regressor to actually do meaningful or um to actually collect uh efficiently for the for this framework so the last question is how do we plan a path across the grid and to use digital Network to achieve this so uh for those of you who might know something about reinforcement learning the reward table is uh it was optimized so the total reward is what it optimizes during a time period and the the design of the reward table is very important so we want to balance the quality of the data and also the time wasted or the time cause by moving when you move the microscope to a different area so uh we defined that uh if you collect a micrograph in the same patch it will cost one minute and the reward if the cpf resolution is lower than a threshold number is going to be one if so basically if you're collecting a good micrograph according to the threshold number then you get a maximum reward if you don't then you'll get a reward of zero so similarly if you press the micropipe in the same spot we're not going to say hash then we Define the time to use it will be two minutes and then we'll get a lower reward if you've got a good micrograph because your question sometimes in the the movement of the microscope and similarly the move to a further away area to uh the time class will be higher and the reward will be lower if you have to go to the background for all the for all the movements which looks like a bad Minecraft which means the CTF resolution is higher than the threshold number then you'll get zero reward so um so the connection between the deep regressor and the price network is shown here uh the progressor will output the positive score which here is uh CPM resolution we use per square per whole and um there's quality sport per whole will be combined with a hierarchical grid structure information it's basically where this code comes from this code come from this patch that's where in this group we take that all the information and also it's quality uh and we engineer uh a multimedia Direction features and use that as the input for the policy Network so this DQ network will output the Q score and based on the kill sport it will determine an action which is uh where the algorithm think issued correct from from the data from and this action will change the stage to basically will change the input of the two networks and then it'll predict another action and once you've uh output and action basically you want to track data from this hole and once you have that we will also obtain the ground fruit of the quality of that hole so it'll also tied up with the with the reward so the goal of the G2 Network during training is that it will maximize the reward and limit it in a different time frame and what will output in the end is that the um the dpu network will output a series of actions basically you have maybe a few thousands of candidate calls and the algorithm will tell you like you should click on this hole first and the next one and then the next one so the alphabet trigoni of the whole set it thanks um so to combine everything together um our file RL first uh uses distributed distributed data connections where uh we want to have a survey across the Grid or at least several um uh squares on this on this Atlas we collect patches and we identify the holes and we segment all the whole images and then based on different whole images we run a deep regressor to Output a quality support purpose and also based on a hierarchical grid structure we combine this information with the politics were and engineer a feature deals as inputs for the policy Network and the policy network will output the action for well basically I'll put a trajectory of actions basically a trajectory of holes that the things you should um so because uh this framework is not integrated in any real uh software yet uh for testing file RL we need to obtain the ground through for uh to actually evaluate and to test the performance and um to do that we need uh to collect a systematic data set basically for example in this data set of elderly we selected 25 scores and collected all the micrographs possible from the Square from all these uh from all the folds in this 25 squares so we end up with over uh 300 3500 polls and for each of the whole we have a round through CVS resolution attached to each of them and these are the example micrographs because we are not trying to um incorporate any human knowledge in this so uh the example micro have some of them are good ones but most of them are really bad ones and probably in the histogram of CCM resolution for all micrographs also you can also see that many of them are really bad ones so the higher this number the the worst applies so many of them are with um so many of the micrographs here are with bad quality but there are still a few some of them that are lower than five or six which indicates decent quality so uh we use a few uh data sets so basically on fuel squares for training and then we use the rest of the um data sets for validation or for Real uh so basically we apply our Trend model on top of that and we can plan it to decorate the power RL designs to take Minecraft from and it didn't really select a lot of my a lot of squares and uh when we zoom in introductory we can see uh it only visitors uh one two three four five six different stars and we can zoom in this first to see what's really happening inside there so this is a trajectory overlay on the Square level image and and um when you zoom in over here you can clearly see that where it's to actually data set from and the trajectory is pattern and because we also have the ground truth of almost all the holes here in this wire so we can overlay the ground through on top of this image and also on top of our trajectory and here we can see uh file RL decision uh decision making and it successfully avoided most of the bad bad holes which are this patch and this patch and this and this patches and it will try to collect as many of the data as possible but also to reduce the time waste of removing across different patches so uh example here would be this patch so moving so this patch contains maybe only one good image here but um the algorithm decided not to visit this hash because it'll waste a lot of time to actually click on this and then move to a different patch which uh reduce the reward and also increase the time cost so also it might it will make some mistakes but that's mainly due to the imperfect of the um of the regressors prediction so how do we evaluate or how do we compare the results from parallel uh we use a very uh naive Baseline to compare this with so basically we start from a random position and then an eBay an au policy uh we'll just collect as many as micrograph as possible in a given time limit so for the naive policy we don't optimize for the data quality but we optimize for the number of the micrographs uh practice so uh we also have some qualifications uh of priority performance this is the trigger that's where you're showing the first 100 micrographs collected by file RL and this is we're trying to connect the data set as uh below for six and from resolution so as you can see although it makes some mistakes but most of the data cracks in our below 6. and this is the histogram of Prior versus all micrographs so the graph so uh all micrographs as the candidates has many um core or bad uh or composed with best qualities but file RL is unable to enrich the parts which is below five Armstrong resolution and this is also shown here whereas the Baseline is the naive Baseline I introduced before and the prior rl's performance is probably not the right and this is applying the number of the micrographs uh in the in the in the uh during that eight hour uh simulated during the late hour time direction for a simulation and uh the power RL here shows um uh connected microgram with much higher quality overall quality compared to the Baseline so it's uh uh has it doubles the um micrograph numbers for the uh for the micro rabbits lowering their forearms around and also we expected a lot more without micrograms with lower than 500. and later we also thought the um Behavior so some high level behaviors analysis of the power RL trigatory I totally affected over 400 micrographs and it collected around 75 micrograms per square it only visited around 50 unique patches uh only six Spurs visited only five units unique grid areas visited and we also compare this with the price Network returned with different professional numbers so if you still remember the reward table this defines um uh so this special number defines the strictness of the reward table when for example you'll get a maximum reward if you when T is equal to six you'll get a maximum reward when you look at the data set I will look like a micrograph with lower than 600 resolution but you when T equals to four you'll get the maximum uh the word only if you press the data a micrograph is lower than forearms from resolution so we can see here that a lower T value which means a more strict reward table will encourage the policy to perhaps less microgram per square and it'll import the policy to explore more of a process bar so it will visit a lot more unique patches a lot more squares and a lot more fluid areas and because it wastes a lot of time during the searching to removing response to a different area so it will result in a in uh low lower micrograph numbers hold on so in real practice so what I've shown before is uh when the regressor and also the policy the two network is trans uh it's trained on a subset of the data itself but in the real practice uh you don't want to say I have to systematically press a few thousand micrograms and train some models and apply on the rest of the data set so we need to prove the transferability of the of the models of this framework so can we train from RL on a different data sets and on say on Elvis it's going to apply on a different data set so we collected this April 13 data set on a different type of grid so this is gold grid and similarly similar as before we also affected as uh systematically we saw 10 autographs true for all the um sweaters here we end up with around randomly 34 squares and over 3 000 posts and here we use the defrogressor which is a general model that I introduced before training around over a hundred thousand of images and you also use the polycentric which contain on the other list which is the data set I shown before so both models are transferred from another data set and this is the progressive performance uh from the general model you can see this is like a human Porter for performance compared to the the one strong before but uh it still capture the overall trend of along this diagram a lot but the good thing about this framework is that you don't need to have a perfect progressor to correct uh efficiently so this is the first 100 micrograms corrected by Carl RL and this is the overall uh distribution of uh data products you know distribution non-ctm resolution uh and you can see that uh the brown true so the data quality for all micrographs uh uh basically a lot of them are not good ones but um final RL is managed to accumulate most of the good ones from here uh also Sean here is that when comparing to the naive Baseline a more efficient view but much higher micro with much higher number of micrographs compared to the Baseline with good qualities and also from the we observe similar patterns from the behavior class where a lower T value will encourage their policy to explore more areas uh across the grid and result in a correcting less number of micrographs totally so uh the last experiment I'm going to talk about is parallel uh compared directly with this human with an experts which is so we obtain this we practice data set we obtain the atlas we have we carefully divide the assets as fairly as possible in perhaps the top part is affected by file RL so we do a systematic data connection and run for our on top of this and the lower part is corrected by Mike which is my pi as an expert for the Baseline uh for for comparison so for the pile RL part we still use the general regressor and the product Network trend on another data that's going to do that attention so both models are transferred and uh for the lower part micro just freely connect a few over five or over 500 micrographs uh on the other half of the grid foreign micrographs collected by prior RL and also by an expert so as you can see your expert makes multiple mistakes but so is file RL probably um shot the number of the micrographs across the total trigatory and this is when an expert uh the expert trajectory translated to our time cost table so during a eight hour time duration uh uh we can see that coil Oil performs much better than the expert and much better than the Baseline and the expert actually failed to identify the squares which contains CTF resolution below Fourier drops so it performs even worse than the Baseline because if the the experts did not have the global information when it started the data connection here whereas in power RLS have the global information with all the candidate hosts predicted uh CTF values so it is able to um um uh efficiently uh accumulate the data uh the microwave high quality and also project here is the behavior Raider plot where um the file RL Behavior as uh shown us and back on so the behavioral required with different key values exhibit a similar pattern where a lower T value will encourage the policy to explore more areas but more interesting is when you see the export expert actually visited a lot more different unique areas in the early stage of the radio fracture because for an expert in this circle and one area type down through the microscope level uh micro uh dive down to the micrograph level and then determine whether this is a protocol or determine whether this is a good sprayer and determine like basically it needs to search for a for the parts with better data quality so the conclusions are reinforcing learning combined with a whole regressor allows successful or more efficient data connection and compared to an expert and final RL learns a policy for attracting images that maximize data quality given a limited time and parameter setup allows for relax or more streaming data connection which basically you can tune the parameter to encourage the policy to visit more areas if you are doing like a screening session some future directions um will tend to do more Cloud RL versus expert comparison to have more uh have more data have more data part basically and um another part is active learning to update it with Western data because as refractive data we learn more about the data quality uh relationship between the square level uh between the whole level images and we can use active learning to update the regressor during the data fraction to predict more precisely with higher with higher accuracy and that'll help with the transfer pen for the Plus network as well and that's another thing is uh we need probably need a better metric for a good micrograph currently we are using CTF resolution because as introduced before it also has some applications and also how do we know when to stop data collection uh if say we have a very bad grid uh that's basically no good data on it and what's the what's the early start point where uh that's we say that before the time duration given time duration how do we call say this is none of this word and I'll just stop collecting the last thing is to incorporate this into a current data connected softwares like Syrian or Legend art and we're looking for grad student post-op and collaborators on this project and on other side of the the projects in the lab so acknowledgments uh this is a collaborative work with social boss from MIT my TI and transfer fund especially uh he's the land contributor of the of the algorithm and the experiment design together from IBM uh and Central collab uh especially Nick and Billy for making a great again facility uh foreign yeah if anyone has questions online again you can put them in the chat box and I can read them out or you can use the zoom exactly while we're waiting to see if anyone uh has any questions I actually have a quick question and you mentioned that you plan to do some more testing with expert diary on people I assume that that would involve experts outside of your higher RL team yes you're a little biased as an expert yes exactly and also structure experiments are expensive to do because checking out data sets like systematically it's takes a 304 days at least so uh we are planning to take uh so uh do the simulator to mimic the data connection process so it saves time and also saves money and we can ask other experts we can basically distribute software to other exports gather their return after economics great another question yeah but sometimes uh questions