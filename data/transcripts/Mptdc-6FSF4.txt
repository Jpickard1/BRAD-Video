dude University of Florida HD in mathematics 1996 from University of California masters in mathematics and Master of Arts in economics as well as BS in economics training yes so he's also a fellow instead of mathematical statistics and what he can start Stickle association as well as in international statistics institute all right thanks in dica for the invitation and it's great to be back and I come back often since still have my house here but I picked the right days it's there's no storm or it needs to be very cold I don't want that so this is a really joint project out of what we were doing and will continue to do at the MRC to a Michigan regional complication metabolomics resource core and so this is joint work with Allah so this building everybody should know Allah will do in the first welcome software Chuck boron that leads the MRC to my current students in Hmong gonna show you some towards the end some things that we're exploring and they are relevant to the continuation of this project and Samantha bathroom my former student in stats with right now post directly not UC Berkeley and I don't know how many of you have seen that a belongings last time I was in this room we were having a summer workshop or MRC - so it's a probably to talk again about metabolomics so I don't know how many of you have seen metabolomics and the reason I mentioned all these people extensively is because through various presentation metabolomics I have freely bordered from their slides so they're gonna recognize what they have written on various slides in my presentation starting with Jack's big picture so this is you know right as an introduction person in this audience everybody knows are these things and basically we're going to talk about data that come after metabolomics and given the fact that I don't know how many of you have really Huntley such data or you know the peculiarities of this data I'm going to spend a little bit of time about a few slides talking about the pipeline so that all of us are sort of on the same page probably most of you have dealt with other types of omics data but do they have their own peculiarities but as you're gonna see you know once you have pre-process carefully the data many of the analysis techniques that you throw at them are very very similar and they come from identity so again another board slide you know metabolites are relevant for a number of reasons basically your interests in metabolic signatures you may be in metabolic plaques very important and some examples of Accession and identifying phenotypes and so on and so forth so it's sort of downstream omics technology and but it has sort of its own peculiarities it shares some difficulties but many of you may have encountered in the proteomics data but you know it has you know its own special features so when you do quantitative metabolomics especially the mass spec you get this type of spectra and then you're trying sort of to identify you know the peaks and assign particular metabolites and then as I'm gonna mention a few slides down the road it depends on some Peaks you're able to identify some other you are not able and basically that's what this talk is all about you know whether we can leverage some recent developments know the high dimensions papistical to help us going from the peaks are the features that would have identified and leverage that information to improve our identification and this is a this is a big issue here and this is one particular approach and basically I'm going to walk you through the methodology of how you can do that um so as I've mentioned just for all of us to see what's going on with this type of data this is a very typical data processing pipeline as usual with all the omics technologies you start with quality assessment quality control type of issues and in one type of mode of operation that's coming up what is known as the untargeted approach or a biased approach you end up having a lot of missing data many of these Peaks you cannot really detected for particular samples so imputation becomes an important issue as with all the other omics type of data that you get argue need to worry about normalization or removing budget tech support and then you start getting into similar types of analysis that many of you are familiar whether you are dealing with microarray data or on data exploratory analysis to see what's going on both from the perspective of your metabolites or features and from the perspective of your samples are then usually we don't go on into depression our analysis especially if you have two or more groups or biological conditions are or construction of classification models or associating them with an outcome variable and at the end of the day you know if you start sort of taking a more system's perspective you would like to get into pathway enrichment analysis and number of things that I'm going to talk about roughly speaking are related to network analysis in this particular case but when I use particular network models that you can contract from metabolomics data in order to help us identify these unknown features that you have in the spectra but towards the end I'm going to give you sort of a preview just a couple of slides or where you can go beyond originality that how you can start thinking about networks across different conditions that people have talked in the literature about differential network analysis so you do not see a sneak peak of where we're going with this project so one thing that you have you know when you process your pipeline of metabolomic data so basically it's not only that you process your samples but in many cases we put this pooled samples in order to control or to be able to say something about the quality of the run and see if everything was correct so basically what I'm showing you here is sort of the distribution across all the metabolites or all the features that you have collected for all the samples and these were sort of the pooled samples and that tells you about the quality of the data but you may need later on for some of the tuning parameters that I'm gonna talk about and this was the baseline and this were sort of experimental condition so the advantage is that in the double omics and similar another mass spec technologies you have this control type of sample you know it was the quality of your analysis and this is sort of familiar with other omics data you need sort of normalizing appropriately before any downstream analysis exploratory analysis depends on what you are interested in principal component analysis a popular technique cluster analysis where you get trying sort of to do join class or know by class and both are your samples and potentially selected features and see whether sort of you have signal basically these are sort of our techniques multivariate analysis techniques we will call them in you know to to get the big picture of what's going on both in terms of your samples and also in terms of your variables of features um then the usual differential analysis pick your favorite t-test under whatever conditions you would like to assume and what type of testing procedure you use depends on how you have normalized the data or another way sort of to start building class fit a Tory model to separate normals from the Z state or treatments for controls and so forth and it shows you know variable importance plots or random chorus I mean all these are secondary to my talk but basically I'm just trying to give you the big picture of what happens with metabolomics data so as you've probably not so ready you know there are some particular issues especially with the quality control and how you assess equal to your on and you know once you have normalized the data basically these are very common techniques that many of you have used when we deal with microarray data or RNA seek data and so on and so forth at some point you just have from a statistical perspective high dimensional data few samples lots of features and lots of variables and then you start using your favorite technique and depending on what question you're trying to answer so there are lots of similarities once you have processed the data the omics data of how you proceed downstream but at the end of the day in main case in most cases what you would like is sort of target in biological insight and that where this business of pathway mapping and enrichment analysis comes in what I'm showing you here is different network that we can construct from the Medscape tool that al has developed and continue working on it and I'm gonna show you how we have started integrating a new method in a new version of the tool so that's all the technology and the tools that were developing are can be sort of seamlessly used and pathway mapping and enrichment is important because you know you start taking as I've already pointed out a more system's perspective point of view um so right now I'm slowly moving to the sort of the coral they talk and you know what we have done one thing to keep in mind is that when you're doing the turbulence analysis depends in what mode you operate if you're doing a target analysis basically you know you you're using an assay where all the compounds that you're interested in you can identify you're going to have obtained data usually we don't have missing data so but the downside is that basically you are gonna have data for those combined compounds that illegal acid all the previous things that I've talked about normalization and then downstream analysis still whole but the big advantage if you're in a targeted mode is that you have complete data or nearly complete data from experience the imputation that you need to do is very light and you know it's on the other hand if you so if you know what you are really after a target can item provided that such an assay exists it's worked fine a lot about it on your hand if you would like sort of to cast a wide net and basically be in a global profile in mode then usually you're going to do an untargeted analysis and that's where a whole bunch of issues come up and that's where we're trying Toto to use some of the metadata and when I describe in a second to help identify a lot of unnamed features so so in the interpretive mode what are the challenges first of all we have a lot of missing data so the first thing is that you detect you have a lot of features because we take a lot of pics in your spectrum and only a small percentage only a small proportion corresponds to identified metabolites so here depends on how you choose your cutoff off because you had a lot of missing data so here you're asking for 70% presence in your samples of every feature they are left with about 4,000 feet if you make your cutoff more stringent you start cutting down on the number of pictures but even you see even if you ask that that feature was present in all your sampled they still have thousands of it and many of them in the majority of load are usually running and therefore for some types of analysis if you are trying sort of to see if you have signal to differentiate your two groups let's say treatment and control or normal and disease you know you don't really need to know that either because all the techniques that I mentioned in the previous pipeline many of these exploratory or other techniques like building class figure Tory models the only thing that they need is this feature and then they're going to select which ones are more capable of separating the two groups and so forth but if you start getting trying to do this business and therefore do a more informal analysis and try to understand from a biological perspective what is going on somehow you need to look to identify your future in order to be able to start populating these pathways and these networks so that's where this is the big challenge because um people usually about 90% of your teachers are gonna be a name and therefore the question is what you do electrodes gonna tell you is you need to work very hard and in order to start identifying features and what I'm going to present is you know basically built of some ideas of how you can correlate and named and named features roles and especially if you have data both from Target in an untargeted analysis how you can sort of try to integrate that information and provide good hypothesis of what these features may be and of course you still need to do sort of the validation basically but where we're going so that's why this this is sort of an important topic because in this type of analysis which is very popular you end up having all these features and then you can do some types of analysis but there are number of other questions that you cannot really answer and especially if you really in public so basically more or less I have covered the first bullet point and where I'm going you know next is the proposed approach is gonna leverage correlations both classical correlations and I don't know how many of you have seen the notion of a partial correlation and the corresponding partial correlation networks primarily between named metabolites that come from an untargeted analysis or in many cases if you have appropriately normalize your data you can pick those from a targeted assay and unfortunately I mean you know in terms of correlation networks that's easy to do but importantly when you start building this partial correlation network there are some technical challenges basically the key aspect is that the number of features that we've seen usually far exceeds the number of some available I mean even if you have a large study you may have 200 300 400 sample but we can see here even with a very stringent cattle basically we're looking at thousand so when you are trying to do this partial correlation business basically you start running to technical problems and describe okay so this is sort of stat 101 so this is sort of how the definition of Pearson correlation many of you have used that and so if we have arm P metabolites and basically we have measurement on the N samples and at this point I'm assuming that I have included the data have normalized the data flavor Finnish knives basically you can calculate the correlation coefficient between any pair of those so we have T squared order of correlation and then in that case basically we can build a correlation network and many of you have done that on some software platform they are very easy to calculate and but essentially they capture co-expression between two metabolites or two features if there are unidentified and from a statistical perspective they capture margin Association and therefore they don't sort of distinguish between Directors in direct connection so another sort of to be able to deal in delineate between these two types of connections technicians have total another way of measuring correlations which is somewhat more involved so this is the notion of the partial correlation and basically what we are trying to do is instead of just looking at the co-expression between two metabolite basically this is what the covariant just basically the covariance captures this association here you are trying to capture this association but conditional on the effects of all the other features in your data and one way sort of doing it is basically you use the measurements on one metabolite as your out variable and then you regress it this is a standard regression model on all the other variable and basically you calculate you know once you have calculated all these regression coefficients then you get the residual then you go on to the second metabolite and you do exactly the same exercise and then you get the residual basically your data minus the fitted value this gives the residuals and then you calculate a present correlation on the residuals so basically what you are doing is you account for the effect of all the other metabolites your data so that's the key idea behind partial correlation and the key aspect that you know what this sort of gives you gives you is suppose that you had a metabolite that was driving the association's of two other ones so basically this was sort of the key driver node in Kyoko in a particular condition so if you were to look given the fact that both these other two metabolites are influenced by this third one when you did this type of correlation what we're gonna see is very strong correlations on all the pairwise entity it was even this third one given the fact that is impacted by this other tool gonna show you a very strong arm Pearson correlation on the other hand when basically you do this type of exercise you have been identified but this was the driver node and basically we start allowing you to identify directors and that's the idea behind marginal associations and conditional as you can see from a technical perspective this becomes a more involved computation because then you need to do all these regressions and the other thing that you need to keep in mind given the fact that you have these regressions that you need to perform this is going to work if you have more samples than the turbo lights so going back to what we were discussing especially on target studies you see where we are going we already start tensing that there is going to be a problem because usually you have 2,000 or more features even after crash holding and towing those that have missing data and we are not going to have that's you should be at least in 20 okay so let me switch gears a little bit and connect all these things assuming that my data come from a normal distribution so right now the assumption that I make is that my tea metabolite follow a multivariate normal and all of you have seen sort of the picture or in statistics you have talked to you with the bell-shaped curve the picture of a normal distribution and in high dimensions basically you have sort of this multinational bell-shaped so and let me introduce a called Omega the inverse of the covariance matrix so as you remember the key idea when we were calculating Pearson's correlations was basically calculating the covariance and then normalize in it so basically this marginal associations come directly from the covariance basically so two of my time metabolites are marginal independent if the corresponding correlation is zero which is equivalent of the covariance on the other hand if you are doing partial correlations basically you are looking at conditional independence because basically you condition on everybody else the definition that we had and the nice thing is that if you make this normal assumption and come well justified this is in your data depends on how you have normalized data so if it up if you are feeling really comfortable but this assumption roughly all your data then it's easy sort of to calculate in one shot both Pearson and partial correlations and therefore them put them in a network so in that case the network you know the edges on the network they are gonna correspond either to your correlations in the correlation network and therefore this the strength of the edge of the weight of the edges and I captured the strength of the marginal Association or if you are calculating a partial correlation network the weight of the edges gonna correspond to the power relation and computationally under this assumption things become easy because either you're calculating the covariance matrix and those that are marginally independent just correspond to entries in your covariance that are 0 or you are taking its inverse and whenever you find 0 entries then basically correspond to metabolite that are conditionally independent so in my little sort of toy example accounting for this driver note these two are conditionally independent therefore there shouldn't be an edge in the corresponding ok so this is fine but as you sense this is going to start becoming a critical assumption because if you have more features more metabolites or more features than samples then your covariance matrix is not what we call not good rank and therefore it's not invertible so somewhere this framework start breaking down and therefore we need to to do something because otherwise in the type of problem that we have where you are always going to have at least at present more picture than sample you never gonna be able to calculate okay so so moving on so this is sort of the general technical framework there is more technical stuff I'm not but basically this gives you what Christians correlations capture what partial correlations capture how you can calculate them under the normal to the multi assumption of your data but the way we're going to proceed is we're only gonna focus on so the problem is that even if you try to use sort of this machinery basically if you have two thousand features basically are trying to construct a humongous network but has basically two thousand by 2,000 M even if you had enough sample and basically that means that any Association and conditional Association the partial correlation that robot is not zero is going to be present but some of them may be very very weak which means that yes you are picking the map but you know from a biological perspective they are not giving you really any information so here come time the point where Jack had a very critical insight and basically the next slides and trying sort of to translate or to justify technically within this framework traction side physical Jack kept saying that well when I look at the correlation network the correlation network I would like to first call them an only concentrate on that part of the correlations that are large either positive large values of negative large body but I don't really want to look at correlation coefficients that are close to zero so basically our strategy is going to be that we are going to first calculate the correlation network that can always kind of calculate because you are not suffering from this tissue and then only focus basically we are going to only screen the strong connection basically we're only looking for associations marginal associations between metabolite that have large positive or negative correlation and then we're only gonna sort of look at that subset of named and unnamed features they eventually were gonna do a little bit more screening and calculate the corresponding partial correlation network to identify the key driver connections as I explained so the question then is I mean if you calculate everything then you know exactly how the correlation coefficients and the partial correlation coefficient are connected under the normal the Assumption through this knot value relationship but when you start sort of just screaming first you know basically you're looking at a particular subject of your data and then the question is and this is exactly what we would like to do in order to further reduce the data but then what is the connection then between the Pearson and partial correlation network under screen basically Adamo so I'm not gonna bother with any technical details but basically these are sort of two key ideas that come from what we call data modelling and special production graphical models and basically I'm trying sort of to show you the key idea in terms of these pictures so we we are gonna have that these metabolites are dissipated given the remaining ones if basically these remaining ones block any flow these remaining ones create this blocking type of path and then there is another key idea basically what comes again from is graphical modeling and those of you that have seen director cyclic graph distributions of a directed cycle graph probably in the idea of faithfulness of the distribution of very direct attacking graph basically traffic translating now setting is if the total effect from metabolite 1 and 2 and from 2 and 3 does not cancel the effect of so basically um by using sort of them putting these two things together under the faithfulness assumption that depends on this idea of this separation the features that have high partial correlations are also need to expect except it's from marginal correlations so you know the mass which gets a little bit more complicated but basically this bullet point is a key idea for armed operationalizing Chuck strategy of first you know Street screening in terms of marginal correlations and then usually known will those features that have some marginal correlations and go into partial correlation so of course in our data we need to be a little bit more careful because when we calculate the partial correlation network we have done a half components that involve only unnamed metabolites and in that case it's very difficult to see you know what happens next so basically the way we're gonna sort of proceed at least now is we would like sort of to find strong marginal correlation special correlations between at least one named metabolite and a number of unnamed feature so we're gonna just start sort of concentrating even further and we call this blocks you know beef seeds and basically we're going to construct partial correlation networks among all the seeds that we have identified that are connected to named metabolites given the fact that you have thousands of unnamed metabolites even after all the screening you still have so I hope that decided is clear basically you know we would like to calculate both the marginal correlations the marginal associations of the pressure correlation and the partial correlations that give us this conditional dependence or independence relationship between our metabolites after a screening step and the previous slide justified and this bullet point justify why this is sort of cautious to do within this framework so so basically here I'm describing the algorithm of how you can construct a partial correlation network and the key aspect is that we'll use a pre specified threshold to screen out lonely correlated pairs of metabolites and you can pick five point six point seven depends on how much you'd like to screen out and then of course you know after we have done the screening you still would like to see depends on how much data you have which plans may still be sort of zero or different than zero and in that case basically you run a test you get the p-value and of course given the fact that you have your testing lots of correlations you need sort of to adjust for Malcolm somewhere and come back and discuss this data set in more detail but even after all the screening this is the picture that we get from this paper this is have kind of the hairball so how do we construct the partial correlation network even after our screen is test assuming that we have more samples than metabolites of future so this is sort of still so again we were gonna use sort of a threshold and we are gonna use one of the procedures that I described before which we get in type of procedure and then we're gonna again run a test to see if our partial correlation between two metabolites is zero non zero and again we're gonna just equality values for multiple comparisons and this is sort of a picture that can again bordered on this paper and what you see is that's what we expect but this is the match more spark network and you can start seeing you know what happen but the edges here are capturing conditional dependence relations that we Scully relation because you have conditions when you look at we said lookup condition on the information from all the other no to your network whereas here you know these are just marginal and this is sort of the interpretation of nature in this context okay so the question then is what happens in the setting that we're going to be in in our study we are the number of samples is significantly smaller on the number of features even after the screening test so so basically as I've already alluded to these operations start breaking down because they require an to be larger than P so over the last five six seven years and in the statistical machine learning literature that has been an enormous amount of work pull what happens in these settings and of course frankly assumption is you assume that the image of your covariance matrix that you're looking at this part so basically we assume that there are not too many connections so this is sort of an assumption that you need to make if you would like to make progress and and in general I mean you know this type of networks are somewhat sparse how sparse that's probably different but basically you need to make such an adapter and in that case basically there is a lot of machinery of how you can estimate your partial correlation network even if when the number of samples are fewer than and this is famous paper that basically solves the corresponding constrained optimization problem and for those that have seen sort of this type of framework given the fact that you're calculating an inverse of the covariance matrix you need to make sure but your final estimate is positive definite and this is exactly the constraints that you need to carefully deal so basically the data that we have after normalization and we have long transformed and centered and scaled appropriately we have the metabolite on this dimension and the sample from this dimension and keep in mind that this is a short and fat matrix because we have many more features on the double light releases and we have significantly fewer examples so this is a really fat matrix and short and this is the mathematical problem that you solve and basically this enforces this l1 constraint and at the same time you still need to operate in the positive definite Col not symmetric matrix so a few comments this is by now very well understood technology there is great code in our or in many other packages in other languages so people you can run it very routinely it's fast is easy to implement it provides a good estimate of the sparks the level even if you have your samples and metabolite one issue but we still fairly open in the literature and there are lots of suggestions is how you tune this parameter because basically this is the parameter but enforces the sparsity so basically it makes all these things work and - this is still a challenging issues from a technical perspective what we know by now is that these estimates are generally biased so that's something you may be concerned about and also you don't have any uncertainty associated with the estimate in this context you don't get distributions about your estimates and therefore you cannot construct confidence interval or until until recently you couldn't so basically this strategy that was working well in this context because basically you could run a hypothesis test and you can construct the confidence interval get a p-value that breaks down you get estimates but you don't have a measure of a circuit so but especially one key issue that people have noticed both theoretically and a lot of empirical work is that your estimates are biased and people have worked really hard to start resolving some of these issues and this is very very recent work it's not out were born from literature and find the gear is one of the top irritations in the field so and basically she proposed with her student a procedure both to correct the bias but we're getting and also forgetting asymptotic confidence intervals for your partial correlation basically based on this work suddenly you have a lot of goodies that you can leverage and this is sort of the Arabic the algorithm that you get and the key aspect is that basically you get in an appropriate normalized form you get uncertainty assessments full your partial correlations even when the number of small numbers so this is sort of a great recent development that we leverage it in order basically so let's start looking at some results and what happened and always keep in mind that the key aspect in these businesses you know how you use this tuning parameter basically this tuning parameter becomes too strong you estimate a very sparse network basically you're asking to estimate very partial correlation if you don't tune it very strongly here your circuit and many many more of those and this sort of the intuition behind how this parameter and there are suggestions in the literature basically you see asymptotically people have calculated the order of these parameters but you know in finite samples that we operate you need to do much more work in order to really and the procedure is very sensitive to how you picked in parameters and as I said is still a challenge in Asia okay so let me walk you to this sort of golden world standard data set that came from a German group and this is you know something that really are you gonna have access to such data because basically what they did was you have an enormous sample size you have 1,000 plus samples and today are looking at very few metabolize so basically we're in a very nice regime where your number of sample is much larger than the number of pictures so you can do lots of things so when we calculate the pressure on correlation and we used a very stringent adjustment of the p-values we get a correlation network with about 5,500 edges the total number of edges that could have been present is 150 Square and therefore still has a certain level apart not all metabolite are correlated with each other when you do the partial correlation network that we expect to be much more sparse again under the same correction for multiple comparisons which is very stringent basically you only get four hundred five edges basically these were these two pictures here you still have 50 500 pages and you get this airball type of network here you have one 405 and basically here you can start I didn't I think we'd rather not go back to what I was saying so given the fact that in these contexts we have lots of samples and therefore arm basically you can invert the covariance matrix and calculate everything here we haven't done in explaining the first experiment is to assess what happens when you don't have that main sample and the number of samples starts becoming of the order of your number of metabolites to see what's the sensitivity of our procedure and how well would you need so basically we start the first numerical experiment that we undertook was you know this is the complete data set then we cut it to crap then we type the half and then basically we took a value that is very close to the number of metabolize that were present and then another value but is you have in this context will have fewer samples than metabolites which is going to be exactly where you're going to operate with your own study we fit the model the way we describe it both by just calculating the inverse of the covariance matrix and then using this the biased and basically we're trying to look at how well we recovered the edges compared to what we had when we had a lot of sample so when you start doing this up sampling how much do you do because basically you don't have as much information and then the other performance metric that we report is well some of these edges or some of these correlations or partial correlations are not as strong so if we only are interested in the strongest partial correlation how many of those did we recover so the upshot and I'm gonna show you the table on the next slide is that if you use a lot of sample you know five hundred of course 1,000 both procedures perform well and therefore basically the key message for us here was that we know how to tune our procedure because that's the metric ERP when you start getting into these settings where the number of samples about the same as a number of metabolite or in such a setting where you cannot invert the covariance matrix anymore our procedure still that performs well relatively well so basically here are sort of the result so if when you use all the samples and under this stringent correction for multiple comparisons you get the network 405 edges and the strongest and percentages in terms of magnitude are 41 so then if we used our procedure we estimate a few more compared to this and we have agreement of 62% so basically we start having some and basically we recover all the top edges of the strongest stages fix our okay this is kind of boring because you could have used this procedure you didn't really need all the machinery here is where things start getting more interesting because here you start having half in your sample size and our recovery is still pretty good and we played the same game with the other procedure assuming that we see sort of the true so in this case when you have your number of samples basically you recover correctly 50% of these 405 edges but basically you recover essentially all the strong one when you go to the next level again here you still have 500 something only hundred metabolites if you're not going to have in most real tally here you start getting into the regime where the number of samples about the same as a number of metabolites our procedure at least in terms of the strong ones is going strong by trying to invert it various metrics you start breaking down here we're basically your n is equal to P this is crap here were still going fairly well and we still have recovery of the strongest edges here we have fewer sample and they'd done the double life so basically the two things that we need to keep in mind is that we know how to fill the procedure and therefore at least as long as you're interested in the stone Age's network we are gonna recover most of them correctly and we have a procedure but right now we can unleash on all real studies so here are sort of the pictures of how the networks look when we had all the samples and there are about four hundred five for act ages and here are how the two procedures perform basically here get very little if you're trying to invert the correlation matrix but well so let's do on another study these are data from well the problem deniz that then you know what is generalizing which is not exactly like the inverse and therefore not like I mean all the things that come under this Gaussian graphical more than linea second calculate some sort of an inverse it has very different properties and a statistical interpretation because here I'm looking for these conditional dependencies so mathematically yes I can take in English but I have lost all the other things that I would like to do okay so this is a sample of so here we have essentially 240 samples of course it is dependent between these samples because these are twenty women on two consecutive visits so here when we tune the model we need to be careful to take that dependent approach duration and exactly how when you have dependent samples how you do all this business that was part Oh Samantha buses dissertation and after we have sort of clean the data we have three thousand pictures all right now we are in a challenging set up and also we had some targeted data so we we run an untargeted analysis and we get 3000 features and then we also have some additional idea metabolic from the target attack and of course both of these are going to be somewhere there but many case we may not identify so I'm gonna show you some snapshots of what comes out of all this methodology one thing that we notice is that basically you start out of your B's features basically you have in sauce fragments so basically the metabolites have patented and basically this corresponds all these fragments correspond to bail-in and distal prolene and therefore this is very helpful to go back and start eléctricas other photo how you can do that but vicious and there are other ideas out there but this is one way that coupled with ideas out in the literature and start helping you identify all these in short fragment and therefore go back and start cleaning your picture so that's one outcome that you get out of all these ah the other outcome that you can get is basically you had a number of amino acids from the targeted assay and then using this seeds and partial coordination that form you can start populating an amino acid Network and this is going to be helpful when you start thinking about pathways and enrichment and so on and so forth so right now here what start talking about taking a more system's perspective and of course the last thing is sort of validation because here by you know doing the seed and the unknowns so basically here with a lot of additional work you start identifying the unknowns and that was sort of the key equation that we asked at the beginning of it of course this still requires a lot of manual labor because once you start looking at all this subnetwork you know you know really identify what are the nodes require quite a lot of work but basically that's where the the framework or we are going with this panel so basically these are the three possibilities that would have identified this is to clean up the data set in terms of the features this is sort of to start populating particular networks that you're interested in and this is sort of the killer application of sort of using all the technology identify listening features um these are flight from build back has worked on integrating all this into Medscape so this is sort of new developments on the net a platform you input your data to select your parameter you you select your normalization you said that your parameters you select your fresh holding and then you run the partial correlation or this is sort of the input file and then we using the slider you can decide how much you would like to um select or the pollution correlation that cook before you start looking at the partial correlations here are some screenshots and then basically you start visualizing the result and start zooming me know that sort of trade understand what goes on a little bit of additional background on Netscape it leverages scag and undo all these nice things be sure many of you have Allah giving much more in-depth presentation about her tool so the last thing that I would like sort of to before closing is this sort of the summary basically we used all these recent developments in high dimensional statistics of how to estimate partial correlation network even if you have your samples done features and then you know we're building a lot of the extensions with the Netscape platform so that everything works seamlessly and of course then the key equation is you know how to sort of prioritize all the things that have you come out of dissonance because you know you still have hundreds of potential targets so which one should you really focus in validate we have a number of ideas of how you can extend it to lipids one issues that still they method requires considered sample sizes so one question then is there are so many metabolomics datasets out there in the public other lab runs how can you start leveraging all that information in order to sort of build such networks and the last thing that you know I would like sort of to mention is we estimate the partial correlation network you can sort of at the start using additional constraint that we may engage with like chemical similarity which helps you sort of constraint the search page so this is from a technical perspective can easily be incorporated in dollar G and the other thing that you may want to do is in all this data that I showed you there might be two different by a lot therefore you what you may start asking is why shall I sort of just estimated any network both of named and unimpeded there might be differences in the network but we assumption that I make is that there are some common or similar components so when you start thinking along those lines the question is how you do sort of bench joint estimation that's basically it so ah my other piece decision that is working on the project Jean MA and these are data from lipids and I'll tell you what they are all about but basically there are two conditions and here I mean if you look at the picture it seems like identical but basically here we assume that under the two conditions the network share similarities but they also have differently that's why I had to lay out because here you see difference between that network and of course then you need to look hard in this situation of you know what which realization could be used in order to identify where the differences are about it means logical perspective and here you know we have colored two different very different lipids classes appropriately so that we can see what happen but basically that's where we're going with this technology of how you can do this type of join estimation of different correlations or partial correlation at different biological conditions some you know Charles Evans helped with some of these identification Josh are asking with the data also Steve Brown looked at some of this data with a Sun data and of course we always the granted thank you very much Chuck what's the ratio so what's the magic ratio so you know as your number of samples get below the number of features that you're looking at you know you showed us that when you went to that plot you know when you went down to a hundred and you had a thousand features you still were doing pretty good but is you know when do you start really dropping off or is this going to be a linear linear decline in the number because if you go to N equals 50 if you go to n to 25 you know what when you're going to start gritting your teeth so I mean if you have two thousand samples sorry 2000 features and 100 samples at most you're going to see able to identify a small fraction of the strongest ages and my point is don't even try so that's so what we are this is becoming key and that's what we are working on right now how to leverage targeted additional target studies so that basically it's not that easy to do because you know it's not like okay normalized on my date and then suddenly I go from hundred samples and 2,000 features to 500 pictures because how do you how are you gonna sort all align the features but the key sort of the key challenge is how you can sort of bring all the information that you have gleaned from all the other studies and therefore you know but an edge between loosing and a solution is there ninety percent of the time basically bring it in your estimation procedure the technical the technology you know how you do the math that's my students one part of my students thesis so technically I know how to do it but you know how you process all these datasets knowledge so to create a background network but sort of robust across a number of studies that what we're thinking working but this is the only way to go I mean if you are gonna unleash this technology of two thousand features and fifty samples I don't dare any ability for children I get out it's all wrong you need that place you know if you really trying to get a lot of stuff out there is a hope with the lipids because the lipids you can do much more if you get into the lipid space the reason is that given the fact that we identify the classes you can start putting in your estimation procedure much more many more constraints because you know about the members of the lipid class are going to be much much more highly correlated therefore basically which start narrowing the search base as you can see I mean when you have fewer samples than features you're doing you are trying to find this network leaving an enormous Lea complex and large space of networks so the moment you start putting any interesting information whether it is information about the lipid classes or chemical similarity basically from a mathematical perspective you start narrowing the search that's many things that work but 50 and 2000 George yeah George I got a question from Gil let me read it to you for many years the peaks reliably identified in metabolomic Sasae seem to have stuck at a few hundred maybe only three hundred compounds or serious efforts being made by chemists to identify multiples of this number our known chemical modifications routinely detected aren't you worried about relying on co-expression with small number a number of confidently identified molecules to build networks I think this is a good question but I think there are people in the room that can looking a child probably can answer a bad patient much better I think it has to do with the quality of the data and you know how company you are about the input tent any thoughts you you published on using variations of Granger causality look at time series gene expression data I was wondering you have an update and use of time series and how that might relate to this and under what conditions might a hundred repeated measurements yield better measures of edges and better more I want to nation than process data so the moment I mean this is a good question when you start blinking time course data you have to deal with the whole battle other technical issues and but still because in that case or to take into consideration how it was basically in time course data you're gonna have these thousands of features across different time points and then you need sort of even to model bay the profiles carefully I mean for any specific you know because in that case you have an even more challenging questions that eventually we would like to go there but there are a number of key technical challenges that when when we do sort of mapping various components in our case genes on a network with the unit on guam their paradigm is basically that the net work is something that she builds once based on the protein interaction to have profile geodata etc and we are just putting some of our data on top you're creating if I understand you correctly the network out of each experimental data set and you hint it at the end that you might want to use the metabolomics existing data as a as a background in the future so how different is this is it that the correlation structure between metabolomics let's say in blood or brain or some other sources it's very different is that why you have to reinvent the whole network every time no I mean basically you know this is sort of the first stand she ation of this technology so obviously you can start incorporating external information about connections on pathways so basically you know I just told basically showed you what you can do even if you start from scratch but you know in order to sort of to start using it more routinely as you hinted basically we need sort of to start creating this bad boy network but in this context given the fact that we have all these amazing the turbo lights there are a lot of challenges of all you're gonna integrate all your data set because you don't I mean if I can't identify the ultimate SS right because I know what the columns of my data matrix are that come study to study I don't know whether this column corresponds to that the other data set so you need to do a lot of additional work get the starting degree in order to create this common so so it's a much more subtle game but eventually what we want to do but so basically what I presented were sort of version 1.2 of how you start with an appropriate data set what are the technical challenges what are the key ideas but we're trying to sort of do and and with the eye on how this technology can be used it at clean your data set or identify me so I mean this is absolutely to what you just said but I just wanted to comment and put some numbers around it so one key difference in sort of you know doing this sort of mapping until essentially what you're talking about I think is sort of mapping your findings onto a knowledge based network okay and so that was the you know first and most of this step the gene expression data you know and so on and so forth important interactions and the trouble with that approach specifically for metabolomics is that the pathway databases they tend to cover primary metabolism reasonably well but beyond that the coverage is very sparse so it results in essentially when you do this sort of mapping in addition to what George said about the unknown features and so on and so forth even for the known compounds it results in coverage basically you know you can map on average between 40 and 60% of metabolites which is really sort of given the small already small numbers makes it even more challenging so I think it's sort of a must by which I have a question so not relevant but so let's see the blosum matrix the blossom can you use the blosum matrix some sort of a prime additional information about that right you know the blosum matrix that is described in the protein field so and you I'm just saying that have you ever thought about that this is not important I mean as long as you know as I point out here as long as you can bring any sort of prior information that you're comfortable with the technical requirements I can take care of that so the question is and that's what could have been discussing where this external information maybe relevant no to cut down on the search assist up is available and you know after what you know once you tell me what this is important and interesting information and relevant to what we're doing I don't have to this is this is a statistical question so it might be a bit boring but in my experience it's it's really hard to get this data to look multivariate normal and so in practice do these asymptotic results about confidence intervals and standard errors are they really applicable I mean from from the examples that you showed we one way of interpreting it that there is a lot of false positive so yeah so I mean people have start working you know if you have soap so basically they the short answer is that as long as these deviations are relatively small but if you really really have a strong spoon then essentially there is another sort of way of going to all these are basically three regression framework what I started describing of how you can get partial correlation from that without making the connection between the covariance and you can still get both sort of conditional dependencies but the idea is that it will lose normality you can still get with partial correlations but you know in this case unless I see you know from a lot of data exploration that the assumption of normality is grossly violated I feel comfortable especially when you concentrate on the strongest edges strong details are going to be recovered as well we you start getting to be smoke gray area depends on coming some without then all these things get a little clearing increase but you have much bigger problem to begin with given the lack of maybe a little need question from PG in point of view so in the beginning of the talk you were talking about like Bayesian networks directly to say if I grab and later in the end in an algorithm and networks themselves that look like Markov networks without directions and they are equivalent only if you remove all the V structures oh if there are two metabolites that control the third metabolite you always have a traditional edge in between them so is that what you're doing yeah the graphical structure what I'm looking at is Marko yeah okay so you want the table you sure look ugly by only moving subsamples like so have you tried to remove features for example even look some of the key drivers I'm missing not in your dataset to get a totally different network Oh Fowler can be a problem in some like he travels are not in a data set yeah because remember here on the correlation network this is not sort of gonna matter because the partial correlation if I start removing different features the conditioning set becomes so there's a project called latent quark model assumes and Xena miss it latent yeah okay so so basically I so in that case basically technically what you you work on we don't work on all of the covariance matrix trying to do all this then there are a whole bunch of issues of exactly how we do the screening if you start having such a triangle so so there are a number of technical issues you know if you do is sort of vanilla latent graphical models basically we work on the food companies or the conveyance that's fine but here we first do the screening you know the sort of the further cut down and then a number of technical issues good idea but as I said this is [Applause]