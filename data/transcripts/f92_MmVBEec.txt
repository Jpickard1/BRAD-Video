good afternoon my name is Kevin Nigerian with department of computation and medicine and by informatics department of emergency medicine department of electrical engineering and computer science at the University of Michigan I'm working with two centers Michigan Institute for data science at University of Michigan also Center for Integrative research in critical care the title of the discussion today would be on future of artificial intelligence in medicine and healthcare focusing on fears and hopes speaking of years this is the first time I'm doing a webinar like that so I hope I check all the technical things hoping that everything works fine but please through the chat let me know if you have any concerns or if anything is not working so we're going to start by acknowledging the sponsors of the research that I will be presenting and I thank our sponsors for funding the research that presented here I'm gonna start by introducing the people in my lab or the actual people who are doing this research and I'm presenting their works basically our lab has a number of faculty associated with them and we have people from the technical side of this mathematicians engineers and also clinicians from different backgrounds who are essentially leading these projects our lab has research manager and dr. Jonathan Creek and we also have research faculty dr. sue Schmidt who's leading the efforts we have a number of postdoctoral fellows and these are the graduate students the people who actually conduct the research in the laughs PhD students some of them are PhD students some of them MD PhD students and we also have a number of very talented smart undergraduate students who are contributing to the research in the lab so I'm gonna start by talking of the fears in using AI in medicine many so I'm gonna outline some of the issues and we get back to them and discuss them later on in a different format first issue is that if you look at solutions designed for medicine based on artificial intelligence and machine learning you realize that majority of what they have developed in in literature stays as publications they don't go beyond publications they you know people cite them people you know discussed them but in reality they don't turn into a commercial product a real product that can help improve the quality of care provided to the patients and all the ones that envision as a product and designed to help with medicine they fail to pass regular clinical trials and never make it to the market and the are the ones that make it to the market or they're getting towards becoming a commercial product some of them they're not if they approve they face to be you know if they approve and all the ones that pass if they approve all the few products that are truly using machine learning and artificial intelligence they get the FDA but they're not widely used by the community so the impact of this product is very limited and that's like in an observation that you see in many publications in many you know discussions that people are having they fail many panels disdain that are up and all of these issues are somehow fueled by a type of hype over artificial intelligence and in particular when applied to medicine and perhaps unreasonable expectations that you know people have in terms of what AI can do in medicine and there is a fear that if we continue having these expectations and not addressing the existing issues one by one we may head into another AI winter and this time mostly in medicine and biology because all those expectations may not you know fulfill and people feel that may perhaps like the problem is with AI so what I'm gonna talk about is I'm gonna show you a challenge treat the major challenges and opportunities that exist when we talk about using AI and machine learning in in healthcare and I addressed some of those challenges in more detail later on so as you see the challenge tree has three types of issues involved in the three types of challenges technical challenges health and clinical challenges and cultural and policy related challenges this one socio-economic and cultural challenges I'm not going to focus too much on those but you can guess that there is a whole lot of you know concerns about how the regulatory and incentive methods and the policies that are put in place can affect the user of AI so what happens if you know AI recommends for certain things in medicine and that course of action that medication that treatment results in a negative outcome for the patient according to the legal system is not clear who exactly gets you know stood for that or who's responsible for that who's the liable entity in it is the clinician that used that is the company that created the the product is the hospital system that acquired the product there's a spectrum of legal and you know policy related issues that are not vague here and I see that you know in at least a couple of cases in the and since the policy on these aspects were not clear that cause total you know an issue for for commercialization through commercialization of the usage of these products what I'm going to focus mainly in here is the technical challenges some of them are data related some of the algorithmic related challenges which I will talk about them next but I also refer to some of the clinical issues and technical issues in terms of the health such as epistemic issues in terms of who defines these these diseases what is the exact definition of the diseases that we're trying to address with AI for instance what is the exact definition of sepsis so if you want to create a computer-aided support system to address sepsis first you have to have a very clear definition of what steps I see and lack of clear clinical definitions can cause issues for the AI then they are is not essentially responsible for that so let's focus on data related challenges in terms of AI in medicine first thing is you have to consider the fact that there is a lot of variability and there is there are a lot of different structures to the data used in in clinical decision support and medicine for instance you have all kinds of images that have specific structure videos that have structure and enough data and electronic health records each one of them having different different sort of format and not just that in medicine the input data that you use the attributes that you use as the input often extremely noisy like you have a lot of missing data you have a lot of noisy attributes that their measurements is not quite certain more importantly the labels that they use they using in medicine are also not very certain as an example the issue of sepsis that I discussed before if you're designing classification technique to predict occurrence of sepsis or even detect the presence of sepsis first you need to know exactly what sepsis is and there is clear lack of you know clarity and and exact definition of things like sepsis a RDS and other things the other thing is the issue of completeness of the data a patient may go to different healthcare systems for the same disease like first they go to the urgent care close to their house they get some tests done and then they go to a hospital then they go to another hospital for the same disease for the same condition for the same issue they may go to different places and none of these databases are truly connected with each other and even within an Hospital within one healthcare system some of the data that we need may be told in a completely different database for example it's on the images are in the packs and clinical data is on electronic health record systems like in a Cerner or epic and there are a bunch of data that are you know sort of collected locally within the you know the the unique that is that is running the test and they never shared or essentially distributed through these any of these databases so completeness of the data is another issue another thing is data security and privacy and data sharing because of all the concerns that different healthcare systems has a bad part about the privacy of the data having personnel has information pH I data in there often they cannot pair data with others and they have to anonymize the data and some of these anonymization will somehow remove the information that might be helpful for clinical decision support system or on the other hand may they may not remove some of the you know some of the data that they are not considered as PHR but combination of these can essentially be a risk for privacy and security of the of the data but there are concerns about that and because of these concerns over data sharing we we have the issue of not having enough large data bases large and comprehensive data bases so in any other things in machine learning and artificial intelligence the quality of the model that you develop to the most part depends on the comprehensiveness and the size of the data base that you're using and these issues that I mentioned which prevent creating the data bases that you can actually use to create a reliable mother then algorithmic side most of our thought would be focusing on the algorithmic challenges algorithmic challenges the first point that I can mention here is the need to customize machine learning algorithm to match any specific application that we are processing one observation that anybody can make is that recently and more specifically in medicine people have been using one size fits all kind of approach in machine learning for example they have this deep learning method that regardless of what exact problem they're addressing they put the data in this machine and they shake it and they expect to have a model that would be suitable for that specific application which essentially is not very realistic expectation in reality every every application every domain requires particular type of mother that matches the reality of that that problem that application and the tendency to use the same machine learning for everything has created you know some results that are not reproducible and some models that are not robust enough so it's highly desirable to look at the realities of medical applications and see if we can Jenny algorithms that match those a specific application second problem is that in medicine law as I mentioned a few minutes ago majority of the databases are extremely sparse in other words like you know you do have a bunch of variables for which you don't have a lot of instances you have some examples but in if you look at the data as a comprehensive like you know entity there are so many missing data points in it and that causes an issue for training neural network or any other machine learning algorithm in particular neural networks and in order to address that we feel that there should be approaches in machine learning that allow the domain knowledge what else mean is known about the application to help address this a sparsity I'm gonna leave it vague at that level and I get back to it when I talk about you know some potential approaches to that and another factor is if you look at what people publish in the field first thing that come especially like you know statisticians and people with more mathematical approach to the problems they they see is the fact that some of these models are not properly validated and assessed they use a certain small amount of data to pain a very large model and then the assessment is is not comprehensive enough to actually show that the model is not doing overfitting many of these methods are not are not reproducible in a sense that they created with the data that is either too small it has the flavor of one particular healthcare system and then tested in larger databases on other databases on other healthcare system data they completely fail and this is because full validation methods that are used for majority of these machine learning algorithm and last item is is the challenge that is becoming more and more evident in in medical field and that's lack of you know ability in terms of like these machine learning algorithms to address the structure and the temporal relation of the data it's in order to see how important structure is imagine that if I show you a structure two dimensional structured data we call it an image if I show you an image and I ask you what you see in that image let's say there's a like you know image of a person you will immediately just by looking at the two dimensional entity you know that this is a person a person that you know perhaps but if I get the same data I get the same image and I flatten the data and make a big vector out of that like I'm concatenate all the rows one after another ending up with a large peg that I haven't changed the pixel values I haven't changed the information in any of like individual pixels but by making a very very long vector as opposed to the structure two-dimensional structure if I show you this long vector and I ask you what is it that I'm showing in this image you cannot say that and it's because because of the fact that the most important information in that image was the 2d structure and by flattening the data you're actually destroying the main information in the data and if you look at majority of these algorithms these algorithms are actually doing nothing but that they get the information flatten the information then we'll try to figure out what was the object that they process and that's an issue that was recognized by the community and and now there are metals like deep learning is trying to in some way address that there are other techniques that are doing the same the same task so I talked about all these issues and fears and challenges I think it's now it's a good time to talk about some of the hopes some of the good thing happening one thing is that FDA and other regulatory authorities I think the improving the guidelines they're becoming more of a dove what an AI based product is and and they're trying to come up with better way of testing and validating these things they're some of the regulations that they have they're still very old and outdated but I think there is a trend to better understand where AI is stands machine learning and artificial intelligence stand on how to test them in a clinical setting the other thing there are so many consurtio that are dealing with creating larger databases even though they're not as comprehensive as you want them to be but there's still so many you know there are so many efforts national and international efforts that are trying to create larger databases that are more useful for AI and and machine learning the other thing is I would say that some components on parts of the medical community they are becoming more reasonable with their expectations from AI they're not expecting magic from AI they don't expect a box that can always tell them what they want to hear and all the bulletins are also improving and addressing some of these challenges so from now what I'm gonna do I'm gonna focus on on how to address some of these challenges I would love to just focus on the overall methods of addressing these challenges and don't talk about our own research but in reality I know the solutions that I'm using better than others what I'm trying to do is talk about these challenges and give you an example of a solution that we have develop in our lab so the first challenge is systemic integration of auxiliary information that we have around around the subject remember I I mentioned that when the data is very sparse in order to have better machine learning or better modeling of the problem you have to use what else is available in the domain you have to use the domain knowledge you have to look at other data that can somehow affect your ability to address this and to model this a sparse data this particular example that I'm going to talk about is an algebraic approach to drug repositioning and there are repurposing that we have you know started working on and it seems to be doing exactly what what we wanted to do in terms of integration of auxilary information so just a quick couple of definitions drug repositioning is user job and already used drug for one purpose for another purpose for example a press card that I mentioned in here was originally developed for prostate cancer and now people are using that for some sort of male pattern baldness as well so the drug was used for some purpose now you're seeing that if you can use it for another purpose as well and drug repositioning is the case that there is a a drug that has failed for certain application and now you're asking yourself can be used for another purpose for another application so this is a huge total effort and direction in pharmaceutical research and we're trying to somehow use algebraic method to help with that so if you look at what why we need like algebraic and computational method towards that you have to think about that the fact that typically they have a few thousand if not a few tens of thousands of small molecules that they use for the drugs and they have a few thousand if not tens of thousands of potential targets that could be you know proteins could be genes could be other factors but we have a lot of you know if you think of a matrix of drug versus targets we have high dimensional matrix but this matrix is highly highly sparse in a sense that one drug was tested against one or a few targets and you don't know anything about the interaction between the this drug and other potential targets so you are dealing with sparseness of sometimes around 99.997% so there are very few known numbers in there whether it's zero saying that there is no interaction or there is one there saying is an interaction and the rest of the numbers in the rest of 99.997% it just unknown so you don't know so if you think of this problem in terms of mathematical formulation it's nothing but matrix completion in a sense that you have a matrix of drug versus targets but we have very few numbers in there you want to somehow use what you know to complete this matrix and estimate the interaction potential potential interaction between drugs and and other other proteins so various techniques have been designed to address that some of them are based on similarity based methods some of them you know they're based on other techniques in machine learning such as deep learning and some of them are using algebraic methods like matrix factorization what we want to do we want to go back to our policy and our understanding that there is a whole lot of you know side informations around this matrix that can help us complete these more intelligently and have a more reliable model first approach is something we call it coupled matrix matrix completion so this is the matrix that we want to complete this is the drug versus target but in reality we know a whole lot about the interaction between the drugs for example we look at the 3d structure of these and we can say how similar the structure of these molecules are to each other so that's some information there is another information that deals with targets the relationship between the targets we have protein-protein interaction matrices that you know very quantitatively tell us how these proteins interact with each other so this is information auxilary information that is highly valuable this is also exhibiting formation is highly value but the question is can we use these additional matrices and make sex and miy in order to help complete mxy so that's one thing that we started working on there are others who are doing that but with the help of dr. harm dicks and was a well known algebraic theoretical algebraic mathematician we came up with an approach that systematically uses the information in these two auxilary matrices to better complete this drug target interaction matrix but not just that keep in mind that if you want to look at the interaction between the drugs between these small molecules there is no just one way of looking at for example you can look at the interaction of these molecules in different pH environment you can look at this from there are different different simulators different models to show the 3d structure singularities there are all kinds of other ways of looking at similarity of drugs to drug so there is in reality that we have multiple matrixes like that same thing with targets you can look at the interaction between the targets in many different ways and as a result we are dealing with another problem and more comprehensive problem called coupled tensor matrix competition in in other words you want to again you want to complete this matrix that's your target matrix but you have two tensors a stack of matrices that identify drug drug interaction a stack of matrices that identify target target interactions so you have a tensor of the target tensor of the of the drugs and you want to use these non value this.value structured data within these two tensors to complete this unknown matrix that is very sparse so I'm not going to go over the details of the algorithm but this algorithm systematically considers what is known as the interaction between them the matrices of the drug drug interaction and target target interaction and the same thing with the tensor to have a better estimation of drug target interaction so the question is let's we developed such a model how we tested how we validated in order to validate it in we repeat this a process that I will describe 100 times or like a thousand times doesn't matter what we do each time we go and remove 10% of the data that we know that are there right known information we pretend that we don't know that and we use the rest of the 90% remaining to estimate the ones that we have removed and we keep doing that for 100 times so each time randomly remove 10% and our ability to predict what we have already removed intentionally tells us with it algorithm can guess what what might be the values other values in the matrix and complete the matrix for us so without going into the details I'm going to show you one these set of results that we have we have done multiple databases of drug interactions these are all the techniques that you know majority of the techniques people are using in the field and this is coupled matrix matrix completion method which you see that in terms of a you see in terms of the computational time in terms of accuracy and in terms of the standard deviation over all of these factors is superior to all that but when you get to copper 10 matrix completion then we have substantially better results compared to all of these techniques and and all across all the measures like you know if you look at F F 1 is score is significantly higher than the rest of them so we have tested that on multiple databases like there are drug Bank is you know one of the large publicly available databases we have used other databases and all like you know it use something around 15 to 20 percent improvement in a you see just by using auxiliary information in a systematic structured way so that's one of the challenges the next challenge that I'm talking about it would be creating generalized yet robust models that can be trained by small databases keep in mind that we have you know techniques that can be they you know robust but you need substantial amount of data to train them can we have models that can be traded pain with small data sets but will be robust and and repeatable and generalizable or that I'm going to talk about one example that we have develop in our lab and algorithm that we developed for detection of in-vehicle cardiac events funded by Toyota and we want to for that we want to essentially detect some severe types of arrhythmia like severe atrial fibrillation afib super ventricular tachycardia or SVT ventricular arrhythmia OVA and bradycardia bc so what we want to do and we look at other types of like very you know or less less common types of editing as well majority of the techniques that are out there they have you know they look at the electrocardiogram and they try to first we take where the major peaks and major features of the signal for example they do our detection pqrst detection and based on the timing between these and the duration of some segments such as st-segment they come up with wait with some algorithm one major issue with these algorithms the fact that in some of these like a hit me that I listed in here like a fib and others in some of these I'll it means you don't have some of these things like you it's very rare that you can ever see you and you wave and and sometimes most of the time you don't see T and P is also missing in some of them so even the existence of some of these waves is questionable it alone like you know you can't on these as features in order to assess the the arrhythmia for instance if you if you rely too much on ST segmentation for your decision-making and as T cannot be computed because T is not very visible this mixed with noise then you have a lot of inaccuracies just because of that assumption so these are if you look at two groups of algorithms used for this the first group of algorithm that I'm going to talk about here are the algorithms that are combination of conventional signal processing and image processing techniques these techniques essentially prior to pre-process the data do the peak detection as I said find where our pqrst are do some feature extraction with you know methodology such as wavelet or HIV techniques and do feature reduction with pca I say other methods and then feed this feature to some algorithm random for a support vector machine artificial neural network to come up with the prediction even Markov models on the features that's the more conventional approach to this problem but another group of methods are using deep learning which is very different from those they just feed the ECG directly to the model to the deep learning technique and then you get the prediction there are good things are bad things about both of them like I said the issue with this approach is that you know everything the conventional approach depends on your ability to get these features and name make sure that this is actually our it's not a different peak and the issue with deep learning is that you know deep learning is works fine relatively fine if you have an enormous amount of data for some of these like SVT if you if you put all the report that is assuming that the quality of the annotation is very high there would be there is no big database on SVT it would be very difficult to create enough cases so that like a deep learning algorithm can reliably and robustly predict SVT I'm going to show that through some results first I'm going to quickly introduce our algorithm algorithm is very simple it's a probabilistic sort of version of a finite automata system that's one way of looking at it another way of looking at it is is some automated version of a Markov model what it does starts with soft symbolization let's assume we're not doing software doing hard symbolization this is what we do we put some levers a B and C we don't care about the waves like R or whatever we just like divide the the range of the values in normalized ECG as a B and C and we change the sequence of numeric data in sequence of alphabetic data for example if you look at all these region is B so you have D BBB then you go to see you get a bunch of C's then you go back to be you give them one B and then you go to a get a couple of a's you go back to C and B and again a bunch of seeds so instead of relying on on the exact numeric value you create a long string of variables like I said this is the hard version of symbolization we're doing a soft symbolization in other words we create probability of each thing being a or b or c so instead of like saying is definitely it's a B we don't want to rely too much you have like sensitivity over what is the like value of threshold here and exact value the threshold between B and C in order to reduce those sensitivities instead of actually you know art code that as a or B or C we trade the probability of being a.m. B and C and we use that to create a tree and we look at the words the sequence of symbols that are more likely to happen and and we spread this tree as long as we have a certain amount of you know minimum value of frequency of these words once we have these words and their probabilities then we create a transition probability transition among these states going from like a to a B from a B to to a BA or something like that we create a finite automata but probabilistic finite automata based on these values and this becomes the mother in this mother we don't care what is all we don't care what is T we don't care whether T is even there or not we just you know look at the range of the values that we dealing with when it comes to prediction we use a window of time look at the start with the first row we look at observation window for the signal so for example we monitor the signal for half a minute for five minutes whatever and then based on the value that we observe here we make a prediction of the event that is happening in the future we put a prediction gap between them so for example we we look at the values that we recorded for the last two minutes and make a prediction for four and a half minutes in in the future right and we change with the values of signal window the window of observation and prediction gap sometimes we get we get the sync signal window the same but we increase the prediction gap we want to make a prediction for well ahead of time and sometimes we keep the prediction cavities as it is but we monitor the signal for longer time and we want to see all these models other related to each other so next I'm gonna show you the comparison of our results in making these predictions compared to some conventional methods so let me show you what we have here the blue line is our algorithm the red line is deep learning the green one is a combination of HRV features and support vector machine and the yellow one is dwt and an support vector machine so we looking at two of the conventional methods one deep learning and our method and the halo that you see around each line is talking about you know standard deviation over that so the whole vertical lines are AUC and area under the curve horizontal line is how many minutes ahead of time we make the prediction after minutes before before the event one minute before the event all the way to four and a half minute so each one of these is like you know for half a minute observation one minute observation and two minute observation but observation for events that are happening in like in a minute in the two minutes and three minutes or four minutes as you can see here our method is essentially better than any of the other methods the blue one is our method deep learning for a thief is very close to others but look at the halo the red halo the red halo is so significantly larger than the blue halo which you hardly see that which means that standard deviation of the results that you get from deep learning are much bigger than the standard deviation that he gets from our method staying that our method even we have a lot a lot of data and you train your reliable deep learning our method is more robust and the ups and downs are far less regardless of whether you're doing the prediction is in half a minute or longer than that another observation is that no matter how long the prediction is how much how big the prediction gap is there is still around 0.8 AUC which is significant you can predict what is about to happen with 0.8 AUC but now if you look at SVT supraventricular tachycardia you will see that you know now deep learning is far less accurate and ours in all three cases you see deep learning is is significantly lower than our mentor the blue one red one is deep learning blue one is hours and look at the halo around deep learning and other methods that are significant they're not as robust so this is what we wanted to do we wanted to do an algorithm that is when you have a lot of data it's very robust but if you have few data like SVT that doesn't have a lot of data you can still have a reliable model and it's robust and repeatable so another challenge that we wanted to talk about is was incorporating the uncertainty of the labels and utilizing the timing of the data for that I'm going to talk about an approach that we are using for prediction of acute respiratory distress syndrome a RDS when you look at a RDS there are two major challenges in in making detection of a RDS which is like a very you know major respiratory issue for all ages one is that nobody really knows what a RDS is if you get clinicians to label any ideas and when a RDS happened you get significantly different results so no matter how you create these labels these drivers are uncertain secondly is the timeline of how the decision is made the majority of the time the clinicians are looking at the patient physiological and you know EHR data they look at you know some physiological things such as vital signs and some EHR data such as lab results and they have to make a decision with it this is a RTS or not but in reality maybe perhaps two days or whatever days after that they will order chest x-ray and Chex x-ray is the most informative data that they can have having chest x-ray it would be much easier to say whether they're dealing with a RTS or not but in reality at the time that the patient is is admitted to the hospital nobody even or their chest x-ray they may not even think of the ideas to order a chest x-ray so chest x-ray you cannot assume it's available at the time that they want to make a decision so if you give this problem to two regular machine learning regular machine learning would always want to get you know physiological clinical data as an X the input X and a RTS no ideas as Y and machine learning regular machine learning wants to develop a function f that map's X to Y input to output and as you see chest XA is nowhere in the picture in other words since you don't have chest x-ray when you're making this decision yesterday chest x-ray has absolutely no impact on how f was generated because you need to have like you know the input and the output and you cannot assume you have chest x-ray the question we're asking is can we use a new way of machine learning we call it learning not we call it others call it learning with privileged information that still tries to do the same thing you wants to look at the physiology X and map it using a function f p2 Y which is a RTS or not but remember pay attention to the fact that in painting data in retrospectively collected data we have access to x-rays right these are the cases that happened last year or something he we have like you know three days after the first assessment they didn't x-ray so we have an x-ray can we somehow train FP in such a way that the impact of those x-rays in the training data is affecting the choice of FP pay attention to the fact that if we can do that FP can be used operationally in real-time usage just like F in a sense that their input their real input to the system is still X but it's predicting Y for you but the difference between F and FP is shown in this diagram that shows the family of functions he like the family functions with big F and yet if you use regular machine learning regular machine learning is trained without the knowledge of x-ray and it gives you f but if you use privileged learning it gives you the knowledge of X esta will pushes the choice from F to F P which is you know the difference between a naive predictor and a more informed predictor that is F P so now putting everything together we design an algorithm that is essentially considers the availability of privileged information not only just the input and output of privileged information and the level of uncertainty over the data over the latest I'm not going to go over the algorithm but this is the revision of SVM support vector machines under the learning method that we call it learning using label uncertainty and partially available privileged information pal Lulu puppy we decided to put like a funny name for it maybe that would attract you know some more researchers to that so with Lulu puppy if you look at how Lulu puppy compares the results of Lulu potty compared to conventional techniques as well as you know that shallow neural networks and Ellis TM as as deep learning approach you will see that in terms of the level of AUC and the closeness the between training and testing a you see we are beating essentially every other technique so sometimes you can have like 9 percent difference between training and testing between like you know in lsdm cases but the training that the difference between training and testing a you sees are really insignificant in this case we are actually getting a slightly higher AUC for for our method which is what we wanted to do we wanted to have a robust technique that uses all the information in there and considers the temporal you know relationship between the data last one that I'm going to quickly talk about is addressing the issue that I said like you know how to somehow keep the structure of the data using tensor methods and for that we are going to talk about an example that is funded by Department of Defense and that's development of algorithms and monitoring systems to essentially predict postcardiac the events in patients recovering from major cardiac surgeries so the question is first question is how to define these major cardiac events we spent a year and a half almost putting a panel of different clinicians from different backgrounds to come up with a definition of what is an event and we came up with the event we quantified each one of them that took a significant amount of time to come up with but it's all worth it then we design an algorithm that looks at physiological signals electrocardiogram arterial blood pressure and spo2 and using some estimation method called total string will create multiple resolution estimation of the metal and for each of these estimations we'll calculate a bunch of features then we use tensor methods to compress this information on by night with EHR data and use machine learning for prediction in order to better explain what happens with tensor ization how we create these tensors we get this signal we divide that signal into some windows for each of the windows based on multiple resolution that we put on on on for testing we create approximation and different levels and we create like a set of features and it could be multiple sets of features like this set of fish it could give a bed and we do another thing with a different type of you know method like Fourier in reality in in terms of algorithmic representation of what is happening in schematic representation of what is happening is that we create multiple sets of features and these sets of features some of them are very very long you have many features in in these tensors what we do we use higher order singular value decomposition to reduce them in terms of those long dimension put them in in all of these tensors in the same or similar size then we put them on top of each other to form the fourth order tensor for each patient I cannot show fourth order tensor so I decided to use a cube but in your in your mind please add another dimension to that to make it a four dimensional cube so that becomes one patient and we have so many other patient each one of them is represented as a four dimensional cube then we do tensor decomposition we put all of that reduce that and get the features and then we feed that to machine learning algorithm with this algorithm we want to make a prediction whether the event is happening in half an hour in one hour in two hours in four hours in eight hours and 12 hours ahead of time so we wanted to have a technique multiple models that each one of them will tell you about the short term and long term event outcome of whether there is an event or not and I can quickly show you your results that you know our technique that is using some machine learning algorithm developing in arla those that in half an hour prediction one other head prediction all the way to 12 hour prediction we are in the range of 0.8 AUC which means that you can predict the results twelve hours ahead of time predict if some event is about to happen twelve hours ahead of time with very high AUC and if you look at f1 values they're also very high which means the power of adding all the data in the structure and keeping the structure while you're reducing the dimensionality is extremely important so those are the items that I wanted to cover in in my presentation so I guess that we have some time for questions and and if you want to use the chat button I guess if you if you are your question or comment in the chat box we should I should be able to read it I'm guessing to wait a few minutes to see if there are questions coming I'm not sure if if the system is is allowing for for people to post their their questions we are the chat so oh I started seeing some some things that people are posting let me see if I can if I can see that maybe not top it's in that case I'd be happy to answer any questions if you email your question to me and I will try to answer them via email so thank you for your time and I look forward to interacting with you about potential questions have a nice day