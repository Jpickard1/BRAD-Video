[
    {
        "start": 0.12,
        "text": "hello everyone so uh today I'm going to talk about open SRH which is our paper um that was in New York data sets and Benchmark track in 2022 right so in one sentence summary of what I'm talking about open SRH is the first and only publicly available SRH data set um that's out there right but before I get into the data set itself um I think it's a good idea to first introduce some background about stimulate Roman histology I'm talking about the AI methods that we have developed developed with um that the Imaging method and then I'll go into the details about the data section of benchmarks right so let's Dive Right In so the problem that we're dealing with is in the field of Neurosurgical oncology and this is one of the most important problem um in the field and is brain tumor diagnosis so what happens is that a lot of these brain tumor patients will come to see a physician at the institution uh "
    },
    {
        "start": 60.539,
        "text": "here like the University of Michigan and maybe they have suffered from a neurological deficit such as maybe a seizure or a headache so what happens is that they'll often get an MRI and they all look something like this right so what we can see on the MRI is that we can see something's going on right there is a lesion here but ultimately this doesn't give us a diagnosis and we don't really know what this is um the only way to get a diagnosis um is to obtain tissue from from from the tumor right and then uh from once we get the tissue then we can put a biopsy on the tissue and then actually take the look um at what the tumor is but there are many ways to obtain these tissues ranging from a small biopsy to a gross total resection so what happens to these patients then oh one more thing I forgot to understand is that this is very important because the diagnosis actually determines the treatment right so for a patient for tumors that respond really well to "
    },
    {
        "start": 121.2,
        "text": "radiotherapy or chemotherapy we actually don't want to cut out all of the tumor because that end up hurting the patient in the long run so what happens to these patients is that um they will undergo surgery and a specimen from the tumor will be taken um uh and right now um this uh specimen will go through a pathology workflow that is slow labor intensive it requires a lot of tissue processing right and here's what I mean um the tissue well the essential Pathology lab and then a histological Workforce will have to process the tissue right um and there's a lot of processing steps including fixing sectioning and sometimes staining and this requires a highly skilled Workforce and and these images can also look very inconsistent sometimes uh that depends on the the technician's preference and their experience and once the image is acquired and then it will get sent to a "
    },
    {
        "start": 182.64,
        "text": "neuropathologist to get a diagnosis and this is a time consuming process and can take anywhere between half an hour to up to two hours and oh oh well the surgeon is waiting in the operating room to get the diagnosis and this is not Optical workflow this is not a optimal workflow for us so the question is can we develop something better right we want something that's more streamlined for intra-operative pathology um ideal uh workflow to evaluate this specimen should be accessible everywhere right even though even when there's no equipment or highly trained staff available um it should be fast right we should uh we should be able to get the images or the diagnoses right away without having to wait for a long time it should be standardized everywhere right so um it should be invariant to institutional protocols patient demographics in the clinical Workforce "
    },
    {
        "start": 242.7,
        "text": "and it should be accurate when compared to the existing standard repair so it should work better than what we currently have and I think where I found something like this and that's why we work with stimulated Roman histology so with SRH we can instead of sending our specimen to a conventional lab we can actually um roll the microscope into the operating room and we can image right there and uh we can get these histology images in uh just under a minute and once we have these images we're developing AI methods to help Pathologists um to to reach the diagnosis faster and this whole process will take uh in about just a minute right so we can see that there is a significant speed up from the conventional workflow to Imaging with standard uh with SRH so um SRH is actually currently "
    },
    {
        "start": 303.96,
        "text": "clinically being used at the University of Michigan and many other Essentials across the US and also in Europe right so let me go into a little more about the Imaging details on for how this microscope works right so once we have a specimen from the uh from the patient um all we need to do is to place that onto a microscope slide and we don't need to do any other tissue processing we just need to put it on the microscope slide and then close up cover right and then this slide can go straight into the microscope um without any other processing um so once the once the tissue is in the microscope uh we then start the image acquisition so um SRH the technology is based on a Ramen scattering right so what rhymes scattering is is we have instant proton on on the tissue and it can gain or lose energy and when it's being scattered and "
    },
    {
        "start": 364.86,
        "text": "when against the loose energy it has a shift in frequency and we can measure this shift in frequency to characterize biochemical composition but the problem with uh Romans Gathering is that it's very slow to acquire the problem the Broadband Spectra um so in 2008 of stimulate Ramen uh scattering was discovered and instead of using one laser um two two lasers are being used and um we can use we can use these lasers to achieve a non-linear amplification of a narrow event Spectrum right so we will choose on this frequency very frequently very carefully um to correspond to molecular vibrational signatures so what we do here is we choose the two very specific um they correspond to uh "
    },
    {
        "start": 425.16,
        "text": "they correspond to nucleic acid and the protein contents in the in the protein lipid content in the specimen so once we once we do that then we acquire um these two channels of images of the signals that comes from the microscope um so the raw image format output is like this and they're in their diacom format um once we have these images we actually do some tissue processing image processing for AI model training so what happens what we see here is the two raw channels and we can see that the channel on the top uh toucher corresponds to protein and lipid content that's in the tissue and the second row here um corresponds to the uberic acid that they tend to reflect on nucleic acid content so what we can do is to subtract these two channels to highlight the nuclear contrast and we can then concatenate that with the two channels to create a three-dimensional three Channel image right which is something that we're used to looking at "
    },
    {
        "start": 486.56,
        "text": "and uh and these images are so these these are a few examples of the of the patches that we generate from the from the whole side images um so we will patch our host that image into smaller patches because these Pulsar images are on the scale gigapixels and they're not feasible for uh our gpus to process and we use the segmentation model to determine where on the slide is tumor and What instrumental right so we can generate this cmap see that appears the tumors and appears from normal or normal content and these and then the tumorable patches right so we're confident about that these patches are tumored um then I can go through a neural network to predict um the tumor probabilities so um next I want you to talk a little more about the AI methods that we're having developing our lab um that go with um "
    },
    {
        "start": 546.959,
        "text": "SRH images and specifically I want to focus on two projects um the first one is on the scope-based analysis and it's actually featured on the cover of the neurosurgery journal in 2022. so uh first I want to talk about why we chose um star-based tumor analysis right um skull base uh is Spokes uh the diagnosis of a cell based tumor is essential to have a personalized treatment and um and so it's very important for us to have this diagnosis um during the surgery and the other uh important bit about small Bas treatment is that it's very diverse right so here are some samples of normal and selfish tissues and we can see that these histologic images look very diverse ranging from a very dense cellular tumors to some of the tumor that's less cellular right so um in this project we use ID we "
    },
    {
        "start": 608.76,
        "text": "use the idea of the similar data should have a similar representation so um for all of these images right we would um we can use a model to compute some representations um for these images which can then be used to generate the predictions um for each for each patch and so what we can do is we can find some a similar patches uh and we will pass the computer model and we'll try to make sure that they're similar on the representation space so on how do we find the patches that are similar so we can do this in two ways um if we know the labels of our images then we then we can assume that a similar the same the images from the same level uh will have a similar representation right so if that's the case I'm done then we can use the images from the same label and make sure that they "
    },
    {
        "start": 669.779,
        "text": "have a similar representation and if we don't know um what the what the ground truth label is for our images then we can actually use different augmentations right so for example we can try to blur the image or maybe shift it around right so um this this way we can know that these images should look are still are still very similar to each other and therefore they should have a similar representations so using this technique which is called contrastive learning we can achieve excellent results um and as we expected right our supervised contraceptive learning where we know the labels during training um all performs um self-supervised methods um which is trained without labels or the conventional cross-centric model that's the Baseline we can also look at some confusion matrices uh for the models that we train and we didn't see that many areas are actually in the processing class and that is uh that actually do correspond to our domain knowledge that metastasis "
    },
    {
        "start": 731.1,
        "text": "is a very diverse class and it makes sense that we have this um slightly worse performance right we can also visualize these learning representations using tissue dimensionality reduction and from here we can see that um they're using supervised contrastive learning we can visualize much better separation between classes uh compared to uh process attribute but we can see that the cell supervised learning um doesn't do as well and in this that's okay because from cell supervised learning is actually training without any supervision uh when we're learning the model okay what are the points here uh there are each patches in like roughly how many images or patients is um so we have about 8 000 points as visualized but that's not all the training or the validations data set um I don't know the number of patients at the top I had "
    },
    {
        "start": 791.639,
        "text": "um we can touch base after I can I can compute the numbers um one of the most common uh questions that we got from surgeon is um how does that help us right how does the AI help us so what we can or what we can use our aim to do is to do something called margin delineation so what happens during surgery is that the surgeon will assemble from different places on the tumor and the surgeon will want to know um where every place is sample is tumor or normal so our model can help the surgeon to decide that maybe different regions that they sample this tumor um and maybe they should keep respecting or maybe some regions that we sample is not tumor and maybe the reception is good all right so that's the first application and the the second AI that I want to talk about today um is molecular diagnosis politically or muscle "
    },
    {
        "start": 853.079,
        "text": "um so emotionally cliomas are diagnosis with histologic features right so um these images will be examined by a pathologists and and they will classify the cliona grading and the subtypes based on histologic features and this can sometimes be subjective and could be varied between Physicians especially depends on the pathologist experience so in 2016 uh thanks to uh tcga right there is a study that showed that actually um these molecular mutations these molecular subgroups correspond to a patient overall survival and treatment response so much better than what the histologic feature enables us um therefore it is so that internally 21 uh the World Health Organization changed the way that we classify gliomas right "
    },
    {
        "start": 914.699,
        "text": "from based on histologic features um to only based on these new patients um but the problem here is that molecular diagnosis will require a immunohistochemistry or next Generation sequencing and these methods are very time consuming and also very very expensive right so there's a need um to develop develop an AI to help us to determine the molecular medication for the gliomas so the clinical relevance for having an AI method here is that it can again help us guide the tumor resection right it can help us extend access of molecular testing without requiring these expensive tests and in in Laboratories and it can also help us inform uh clinical trial feasibility and enrollment for early post-op Therapies right so we call our algorithm d cleoma "
    },
    {
        "start": 977.399,
        "text": "so how do we train right so there are three steps that we need to do we need to First learn the images um we need to learn the genomic landscape and I'll get into what that means in just a second and we and and then we'll take other components and we'll put them together to classify images using the genomic landscape that we're learning so the first step is to learn the images right so this figure looks very familiar right we'll use the same idea that we used earlier where similar images should have similar representation representations right so we'll have a patch we'll take the patches and we'll do either augmentations or we'll sample from um patches that are from the same terminal same terminal class and from here we'll have the representations for the images and second is to learn the genomic landscape this is so when we when we do know about a lot of these mutations is that some of them tend to occur together and some of them "
    },
    {
        "start": 1039.919,
        "text": "maybe don't occur together and we actually have a lot of data about on this genomic data that's actually publicly available from tcga cgga and other sources um so what we can do here is we can use a we can train a machine learning model to um as a genetic encoder to basically learn these core occurrences and we can leverage these co-appearances to help us predict on the mutations together Downstream right so once we learn these two components we can put everything together so what we had is the image encoder here and we also have our learn genomic encoder and we will put all the components together and we can use a Transformer architecture to help us predict the mutation of each subgroup and once we have the classification for each subgroup we can then combine all of them together to have an overall molecular diagnosis "
    },
    {
        "start": 1102.98,
        "text": "we tested um biblioma International multi-center infrastructive testing cohort including institutions here in the United States and also in Europe and we can see that overall our model achieves excellent results in the three molecular subgroups that we are focused on right so these Spirit subgroups are the three molecular mutations that the who have defined to help us classify the poliomas right um we use many metrics to evaluate our machine learning model and we can see that we achieve excellent results in over 90 for pretty much all the metrics except for um but um and from from the molecular subgroups we can then combine them together to get the final diagnosis of the molecular classification right and we can see that we still achieve very good results once we combine these uh once we combine "
    },
    {
        "start": 1165.08,
        "text": "these molecular subgroups together and especially in patients that is in the 55 years old [Music] you take a genomic sample and then you're predicting with the histology and you're also the predicting the genomic substitute no and the testing this time we only have the in the clinical setting we only have the images okay so when we have that arrange training during the training we have the the labels yeah a training we have the molecular labels but in a clinic during inference time we only have three images so how does that compare to the existing methods uh to get the molecular diagnosis right um so one of the challenge uh for the current current diagnosis that the testing needs to be go through immunohistochemistry or IHC um I see tests for a very specific "
    },
    {
        "start": 1225.14,
        "text": "mutation right it doesn't sorry there's a question online I just want to make sure in case it is specifically wrong but uh because how does it do in terms of FDR back you are supposed to Discovery results um I I don't I don't have the numbers I'm sorry all right if you or contact s you want to unmute yourself and ask that would be great yeah hi so technically like when you um if you can go back and go on to your last slide uh again the last one um the before yeah so so now here you are only considering the true positive rate now like and in along with your kind of plotting the false positive traits in a way you are looking into like AUC or similar kind of stuff so like do you know that if your method kind of "
    },
    {
        "start": 1285.5,
        "text": "controls false discoveries or is the power Just because like it finds like way too many [Music] um like way too many options to just get considered um I I don't know if you if you send an email I can follow up with you okay great thank you I'm not familiar with the the metric that you're talking about okay so so yeah so let's compare um How Deeply almost performs compared to um the current weight that we're diagnosing biblioma patients um and that's your immunohistochemistry the problem with immunohistochemistry is that it only um tests for a specific mutation and it doesn't really account for these non-canonical mutations or sometimes idh2 mutations um so um this is especially important "
    },
    {
        "start": 1347.24,
        "text": "for patients with low grade gliomas um which is very common in patients uh with with a younger than 55 years old and that's why it's often recommended to in addition to immunohistochemistry to also do uh uh sequencing which is more expensive and takes more time um and we can see that so so we we can see that I think we all outperforms the standard immunohistochemistry uh testing and especially in patients that's younger than 55 years old um diclioma has the potential um to improve the way that we do molecular testing and in just a fraction of the time so next up I can show you some heat maps of how igneoma performs right so like I mentioned earlier we kind of patched our whole side image into smaller patches and uh we will have a patch level prediction and then these predictions are aggregated together to have an overall diagnosis and also this heat map "
    },
    {
        "start": 1408.799,
        "text": "that we don't see and we can and and it's very interesting to see how in these positive regions well we can see on histological features that correspond to ADH maintenance such as uh anaphasia in other features right and we also see that in this region that the model predicted to be less likely um as the idh mutant uh we can see that it has um these microvascular proliferation which tend to be correlated to uh with idh wild type uh type of slides and we can also see that our model performs really well with these some of these non-conomical ideation mutations where immunohistochemistry predicted um that this slide is ideation idh wild type but it's actually an age two mutants right so you know histochemistry does not test for that so the diploma is "
    },
    {
        "start": 1470.659,
        "text": "able to um was able to predict this to be a ideation return as well we have many other examples uh from deeply over on our website so um if you're interested I recommend you to check it out all right so I have talked about SRH and how we have developed AI methods to help um to help positions um to diagnosis to to diagnose with SRH so next up I want to talk about the actual open SRH data set so like I mentioned earlier open SRH is the first and only publicly available SRH data set that's out there and we've selected seven classes um that are the most common right so that includes 5 Min and lower glios that we use in the nucleioma are study and we also included four other tumors that are skull based tumors that are very diverse "
    },
    {
        "start": 1532.279,
        "text": "and we also include a normal brain idea and so here are some examples and again these patches are very diverse uh we can see meningiomas in the pituitary anomalas are very cellular and we can see some other treatment type are less cellular and we can see that strong armor has their elongated cells on which is the histologic features that tend to be associated with strong normal and also metastases is very diverse on their own so um we divided um so we randomly selected from our training set um about 60 uh patients per majority tumor class and the minority process are strong normal and all so we get a lot less of them so we selected about 20 patients for each or each of these classes right so we randomly selected these patients from our data set um and we divided them into training and testing cells and here's kind of like the breakdown "
    },
    {
        "start": 1593.779,
        "text": "again it's it's overall uh pretty balanced on the on the patient level I mean except for the terminality process um in uh each um each slide is then a patched into patches and we can show a distribution we can visualize how the number of patches and slides varies across the patients right and we can see that on most patients tend to have five slides and that just that is just how many slides that we're Imaging during the surgery and the differences between these two distribution uh uh can be caused by the specimen size um the the size of the non-diagnostic regions and also the certain preference so some surgeon will stumble differently than the other right we can also visualize the age distribution for the patients that were released in open SRH contains patients ranging from newborns to uh 87 years old "
    },
    {
        "start": 1654.2,
        "text": "and we can see that different classes um are all what pretty well distributed among the age groups um so we can also see that some tumors uh tend to have different age distributions right for example the low brick gliomas tend to have a lot more in younger patients and that is um that's how we expect them in hybrid gliomas in the metastases tend to happen more in older patients right the Aged also part of the data uh it's just not released as part of data yeah um the absence email uh we can we can talk about collaborating and releasing the ages as well so next up I wanted to show you some Logistics that's associated with releasing the data right now everything is just machine learning a lot of this is actually um in the data readings so um that's one more question yeah is that all data is labeled as patient editable "
    },
    {
        "start": 1715.46,
        "text": "yes patch level yeah so all the all the data is labeled at the patient level and I'll get into more differs right so I want to talk about the website that we have created the data use agreement and how we're hosting the data and how the data is organized and also the source code that we have released with that of the website that I have created so this is hosted on GitHub uh using a jackal template so uh it basically contains the information about the the paper and some have some abstract on the front page and what's important about this website is that it has some samples of the images that we can visualize and you can zoom in to some of you can zoom in or zoom out on some of these images and see the shower to the cellular details for yourself right so "
    },
    {
        "start": 1776.419,
        "text": "we included an example for each class then we have so we can see these very beautiful images um that are that you can zoom in to see all the details another important thing for our website is to have a data use agreement right so uh we have a agreement to get the data so basically anyone online can fill out this agreement right it's we basically don't provide any guarantees about the data it's important on commercial used so once you fill out this agreement you will see a screen that looks like this right which will take you to nothing so we went with Google Google forms so it's pretty simple to implement and it's also really easy to change if you need to um so as you can see we have our data hosted on Dropbox uh Google Drive uh and AWS S3 right so AWS were probably the "
    },
    {
        "start": 1837.02,
        "text": "easiest way for someone to download this data in a very Speedy fashion um but Google Drive and Dropbox has their own convenience and we don't have the patient archive from Dropbox yet but we will be putting a copy there really soon okay so open SRH is sort of organized uh in a way that we contain the metadata file and we also have um basically the raw data for for every patient in their own directories right so we will we are releasing both the raw of the icon files so that's the the raw strips that we can get from the imager and also the patches that we have generated uh to to training machine learning models yes that's right so um the the patches will basically just um divide the dicom file into smaller regions so we can actually fit that on the GPU this is "
    },
    {
        "start": 1897.26,
        "text": "why yes yeah right so in the metadata files we released um both the patient information we also released a training validation split right so in the page and information we released the final diagnosis right so high grade logarithms hpg here is a hyper glioma and we also released uh the results of the segmentation model that we used um for prediction so these are uh so basically these are patches that we determined to be tumor normal or non-diagnostic so we release the prediction and these predictions are the same ones that we used to generate the benchmarks that we have with this paper um so and we also have a Json file to to signify the twin validation split and this is here um so that it's flexible enough so you can easily change the display if you want to right so this is a release "
    },
    {
        "start": 1958.46,
        "text": "that's part of the data set but can be easily changed we also have a companion of source code and that's released on GitHub and our source code is implemented using High torch and high touch lightning so high pressure lighting is a wrapper over High touch that easily allowed us to do multi-gpu training and parallelization so going back to our demo I can show you the repository that we have so on GitHub um we have um our basic information and our package is called open SRH um there are there are instructions on how to install these on how to install these packages and also how to run the training and it's it's a pretty simple trick it's a pretty simple command you just need to modify the config files and also just just run the trim uh we we provided all the components "
    },
    {
        "start": 2021.22,
        "text": "that you need to train a deep learning model right including custom data sets uh that we need to use with our with the or custom data sets that you need to use on with the data set right so the reason the Json files and reading the images for you so um that's not something that you need to worry about um this is important because SRH isn't organized like image now like everything else right because um with SRH image there is this inherent hierarchy of patients slides and then patches right so we can just put everything in one folder like how a lot of these data set is structured so that's why we included our own data set narrative learning model acquired constitute between the incidentals data sets do you have an estimated training time or like a single model yes so uh the the length of our benchmarks is All released as the papering objects and just a second "
    },
    {
        "start": 2082.06,
        "text": "and we also have the training script again like I mentioned earlier we use high push lightning which allow us to remove a lot of the training Loops but we only need to identify the inner workings of the functions so it's very convenient and if you're interested I can walk through the code with you more details um later all right so next up I want to talk about the benchmarks that we included with our data set um we included histologic classification um and we also uh Benchmark contrasted representation learning on a data set so um first let's talk about the histological classification so we have our image so this you this is basically the Baseline for for all of the classification tasks we have a image and go through some kind of neural network and then at the end of your network the model will predict a distribution for different type of tumors "
    },
    {
        "start": 2142.9,
        "text": "um this classification is learned with past entropy with which maximize the similarity between the model prediction and the brown shoes in a basically a one-part Napoleon as a histogram uh we benchmarked um the classification with different architectures um such as uh residual networks right so residual network was invented in 2016 um so as we've seen once the convolutional layer a convolutional gets deeper and deeper as we add more layers um these models tend to be harder to train right so what resonant did is that it added these skip connections um to help the gradients to easily or flow through our Network um so that it's easier for us to train these very deep Networks um we also included Vision Transformers so Vision Transformer is a newer architecture um that models the image as a sequence so um the images are being patched into small even smaller patches and then all "
    },
    {
        "start": 2203.619,
        "text": "of these patches are modeled as a sequence and these sequence are learned using a Transformer encoder and it can predict um the final labels with the transforming encoder the advantages of a VIP encoder is that it has less inductive bias and it allows us to learn these images more generally it doesn't confine us to the convolution but the problem with that it requires a lot more data to overcome this low inductive bias um it has been shown that with enough data enough compute vit can outperform all the convolutional networks so here is the the benchmarks that we have uh here are the benchmarks that we have um and we can see the overall resonant performance a lot better than vit um and and that's the potential reason is that vit often requires a very very large scale data sets um to to overcome the slowing Dr bias I was "
    },
    {
        "start": 2265.78,
        "text": "talking about so efficient training of the vits is one of the open question in the field for both computer vision and also Medical Imaging and we can again visualize the confusion matrices where again we see a lot of errors are being made in the metastases passing again it's because it's very diverse and that Benchmark did you use augmentation when you did the main contrast important yes so um for for cross-centric people we only needed to use weak augmentation so only flipping um only only flipping is really needed to achieve a really good result achieve a reasonable result in the uh in this training and we actually do release like the details of how other augmentations implemented uh in the paper so I'll filter that in just a second yeah oh and in addition to uh in addition to classification benchmarks "
    },
    {
        "start": 2328.06,
        "text": "we also performed benchmarking on these contrastive representation methods so I'll actually like to go a little more in depth into what contrastive learning is doing right so your sort of two variants of contrastive learning that we are that we benchmarked we have the self-supervised contraceptive learning and also supervised so like I mentioned earlier in self-supervised contrastive learning we don't actually have the labels for each image that we have right so for example we know that these are or pictures of thoughts but since we don't know that they're dogs right um the model will actually uh try to pull these two images far away only representation space um so contrasted learning is important and contrastive loss and so we can see that in the numerator so the numerator here is what we're trying to minimize well we can see that we use the dot product similarities the dot product "
    },
    {
        "start": 2388.119,
        "text": "similarity score um to to compute the how similar how similar or positive pairs are right so in this case the positive pairs are basically two augmentations of the same image right since we don't know the labels and in the denominator is what we're trying to maximize uh we have uh we have a DOT product similar similarity score of any pair right and we can see that this similarity score is being summed over um all the samples in our mini batch right so that includes everything else right so um but also uh notice that there's the other developments here since we don't really have this information during a self-supervised study and we can see that this loss is some over um all of the anchors right all of the images of temperature having perimeter to help us to train them all together anger is the Ice image right yes yeah so it will be something over every anchor in your mini branch "
    },
    {
        "start": 2449.64,
        "text": "temperature is a hyper parameter um it basically changes the behavior of of the loss function there's a um depends on different temperature um the the loss function will behave differently um in the way that the Clusters are being formed so you can that's a parameter that we can turn over um to to basically adjust it based on the data sets that we have right so in order to train uh self-supervised contrastive loss uh we need these very strong augmentations right so here is a list of all the strong augmentations that we that we apply so we just apply every augmentation in in sequence with a 30 chance right so here are some of the resulting images and we can see that some of them don't really look like SRH images and that is okay because um a lot of these very strong augmentations uh tend to help the model to learn better "
    },
    {
        "start": 2510.46,
        "text": "um for for Learning Without available um so but we also use a supervised contrast deployment right so in supervised compressive learning uh we have we actually had the labels for all the all of our samples right so we know that these are both images so when we are Computing our positive pairs right we can we can know that all of these images should be similar right so the last function actually looks quite similar we have identified these terms that we already talked about earlier right we heard that they're probably similarity score um of a positive pair um but this time oh sorry there's something to talk about denominator first right the denominator is the same as before right um except um in this case we're only uh summing over samples in different classes so we don't see the underground image here that's not being compared and "
    },
    {
        "start": 2570.64,
        "text": "in the numerator we're summing over all the positive examples right and these positive examples are acquired based on the labels of of the images right so we know that these are all images of dogs and they should be similar right and and like before we are sending over all the anchors in the mini batch and we still have our temperature parameters um here as a hyper parameter so in here is the contrasting learning metrics that we have and we can see that uh overall supervised method you still perform the best um because label help us learn a lot better um but we can see that uh with simclr right so this model is trained without any uh supervision from the without any actual labels right we're just looking at the images and using documentations and we can see that we're still achieving really good results another thing that we can do with these contrastive learnings is we can visualize these learn representations "
    },
    {
        "start": 2630.7,
        "text": "and we can see that we have a much better discernible uh clustering in in the suit part and the same CR when compared to randomly initialized with sorry these are not right now these images are not trained initialized model so basically these are representations learned are natural images we can see that there's a very big difference um when we are actually trained using contrastive learning on our data with or without the need for the treatment classes um we can see some mixing in on our images so everything inside here so a lot of these on mixing happens in uh images that have pretty similar histologic features such as between hydrogen and logarith gliomas and sometimes hydrogen passes um so as part of our Benchmark release detailed training parameters for all the experiments that we have run right so "
    },
    {
        "start": 2691.599,
        "text": "we also included uh the gpus that we used and also how much time it took for us to train these models um you know we can see most of these models can be trained probably within a day so the training of this data set that tends to be quite feasible for a lot of us all right so in conclusion uh there is a there's still a lot of open questions um that we have with our data set right so uh here are some of them right domain adaptation so a lot of existing tasks in domain adaptation um it tends to be very artificial so uh some of these data set is comparing uh the MS the hearing digits to the street being house members right so a lot of these tasks tend to be very uh artificial and another example is taking a picture with your phone and taking a picture with a DSLR right so there isn't "
    },
    {
        "start": 2751.599,
        "text": "much difference here and the tasks are all very artificial but we can actually trying to adapt um SRH to the conventional hne histology and adapting these different methods is a very large scale Benchmark that's actually uh based on the image properties right from Optical Imaging or from staining um there is uh multiple instance learning where we want to have a good strategy to learn from these weak labels is that we included with our data set right so I mentioned earlier that we are releasing a label for every patient instead of enabled for every patch because um it's infeasible for someone to basically enter every patch right so we have this weak label where all of the data from the same patient right so how do we better leverage this um label to use uh to to train our models using multiple instance learning it's another opening question "
    },
    {
        "start": 2813.76,
        "text": "um in all of our benchmarks we had a very simple aggregation method that was simply average all of our predictions right so a better aggregation method to get us a sign and patient level prediction um is also a open question um earlier we looked at some of the augmented images and in these augmentations a lot of image doesn't really make sense right so how to do self-supervised learning um without these augmentations or with more domain specific augmentations there's also an open question and finally uh we have seen that vit tend to have a worse performance on our data set and that's because of these models requires a massive amount of data to train so how to train these models efficiently right with uh without a lot of data is also another one question that um does another future direction of research so in conclusion we can see that SRH is "
    },
    {
        "start": 2874.599,
        "text": "a rapid label-free Optical Imaging method that provides this very high resolution images of unprocessed biomedical specimens um the combination of SRH and modern artificial intelligence can enable us to have rapid and accurate brain tumor diagnosis open SRH is the first and only publicly available data set of SRH and we hope that open SRH will help promote open science translational AI research within the field of physician oncology to optimize the surgical uh management of brain cancers right so more details again it's on our website and I would like to thank all of our collaborators in the lab and all in our uh sponsors which generosity make this research possible so that's it any questions a reminder if you're online you can put "
    },
    {
        "start": 2935.5,
        "text": "your questions either in the chat box or a razor Electronics yeah I'm going to talk to him I have two questions three nine questions but um the first one goes to your simulator Roman histology at the beginning um I remember you talking about how I think the reason why we generate um pursue this project was because um uh wearability in the technician obtaining the sample and such um you know I'm thinking about obtained tissue even for this project is very important because we're going to try to process begins so my question for you is whether it's leaving for the surgeon obtains the sample when you have like a requirement or some kind of threshold saying this tissue sample needs to have this amount of cancerous tissue versus pelvic tissue all Living Essential for those that can't be 100 and it's like part of the tumor doesn't matter where the surgeon takes it from because I know I mean if you're working "
    },
    {
        "start": 2995.8,
        "text": "with a three-day options like if you can't deeper in a thing for tumor versus someone else works you could have potentially different being with tipping Nostalgia so what's your take on that yeah so that's really up to the surgeon so sometimes a certainly would like to sample around the tumor right so and and that that goes into the margin delineation that day I can help solve right so it's really up to the surgeon on which part of the tumor that they want to have it visualized um and in general there isn't really a lot of requirement on where the tissue comes from it can literally be anything um the the the imager works really well with tumor normal tissues and and we can also see like a necrotic tissues and other other regions right some regions are very fibrous like we can see all of that on our images it's just up to where does the surgeon want to see yeah and there are some guidelines "
    },
    {
        "start": 3057.72,
        "text": "um they're like pretty simple like don't sample very bloody things right so um there are some guidelines but it's really up to the story well then my second simple question is um not call it but what is your take on your research and adjusting rare and being able to survive those bills yeah yeah so a lot of the methods that we have right now is focus on the on the very like majority classes of trainers um but there are also strategies to identify these rare tumors um so the first thing that we can do is to basically identify the rare tumors right so um basically um during inference time if we see a trimmer that we're not trained on right can we identify it right that's that's like the first step and uh in addition to that we can use a "
    },
    {
        "start": 3117.78,
        "text": "lot of this representation learning techniques or other techniques to basically help us to learn these better representations of tumors and ideally at the end we can we can have a one shot or a few shot learning of these of these trainers right so for example in our database maybe we have one or two patients of a very rare tumor right so um the the goal that we're going toward with representation learning is that with only one or two patients we can identify them separately on the representation space um so when we see them in the wild right we can identify these red tumors as well yeah it's really really fantastic workings for this data set it seems like a really powerful for people to go out and train their own models I'm curious as people do that if you have a way of collecting back like a leaderboard or some sort of um like summary of their model performers yeah so as of now we haven't "
    },
    {
        "start": 3180.96,
        "text": "decided on how we want to um have a leaderboard of sorts um we have talked about maybe we've been potentially host a competition um at some point um especially uh with these molecular diagnosis right so we can release additional labels with our patients and we can potentially host a competition to um to predict these um additional labels or we can release another testing set right on to to host a competition but that's not something that we have right now but we think that's something that we certainly have talked about so we're just doing other contexts that really is simple it works well as at the end of your training for example with the paper Titan there's a function that can just generate like a yaml I guess like in a format that is standardized and then say please send that to us and then we can add it to the website I guess but that way you're at least getting kind of like a consistent performance and if people want to share "
    },
    {
        "start": 3241.559,
        "text": "like something that you can gather that it's not just kind of free viewing all over the place yeah so the so our base has a pretty um say thorough logging process for for the training process and also evaluation so what we can what the what our code base doing that standardized is that it saves the uh the checkpoints right it also saves the state of um pretty much everything so including the optimizer or the scheduler in the data sets and we also have a prediction pipeline that saves um all of the predictions so including uh like the Learned representations and also the the locates by the selling process so all of that is being saved um in a standardized way obviously people can identify that um but we do have some kind of a way that that's being released with the code base to save these essential uh "
    },
    {
        "start": 3303.96,
        "text": "predictions in the models so hopefully we can we can use something like that in the future to to maybe do representation sharing or mobile chain the questions okay don't say anything on them yeah so yeah so the ground truth is from I uh from I've seen also the yeah and also the sequencing so um the ground shoot that we that we do is the is the final diagnosis for each patient those [Music] yeah so "
    },
    {
        "start": 3365.16,
        "text": "um different different tumor will basically be have a different channel on during during surgery right so um so in in the future we can well the goal is to integrate this AI into into the microscope or into the or so we can inform the surgeon um like right away that um we think this is a surgery "
    }
]