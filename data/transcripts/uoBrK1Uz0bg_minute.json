[
    {
        "start": 8.809,
        "text": "I get some interaction we think so I "
    },
    {
        "start": 103.2,
        "text": "appreciate the nice introduction so yeah this funny um I guess I hope that most people if they're lucky at something at least in their careers whether they actually enjoy doing enough where they can probably do it for free which I shouldn't say with my boss in the room but but all I know is that I really do not only not really enjoy doing this optimization work and so this is probably gonna be maybe pretty far on the tech side of tools in fact today so if I get too carried away in the details forgive me or stopping or some and feel free to interrupt anytime you want with questions so let's get started so basically I'm going to talk about a what ends up being a small portion of the tool that we're developing this part of all is you were one grant and so you lasso is the name of the art package that I'll be talking about in particular I want to get across the message that implementation really does matter it's not good enough just to say I'm using this algorithm and I'm gonna plug it in and run it there's a lot that can be done if you have access to the code and "
    },
    {
        "start": 164.3,
        "text": "know what to do with it um so it gets all and then he talks about or I can just talk for this this on this one slide the whole time it I can fill an hour on anything you just need to keep what if that works okay so it's probably just putting into these and "
    },
    {
        "start": 224.55,
        "text": "I'll have to hit the spray button alright so let's try that again alright anyway so it's if you last so does anybody has used G lasso who's heard of July so as anyone in the room done anything with it or actually that's good surprising bit okay so good actually that's you don't need to know anything about it going in and suspect I'll discuss in some detail but not a ton of detail like exactly what the algorithm work is is about so D lasso is an r package use for estimating partial correlation networks we use it extensively in our project and a lot of other groups use it as well the original paper that started all this is Jerome Freedman forever hasty and Rob tips Ronnie in 2007 Park inverse covariance estimation with the graphical SOS the G last G and the G last service for graphical and it's used in a lot of applications basically there's in the title you notice inverse covariance "
    },
    {
        "start": 284.91,
        "text": "estimation the reason that it's an estimation is that you can't really invert a matrix you can't just invert the covariance matrix in cases where you have fewer samples then then you have measurements and that's often the case certainly in the tableau make sister cases it happens a lot in in different fields and in that case you need to do some numerical methods to estimate the inverse so that's this is all because I'm saying metabolomic neural modeling differential gene expression and it's used in a lot of different contexts and it can be well it's it's I I don't know I'm not sure you know how how wide the applications are but I know that people that use it swear by it our collaborator George nikolaidis is a statistician at University Florida won't let us touch anything else basically something actually took a long time to talk him into letting me do that do the optimization on this okay so this has a very basic idea of what happens it uses "
    },
    {
        "start": 345.52,
        "text": "an l1 or lasso penalty to induce sparseness in the matrix the idea behind that is that you basically have a ton of variables not all of them really matter in the in the analysis you're doing but you can't just throw things out arbitrarily so the numerical method decides excuse me decides sort of up front what isn't is not important and anything that's you know is not important gets forced to zero and that buys you degrees of freedom by Z power in your statistical analysis and allows you to deal with the original not having enough observations not have enough samples compared to the number of features that you have so this is really kind of groundbreaking work at the time it came out in 2007 2008 from the from the abstract it's a simple algorithm replica lasso that was at the time 30 to 4,000 times faster than competing methods that you can imagine that that was kind of a big deal basically things were I can't "
    },
    {
        "start": 407.169,
        "text": "imagine what people were able to do for this kind of method before this came out but interestingly this is an example of people being hard on themselves I think in a way but one of the co-authors Trevor hasty wrote just 5 years later that that you have basically n samples feet measurements so in other words of P no network that G lasso basically is almost impractical computationally when P is over a thousand only 2,000 rather a metabolomic fortunately we don't run into that very often because typically today I'll talk about today as 500 features in it so it's starting to get to the point where it's takes a long time but it's not out of the realm of what's reasonable so some people try to solve problems that have maybe a million nodes in them and this can be done as I'll show in the next slide there ways in certain situations that's still feasible but certainly in for the data set I'll talk about today but you couldn't do that a "
    },
    {
        "start": 468.33,
        "text": "lot of people because of these obstacles have tried coming up with approximations their number of those that almost calculate me verily the inverse covariance matrix but they don't do a really great job so that you know it's a trade-off you want to do an approximation you get what you pay for you probably don't get nearly you know as good and reliable and answer as you would otherwise and the other thing is that PS has been studied a lot and it's been its properties are well known well understood the convergence properties and so on so you can be pretty comfortable when you're plugging it in that you're getting something that will handle a wide variety of data sets as I mentioned for instance if you go into our studio and and type in G lasso and see what else people have tried to work on there's something called G lasso fast which sounds great and it advertises itself it's an improved G lasso and I'm not going to slam it because I'm not supposed to be doing that but um it's it's really not the same thing it ends "
    },
    {
        "start": 530.589,
        "text": "up being an algorithm that basically does the same thing but then in my local properties are totally different and I think it's actually I don't really think they should call it the same the same thing so that's an example of something where you could think you're getting a nice improvement but you're not really getting the same thing talk about one of the improvements that has been made by some of the original co-authors and some of their collaborators is the idea of using block diagonal matrix it's sort of a standard concept if you have a matrix a large matrix that you know we're in this case it's a P by P matrix in the algorithm depends strongly on P well you don't want to have to use the whole matrix if you can use if you can break it up into pieces and use the small pieces along the diagonal it's not quite that simple we're starting with a covariance matrix where there aren't actually those off the diagonals this is a representation where the colored parts are nonzero and the author agonal are "
    },
    {
        "start": 592.92,
        "text": "zeros well you don't really have that to start with but so the people came up the first author's original authors came up with a pretty quick test where you can determine whether this is going to happen basically whether the solution will whether the solution will will break into block diagonal form and if it does then you can reduce in this case obviously a baby example but instead of using a seven by seven matrix you can solve four separate problems none of which is bigger than three by three and so if you scale that you can imagine 500 by 500 if you maybe you can break you down to nothing bigger than a bunch of ten by tens then you save an enormous amount of time so this is how people handle very big networks but comes with the conditions so again just explaining how these work first paper was written Friedman and Simon Friedman was one of the original authors of the and then was under and hasty hasty was one of the original authors and I'm not sure what happened "
    },
    {
        "start": 653.43,
        "text": "here why they ended up in two different groups writing two different independent papers on the same thing five years after the original I won't ask I don't know anyway the the idea they both came up with the same idea of breaking up a matrix into its connecting components and thereby splitting it into independent problems smaller problems and for sufficiently sparse data sets and that's key in other words where the covariance structure is fairly loose where things are not very strongly correlated and you can more or less handle large networks at this point if you can do that and in fact the optimization that they did is now part of the official July so R package so if you're using that and you have a sparse data set you'll be in pretty good shape unfortunately we don't though not all data sets are cooperative that's one of those things so when you don't have a sufficiently sparse dataset you can't "
    },
    {
        "start": 713.61,
        "text": "break things up into connecting components one of the data sets that in all his lab that is is actually part of the paper that George McFly is the student published the the datasets really dense the correlation structure is dense and initially it doesn't break up in at all it's just big five hundred ten by five entertaining matrix that you can't break up you have to try to solve the whole thing at once and then even as the as you start inducing some sparsity it just barely breaks up and there's a big Dunkin then a couple of those tiny ones but you still end up with fundamentally the one big problem to solve so you can't use the the optimizations that the black bag so I'll talk about that application in particular for the rest of the talk because that's really where it matters too you can't do much about the algorithm at that point you can really do is change the implementation who's working on this I mentioned George McFly tiss who was again in the stats "
    },
    {
        "start": 773.879,
        "text": "department now is at University of Florida and his huge ingmar was the first author on the on the paper that introduced our method and then duck Brandt um probably a lot of you know Chuck but he's currently the director of the I guess at alman senator Feldman Institute I'm not sure which is Paul but I was also the faculty director of the metabolomic score and he's been a longtime collaborator in the Fallas and as tremendous ideas we've been really he energizes meetings it's great when he shows up everybody knows Allah better than the department Gayatri is one of those beats D students has been doing a lot of really great work in I mean Alice aside mostly although I get the sense the methods are coming too and then Dan and I owe the the primary software developers on the project and Marcy you know and when she's not doing tools in tech she's also writing fantastic "
    },
    {
        "start": 835.259,
        "text": "documentation and tutorials for schools she's done a lot of admin past with Netscape and some correlation calculated which was an earlier partial correlation that did but it's it's great to have Marcy around because we don't get people calling us all the time saying how do I do this because she's already very handle it okay so just a really quick overview of what our project is and why where'd you last so it fits into it method we the algorithm we call DNA which is differential network emission and analysis the basic idea is that you get a new tablet omics data set which is the input data up here which you probably can't see but it's a bunch of metabolites in this case typical days said is 500 or so and then I'm samples where plasma or something is measured for for all those metabolites and in this case disease conditions as well currently we're working with two disease conditions but we've got plans for being "
    },
    {
        "start": 896.1,
        "text": "able to have them work elaborate designs as well the idea is that I'm basically first you have to make me the partial correlation networks for the data and that's actually where the G lasso part comes in once you've done that then you can you can perform grace clustering algorithms to determine which parts of the overall network are significant between the disease group and the control group and then where their methods for this net gsa method which is a base initially on a gene enrichment method I guess is GSE a is that original but basically there we can you know assign p-values and adjusted p-values to the clusters to say how significant they are and it just rather than univariate analysis where you just get a list of metabolites that might be important this really by being able to do a network and show the the correlation structures between the metabolites that important you really can get a better biological story and a deeper understanding of "
    },
    {
        "start": 958.589,
        "text": "what's actually going on we think it's pretty promising but it is computationally intensive which is where I get to have fun so the data set that this is um people it's referenced on this is big mouth as I mentioned George's don't years ago but first author on the paper that pointed out used we used our early method the early versions of the method on on a couple of data sets and came up with some nice results in one of the data sets that was uses from the Crick dataset you don't actually remember what the thank you chronic renal insufficiency cohort and disease with the other ok great thanks yes okay so in "
    },
    {
        "start": 1030.139,
        "text": "case that didn't happen for people who were listening at home or whatever that so what I was explaining is that the essentially the two groups are disease and a group that had renal disease but was not at that stage yet and the idea was to build is to build a correlation network that ideally shows some some metabolites and some differences between the two groups and allows us to come up with a good biological story and that was it and it did in this case on there definitely this was a good story be told which I won't go into actually the young I should mention that this work is probably going to be the subject of a later tools and techniques that that the idea at this point maybe next semester so for people who are interested in in the ongoing work I'm trying to talk Gayatri to be one that does it he's not but maybe all we'll be able to do it anyway somebody from my lab nah not me somebody else from our lab will be "
    },
    {
        "start": 1090.23,
        "text": "will be giving that one so in case people want to follow up more I'm just going to go don't go into any more detail and what we did what we're doing now so basically running the algorithm I wanted to go back to the structure the data said there 200 samples hold together I think it 121 and one group in 1979 and the other group something like that and 510 metabolites that were seen as as well that were measured and identified and the correlation structure is fairly dense but I think that's probably likely because we have quite a few lipids in the industry and they did gainsaid and it's generally lipids tend to be highly correlated with each other anyway amino acids also for instance tend to be highly correlated with each other so there's there situations like this where you just inherently have a dense correlation structure where things are related already and you find peas out what's well what's what's relevant to disease state versus non disease State given the underlying structure so "
    },
    {
        "start": 1152.479,
        "text": "because the structures dance we can't do that nice box agonize a ssin trick where we break everything up into little pieces so we're basically stuck with the original algorithm and it can take for one run of the data a couple out of several hours for sure maybe longer and about 95% of the running time is actually spent directly in G lasso so clearly that's the place to look if you want to speed it up and I want to mention that in addition to taking several hours um basically it's possible to parallel lies a little bit in the sense of sending things off to multiple cores but we're trying to build a tool that will be used by the community in general so it's not a matter of you know us sitting in the lab wondering about you know how many hours is gonna think it's a matter of potentially buying more hardware or renting cloud computing time or something like that in case we get enough users where they're really putting computation of pressure on so any times time saving we can get from "
    },
    {
        "start": 1212.899,
        "text": "implementation or will save this later in terms of resources okay so what what is the structure here so not much low-hanging fruit basically I've been doing this forever and as I've been mentioning I actually enjoy it usually part of the reason I enjoy because there's stuff up there that's just really simple and I can go in you know a couple of days and have ridiculous meetups because often and you know often people would write these things or not computer scientists or not programmers they're really good with the methods but they implementation and in that's fine it's that's why we do collaborative work but in this case these people were really good they really know what they're doing so most of the tricks that I need that I know at least were already sort of thought of and handled really nicely I'm in particular as they say it's our wrapper around Fortran well though I "
    },
    {
        "start": 1274.01,
        "text": "don't know how many people are aware the extent of this but R is really slow so Fortran C and C++ even even Java are all probably fifty to hundred times faster than R on a typical application so if you have the availability or the knowledge to write in a underlying language and Ohana's been doing this for doing the methods you've been working on now and would probably tell you that it's made a big difference so it's one thing you can immediately get a huge speed up just by changing the language you're writing another thing that happens a lot that it's possible to parallel ice that by that I just mean split the algorithm up so you can send off to multiple machines you shared calculations get the results back and put it all together at the end well here you can't really do that because early calculations feed into the later calculations either sort of walking down a matrix and at each step you're doing a calculation which then you need in order to do the next step so it doesn't really "
    },
    {
        "start": 1334.34,
        "text": "do any good to parallel wise because you're gonna be sitting there waiting for the first calculation and you can't really go on until you have it there also yeah okay that works I might not use math works in MATLAB is that related at all there seem certain ly some other languages of that sort that are more in here or matrix languages that are also very fast so those those are definitely good choices too works I believe the ones that make MATLAB yeah okay so okay okay so I saw "
    },
    {
        "start": 1394.35,
        "text": "I'll just repeat that in case that wasn't clear to everyone so I believe the comment was that MATLAB is actually implemented in Fortran underneath as well and so so you'd expect it to get similar so yeah that's a good sure yeah and and a lot of the matrix algorithm packages also do special optimizations based on matrix structures and things that that you know basically a lot of research has been done on how to do fast matrix multiplication so on any number of things fast matrix inversion I mean the whole field of computation based on yes so definitely picking out the tool that you're using is critical are is great I don't I'm not trying to you know disparage our I use it a lot myself and it's it's great it's a really nice way to develop things in a hurry and to experiment and work interactively as he develops absolutely not saying you shouldn't use or should you you should use our but if something's really computationally intensive then you should consider writing this speed "
    },
    {
        "start": 1455.82,
        "text": "critical parts and something else okay so what happens how does it you know in the sort of computational terms has the algorithm work without explaining all of the details basically there are a bunch of P by P matrices where P again is the number of metabolites in this case and the graphical asto algorithm calls the underlying lasso l1 penalty method repeatedly as this is a little oversimplifying a little bit but effectively it's progressing each feature against all of the others so for people who aren't familiar with partial correlations the basic idea is that with Pearson's correlations you get indirect associations and you usually get a whole lot of things that appear to be related to each other but a lot of it is superfluous um indirect relationships for instance you can have a and B and C all of which appear to be highly correlated but maybe "
    },
    {
        "start": 1516.46,
        "text": "B is not part of that formula at all maybe a and C or the real connection that you care about and Pearson's correlations won't tell you that at all the partial correlations are I'm able to distinguish essentially direct relationships from indirect ones and the way that they do that is by effectively regressing each variable against all the others and then doing Pearson's correlations on the residuals from the regression and that essentially takes takes away all the indirect effects and gives you direct correlation values that's like one of the reasons that partial correlations are very appealing but any rate walking down up excuse me calling walking down these P by P matrices P times basically wants to reach feature results in order of P cubed performance I'll talk a little bit about computational complexity but I most people familiar with Big O notation or what that means in computational "
    },
    {
        "start": 1577.72,
        "text": "terms I can describe a little bit I will in the next in the next slide but basically it's just a way of evaluating how long an algorithm will take relative to the size of the set that you put in in this case um it essentially performs as the cube of the number of metabolites which is really slow most most algorithms that are there's nothing that they can do about it obviously a little about that it's not their fault it's the way the algorithm is but it's it is just computational algorithm so but fortunately for people like me you like to go and muck with the code there are only a relatively few places where this kind of you know where these deeply nested loops that are actually taking up the time so optimizing really involves just paying attention to some of those internal the inner loops and leaving the rest of the code alone which means not breaking it and then thereby sir being more well it's a lot easier to "
    },
    {
        "start": 1637.779,
        "text": "make sure that the results match exactly with what was there previously it's one of the really important things I mentioned before about approximate solutions or other people's efforts to bead this up one of the things that I think is critical is that then in my book if it doesn't do exactly the same computation I did before it's not worth it so in other words the results have to match um if has to be reproducible that way people can understand that it is actually doing the same thing and excuse me and basically if you if you have slightly different numerical properties you can end up where it appears to work well and one data said but another data said it could behave completely different of the deal a so fast that I'm actually for instance the numerical methods for July so might converge in four or five iterations you got so fast it was taking thirty or forty iterations and so it's just a completely different hour it just doesn't behave the same way so I always make sure to me it's not it's kind of cheating if it doesn't "
    },
    {
        "start": 1699.07,
        "text": "actually do the same things so I always make sure it is exactly the same so the idea is to get those critical areas hard and leave everything else alone and talking about computational complexity just a little bit what is order peak you mean basically in terms of definitions it means that to do one run at the section 80 cubed operations or some constant K greater than zero and that sounds all innocent but K matters a lot okay is not just some something it's not just two to three it can be ranged from you know two or three to several million several billion but it doesn't it isn't affected by the size of this and I say we can't do anything about the PQ part would anybody is that the algorithm has been studying I'm trying "
    },
    {
        "start": 1765.29,
        "text": "to come up with faster algorithms I can't say for certain that it's not possible but it seems highly unlikely that this operation in less than order PQ just because on the P squared elements P times and a lot of getting around that we can't do much about the PQ part if he gets too big it swimming is low regardless in our cases I mentioned 500 or so that wall music tends not to be too too terrible well we can do a lot about how large cages that's that's what's critical here in this case I personally consider consider it a failure if I can't get us factor five or so to speed up anything sometimes I'm able to get factors of 700 or even several thousand by implementing things better and that's about making a double hundred and seven thousand times similar that actually changing the underlying algorithm okay so getting "
    },
    {
        "start": 1827.39,
        "text": "into a couple of details I won't go into everything that I did I worked is to give you some kinds of ideas but to look for what kinds of things show up I'm putting this up on burgers a nice thing anyone to understand antenna and at the point it is important it's kind of messy or they don't need spaces after their comments right away I'll do their art or Fortran people as they have to worry about making some punch cards Brooke kind of joking but the people that wrote this we're really good fourth in programs through 20 parents 20 or 30 years of experience folks in this case they missed something this is just a function call to the underlying last so and it does the penalty it does leave you know most of the calculations I'm not talking about how long it takes to run laughs in here I'm talking about it's just how long it takes to do the calls in my past so I think it's cleaner it's just this is fun "
    },
    {
        "start": 1887.99,
        "text": "Multi privately privately takes about 30 percent of the entire algorithm just it's a well-hidden this sum of the absolute value matrix which thanks support renting flexible about these sorts of things you don't have to write a bunch of loops to do to sum over matrices sum of the absolute value of the matrix R does the two most losses do that doesn't come for three misses small some it's actually I mention this beauty matrix is P by P and I mean the absolute values of the components we have to fill through them at a time so it's worth where and it's doing this for every feature of the dataset 42 so it's one of those places where it actually makes a big difference for our current dataset "
    },
    {
        "start": 1948.07,
        "text": "this particular code gets run anywhere from 2,000 to over 15,000 times in one run up to that so so you can see that it not just this it takes a long time to some matrix that takes a long time to do it really so what can we do about this we're sort of have to understand a little bit of how this matrix is used and this is understanding a little bit more conceptually oggi lasso works I think I mentioned a little bit of it locks down matrix and page thanks the previous calculations and use those in for the next level of the calculations so V V is the matrix that is used to keep track of the progress as you go walking down that matrix and eventually when you're done it's going to turn they think it'd be on purpose cuz it's what you gather to play W but eventually becomes W which is the partial correlation matrix they calculate and I'm saying for a given block in the "
    },
    {
        "start": 2008.92,
        "text": "bottom structure so if you were able to break it up in the blocks at one of these streets of this box so I'm basically in order to you I do this operation in general when Damaso runs through all the features it operating slows down the current pain trippy and everything else in the dataset so against the idea of the fact I've been taking one feature and regressing it against all the others that way they're taking into account the effect of all the other features on the one yeah picture the probably good idea here so this is you have some in one picture of slide oversimplifying but imagine here and you have the whole people at the entrance think of the yellow part as rotating row and column M one feature basically start at the corner and then we just walk down row and column in this exception and doing that you can see "
    },
    {
        "start": 2071.74,
        "text": "that we operate on the yellow part which is the current picture and then the green part everything else that makes sense and that's that's really underlining it what ends up causing this thing to be slow where their possibility I'll go back to that a little bit but basically this VD matrix that I was talking about is only modified with the yellow parts for the current picture that each step so it's not necessary to go through the entire matrix each step in other words you don't have to go through look at I'm saying this step the next step well most of those green squares are not changing not you're not touching those guys so you don't have to recalculate those only as you keep typing do the bookkeeping carefully you already have those sitting around so what you can actually do is subtract out previous row and add back in the next the new row rather than matrix and "
    },
    {
        "start": 2135.12,
        "text": "careful with the diagonal because basically now instead of having order D cubed operation to go through the whole matrix you have an order to squared operation part that's you're only going to one row it's such a cheating is one operations instead of these squared operations so that particular part again going back to the one line from here 30% it now takes essentially zero negligible amount of time because you don't have to agree about something every time you do some careful bookkeeping and keep track that took care of my but hidden bottleneck that I think was kind of in the way of understanding what else is going on at masui this I didn't expect to see something like that but it you might people take advantage of and do some "
    },
    {
        "start": 2195.64,
        "text": "things so so now we're basically down to the heart of the ditch here fundamentally going back to the picture and you're operating on fell apart portion and then we're operating that all the green abortions there are lots of ways of thinking about operating on those greedy portions you can think about operating on them each separately for separate loops first being part you go to tip over again the second one the fourth we got sound very different but the implementation of time really are another way the way the original out really did it actually actually stripping out in the old part entirely and amazing all the green part together so they just copied a bunch of data around but just the green part thereof it will operate on the others and get that's one block which they most much more efficient than operating on four separate blocks so "
    },
    {
        "start": 2258.97,
        "text": "really clever wrong but better be really clever too instead of what they were doing they sleep it's the other basically they'd be clever just to give it four small loops at once but working really well in job actually implementing this in Java C++ or Fortran to here sometimes a little bit about that but you don't know what to try which scaly bastard it depends a lot of situations and those are somewhat interchangeable languages and I'm old then and you learn Fortran until actually had to anyway so this idea of splitting into four pieces work really well in the job would actually call the handles loops quite nicely that way but didn't do any good in Fortran almost wash pull that away "
    },
    {
        "start": 2322.21,
        "text": "try something else there's a lot of that in this kind of work which I guess it's frustrating but to me B that's the goodness okay I'm gonna try something else I'm definitely not giving this up so back to the drawing board what's a better way so yeah he brings a lighting analyst up being around a lot I think I said really explained what was happening in the original algorithm when they were copying data around the reason that they ran into trouble with that what's that on as long as lasso the in theater algorithm takes a long time to converge so you're spending a lot of time in there you can afford to do some sort of setup work at the beginning which in this case involves hopping around a bunch of major seasons of getting involved positioning for lassos algorithm so it's nice early on but then eventually they start getting worse parts into matrix lasso starts converging a lot faster generations set apart the new ball is always all the data around overwhelms at the outer "
    },
    {
        "start": 2390.22,
        "text": "limit is what taking much more time and setup it's more it gets really expensive so thought was maybe we can do something about both of those situations take advantage of their original implementation of just iterating over this space once but do it in such a way that you don't include the yellow part and they won't do that by looking at there's some more easily rearrange that or not he squared matrices but just single rows and column row and column makes me sort of careful with things you can introduce zeros in those such that when you do the calculations it's as if the row and the column and more not include actually end up contributing zeros to the process iterate over the entire thing over this entire matrix by the yellow parts when you get to it will just be adding zeros "
    },
    {
        "start": 2454.21,
        "text": "computationally just faster that is basically what Fortran Java was about a factor to speed up to 30% earlier then we got rid of so now we're really getting someone on this particular speed up what's even better for larger values of one of the original authors erasing again mentioned that not only is that's an order P cubed in general but for Hoover 10 state sets it starts or so I think we're hitting some of those areas for reducing the complexity based on the dataset sizes as well well that's nice and then a bunch of other tweets - they're just trivial things like overall it's the bottom line is how fast did this what did you get out of this how "
    },
    {
        "start": 2515.51,
        "text": "much improvement did you get interesting what I want to stress is that it doesn't change even depending on the operating system as well as so the top of the table and really one thing is that the quick dataset 500 futures the original version in Fortran I'll explain what these columns are so Linux our server that we share this runs Linux boxes and basically this staff cell is a stability selection fortunately algorithm which does multiple calls to deal a sueance with some randomization to determine which going to get stable Network basically by multiple chemists and finding out what sticks around that's what still though that's all that's the most computationally intensive part of the algorithm is also the tuning of course you may have to tune the penalty parameter as well let's see there's a "
    },
    {
        "start": 2575.73,
        "text": "portrait and you can see Linux was actually about 20 times smaller than the Mac which surprisingly was so one could take up to 10 hours doing the original signal is lecture a half the Mac tuning is quite a bit shorter now we're gonna have fun thanks but half an hour Mac but ankle named indoor train just talked about Linux those lectures released reporters so active little over six like we had passed an infection vector v and I can see that as you can see though it varies a lot Mac that's seen that same information is only three three and a half times faster and then the tuning is a similar ratio or its considerable speed up and what is interesting also "
    },
    {
        "start": 2640.2,
        "text": "the blue highlights are the fastest versions BC or did CNC version was actually faster the Fortran wait a bit on the Mac lower Thanks here this is the buildin Barisan what I did it's 50% or 30% slower DN C and Fortran so I didn't actually use the police actually DNA is organized based on other data I realized it wasn't worth the effort didn't you - moving slower first time in Java I give up cos instead of a lot slower but you can notice that C naught for back a lot better than they were trained in some that's probably implementation-specific no that's "
    },
    {
        "start": 2701.73,
        "text": "compiler specific it's just the way the operating systems happen to work with slightly different configurations so you really have to be willing to try a lot of things or experience what works well and what doesn't in different situations he's obviously all significantly better than the original so you might say out of it this over former thousands of runs from thousands of users over lucky and this actually gives that working well over the next few years it's not up to anyway so this is now a package not actually package does not package something that we're using internally our scripts so it will be contacting the initial office lately and tell them call us in any word with it here's something "
    },
    {
        "start": 2763.41,
        "text": "that's even faster so didn't know anything the original authors they miss stuff there's no respect for them this was a better thing and well they had already gotten speed up 30 to 4,000 or previous algorithm says you could forgive them if they didn't think even deeper than that then understandably happen anyway I think this obvious illustrates some things that can go into implementing things carefully answering questions about this "
    },
    {
        "start": 2830.79,
        "text": "or just in general what we do be all set boys interested to learn people have things that they frustratingly slow and then get together discuss or actually thank you thoughts about me to even talk about too much but the possibility of some projects one thing I find really frustrating is nature of it but a lot of you PhD students and you're right nice method up and it's a really great method but then you go on to your careers and it dies nothing happens with it I think that's a shame I think there are a lot of really good methods that don't eat my day after people graduating eventually you can do some things of that rating tools with nice interfaces around things or additionally feeding things up great things really efficiently to make something useful something you don't want there your pieces come back and "
    },
    {
        "start": 2891.599,
        "text": "haunt you later I don't know I quit three PhD programs anyway but that is actually going to be nice to some people's projects long past their graduation it's unfair it's that anyone who graduate and will revisit their own project for better things to move on to that questions yeah but and then also Fortran compiler that's a good question I believe I was using g77 for this so it must be g9u but it's basically new Fortran compiler so it's with a bunch of optimization flags one that you use that you like which is you thought had been great yeah so it's "
    },
    {
        "start": 2953.67,
        "text": "it's it's a good question we no longer see the wrappers so I was getting the line would say g plus plus or could you essentially they're wrapped inside so i might call it g77 but was actually being used one of the more modern versions of that I'm pretty sure that not positive appreciate it probably check that so the question is what compiler options doing is they're typically got big oh it's not standard because compilation and I'm always it compared to the time the expending everything else just takes "
    },
    {
        "start": 3020.24,
        "text": "care of a lot of under the hood things that you could do manually "
    }
]