[
    {
        "start": 2.32,
        "text": "thank you Mercy for the introduction and I'm glad that we are back for the first chos and tag of this semester and as Mary mentioned I'm a recent graduate from F lab and if you went to my the defense you might find some of the things pretty familiar but um we're just talking about how to and Tech is like more like you can TR it throughout and you can talk about developing tools that are still actively being worked on so I would like to hear all of your questions throughout the presentation so um my talk today is going to be on a flexible modeling framework that use a zero inflated negative binomial mixture model so before we goes into the Toth part of it I want to introduce this whole thing with the data that led us to this tool so we have a collaborator in the department of biological chemistry that work on chis "
    },
    {
        "start": 63.68,
        "text": "cast in neria and um chis cast is in bacteria giving it kind of like adaptive immunity it can be kind of like uh set into three different stages the first one is adaptation where the foreign DNA is first recognized excised and then integrated onto the native uh Native crisper array and become new spacers and now it is part of the immunological memory that the bacteria can use to defend from F future invading dnas and the second stage is crisper RNA biogenesis and the third stage is probably uh a lot more people are familiar with is the target interference stage where the foreign DNA is recognized and then clear so as we can see here that new spacer is really a very uh BAS basic part of this process because that's where the memory comes "
    },
    {
        "start": 123.759,
        "text": "from and where this immunity is going to work on and if we really tune up the activity in the of the crisper care system in the bacteria it is possible that it take up self DNA as new spacers because that's probably the most abundant DNA lying around is inside the bacteria itself genome so with that we are more interested in the adaptation process and we want to know how our DNA donor selected for the immunological memory what criteria makes a DNA more s susceptible to become uh to be recognized by the chrisper care system so uh what we're interested in is basically the newly integrated new spacer as in the yellow box what are the surrounded genomic context and what are the sequence uh features that comes along with it so one of the first thing we think "
    },
    {
        "start": 184.4,
        "text": "that we will check is that across the Genome of the self DNA is there a differences of where new spacer is coming from because if there's no difference is most likely the selection is random and we've looked across the genome coordinates and uh plot how many spaces are there at each genome coordinate that we found so the first thing we notice is that there is uh a range of uh different level preferences of different genomic side being uptaken as new spacers and some of it we uh some of the hotpots like the very high pics we can associate with genomic features that we already know for example on the very left there is the origin of replication and in the middle that is roughly where the Terminus is and uh the third one is where the native crisper rate is so we think that it is likely it's related to single stranded DNA availability to uh for the Chris for "
    },
    {
        "start": 245.68,
        "text": "care system to uptake a strand of DNA into as new spacers so that is the data we see but we want to know with The spse annotation and everything and we want to know does do the genomic sequence and the chromosomal features play a role in the space of acquisition preferences so that's the data part and that's what lead us eventually into dealing with this question on a more statistical point of view so the first thing we did is that we created a list of genomic features including that if a genomic site is in the genut that is local GC skill is AP content was the DNA shape predicted around it and uh what is what is a distance to um some important genomic features such as origin of rep location Etc and we calculate the experiment correlation of such features with the spacer tracks that we just saw "
    },
    {
        "start": 308.52,
        "text": "and we want to know whether or not this uh correlation is actually meaningful what we did is that we did a permutation test which Shuffle the blocks of genomic features and recalculate the experiment correlation over and over again that establish a baseline of what we expect if there's not really any correlation so we can see that in this example of H content there's a small correlation but it's robustly um away from what we expect as the Baseline similarly we do that for the rest of the fature and many of them as for example the distance to the origin is has like a modest correlation that is robust but in the case of like uh the distance to the origin replication that is intuitively similar to what we see visually in in the track where there's a Hotpot at the Terminus so these are the features we're currently working with and we cannot just do streaming correlation over and "
    },
    {
        "start": 370.44,
        "text": "over again with all the features we find so our question becomes how can we systematically identify uh possible factors that affect this chrisper spacer acquisition preference so this is where the regression model comes in uh basically what we want to is establish is that given an independent variable in our case that will be a certain genomic feature we want to establish it relationship with the outcome in our case is the space accounts so there are a lot of like benefits of using a regression uh type of approach instead of again doing correlation over and over again for individual uh features for example uh we can actually investigate the interaction between uh different terms terms different independent variables it will give us better handling of replicate data and also disc variables and uh a little bit along down the line uh we can compare different "
    },
    {
        "start": 431.319,
        "text": "models to really understand if a variable is really contributing to the model and contributing information to the space account so that is when the regression model comes in and to do regression model the first thing we want to check is that what is the distribution of the space account what is what does it look like statistically so what we did is that we take the space account and we count the number of genomic sites given a certain space account within different uh interval St on the xais so here is the bar chart showing uh the space accounts in different bins and very strikingly we notice there's an assess of zero uh of space account it is possible that coming from it is possibly coming from either a lot of genome are not taken up as new spacer at all or they're not detected there are a lot of reasons for that but given the big "
    },
    {
        "start": 492.96,
        "text": "genome this is not surprising that we end up with a lot of like sites with zero space accounts but this cludes a see that a zero inflated model may be a good place to start so we wanted to start with a zero inflated Nega the binomial model and to use a mark example so let's say you have a pile of metal beads and you have a plastic cup and you are going to scoop the beads using the cup and then count how many beats you get in each scoop so in this process you will end up getting a person distribution and now let's say those met be are magnetic and they tend to Clum with each other now you have like over dispersion coming in and the same process will give you counts that follow a negative binomial distribution but for zero inflated negative binomial distribution you think about it like you also have a nonfunctional c that has its "
    },
    {
        "start": 554.959,
        "text": "bottom cut up so every time you try to sco be you just end up getting zero counts and you repeat the same process now you have 20% chance getting the non-functional cup and 80% chance getting the functional cup well we get is a zero inflated negative binomial distribution where there is not only just a count component but also a zero component as color coded here so this is basically the model we want to start with because given the uh SS of zero and given that we are dealing with count data with a considerable level of dispersion so what we did is that we fit a zero inflated negative binomial uh model and compare the observed space accounts with the predicted counts so we see that in the predicted counts which is the purple histogram there is a dramatic rward shift uh compared to the observed CS we think it's probably coming from "
    },
    {
        "start": 617.12,
        "text": "the negative binomial component of it trying to account for the very high count we see in the data and end up being very dispersed in this process so when you look into our data and we concluded that this model is probably not sufficient to handle the complexity we see in the data if it has to use over dispersion to kind of compensate for it so how can we make our model more reasonably complex but also useful and statistically makes sense and this is where the zero inflated negative mixture model comes in so going back to our Mark example here so now besides a nonfunctional cup and a functional cup you get a third option that you can also randomly grab a bigger cup so now you repeat the same process of scoop scooping these and then counting them but now you have 20% chance getting the non-functional club "
    },
    {
        "start": 677.68,
        "text": "cup which is going to be the zero inflation as you already seen in the spoiler and uh the small cup for 50% and 30% chance of getting the bigger cup then you'll get is something follow a negative binomial mixture model there is not only a zero component and there also two negative binomial components that make up the counts it's also worth noting that uh the negative binomial component is possible to generate zero counts so not all zero counts come from the zero inflation part of it and this is what we landed on on a zero inflated negative binomial mixture model and we build this model and implement it with a variable number of um negative binomial components available and that is where the tools andex part comes in so we we got there and one thing we tested on is an aristic data of an "
    },
    {
        "start": 740.24,
        "text": "anerobic biofilm culture so what I want to see here is the some of the results we see using this model with a con considerble higher level of complexity that you can deal with so we we analyze this aristic data we quantify the gene expression with risk per Gene here we plot the histogram of this gene expression count and we end up with something that looks like this and uh by this time of the presentation I probably talk about zero way too often so you might notice that there is an acccess of zero at the very far left of this graph so it wouldn't it be nice is if the zero inflation got taken care of and the distribution roughly follows a negative binomial distribution but again given the inflated zero if we use existing tools to fit either a negative binomial distribution or a zero inflated negative binomial distribution which has only one "
    },
    {
        "start": 800.48,
        "text": "cam component will get the best line for the negative binomial distribution and the solid line for the zero ination so one thing we notice is that if you look at where the pick is in this fit it is level shifted and is a little bit lower than what we observe we think it's likely that it's trying to account for the zeros by Shifting the negative binomial distribution and end up with this fit so this is not ideal and after we now have an implemented zero inflated negative binomial mixture model we fit this with to the data with from one to four different number of negative binomial components in the model so here is the fit that we see and um in the Orange Line we with one negative binomial component it still look better than the existing tools that's because in our tool we Implement a noise term that can handle outliers a "
    },
    {
        "start": 862.04,
        "text": "little bit better than what is currently available so that's also why our zero inflated one negative binomial component fit looks different than the existing tool and the noise uh handling component so far has been proven pretty useful in our um testings so what we see here is that if we look at the P if fits a lot better than the other ones um but also more importantly around where the zero is um our model is more successful in accounting and predicting the inflation in the zero counts so that is the distribution part of it so we have a statistical model we have some data with count we can fit this distribution uh of the Gen expression data pretty well with our model so now let's introduce some independent variables and see how regression works "
    },
    {
        "start": 923.32,
        "text": "so we created some gen level features for this data and try to do regressions on this set of data to see if we can establish a relationship between the independent variable that are the gene features and the uh gene expression counts so in this process there are two sets of parameters we can fit to and there that means that for an independent variable there are two ways it can affect a specific count one is through the means of negative binomial components so if we go back to the mark example we have that would be the size of the functional cups basically for the C components what is the mean of the uh negative binomial distribution force and in this Mark example for example on the right you see the distribution the blue has a mean of five and the uh green one has a 15 you "
    },
    {
        "start": 984.44,
        "text": "can see they distribute uh quite differently and if you fit an independent variable to the mean of these components it can affect the eventual outcome through the means so that is one way that an independent variable can play a role so what we did is that given the uh Gene level features we created just for examples uh we have the length of the gene the at content was the Gen distance to the origin of replication and also so whether or not a gene is annotated with anerobic respiration functions and we look at the values of coefficients that come with this regression model which means that uh if we have a positive coefficient the increase in the independent variable in our case Gene feature um would have like a positive effect on the eventual mean of the camp and I see that Marcy has a "
    },
    {
        "start": 1046.36,
        "text": "question yes there's a question online he said what each component in the model represents in the data acquisition process so that is actually a really good question so uh one of the reason we end up uh first finding this RNA s of biofilm and aerobic culture data is that this kind of complexity works really well with either clusters of genes that cor regulated but in our case we are looking at possibly subpopulation of cells that has different uh gen expression patterns and that is a possibility what different components can mean and it varies from data to data and in our case for space acquisition where we eventually want to land it on uh our other studies shows that there are more than one possible kind of like mechanism space that can be update updated if we see there is the complexity more than just one C component it is possible that those "
    },
    {
        "start": 1108.96,
        "text": "counts are following kind of a different mechanism with a different path that end up being uptake as a spacer that follow a different set of molecular rules so I wonder if that answer the questions online we will assume it does if that doesn't something AIT yeah um I was wondering why did you um test zero inflated Bon models as well or what was was the reasoning behind choosing negative binomial so at the beginning of this project where we started the development of the actual tools we try Plus on negative binomial uh distribution both of them and zero inflated percent and zero inflated negative binomial distribution and we go with the statistic root of like checking the residue checking the goodness of it and it has been pretty consistent between s and negative binomial distribution "
    },
    {
        "start": 1170.799,
        "text": "negative binomial works better I think it's coming from the dis uh the disperse nature of our accounts because we have a lot of small accounts but we can also end up accounts that on the scale of 5,000 for example so um with that it is probably why a negative binomial uh distribution compared to pan handles it better and we see similar result with the zero inflated ones that answer the question yes all right thank you I appreciate the question so definitely let let me know if you have any thoughts so in the regression model the coefficients here is that if we have a positive coefficient as a independent variable increase the outcome also increase and the reverse for the negative ones your independent variable increase your outcome decrease and the further away this coefficient is from zero this uh effect is more dramatic okay so uh with did several runs and uh for "
    },
    {
        "start": 1235.679,
        "text": "this specific example is from the third negative binomial component in the model that has three uh count components and what we see here for example is that features such as the distance to the origin of replication and uh the at content of the gene has a small negative effect towards the the mean of this specific negative binomial component and this goes back to the very good online question about like what this different components mean biologically so it varies again from data to data but it is also worth noting that in other negative binomial components we can see that the same gen features can be affecting the means uh in different ways so is it because it's through a different pathway through a different mechanism or this is a totally different subpopulation that is like a question that is "
    },
    {
        "start": 1296.76,
        "text": "interpretation that we need more investigate on but it is interesting definitely to see that there's a level of data complexity here that is accounted for by having more than one just uh more than just one C component so this is uh the first way an independent variable can be fitted to a mean and therefore uh affect the counts in the end it's man it also fixed my screen so that works out nicely in a way um so we're just saying that there are two ways and the second way an independent variable can affect the count outcome is through the component prob ility from which a data point is generated so in our Mark example that would be that was L sorry that would be the uh probability distribution of like "
    },
    {
        "start": 1359.279,
        "text": "which cup you end up grabbing and um in this case if you for example have an independent variable that increase your probability of getting the nonfunctional cup and give you a zero accounts then your uh space account is spe well gen expression which is recount P here is expected to uh be lower so with this second um method where the independent variable can affect the Gen expression counts we can fit this variables to the um component assignment probabilities and again if we look at the value of coefficients and this time we have also the zero inflated uh component here one thing thing that is really interesting to me at least is where the Anor robic respiration function membership uh has a negative has quite a high negative uh coefficient "
    },
    {
        "start": 1420.76,
        "text": "which means that it has like a strong negative back on the counts and that means if a gene is annotated with an anerobic function it is less likely to be end up uh to end up being a zero count in a it makes sense because it comes from a biofilm anerobic culture so expression those gen is certainly helpful in that environment so this is the second way where a g feature can affect our outcome and it's really interesting especially looking at the well personally find it really interesting to look at the zero inflated component probability coefficient because they're really like Clues you in what are the features that probably has the biggest effect at the very fundamental level of like do you end up with a zero or like account and that is a very interesting part for me to check out every time I do a f hear me "
    },
    {
        "start": 1481.36,
        "text": "your CH I'm sorry you may have covered this uh how are you dealing with the potential issues of identifiability that could seem to me clearly arise here like if negative binomial component three shifts a an estimate downward two could just shift it up word yeah so let's see so in our model most of the time we use a soft assignment which means that um like you said if negative binomial component three has a high mean and we uh multiply with the probability that the uh that the can comes from it and which can be counteracted with like a lower mean from a negative binomial component like two for example so one thing is that they are all positive counts so good thing that we don't get a completely like negative effect and also with a soft assignment we end up doing that uh because it kind of makes sense "
    },
    {
        "start": 1544.72,
        "text": "numerically um we also have results that is from the heart assignment that has similar results the heart assignment is that you just only assign it to the component with the highest probability and um in some of the data we are currently working on hard assignment is having a difficult time because of the dispersion of a negative binomial component tend to push it to very low counts for example um so going back to the identi identifiability uh that we're talking about so we can consider a each count here is like a composite of all the effects from different negative binomial components but if we use heart assignment um we can more focus on like a single independent uh a single components effect on the count does that help somewhat I guess I "
    },
    {
        "start": 1604.799,
        "text": "can see potential strength to what you're doing here if there is if there's sufficient evidence in the data to allocate certain identifiable associations of data to specific component of your model that's really interesting just with the width of these I'm concerned as not yeah so this is a FAL cross validation data yeah so you can see uh some variables well some some components with end up with like a titer distribution which means that in every fit it pretty consistently end up with the same results and some of them are not and we are thinking something more like a there is a group of gene or the subpopulation that more consistently uh like a core gen or core population that has similar um statistical properties so like I said some of them have like a wider range of like where the coefficient end up landing on and I think it is worth doing "
    },
    {
        "start": 1665.399,
        "text": "more like validate validation type of analysis and see how much we can trust if we used like a heart assignment for example examp all right so that is basically how a gy feature can contribute to the observe expression levels either through the means that we covered previously and through the component probability here and what we have landed on at this point is that we have a model that can um turn that can connect our independent variables with the outcome and one thing we are really uh GL that this statistical model being able to do is that is more a flexible model so it can basically connect any features uh that is connected to account based data and specifically it has some particular advantage to of course zero "
    },
    {
        "start": 1726.36,
        "text": "inflated data and also integers um counts count data that end up with a lot of small integer counts that doesn't seem to fit into a certain uh classic statistical distribution so with that at this point we started with the data where we want to check out the acquisition preference across the source genome and we established some chromos chromosomal features that correlated with the space account and those are the data that led us to the tools that we use for zero inflated negative binomial model and found that it's insufficient for our purpose and we develop our own two for a negative uh a flexible model for a zero inflated negative binomial mixure model so going back to where we started uh our next step is definitely apply the modeling framework to the where the question all "
    },
    {
        "start": 1786.48,
        "text": "started the space account and the genomic features I haven't uh got all the results back yet but I'm very excited to see what we can see here so that part is uh the statistical modeling part and to say step a little bit at the uh at the beginning of this presentation Mar and I were talking about how nice it is to be able to present a tools that is still under development that is actively still being worked done and I want to talk about some implementation side of things of this modeling framework that's still have room for improvements so as you have seen the history of this model started with a data analysis project and end up landing on optimization problem so given this history It Was Written in R and uh we soon ran into a couple different challenges for example we end up having to use because of the model itself a derivative free optimization "
    },
    {
        "start": 1848.08,
        "text": "and we need tools for that and we also observe that our optimization landscape is a lot Jagged a lot more Jagged than uh the the optimization problem that algorithm that R comes with can handle so that's when we ask if we can find a different Optimizer that perform better than what we have so we end it landed on never GD which is never Grace say the grace stands for gradient it doesn't mean that you never graduate um but it's actually written in Python and by it's an open source tool by by the Facebook research team so in this convoluted history we end up having our into never grade and go back to our so there are a couple disadvantage of course in this process so the first thing is that we unnecessarily have to transition between coding languages and that also means the environment is very hard to maintain you basically have to "
    },
    {
        "start": 1909.2,
        "text": "maintain a good python environment and a good our environment and hope that they work together and the runtime is not ideal neither python or r is known for you know fast runtime and especially in art it's very hard to manage computation resources so what we are doing right now is that we're are switching to NL op for a control random search algorithm instead of n Break um we have some Benchmark benchmarking results that shows a control random search algorithm in the majority of the cases perform better or as similar as net grade and it has a lot more option when it comes to which programming language we are going to use because it is implemented with a lot of uh interfaces from different languages and we are from there we are refracturing the code in Rust so hopefully it will help us to get "
    },
    {
        "start": 1969.24,
        "text": "a better Standalone tool that is good for release and also easy to install and use and we certainly want to take advantage of the flexibility of the model and eventually look into more different types of data for example uh we can imagine the flexibility of the model would do well in ecology or um met genomic data for example it would be interesting to see how this developed tool can uh work under those uh situations so it's once in a lot it's fun to talk about implementation of the tools so uh that is my presentation I want to acknowledge especially everyone in fredin lab for all the help and also uh dcnb and the department of biological chemistry and also funding that makes the research possible and with that that's all that I have for today and I will take any "
    },
    {
        "start": 2035.96,
        "text": "questions if anyone online has a question you can put it in the Q&A box I guess I have a question about kind of the initial problem that this started from so yeah it looks like you made a great model for that you've got the zeros and then slope coming down but in the genomic model you've got it looked like three really big Peaks how can the zero inflated model you made kind of come back to that it just it looks so different from my view at least yeah so let me think about so in the Gen expression data example is uh you're mentioning that it looks like there are three bigger components in each time so um that is when we look at the effect separately if you kind of like call back to the original count distribution we see we see a lot of zero but you roughly look like a negative binomial distribution which is the tricky part so "
    },
    {
        "start": 2099.44,
        "text": "um a mixure model doesn't necessarily end up with like for example a three component that you see3 PS sometimes they look like something that is just from one components we end up uh we end up we test components from one to four and we end up selecting three as appropriate for the data because the lot likelihood of the model improves basically says the model fits better to the data but at the same time it's not overfitting because we check that information criteria when we reach four components it start to overfit the extreme cases that you fit each model each data point with one model so that's how we end up uh landing on three components so that's also one thing that's kind of amazing to me because visually it really looked like just a negative binomial distribution but when you actually fit data and look at the number it has complexity that I cannot see visually so when it comes to space account data I "
    },
    {
        "start": 2160.119,
        "text": "think what you're thinking is the distribution of it across the genome but if we plot the count Itself by the number of genomic SE we'll end up with a histogram that probably look like something that we'll see in the count component or something totally different um that's why like a good flexible model can help with this process to figure out what actually works yeah that makes sense thank you of course thank you yeah just this model for uh spacer count um is there any possibility that it varies by the genomic location that we're looking at yeah so that's basically the biggest question we are asking is that well we have a space account associated with one genomic location we want to see if any feature associated with that genomic location affects the space account for example if it has like higher at content near that location or "
    },
    {
        "start": 2220.359,
        "text": "if it is landed in the gin or not and those can become part of the Gin features that goes into the regression model so at the end of the day we are basically building a model that explains that the explain the space accounts by those like gen genome position dependent features so basically what you're asking is the biggest question we're asking that when we eventually uh Factor the code and start to apply the data to our to apply the model to our space the data so that will let us know not just by the coordinate of the genome but by the feature around a genomic site so I hope that makes sense yeah thank you yeah so rich you mentioned the I think you call it soft selection uh soft assignment yeah soft assignment um is that are those assignments global or do they vary by genome position they vary "
    },
    {
        "start": 2280.88,
        "text": "by genome position because you can also so this model if you want it to be Global you can fit it with only an intercept and no independent variables that varies across different genomic locations then you get something that is global but if you fit uh independent variable to the assignments because for example the me measurement of at content varies from position to position and uh in your model this assignment will end up vary from position to position so uh the flexibility of it I guess it comes down to and you can either fit just The Intercept then everyone gets like a 20% 50% 30% distribution for example but then if this distribution is fitted with the independent variable then each uh independent data point will end up with a different pred icted component probability yeah so I guess that makes me really curious have you looked at how "
    },
    {
        "start": 2342.28,
        "text": "those probabilities change over the genome like Are there specific components that predominate in the Peaks and so I haven't specifically look into that but I think that is part of what we're trying to get at when we show the result with the zero inflated component assignment probability is that for a zero inflated component is has like a more easily interpreted kind of aspect to it and uh with different C components it's definitely worth looking at like what oh we actually for the same data we have the um goterm analysis of each one of them assigned to different components where we are doing that analysis because we want to do it more like a differentially expressed Way by fitting the model considering the uh biological condition it was spending for the "
    },
    {
        "start": 2403.24,
        "text": "experiment but we can definitely do like by component to component analysis if that's what you're thinking I think I was just thinking more yeah simplistic than that just you know in the is there something does component membership yeah um is that predictive of whether you find yourself in a valley or in a peak oh I see what you mean yeah I think given the means vary from different components if you have a higher probability have a membership with a component with lower mean you end up more likely to be in the value so I think this definitely related but um numerically how it is related I hav't look into it yet cool that's it's like a interpretable dimension reduction yeah that would be fun yeah "
    },
    {
        "start": 2468.839,
        "text": "cool any other questions got a comment from maren saying very cool work and thanking you thank you I'm not seeing any other questions online or in the room so let's thank and thank you all for the questions it's always fun to present with feedback and just hear every so thanks everyone for coming and I hope to see everyone next week I really "
    }
]