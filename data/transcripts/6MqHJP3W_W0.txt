my patient thanks for coming so thanks thanks for showing up I guess and Thank You indica for inviting me here and this is I'm gonna talk a little bit about work I've been doing with various colleagues and the title is modeling complex phenotype in a way this is a little bit of a kind of play on are two versions of complex phenotypes so the first half of the talk I'm going to talk about complex phenotypes very much from the perspective of a kind of quantitative genetics statistical genetics in that prediction of what we mean by complex inner types in the second half of the talk I'm going to talk about how we get towards trying to do genetics of shapes and here what I mean by complex phenotypes is a more lack of better term form and function perspective of phenotypes this being things that are like brains ankle bones hearts right these types of phenotypes and also that's going to be the two parts of it so so the first part again is quantitative genetics very classic idea is lots of people ask questions about ok how irritable is height it's the very old question various answers of this and more and more what we're realizing what we care about is a lot of phenotypes I like dogs for example dogs right they're there many different really there are lots of traits and we will understand jointly how these traits are behavior and so kind of a classic idea in quantitative genetics is if I give you 15 traits 20 traits a thousand traits right if I give you phenotypic variance covariance matrix how do I break that up into a genetic part an environmental part which is just what we're talking about here classic notion is something called broad sense heritability the genetic effects on the phenotype and that can be further partitioned into more fine ways things called additive dominant and interaction effect then a very kind of classic version of this and this is goes back to something called Fisher's fundamental theorem and is that the only the additive effects can be passed on to offspring is called narrow-sense heritability and under this model the rate of increase in Fitness is equal to a genetic variance in Fitness at that time so a way of writing this down is this is called the breeders equation is if I have I'm going to measure a Fitness H squared if I have a selection differential between two phenotype if I ask what the response the selection will be this is the point okay this is just really just partitioning the variation now if you asked about the multivariate version of this you get something very similar if you assume that your traits are continuous okay you can think that this comes from from a normal covariance matrix and then what you get is this equation right where again I said my choice pretend there are multivariate normal s is now a selection gradient at the direction that new pushing towards and comes with a phenotype space and now what we call Delta Y is the change in the Loess part and we're saying it's mediated by this matrix G which is a matrix of additive genetic covariance and so the point is it's he push in a direction that's in the null space right or orthogonal to G you will not get any response so this is very important for breeders for very obvious reason okay so we're going to think about and talk about how we can infer this G matrix given data and this is kind of a classic problem and in quantitative genetics so previous approaches have been trying to do clustering approaches not at the moment and the most common approach is what's called the linear mixed model and this is the approach that is used for most cases because you can use it when you have design study you could when you have actually designed greeting or in the field or even random and so the question that we ask that is there a way of extending this linear mixed model and scaling to thousands of tricks so that's a basic idea so just as a picture my traits are gene expression and more and more we're looking at things like proteomics we'll come back to maybe morphometrics right how do we take advantage of these more complicated traits that we're measuring a bunch of them so one idea is you can reduce the high dimensional data into some underlying structure this is going to be where we start talking about factor model you can estivate possibly evolutionary patterns parameters this is these estimate the parrot ability we want to be able to handle complex experimental design or complex pedigree and you want it to be scalable with a large number of trait ok and so what we're going to do is we're going to use the Bayesian method and we're going to get a sparse estimation of this key matrix and it's based on something called an animal model which I'll write down for you very soon and the application is going to be the gene expression so one of the ideas is that if I look at gene expression in some loose way you can think about it's broken up into different parts there different cellular activities that give rise to gene expression and so maybe it's reasonable to think of this as a modular process so in this context these are my cleaner type this is this is what's called the animal model these are my phenotype these are my fixed effects for example this could be gender to do whatever else you are thinking about modeling this could be the random genetic effect this is just the residual error so you can think about this random genetic effect comes from a multivariate normal named zero and that the covariance is this G matrix right this is my additive genetic variance covariance without that the model now one way of thinking about that a perspective of development you can say well this is be composed into some factor this could be from one pathway the poor pathway or the RV or p53 and then we have some explanation and so the idea is these are sparse there aren't many factors are many developmental pathways and then what we're saying is in each pathway we have not that many genes affected so that that's going to be loosely speaking our notion of the biological model now if I think about this right I have these these factors and what you can do is you can say that you have loadings so basically you're taking a linear combination of these factors to explain the additive genetic variance covariance that you have to have genes by pathway it's my loading and then again we're saying they're very people parameter destiny okay so going on so you can think about this this is my measure trait this is my loading these are my underlying crates and receive additive noise and so from this you can pull out the disco parent image so we're going to be using a beta model to do this so my I have my likelihood this is my data this is my a priori assumptions on these are genetic code variance matrices and this is my prior and so we can write down the animal model likelihood in the following way which is I have I have a normal model and then what I have is in one direction right this is this is for any individual if I look across the trait they're explained by this G matrix this is an additive genetic variance covariance and now if I look across individual they're related and this matrix a is sometimes called the kingship matrix this is how they're related so if you know the the kinship structure or if you have genetic data you can again in for this so this is the information that you have so for a single trait this is just what I said you can write down this is this model and then what you can do is if you think about multiple traits you can just extend this up so what you have is this is now a matrix and B is a matrix it's also what you're saying is that why is it n by P vector of the medical trait U is coming from our is the genetic part and then e is just the environmental noise and now these come from what's called a matrix variate normal this is just a generalization of a of a multivariate normal and what you're just saying is that you're drawing a random matrix got a mean parameter m and it's got two different covariances one is for the rows and one for the cough the ones who related it and one is for the how the traits are related to each other yeah this is where yeah this is yeah so this is it's called but the formula for this is gets a matrix very normal so then you write this down and this is the hierarchical model that we're going to write so we have a model or the use and the ease these are for the environmental part and this is going to be coming from the genetic part we write them down as back to model the factors again coming from this matrix variate normal and then the key is that our factor loading the way we load these factors we're going to impose a prior on them which will make these spots so that's that's the key part now then you can further partition the part that's environmental and the part that is genetic you can think about these in terms of diagonal and so just for specification you can just rewrite it this way in terms of this is your estimate of heritability so you can think of it as an identity minus the environmental part and so this again is an ocean of narrow-sense heritability you're looking at how much variance is explained by the genetic component divided by the amount of variance explained by the genetic component plus the environmental component okay so again this is just stating we can recover these des matrices recover G which is the additive genetic variance covariance this is the environmental partition of the variance-covariance you look great and then from that you can even get back to estimate of a phenotypic okay so this is the idea the idea is that what I want to do is I want to model the sparsity of this covariance matrix by modeling the sparsity of the loading so the first thing we're going to say about the loading is that as I move down the factors as I move down from one mode to another world the weights the new how big the numbers are ready to call them are gonna go down and it's going to go down according to this function and I'll specify that in a second and the other thing is within each column I'm going to put down a heavy tail distribution which is going to force most of the elements to be zero okay and then the residual variance is going to get in decay as we go down the diagonal and it's going to if I order them and it's going to decay like this the more formally this is what this model looks what its multi-level models so things are normal and it's got these two different parameters this powe parameter is what's the shared across each column and what it's all that thing is as I move down the column right shrink down the values of the column closer and closer to zero this signal parameter is based on columns and on the indices and that's what basically imposing sparsity what they need to call them and then now this is just specifying how we get these Sigma and then again the reason why these tiles get smaller is because we're basically multiplying things but than wanted we need further de and we put a prior in terms of the heritability which is just a point that prior which was taken from the more dead zhangzhou and math you had done one one thing I'll note is this type of animal model is also showing up more in one statistical genetics and and then this was a Content the remaining variable who does that at 5:00 so in a case study what we actually looked at this in terms of data is there's a cross that was done from Drosophila they're taken at the farmers market in Raleigh North Carolina and the cross was done and we had 40 lines and in this case gene its measured gene expression the measured gene expression over 10,000 Yi and they also measured 7 Fitness related traits I'll come back and tell you at least what two of those traits are and so one example was this was a previous analysis done using a clustering method for the clustering method is night but if you look at this matrix it's actually not positive semi-definite it's not a proper covariance matrix but they looked at five hundred and three hundred fifty five greens ever correlated with starvation resistant and this was there at MIT and this is our estimate and the point is that we get very similar estimate you can yeah so this is a covariance matrix and what you have is you have three hundred and fifty five genes and you have forty sample okay okay and so again you can fit pretty large matrix with not that many parameters and the point is you can and we do we cover two covariance matrix here they do not now here's another example you get these loadings back in the estimate and factor one content but you see the other ones or not if you look at some of these factors and you do a kind of correlation analysis to see what types of genes that these correlate here you'll find that these are correlating to defensive immune responses and one of the things that's interesting is you can look at these are the top genes in factor two if you look at factor two and you ask what are the genes most correlated with with the response right this is what you get in these are 95 percent quite a credible interval and again the thing you see is that while any gene itself may not be very strongly correlated the factor itself is much more correlated so I just want to show you one other example again 414 now we looked at a another fitness measurement on competitive fitness so you take the Drosophila put it in a little jar put the bunch of them in and then keep them in there for I think it a good day or two and then at the end you see the counts of which of the lines come out right this gives you a notion of the competitive so again we use the same model you use the fixed effects of sex and random effect unaffected by line interaction and let me just go through this so this is going across this is a heritability across the gene okay that we looked at this is again our estimate of the additive variance covariance matrix and what this is what we're looking at is the genetic correlation with Fitness for each gene so you can get an estimate of that so this is what's happening on the gene double so we can do the same thing on the factor level so these are my factors as you see they're going down magnitude the order than magnitude concerning you how the loadings are for on these each of these latent rates okay and then what I'm showing you here is each of these factors and show you how each of these factors are correlated with fitness so it turns out factor 2 in fact that 16 are correlated with Fitness now what you make of that further is a little bit tricky but the point is again you can actually pull out of these models possible factors that are that are related to fitness we have software and one of the things we've been doing now is an interesting and challenging possible problem is getting estimating the covariance matrix is not horribly hard I think we can do that ok trying to get an exact estimate of the number of factors is extremely hard and if you want to do that that's tricky and I think there is there's some subtleties there which I'm happy to talk about one other things we've been working on recently what do you think is interesting is currently if I take a factor of a covariance matrix I can look at the subspace that it's been and let's say I take another environmental situation I get another covariance matrix and let's say the subspace expands is of a different dimension like how do I compare this how do I measure distances between subspaces of different dimensions or how do i model these types of structures if they have different dimensions that's a problem we've been working on and I think we've made some very interesting kind of recent progress then other things you can get estimates of response to selection you can look at percent various variation in fitness and one of the ideas that's really interesting is incorporating this type of thinking and the G was if you go back and if you read Fischer 1922 right where he talks about the infinite alleles model you will see in talking about these types of models both for quantitative genetics and for statistical genetics and I did you know plus or minus sometimes around the fifties I'd say these two fields you know went away and now they're really coming back together and I think it's really interesting to think about how you can incorporate this quantitative way of thinking about partitioning variation with more classical do you and I know they're people working on that but yeah I think that that's a really interesting problem so okay so and then and then this is a very interesting thing what how do you think about this great trait in time varying traits so if you look at back at the landing equation right it had this thing where Delta Y is equal to G times that response gradient right and this we know is true for continual trade but a question about is what happens if you have discrete carry what happened here be a multinomial in terms of flowering color right how would I go back and derive a similar equation would I work in a latent space the classic models have been doing looking at threshold Amada but you know there might be something robust with respect to some type of latent trait so that's also I think something that's really interesting and more and more people are looking at time varying traits so if something happens early in life how does it affect the you know older age later in life effects if you're looking at development of plants right and you want to understand you know what your seed distribution of seed profit thinking about those problems to get I think there's some really really interesting ideas and I'm very clear so that's the first part of the talk now the next part of the talk is a little bit different but I will relate them at the end and I have a colleague he gives me both okay specifically he gives me primate but actually doesn't give me the bones it gives me CT scans of the bones that are measured okay so we have a hundred and six bones some primates some of them are extinct some of them are extend and what we want to do is measure distances between them but what we want to do more is understand their evolutionary history now there are two ways of thinking about this one way of thinking about this is you can if you can take the bones you can construct a matrix from that matrix you can possibly get a tree you have a species tree in terms of the phenotype which is a bone you can compare that to the analogous species tree that you'd have from the genotype and if there's something you know different in those possibly that could be selective pleasure a little hand waving there's great desire to make that more formal but that's an idea so that raises a question how do you measure distance between shapes circles okay so my colleague Doug had been working with another colleague of mine at Duke and ingrid daubechies to do this and the way they were doing this is loosely speaking you can take a tooth and turn it into a I guess a disc tool or a spherical case let me go so you can do a conformal map you can take the to put down a bunch of pointer back to take a little disk around them and then basically you knowwe flatten it out in the particular particular map such that angles right volumes are preserved and then what you can do it you can do this to a bunch of teeth once you've mapped it down flatten it out you can very basically measure the distances basically these maps using something called a variational distance so this is what they've done it's nearly beautiful work it's really nice and it's really elegant but the problem happens is what happens if your shapes are not isomorphic if something breaks but for example if I have broken right because fundamentally if you don't have a map in those methods they'll fall apart so that's the question we started asking what can you do then other examples of this is if I have fruit fly wings they look at images of them and suddenly something qualitatively changed right under one environment they get an extra load right what do i do then what do I do in those studies so that that's a question we started looking at and he approached we took is using something called integral geometry and I'll explain what I mean by that really soon but there have been these older ideas they've had a blogger and a girl and colleague functionals they're examples of something called Euler integration which I'll show you an example of really stood and the classic example of this is something called the radon transform which is how a lot of CT scan doctor they work okay and roughly the idea is you take an object a bone and you're basically going to take scan it blue and ask how much mass gets through okay that that's roughly the idea but I'll make that more formal so the way we started thinking about this is there is there something geometrical or more specifically topological are there summaries of these surfaces that we can use to kind of characterize them that we can write out the surface instead of having something which is really complicated can I transform it into something simpler right that I can then use to compare distances between okay so one idea there's something called Betty numbers which are just how many connected components of it Betty one is how many holes are there 32 is the number of coils another summary that you can use and we'll actually use a version of this cover is what's called the Euler characteristic which is alternating sum of the Betty number or the number if you have a 3d mesh it's a number of vertices minus the number of edges plus the number of faces and just to show you that this is a topological invariant it's going to be zero for all of the poor I go Swiss cheese it's always negative 34 it depends on the Swiss cheese but when it's solid out there it's gonna be okay so one that we used was well one that we also use this something different but I let me explain it to you I'm gonna tell you in a second about something called persistent homology but just to motivate it there's something called critical if I have a curve got a minimum maximum can have an inflection and these are called critical now what what can I do is this is called Moore's theory in one slide okay I'm gonna take this function and just think about raising water up this function okay I'm just going to think about filling it up I'm going to ask when the sub level sets change when do the connectivity of the sub level sets change the question of that but the first point is here if suddenly got a feature and so we call this a burp so I something just started yet again here something else started again here we had a new feature when I go up a little bit further that feature is gone because it merged with this feature right so this little feature disappeared up here so that feature didn't live very long I was born here and it died here and we can similarly just go up this thing and we can look at when these topological features aren't being constructed and one they're disappearing this is just four connected yet or buddy zero and this is what kind of persistence diagram it's basically giving you a topological scan of the object okay now another way you can think about it is if I have data this is a time series I can take little windows of it each window gives me a point cloud now I can take these point clouds and I can start picking this is called a filtration because as I keep picking of them I get proper subsets and I can ask you when does something change topologically before we looking at connected that now I can also ask you about cycle right one little boy formed cycle well right here I formed a little cycle right that will last very long the bigger cycle is going to last long so I could have made that same picture for the cycle and formally this is just what it is this is a filtration these are these things get thicker and I can ask when do the homology groups of these change so the point being is if I give you two of these diagram this is a function one is in blue one is in red I can give you a persistent diagram for each and the ways of measuring distances between them and the distance is really what's the l2 distance from moving a red dot to a blue dot right and if things don't match you move them to the diagonal and this is just called a l2 vajra diametric sometimes called an Earth Euler's method but it has a four moment okay so now let me tell you what we actually do okay so the idea is I take a surface I take an object I take a plane and I'm going to take the perpendicular to that and run it right through the object okay and I'm gonna do it from any direction so if you do it in with the persistent technology I have a simplicial complex and RD so in three dimension I'm sampling from all the directions of a sphere but as many as I can okay and so take a direction B okay X is my simplicial complex okay this actually I'm sorry this is a persistence diagram of dimension K but what are you doing you take a direction V you move are into it right you're looking at the sublevel set below are and you're looking at the persistent diagram as they move through it and so we call this a persistent homology transform it's a map from all the direction into a bunch of diagram one for each direction and you also get them for 3001 ected miss betty one which is holes and Betty two now you can take these meshes and now I can define distances between two surfaces basically I can integrate over all the directions just a distance between these two objects and sum it over the different dimension you can do something simpler which is sometimes good you can take us in push or complex okay or a mesh and you can do the same thing that we did before but you can compute something called the Euler characteristic curve and let me show you a picture of that that's a little bit easier so this is a mouse embryo heart this is one of our directions B okay so we've gone up to the point a looking at all of the sublevel set all the meshes in the sublevel set and I'm asking you what is your other characteristic of that which is the number of faces number of vertices minus the number of edges plus the number of faces and so that's that number then I move a little bit more compute that number that gives me this curve what I've done here is I've taken this earth ID integrated it and then zero-mean it and you'll see why I do that in fact now there's there's a classic notion of a sufficient statistic basically if P of X is sufficient statistic it's something you can compute from data that means that the distribution of X given P of X and your parameters is equal to X given P of X you can throw out the parameter in the classic version of this or normal distribution would known variance just the sample mean of the solution so the first thing you can show is that technology does transform is injective in 2d and 3d we believe it's true for higher dimensions but it's true in two in 3d now if this is true what you can say is I'm looking gonna look at the sub base of all shape that have at most n vertices okay so this is not so restrictive because if it's in a computer it's going to have at most n vertices now let they have a distribution or a density function over the bees are beastly shape right what we can say is that this persistent molecule transform is a sufficient statistic for this density and we can say the same thing for the oil now what's interesting about seeing it for the Euler characteristic is the following is a reason why a lot of people look at sufficient statistics is in statistics it relates to something called the exponential family which means that if you take your parameter do an inner product of that with the sufficient statistic it has this very nice form you can write things in terms of inner products right or transposes right so then I could actually write down a likelihood for these complicated surface model now think about what we're doing I have an Euler characteristic curve for each directions I have a curve for each direction and integrated it it's a smooth thing so now I can just take T discrete points of it okay so at capital P for each curve I have K curve I think of this as a major this is a capital K the capital T matrix in a way you know that's lost almost no information about the original shape okay now given that matrix what can I do I can go back and I can model it as a matrix variate normal so I've taken a quite a complicated shape putting any type of coordinate system one that would be a pain we've transformed it into a matrix and we've given it a likelihood object and so this is what we're working with now we're trying to manipulate and use these likelihood function to do a Pelle in a second but to do more model okay so this is just what I told you I'm sorry because this is the okay so this because it is in a way of so it's like a normal distribution right it's just a normal distribution for a matrix all right you're saying the columns have one particular mean so you have one covariance structure going you know going between the curves right and you have one it wouldn't occur now it could be that maybe the matrix very normal isn't the best model there might be other type of models right that might be better at that's something we're trying to understand but in a way this is a simpler okay now this is something which is really interesting which we recently realized it don't read this line it's under what you can if you want to but let me explain it to you one of the things we realized if you have a bunch of object okay and let's say they're in general positioning which means they're not crazy symmetric okay take the object just take random direction take random directions along the sphere and get curved okay and what I'm telling you the map from the object on to these random curves is injective so you don't need to align the object you just need to get the scale right and if you just collect a distribution of curves for an object a distribution of course or another object and if you can measure the distance between these distribution you actually have a system where you can start looking about distances and even a bit about them without having to align them now it may be better if you align them we're not sure of it but aligning these objects is a pain in the neck you know we we have papers we have soft cool to do that as well and that's that's really really kind of non so that that's kind of another interesting point so let me show you some result I told you we have a hundred and six primates and what we did was we took this persistent homology transplant we took this distance measurement that I told you on it and what am I getting is a bunch of distances and this is multi-dimensional scaling just plotting it in two dimension and then what you see are the primates non-human primates kind of show up together these are old world monkeys they show up together this is an extinct old world monkey he's a numeral monkey school of doctor a monkey this is an extinct again new world monkey and these are all lemurs and one of these I think is an extinct them all right so you start seeing kind of a structure that you'd expect we also did this on more controlled data we did this one little shape okay so this is what my colleague Doug did a much nicer picture using some software called past and kind of putting in the actual file in the phylogenetic will write these different groups so so this is what it looks like there now he told us something which was interesting which is at least in one way our method did better than the others in that the high/low babies were linked with the other eight and previously the hilum babies were linked would be all owada I got this I had no idea what the hell it meant my wife's a biologist she explained to me that it turns out that we got the Gibbons near primate previously they will you know I think the spider movies so so there seems good more visually this is what our method gave this is what a classic shape statistic method would give the classic shape statistic methods are based on Leslie speaking take your object put down a bunch of landmark point put a perpendicular on each of these landmark points right cut down as a vector as a matrix right now if I want to measure distance between these two there's something called a purpose these distant which is how much the distance is to align these two matrices nulling out rotation translation and scale okay but for that you need a fixed number of landline this is using that type of approach okay and this is using an automatic place landmark approach which is more like the informal on mapping approach so at least qualitatively looks like ours is better but we need to do more work on this one of the things that I think is really exciting is that morphologist and also archaeologists are going from this collection of data and keeping them as landmarks five point six points and distances between them they're doing CT scans of them so we're getting a lot more data which is much richer and BCPs get what you think is really really kind of interesting and exciting so we've been asking this question so I've started to think about shapes that are modeling surfaces and shapes but you can ask the same about networks so a problem that I got from my from my wife is she studies baboon and they have collected behavioral networks which are how much these baboons groom each other and from the same troop they have very not on the same diamonds they have microbiome networks how similar the microbiomes are and you have a collection of them and you want to say how much does variation in one network explain variation in another network so you can actually think about networks again as phenotypes so how do we think about modeling right and so you can ask the question of how can I think about a graph so you can apply it slower idea so this is one graph this is another graph there's something called the graph laplacian which is nothing but doing fourier analysis on a graph really and what it does is it gives you different values on the graph note write different kind of frequencies different directions on the graph note and so you can apply the same idea that I just told you through this graph turning in a matrix and you can ask the distance so this is just doing perturbations of this graph the reddest perturbations of this other graph and we're just looking at how these perturbations move through and away perturbations of graph space and then it's looking am I not a 2d projection of that so so that's one of the things we're working on and we're trying to say we know that we can't solve the graph isomorphism problem we can't tell you how similar graphs are but the question is how far can you go modulo what invariant we're starting to do Association study so these I think are brains of stickleback these are fish brain okay and what we want to do is take these brains use this type of approach and then do a genome-wide Association study to ask you know a variation in these shaped are they related to variation along particular genome particular variants and here's another example this is this idea of looking at the variation in microbiome you don't have data for this yet but one very specific example is one of my colleagues at Duke they did a huge sea urchin Club so they took a bunch of sea urchins they crossed them and they did it under two different environmental condition conditions and one of the things they did was they took pictures of the sea urchin embryos the one other thing we want to understand is get estimates of heritability under these two environmental condition so that brings together first part of the top of the second part of the talk is we're building up more and more of these using these methods to try to do quantitative genetics and statistical genetic of these shades though there's a lot of interesting and open questions so one is how do you automatically align these we're thinking about that related to that how do you get correspondence between points on them that's interesting so number three is a really interesting question is that this idea of running the surface through a shape is the crudely speaking you can think of that as a form of integration so the direction is an analogue of a frequency you can think of and then right now we're running it through the entire shape that's a basis you could look at local basis right and so this is a form of Euler integration and you can a signal processing question is how many directions do you need for a particular shape so this would be the analog of a sampling theory for surfaces or shape and then if you want to do this you know find our multiscale you can ask about more local versions I think kind of that that that's also an interesting in think about maps between networks and then lastly and a combining the two parts of this talk so more and more and how they do statistical and quantitative genetics of things that are surfaces or networks or these more complicated objects that we don't have a good coordinate representation of so before I just I think one of the ideas here is that sometimes when we try to model something solely statistically it might be really useful to think of them just not as an object but maps between the objects and how those behave like so so that's that and thank you for listening and there are people who under this so thanks a lot for the shakes yeah we have I'll show you an example this one Krusty's yeah I mean that's effectively what the clock Rusty's is doing modulo the rotation yeah so yeah so we have the I mean the motivation partly for this was a case where that really breaks badly right if you have you know a new lobe on your butterfly or your you're a fruit fly wing right yeah which one okay to let me yes you're right so these matrices are not positive they're not they're absolutely not but they don't they need not be so the mean here when you parameterize it the mean is not positive semi-definite but the two covariance structures are okay okay okay thanks very nice huh so I have two quick questions and first apart so the first one yes so where you do the space vector analysis decomposition on the G and a year both matches do you use the same low image it's lambda for bones of that I'll use a different than that oh god I like went through that right so I use the same loading matrix I see is there any explanation cuz your transmitter can do your you could do them both separately it's more parameters we didn't find that it did much better so but yeah it's a very reasonable quantity yeah and then the second one is yeah you know people are very interested in doing a mountain Pocono type of Nyepi Association studies with their phenotype or with this net so you can imagine if you have hundreds of phenotypes you can do a c2 effect analysis and put the loading vector for a couple first a few men it doesn't do Association study so my sort of scalable to accommodate thousands of individuals and this kind of thing in standard she was started okay so let me let me repeat the the question is the shape things are that we're doing right can we do this for a thousand phenotype and put it into a Gy study let me tell you how we're envisioning to do it right now and from that you can tell how scalable it will be or what do we need to do something smarter so the way we do it right now is you take these matrices right what you've reduced the shape down to you can measure distances between all of them right now the easiest thing to do is take that and then basically do multiple dimensional scaling to turn those into vectors now that you have them as vector you go ahead and do your classical jeewa right so the question that really arises it how quickly can you turn these into a bunch of vectors right how quickly can you stack their vectors how much low dimensional structures are across those vectors right to be able to do what you just said and I don't know because I haven't done it yet but I hope they do it soon within the but yeah oh I guess there's one last thing that I want to say this is maybe kind of cool or not this problem of of how many directions do you need right there's a really related beautiful problem it's called snake healthier so if I give you a bunch of critical point birth right not say I don't know Quenya tell you where they are and then I ask you what is the cardinality of the number of functions right that can satisfy this it has a beautiful kind of cosecant expansion and this is work by b.i Arnold which is really gorgeous but there's no generalization of the multiple dimension so oh yeah okay do you use anything else than shape like texture the way we're doing it right now we don't because we've been looking at bones right and and there's not that much roughly on bone but you know they're examples where you definitely want it I'll give you an example one of the gy studies we're looking at are the faces of fourteen hundred tens of me and children and all their hairs been chopped and the reason why the hairs been cropped is to pick up extra right and there's certain cases where you really wouldn't want to take out texture right and for there I think it's interesting what do you do do you build up kind of a shape model and a text to model but I haven't gotten that yeah what about density additions so that wouldn't be hard to do but that could be that could be done so like I said the way that we did things right now is you're just looking at the sub level basically looking at the meshes in the sub level set right you could totally do all of these waiting but they're ways of doing this weighted and I think that would be extremely interesting and that's why I think if you think a little bit of this as a signal processing more generally idea and thinking about what a good basis for this I think there are a lot of interesting things you could do yes distance and these are not really positive definite so then how these like classical likelihood ratio statistics like Bartlett statistics will work here oh yeah okay so first of all I don't do classical like so so so you can measure it okay so let's let's you have this matrix very normal right I can measure distances between them one way is I can measure the KL divergence between two different matrix very at all sure right so you can do that now if you want to do something like let's say you want to do something more like hypothesis testing yeah which I tend to avoid but that's a religious issue what you can do is you can do a permutation test there are other things you can try to do okay I would be very very very very very afraid of trying to do a likelihood ratio statistic to say something about one model versus another model within this context because I have no idea how sensitive but these are two initial okay you know I mean mostly speaking this is what I code what I've told you is there is a likelihood there's a probability space on these surfaces right there's some density there it's it's harder than hell to write down to pain in the asteroid that I have no idea how the hell they do it in general so I've told you that I've given you a map a deterministic map from that space in another space or I can write down some reasonable mod okay now if I were a reasonable statistician say maybe I am somebody right what I would be asking it you have a likelihood model here I have a push forward onto the likelihood model here but what can I say about this from this right and I you know I can't give you an honest answer thank you thank you very much